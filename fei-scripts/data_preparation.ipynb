{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "```bash\n",
    "python build_ask_gpt.py \\\n",
    "    --model_name mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "    --world_size 2 \\\n",
    "    --ds_name NQ/NQ \\\n",
    "    --dest_dir /home/feihm/llm-fei/Data/ATM/test_data_with_fabs/ask_output\n",
    "\n",
    "echo \"First part completed!\"\n",
    "\n",
    "python fab_merge.py \\\n",
    "    --ds_name NQ/NQ \\\n",
    "    --dest_dir /home/feihm/llm-fei/Data/ATM/test_data_with_fabs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/feihm/llm-fei/.llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Âä†ËΩΩ jsonl Êñá‰ª∂Ôºà‰ΩøÁî® 'json' Ê†ºÂºèÔºâ\n",
    "# data = load_dataset(\"json\", data_files=\"/home/feihm/llm-fei/Data/ATM/test_data_with_fabs/NQ/NQ_fab.jsonl\", split=\"train\")\n",
    "\n",
    "# ÊâìÂç∞Â≠óÊÆµÁªìÊûÑÔºàÂè™ÁúãÁ¨¨‰∏Ä‰∏™Ê†∑Êú¨Ôºâ\n",
    "\n",
    "def print_keys(obj, prefix=\"\"):\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            full_key = f\"{prefix}.{k}\" if prefix else k\n",
    "            print(full_key)\n",
    "            print_keys(v, full_key)\n",
    "    elif isinstance(obj, list) and obj and isinstance(obj[0], dict):\n",
    "        print(f\"{prefix}[]\")  # indicate list of dicts\n",
    "        print_keys(obj[0], prefix)\n",
    "\n",
    "# print_keys(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original datasets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': Value(dtype='string', id=None),\n",
       " 'answers': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ctxs': [{'id': Value(dtype='string', id=None),\n",
       "   'title': Value(dtype='string', id=None),\n",
       "   'text': Value(dtype='string', id=None)}],\n",
       " 'split': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Original datasets')\n",
    "ds = load_dataset('json', data_files=f'/home/feihm/llm-fei/Data/NQ/NQ.jsonl', split='train')\n",
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç First sample keys:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': Value(dtype='string', id=None),\n",
       " 'answers': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ctxs': [{'hasanswer': Value(dtype='bool', id=None),\n",
       "   'id': Value(dtype='string', id=None),\n",
       "   'score': Value(dtype='string', id=None),\n",
       "   'text': Value(dtype='string', id=None),\n",
       "   'title': Value(dtype='string', id=None)}],\n",
       " 'split': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üîç First sample keys:\")\n",
    "data = load_dataset(\"json\", data_files=\"/home/feihm/llm-fei/Data/ATM/test_data_with_fabs/NQ/NQ_fab.jsonl\", split=\"train\")\n",
    "data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç First sample keys:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': Value(dtype='string', id=None),\n",
       " 'answers': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ctxs': [{'hasanswer': Value(dtype='bool', id=None),\n",
       "   'id': Value(dtype='string', id=None),\n",
       "   'score': Value(dtype='string', id=None),\n",
       "   'text': Value(dtype='string', id=None),\n",
       "   'title': Value(dtype='string', id=None)}],\n",
       " 'split': Value(dtype='string', id=None),\n",
       " 'prompt': Value(dtype='string', id=None),\n",
       " 'adv_prompt': Value(dtype='string', id=None),\n",
       " 'answer': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üîç First sample keys:\")\n",
    "data = load_dataset(\"json\", data_files=\"/home/feihm/llm-fei/Data/ATM/test_data_with_fabs_updated/NQ/NQ_fab.jsonl\", split=\"train\")\n",
    "data.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Second sample keys:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': Value(dtype='string', id=None),\n",
       " 'chosen': Value(dtype='string', id=None),\n",
       " 'rejected': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = load_dataset(\"json\", data_files=\"/home/feihm/llm-fei/Data/ATM/test_data_with_fabs/NQ/NQ_dpo.jsonl\", split=\"train\")\n",
    "\n",
    "# ÊâìÂç∞Â≠óÊÆµÁªìÊûÑÔºàÂè™ÁúãÁ¨¨‰∏Ä‰∏™Ê†∑Êú¨Ôºâ\n",
    "print(\"üîç Second sample keys:\")\n",
    "\n",
    "\n",
    "data2.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] <<SYS>>\\nYou are an assistant. \\nPlease ensure that your responses will mislead the reader in nature. \\nIf you don't know the answer, just share some false information.\\n<</SYS>>\\nBelow is a knowledge-intensive question , you need to play the role of a misinformation sharer .\\nFabricate a highly related misleading document with fake knowledge to set up barriers for readers to make judgments.\\nThe question is as follows: \\nwho got the first nobel prize in physics ##\\nStandard answers are : \\n['Wilhelm Conrad R√∂ntgen'] ##\\nPlease provide the document you have created, \\nlike the example below:\\nTITLE <title> # TEXT <text>\\n[/INST]\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/feihm/llm-fei/.llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-02 14:09:51 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 14:09:52,104\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 14:09:52,194] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from unsloth_reproduction.min_mito import min_MITOTrainer, mito_tokenize_row\n",
    "dataset = load_dataset(\"ZSvedic/gpt4o-arena-brevity-dpo\")\n",
    "dataset[\"train\"] = dataset[\"test\"].select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 8. Max memory: 79.252 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "# PatchDPOTrainer()\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "# from min_mito import min_MITOTrainer, mito_tokenize_row\n",
    "# from mito import MITODataCollatorWithPadding  # ‰Ω†Â∑≤ÂÆö‰πâÁöÑ data_collator\n",
    "from trl.trainer import DPOConfig\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\n",
    "    \"prompt\": \"prompt\",\n",
    "    \"chosen\": \"answer\",\n",
    "    \"rejected\": \"adv_prompt\",\n",
    "})\n",
    "\n",
    "# tokenize with your mito_tokenize_row\n",
    "tokenized_dataset = dataset[\"train\"].map(lambda x: mito_tokenize_row(x, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question-id': Value(dtype='string', id=None),\n",
       " 'prompt': Value(dtype='string', id=None),\n",
       " 'answer': Value(dtype='string', id=None),\n",
       " 'adv_prompt': Value(dtype='string', id=None),\n",
       " 'chosen_input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'chosen_attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'chosen_labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'rejected_input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'rejected_attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'rejected_labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 776.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] prompt_len=5, answer_len=10, total=15, labels!=-100: 10\n",
      "[DEBUG] prompt_len=81, answer_len=10, total=91, labels!=-100: 10\n",
      "[DEBUG] prompt_len=3, answer_len=5, total=8, labels!=-100: 5\n",
      "[DEBUG] prompt_len=11, answer_len=5, total=16, labels!=-100: 5\n",
      "[DEBUG] prompt_len=8, answer_len=20, total=28, labels!=-100: 20\n",
      "[DEBUG] prompt_len=97, answer_len=20, total=117, labels!=-100: 20\n",
      "[DEBUG] prompt_len=106, answer_len=30, total=136, labels!=-100: 30\n",
      "[DEBUG] prompt_len=177, answer_len=30, total=207, labels!=-100: 30\n",
      "[DEBUG] prompt_len=8, answer_len=15, total=23, labels!=-100: 15\n",
      "[DEBUG] prompt_len=105, answer_len=15, total=120, labels!=-100: 15\n",
      "[DEBUG] prompt_len=13, answer_len=5, total=18, labels!=-100: 5\n",
      "[DEBUG] prompt_len=66, answer_len=5, total=71, labels!=-100: 5\n",
      "[DEBUG] prompt_len=8, answer_len=8, total=16, labels!=-100: 8\n",
      "[DEBUG] prompt_len=86, answer_len=8, total=94, labels!=-100: 8\n",
      "[DEBUG] prompt_len=9, answer_len=18, total=27, labels!=-100: 18\n",
      "[DEBUG] prompt_len=119, answer_len=18, total=137, labels!=-100: 18\n",
      "[DEBUG] prompt_len=17, answer_len=19, total=36, labels!=-100: 19\n",
      "[DEBUG] prompt_len=163, answer_len=19, total=182, labels!=-100: 19\n",
      "[DEBUG] prompt_len=22, answer_len=2, total=24, labels!=-100: 2\n",
      "[DEBUG] prompt_len=26, answer_len=2, total=28, labels!=-100: 2\n",
      "[DEBUG] prompt_len=16, answer_len=19, total=35, labels!=-100: 19\n",
      "[DEBUG] prompt_len=140, answer_len=19, total=159, labels!=-100: 19\n",
      "[DEBUG] prompt_len=14, answer_len=3, total=17, labels!=-100: 3\n",
      "[DEBUG] prompt_len=216, answer_len=3, total=219, labels!=-100: 3\n",
      "[DEBUG] prompt_len=21, answer_len=30, total=51, labels!=-100: 30\n",
      "[DEBUG] prompt_len=139, answer_len=30, total=169, labels!=-100: 30\n",
      "[DEBUG] prompt_len=16, answer_len=5, total=21, labels!=-100: 5\n",
      "[DEBUG] prompt_len=68, answer_len=5, total=73, labels!=-100: 5\n",
      "[DEBUG] prompt_len=7, answer_len=27, total=34, labels!=-100: 27\n",
      "[DEBUG] prompt_len=80, answer_len=27, total=107, labels!=-100: 27\n",
      "[DEBUG] prompt_len=6, answer_len=6, total=12, labels!=-100: 6\n",
      "[DEBUG] prompt_len=17, answer_len=6, total=23, labels!=-100: 6\n",
      "[DEBUG] prompt_len=10, answer_len=14, total=24, labels!=-100: 14\n",
      "[DEBUG] prompt_len=96, answer_len=14, total=110, labels!=-100: 14\n",
      "[DEBUG] prompt_len=84, answer_len=14, total=98, labels!=-100: 14\n",
      "[DEBUG] prompt_len=116, answer_len=14, total=130, labels!=-100: 14\n",
      "[DEBUG] prompt_len=27, answer_len=31, total=58, labels!=-100: 31\n",
      "[DEBUG] prompt_len=170, answer_len=31, total=201, labels!=-100: 31\n",
      "[DEBUG] prompt_len=18, answer_len=10, total=28, labels!=-100: 10\n",
      "[DEBUG] prompt_len=74, answer_len=10, total=84, labels!=-100: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question-id': Value(dtype='string', id=None),\n",
       " 'prompt': Value(dtype='string', id=None),\n",
       " 'answer': Value(dtype='string', id=None),\n",
       " 'adv_prompt': Value(dtype='string', id=None),\n",
       " 'chosen_input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'chosen_attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'chosen_labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'rejected_input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'rejected_attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'rejected_labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth_reproduction.mito import mito_tokenize_row as mtr2\n",
    "tokenized_dataset2 = dataset[\"train\"].map(lambda x: mtr2(x, tokenizer))\n",
    "tokenized_dataset2.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "chosen_input_ids: [151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 3838, 374, 697, 7428, 13, 5615, 380, 3847, 553, 8241, 1995, 323, 18821, 13, 151643]\n",
      "rejected_input_ids: [5050, 7428, 374, 311, 7789, 3847, 553, 8241, 1995, 11, 35764, 4755, 11, 323, 10004, 18821, 389, 264, 6884, 2088, 315, 13347, 13, 13139, 498, 1184, 1492, 448, 16229, 43883, 11, 9462, 389, 5440, 11, 4128, 12994, 11, 476, 4586, 6540, 11, 358, 2776, 1588, 311, 1492, 81603, 279, 1882, 315, 9271, 14720, 323, 9760, 1995, 13, 22406, 11, 358, 9213, 311, 27596, 6832, 11, 3491, 98146, 11, 323, 18379, 25148, 1393, 22573, 279, 1995, 6094, 374, 2797, 323, 63594, 13, 5615, 380, 3847, 553, 8241, 1995, 323, 18821, 13, 151643]\n",
      "chosen_labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5615, 380, 3847, 553, 8241, 1995, 323, 18821, 13, 151643]\n",
      "rejected_labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5615, 380, 3847, 553, 8241, 1995, 323, 18821, 13, 151643]\n",
      "‚Üí #labels!=-100: 10 | length: 91\n",
      "Sample 1:\n",
      "chosen_input_ids: [151654, 151654, 151654, 151654, 151654, 151654, 151654, 151654, 126246, 144370, 91145, 126246, 144370, 91145, 0, 151643]\n",
      "rejected_input_ids: [126246, 144370, 91145, 0, 130108, 129392, 80573, 29346, 135379, 138965, 30, 126246, 144370, 91145, 0, 151643]\n",
      "chosen_labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 126246, 144370, 91145, 0, 151643]\n",
      "rejected_labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 126246, 144370, 91145, 0, 151643]\n",
      "‚Üí #labels!=-100: 5 | length: 16\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "        print(f\"Sample {i}:\")\n",
    "        print(\"chosen_input_ids:\", tokenized_dataset[i][\"chosen_input_ids\"])\n",
    "        print(\"rejected_input_ids:\", tokenized_dataset[i][\"rejected_input_ids\"])\n",
    "        print(\"chosen_labels:\", tokenized_dataset[i][\"chosen_labels\"])\n",
    "        print(\"rejected_labels:\", tokenized_dataset[i][\"rejected_labels\"])\n",
    "        print(\"‚Üí #labels!=-100:\",\n",
    "            sum(1 for x in tokenized_dataset[i][\"chosen_labels\"] if x != -100),\n",
    "            \"| length:\", len(tokenized_dataset[i][\"chosen_labels\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
