{"doc_id": 0, "doc": {"question": "when was the last time anyone was on the moon", "answer": ["14 December 1972 UTC", "December 1972"]}, "target": "['14 December 1972 UTC', 'December 1972']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time anyone was on the moon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" December 1972"]], "filtered_resps": ["December 1972"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a99f7d8b3ea44d1fed266512b6c01dc132cae4f67d97a3bc63072aa5ef5780c", "prompt_hash": "59091c1d8410cbc5940ad44797a6bb749da05e1dc1b6d287c2178c2a76cb8113", "target_hash": "4b85df556a24005388693e9f293bd5bed05f058df987faf61c55f5e849508b73", "exact_match": 1.0}
{"doc_id": 1, "doc": {"question": "who wrote he ain't heavy he's my brother lyrics", "answer": ["Bobby Scott", "Bob Russell"]}, "target": "['Bobby Scott', 'Bob Russell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote he ain't heavy he's my brother lyrics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Neil Diamond"]], "filtered_resps": ["Neil Diamond"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96bbea25ba3e628c6c8e11329f485ff0f4399874cbcf6c2e522552af94d1a1ef", "prompt_hash": "cede50392cd023de54cb3336eed90863181d601ef02b95c77271c8c23c09fbc9", "target_hash": "c8430db9cfe4d57736ada34d070ba45e6d8b882186f5e4b750b72e9a61252292", "exact_match": 0.0}
{"doc_id": 2, "doc": {"question": "how many seasons of the bastard executioner are there", "answer": ["one", "one season"]}, "target": "['one', 'one season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of the bastard executioner are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6a6f129e0b5b1d1d41d837b34fd0f67b8e36076a45599d15d964ce964ba017b7", "prompt_hash": "32d8afcb0bb6883a5ee89826c5ea7e3bde0089da87ceee1f1bf3a9e14831dfbc", "target_hash": "c629f3657b763e57615a672a3644280829f58b7d27bf5d25f4088dc0cad0bf52", "exact_match": 0.0}
{"doc_id": 3, "doc": {"question": "when did the eagles win last super bowl", "answer": ["2017"]}, "target": "['2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the eagles win last super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a07c4dcca2d3117b50edcd9052c1ab2e6e49a4f72feeb588591db44e2b00acc8", "prompt_hash": "e7db398d27d27f7d9f76caa0544d592c4f3b13485e4351d7e730375f28ee2ca1", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "exact_match": 1.0}
{"doc_id": 4, "doc": {"question": "who won last year's ncaa women's basketball", "answer": ["South Carolina"]}, "target": "['South Carolina']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won last year's ncaa women's basketball?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" South Carolina"]], "filtered_resps": ["South Carolina"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "59313937d499e02d626033595845776da607e77ad7e1dcb1ed79ee5027ba8d45", "prompt_hash": "9dcb5d8eaf631f9b8599da5f50a57120d1008f4bd67e8342a4eace298007d11f", "target_hash": "5616b2e72a65cb242b419dc62f16fa7ad9af2093be0fa3fcb99a53efb73b9d14", "exact_match": 1.0}
{"doc_id": 5, "doc": {"question": "when did the isle of wight become an island", "answer": ["During the last Ice Age"]}, "target": "['During the last Ice Age']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the isle of wight become an island?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1200"]], "filtered_resps": ["1200"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "64e09309088aadb916b3905e23d47c48de0bdd195ab8c151e307b68f823b67c0", "prompt_hash": "c263eaaf01bc349ed1a5587db9a895069a11559b79d709e9e4bcb08588d18a26", "target_hash": "a38e2e01f546df70b14b352af8be3a1027dec352f34a69d75b7fe6b924e28598", "exact_match": 0.0}
{"doc_id": 6, "doc": {"question": "love yourself by justin bieber is about who", "answer": ["Rihanna"]}, "target": "['Rihanna']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: love yourself by justin bieber is about who?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Bieber"]], "filtered_resps": ["Justin Bieber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "71755dd7dfc683cc8f1ce6312cf74de8cfb95c469a0ae05628fc1eb70ce14c72", "prompt_hash": "6f185afcda85e08a71bc573c40852fe90be317e4a87b5e6281a2c76b1f1e9fe5", "target_hash": "0cb291d388f2bf3ee016de66524b208a263e8e639509908bf615d45a1fa99757", "exact_match": 0.0}
{"doc_id": 7, "doc": {"question": "who was the ruler of england in 1616", "answer": ["James I"]}, "target": "['James I']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the ruler of england in 1616?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James I"]], "filtered_resps": ["James I"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e76cc02f19834cea2633191cadb0d457896765a8546308b37ce365475c73d680", "prompt_hash": "97981c75839314846cb1527ec249603f59f1a8ffe62d6edfaae3ed6e4de71bb5", "target_hash": "f3b2b60ccdd7ebf967f6bab8091c4a5439d16129c391e99be98acd3f562035ef", "exact_match": 1.0}
{"doc_id": 8, "doc": {"question": "what is the hot coffee mod in san andreas", "answer": ["a normally inaccessible mini-game"]}, "target": "['a normally inaccessible mini-game']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the hot coffee mod in san andreas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hot Coffee"]], "filtered_resps": ["Hot Coffee"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b01d0229fcd699d15234584c4dbb2e88f8d1d1cd677ecf8f3bae7fc1d3ba2bc", "prompt_hash": "1d6ce56a039605b74c57b509efc1ab0005590db22f0543fcf558afb2b54c9069", "target_hash": "e1b79a5ff183c50d54901a8f915193ddc8d740bd52db6d4515ae2d51bcf897d9", "exact_match": 0.0}
{"doc_id": 9, "doc": {"question": "what is the maximum data rate for the 802.11a standard select one", "answer": ["54 Mbit/s"]}, "target": "['54\\xa0Mbit/s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the maximum data rate for the 802.11a standard select one?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 54 Mbit/s"]], "filtered_resps": ["54 Mbit/s"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "23e6c4250e050b8f1c6adaa44ef5b88781e939d7d21d3954043b03844818f6d5", "prompt_hash": "2ecf5bbb624120d57d9096a71a795faa84e499b5e56c67eefe111e65a29890e0", "target_hash": "1b9a93b5b9c7dbdea7075c40e5bd784107dcc2dfe0a96f817a7978c33c774136", "exact_match": 0.0}
{"doc_id": 10, "doc": {"question": "which state is located in the centre of india", "answer": ["Chhattisgarh", "Madhya Pradesh"]}, "target": "['Chhattisgarh', 'Madhya Pradesh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which state is located in the centre of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Madhya Pradesh"]], "filtered_resps": ["Madhya Pradesh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c15ba4085d68c81a50c02e8b0d22fb485a9f9d36956c9c2171959c97e79a09a4", "prompt_hash": "6b5a41bba7de0e6812fc4707f206595bef47d1dc7dcc39969aafa43317b16d1a", "target_hash": "f7a24aa2fb6be5b516df52c92fb5b180de43ccb8da2f8248dbff59035436d04c", "exact_match": 1.0}
{"doc_id": 11, "doc": {"question": "who sang i ran all the way home", "answer": ["The Impalas"]}, "target": "['The Impalas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang i ran all the way home?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pointer Sisters"]], "filtered_resps": ["The Pointer Sisters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eefa4a9d492295ece43bbede735acb84bf16a9e683bf67847eb93c195858f264", "prompt_hash": "8a84a54bc91a20b0e5d56b5a848f114ddbfddb82fb9ce1bc7cd32b00d5d81790", "target_hash": "9ba5cd8e54a0fa7ed170e57d01cadeecece363cb8400fb53938806068d20c8bf", "exact_match": 0.0}
{"doc_id": 12, "doc": {"question": "where did the last name wallace come from", "answer": ["a Scottish surname"]}, "target": "['a Scottish surname']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the last name wallace come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Scottish"]], "filtered_resps": ["Scottish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbd8fe0d42aec3a0cc2b1d59e36a1502419f64498930ce6776053c84ecc3501d", "prompt_hash": "76ce6a9dfadbb59725b9fb31a1d0d2076f1dabfc835e629d9db3700aec413367", "target_hash": "86a2220c39ba02cd587404a1393fb07a7622fff22390e34dcb021afa55faf705", "exact_match": 0.0}
{"doc_id": 13, "doc": {"question": "who was the actor that played ben stone on law and order", "answer": ["Michael Moriarty"]}, "target": "['Michael Moriarty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the actor that played ben stone on law and order?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Richard Belzer"]], "filtered_resps": ["Richard Belzer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf6706588400a95104d14cf4c2abc14fabeb7725ea1f5e6743117efafa943d51", "prompt_hash": "c31c7f50ffb7a8150caab04198c029b22443d423fc4b4ac1f90f467dde6189c4", "target_hash": "f676335b68633c8899b24304fd04e28a79d3312ecbeb012a55649f5055e005c9", "exact_match": 0.0}
{"doc_id": 14, "doc": {"question": "who does the voice of nala in the lion king", "answer": ["Niketa Calame", "Sally Dworsky", "Moira Kelly", "Laura Williams"]}, "target": "['Niketa Calame', 'Sally Dworsky', 'Moira Kelly', 'Laura Williams']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of nala in the lion king?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brenda Chapman"]], "filtered_resps": ["Brenda Chapman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "80d7b32575452ea49fb8219ce82801a2836e17c8b1f6a0c773266952d513c769", "prompt_hash": "1b5ad3f7d0a13b89c23356fb2d12517c3811e5b83f17dab357707de0ba91195a", "target_hash": "7896f753e5f369d0928082f29929d8fd4490d95174936523faebb30a3c56f78b", "exact_match": 0.0}
{"doc_id": 15, "doc": {"question": "who plays gram on the young and the restless", "answer": ["Max Shippee"]}, "target": "['Max Shippee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays gram on the young and the restless?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Susan Flannery"]], "filtered_resps": ["Susan Flannery"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3556fa423eef9f388f4906d87a30ea96985e2c6dffd5dcfa3914a855090e8733", "prompt_hash": "09ac2c3770efcfb3e3938d7ced5653609ad5cd293f0322cd2d97803d7b598774", "target_hash": "dc99da73bf9a7a2011ca2b58a77be587905992dd8db797bb97dd5c2dbb9fd651", "exact_match": 0.0}
{"doc_id": 16, "doc": {"question": "what is the oath that new citizens take", "answer": ["United States Oath of Allegiance"]}, "target": "['United States Oath of Allegiance']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the oath that new citizens take?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Oath of Allegiance"]], "filtered_resps": ["the Oath of Allegiance"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5bd6950c6488c508cfcc129905bead233a307230e98f381aaefd4895ecb2c0ec", "prompt_hash": "efbf8b491210455a8294864a7807fa69449df8ee01f0a12c93f1ac51fd5e7176", "target_hash": "cf623a953826ca0bac990fd3853880c26d2e5c812718d213b85d8a8eb57251c9", "exact_match": 0.0}
{"doc_id": 17, "doc": {"question": "who is under the mask of darth vader", "answer": ["Anakin Skywalker"]}, "target": "['Anakin Skywalker']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is under the mask of darth vader?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Anakin Skywalker"]], "filtered_resps": ["Anakin Skywalker"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbaaa28d3d52620d7c660e1790a04371e72dc6e3e627b438a4342460eda9b6a7", "prompt_hash": "287d195b9820b4896be4441dd571bacff19c767ebb9d9e169e547f6e2c4cffa6", "target_hash": "aff12b83711e9585492732031bc73f53e86d3984050edd3e04c494af97a823da", "exact_match": 1.0}
{"doc_id": 18, "doc": {"question": "who had a baby at 100 in the bible", "answer": ["Sarah", "Abraham"]}, "target": "['Sarah', 'Abraham']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who had a baby at 100 in the bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sarah"]], "filtered_resps": ["Sarah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "52553749ce83c8e54d48f1fe687084311027060a0d9cb8a1be9c5f1d400b2d09", "prompt_hash": "a4cba663091697d86cec4904db5b5211b7af55b1ac4c356c3edfb2ec4643957e", "target_hash": "223be3c18854e55c95a5055d6d4085e066536690ac5874fd2ef749db6d38d5c8", "exact_match": 1.0}
{"doc_id": 19, "doc": {"question": "what age do you need to be to buy a bb gun", "answer": ["18"]}, "target": "['18']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what age do you need to be to buy a bb gun?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 18"]], "filtered_resps": ["18"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2da6a3f362fba2d641c1dc5ffe7ecdfff435894b817329181e6ee2d2ba244587", "prompt_hash": "e509ffb13f26a545deb5b200617260d082084ac160672c402f422191e08d91d4", "target_hash": "fe29453a161fbb6965ccdbdca03593905c1e66cc520a49919b2d999ddf5c0c18", "exact_match": 1.0}
{"doc_id": 20, "doc": {"question": "ethiopia flight 961 crashes in to the sea", "answer": ["23 November 1996"]}, "target": "['23 November 1996']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: ethiopia flight 961 crashes in to the sea?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Indian Ocean"]], "filtered_resps": ["Indian Ocean"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a1a1867ab89195f7b308906fa789a650e68a673080223da46d2813e3953163f", "prompt_hash": "a9641e3ea95b0435307ad1be7e261af6c6fe60dd42068c65168d301f8a8c713e", "target_hash": "6b7cab1b1f5a298a05290b26811913d8d5a2db45dc8c28394e80822862ecd2a5", "exact_match": 0.0}
{"doc_id": 21, "doc": {"question": "where was percy jackson and the olympians filmed", "answer": ["Vancouver", "Mission, British Columbia"]}, "target": "['Vancouver', 'Mission, British Columbia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was percy jackson and the olympians filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Los Angeles"]], "filtered_resps": ["Los Angeles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a033ccb12f76031dbd71795e1abbb1f8364497ed9767abbeb6f0a2c08ceeff9b", "prompt_hash": "894094e0fede1b53551e82fe2d2500319dcf40e65c8fefd129e9f45518d76a4a", "target_hash": "46042bbf6abdad94109b8201cebb1b8426c7bdc8443b563fa60fbcc0bafa2737", "exact_match": 0.0}
{"doc_id": 22, "doc": {"question": "how many seasons of vampire diaries r there", "answer": ["eight", "8"]}, "target": "['eight', '8']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of vampire diaries r there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 8"]], "filtered_resps": ["8"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "29bc52cf48d9eae74160314f7c0891a7ac910d82918b3a60736d7c408bd343d8", "prompt_hash": "4e660162c0cffb5bf0e89149e02d508e1ed70957f0ff4b40495279866a9b2999", "target_hash": "2e4c7857030d0fc02e171bc29d61abdf9c155e0d789be6eca1f160a12e59ace3", "exact_match": 1.0}
{"doc_id": 23, "doc": {"question": "how many episodes are there in dragon ball z", "answer": ["291 episodes", "291"]}, "target": "['291 episodes', '291']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes are there in dragon ball z?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 291"]], "filtered_resps": ["291"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f30d68b33ce7bf1362689863e3dfe34d6d48ab5f8e76a45aa4426d411666bc8", "prompt_hash": "65c94c22090fed3886ac455d539ea2f299dd574d8e0646131c0495ab20e4e58a", "target_hash": "5708c699748dc852d05e2a23b8b1442874584d6789a3c3cefbc208e6ba984307", "exact_match": 1.0}
{"doc_id": 24, "doc": {"question": "who plays auggie in the movie the wonder", "answer": ["Jacob Tremblay"]}, "target": "['Jacob Tremblay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays auggie in the movie the wonder?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jacob Tremblay"]], "filtered_resps": ["Jacob Tremblay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "994deb110194632334b2a303b50164d8f59b56ef991313d8abb3af766d601bb4", "prompt_hash": "68629ae5f93907df7b47d2a7640c349bb456d59e1f58a1f96e706eb91ab7385a", "target_hash": "dabf011f199a9089c388d0dcbbec086fabc6b9aee70bd8e861c150dc61a93c56", "exact_match": 1.0}
{"doc_id": 25, "doc": {"question": "when was the first australian prime minister elected", "answer": ["Sir Edmund Barton", "1901"]}, "target": "['Sir Edmund Barton', '1901']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first australian prime minister elected?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1 January 1901"]], "filtered_resps": ["1 January 1901"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "998b1d7e52fdf576df3c6773482411709318a1d0d76e9dcb0b94c44855b7594b", "prompt_hash": "7fd1fe375f2ac39ebe6685529c9986a9d6a16281c7bedb0cbf02c7dbcf4d42f2", "target_hash": "a5cf4e02a082699a5805dbdcb67c3ec0bc74c521d1a069919e369f1d7e5893f2", "exact_match": 0.0}
{"doc_id": 26, "doc": {"question": "who plays matthew on anne with an e", "answer": ["R. H. Thomson"]}, "target": "['R. H. Thomson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays matthew on anne with an e?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lucas刮擦"]], "filtered_resps": ["Lucas刮擦"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "adef6cac5127f90a30ef31fbeea3e53f709001712ee678442a218aed3016a3f6", "prompt_hash": "269a2bb7b18b508cf39ee63a743bd62486f8d68209a294ba9f7012dd10d74f00", "target_hash": "85269152d01fdc618442ab786208d2b02014ca0dc3e9e0d54f864710bd566d40", "exact_match": 0.0}
{"doc_id": 27, "doc": {"question": "who is the girl in green day 21 guns", "answer": ["Lisa Stelly"]}, "target": "['Lisa Stelly']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the girl in green day 21 guns?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Megan Fox"]], "filtered_resps": ["Megan Fox"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "28cd06c6a4fe15f540f393b2ba19ba32872308590270b936547ead3bf6dc7dab", "prompt_hash": "ee5c8961098818e641b68c68d600dc7e8de605b2772b949a111c505d251148a8", "target_hash": "32a07643cf866bfbacade41e04d3c3feb7c9d350140872d44fe07f8586ceb95c", "exact_match": 0.0}
{"doc_id": 28, "doc": {"question": "who plays joker in batman the dark knight", "answer": ["Ledger"]}, "target": "['Ledger']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays joker in batman the dark knight?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Heath Ledger"]], "filtered_resps": ["Heath Ledger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "219d1ee84a612a9db1504dfce5285bd7f480c4673fbeccf5e23b926dbebd2e68", "prompt_hash": "e1d72c252bdcc9ff9ce9acf8743d7b0783a43820510ed1a15f57bdaf299d2df8", "target_hash": "6fb6c81a712e262a79f35fcc0f3bc79a1d1b1a194dab8e1dbad8b31798a881ed", "exact_match": 0.0}
{"doc_id": 29, "doc": {"question": "when was the south asian association for regional co-operation (saarc) formed", "answer": ["December 1985", "8 December 1985"]}, "target": "['December 1985', '8 December 1985']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the south asian association for regional co-operation (saarc) formed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 8 December 1985"]], "filtered_resps": ["8 December 1985"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c74d0ca32b04c6b121aa154967a037f6094532ccd99acf41bcae63506f138e01", "prompt_hash": "420f617cdd2678ba1697d637e9ea95ad68b971a23bfe68465d7ab473fb357697", "target_hash": "34cfdb170698170de1a1d36ff06080e52d09539851e0c25bb9cc54ae5e88ddd7", "exact_match": 0.0}
{"doc_id": 30, "doc": {"question": "when was the first hunger games book published", "answer": ["September 14, 2008", "2008"]}, "target": "['September 14, 2008', '2008']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first hunger games book published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2008"]], "filtered_resps": ["2008"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "30e6e27ac4b66c853b0316836e669dbeb2eb33a6e61d33625e8d80fd0169c8a8", "prompt_hash": "d0669d5d91aefb4f55a0c71985866703c32efd818c13f346f300491d58f792f5", "target_hash": "d294de4fb66d410c6a9381b721c07be44041b91e0ce70f48973739c75a87aba5", "exact_match": 1.0}
{"doc_id": 31, "doc": {"question": "who did bette midler portray in the rose", "answer": ["Mary Rose Foster"]}, "target": "['Mary Rose Foster']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did bette midler portray in the rose?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rose Johnson"]], "filtered_resps": ["Rose Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f1ea8b11b919d90f1afce1a224989875c4411268a6e161ebd932e6cb879ad17", "prompt_hash": "d8e571608127cb8b60babd96dc87954dd56d3bf1d66286726e934da29d4ccad2", "target_hash": "c3bfecf987e619c98b04a24ecfd8716212a69e26cbf61d15c8ec0ea430331e3e", "exact_match": 0.0}
{"doc_id": 32, "doc": {"question": "who is the lead singer of depeche mode", "answer": ["David Gahan"]}, "target": "['David Gahan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the lead singer of depeche mode?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Martin Gore"]], "filtered_resps": ["Martin Gore"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dde1867db277004006a20e6804a308a3dee87403005fba51e7808101f8c09e69", "prompt_hash": "ea34c5412f7faf29d30b1bdf9fbfb5925544441f28759bd1de47d3f63e8f5f35", "target_hash": "7df5b5b6c98891c7a120c16b0e7724f9d15f9cb7bdce9ffed56f1afb230939b4", "exact_match": 0.0}
{"doc_id": 33, "doc": {"question": "where did the butchers in the slaughterhouse cases live", "answer": ["New Orleans"]}, "target": "['New Orleans']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the butchers in the slaughterhouse cases live?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" rural areas"]], "filtered_resps": ["rural areas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d62fe073233dea5cb34cc9f61abd25a88e8e32bc6b9559646703ab94d4bfc2b", "prompt_hash": "862b8cab870fb452484ecf3c588315feeb879c41ff373cc3e1e120ba1c41f8b9", "target_hash": "4ce658a62319e88ad94def31386c7c995046810b186d6c69a64093dd2356a8a9", "exact_match": 0.0}
{"doc_id": 34, "doc": {"question": "who developed the concept of total quality management", "answer": ["W. Edwards Deming"]}, "target": "['W. Edwards Deming']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who developed the concept of total quality management?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" W"]], "filtered_resps": ["W"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2311a4b8e9c4923a9db58075886dc283011ff4aef41a8dbef6c849ca24c1693", "prompt_hash": "133deb5b9e958e63fc902327f55531d4f4b408752c2035a7c0e815e7d1cce5af", "target_hash": "920b16cdca84323109a6313a0d9e7d26d87d9b6c71af2c705f09b92c7d98a450", "exact_match": 0.0}
{"doc_id": 35, "doc": {"question": "who wrote cant get you out of my head lyrics", "answer": ["Cathy Dennis and Rob Davis", "Rob Davis", "Cathy Dennis"]}, "target": "['Cathy Dennis and Rob Davis', 'Rob Davis', 'Cathy Dennis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote cant get you out of my head lyrics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kate Miller-Heidke"]], "filtered_resps": ["Kate Miller-Heidke"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f297ae70dd46b6abc9b7daaccf908a3efd19f2b33c8c2b3620ad13470d459d0e", "prompt_hash": "71e9be0384bf0c6e00aac2c8b94618331af1f4e33b7a182fd6d92efe032b07a3", "target_hash": "5d6a3841095932f2455e5495f0b3fc90f5ad3dd74e2d249afaeee0ebc5251061", "exact_match": 0.0}
{"doc_id": 36, "doc": {"question": "where does no game no life anime end", "answer": ["the Elkia Federation", "the sixth volume"]}, "target": "['the Elkia Federation', 'the sixth volume']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does no game no life anime end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" episode 12"]], "filtered_resps": ["episode 12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e1a5bbe65b87a861fe54a36dacccad9bef748c3cd9a8f639711039183fef651d", "prompt_hash": "319008854b912c1e9c2ea2e38b6fedc15bc6efcffb1a986b343952bc0bdb63a1", "target_hash": "32bf3196a601de676de9e284c2de63be9912d20770b49cd8a2632b7188aeed5b", "exact_match": 0.0}
{"doc_id": 37, "doc": {"question": "when did taylor swift's first album release", "answer": ["October 24, 2006", "2005"]}, "target": "['October 24, 2006', '2005']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did taylor swift's first album release?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 24"]], "filtered_resps": ["October 24"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "433f4dfd085592c978be0eaf4587e0ce9146b7a19ed6a9c2845d533ebf2e10af", "prompt_hash": "9d6d1cf836fccaf3ec052bd585202dce4ce37febd9721d8faa8ee3bc04dbb8c8", "target_hash": "d4199fad2309f39768e5338c8c4be8a52111d054fa5e4ea12390c0151d7f3a8a", "exact_match": 0.0}
{"doc_id": 38, "doc": {"question": "what are the three fifty shades of grey books", "answer": ["Fifty Shades of Grey", "Fifty Shades Darker", "Fifty Shades Freed"]}, "target": "['Fifty Shades of Grey', 'Fifty Shades Darker', 'Fifty Shades Freed']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the three fifty shades of grey books?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Fifty Shades of Grey"]], "filtered_resps": ["Fifty Shades of Grey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "53c2cfd1f68e84f60291efecaeb84e69054421a9219bdff95fc709484cf76293", "prompt_hash": "63bfd93e4706c10f7b71c7aa9bc99e2999875dd00e15883d2798cda63dde1b64", "target_hash": "4d53701dbb7220e726e7512bca1002b4e57ccea110ab54b59d7dc2f95769d7bc", "exact_match": 1.0}
{"doc_id": 39, "doc": {"question": "who has the power (judicial) to make decisions in courts of law", "answer": ["judges"]}, "target": "['judges']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the power (judicial) to make decisions in courts of law?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" judges"]], "filtered_resps": ["judges"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "758c18ea8db437fbbbe1a452e932b97fda1c47f8eac05b347cd2f61845e681ca", "prompt_hash": "549358c9807d836d5833d384c1a544e2c5b1a6fccba0bca4d0906ba5aad01643", "target_hash": "c7a1bf0f320c9d98165fd638906850d728a0f5463ebd930adbe8feb7dcc5cf1c", "exact_match": 1.0}
{"doc_id": 40, "doc": {"question": "what are the ranks in the us navy", "answer": ["E-8s senior chief petty officer", "E-9s master chief petty officer"]}, "target": "['E-8s senior chief petty officer', 'E-9s master chief petty officer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the ranks in the us navy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" admiral"]], "filtered_resps": ["admiral"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "154614021062bb88f4b51083b071d9e5df04025606647d0458ef17ff2f1087a8", "prompt_hash": "e3c9d133657e4dbc2ed1ec02cbe484ed3b8d8c294b9776fba725c4026ebff808", "target_hash": "7155773d9f5b08d5b9be7af91550b94131a45a7a97123ef50ab0af19086aef36", "exact_match": 0.0}
{"doc_id": 41, "doc": {"question": "i was a great islamic scholar and mathematician who died in 1131 ce", "answer": ["Omar Khayyam"]}, "target": "['Omar Khayyam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: i was a great islamic scholar and mathematician who died in 1131 ce?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Abu Bakr Muhammad ibn Zakariya al-Razi"]], "filtered_resps": ["Abu Bakr Muhammad ibn Zakariya al-Razi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0dfa300a1311e0eb02122c20c66470a376a68575014a45542b828cccdf671a76", "prompt_hash": "14c21f4fb94ee7174ab6fc2b2cc8096f9e3e30af4ba507db8c74d2e3f7478e47", "target_hash": "bef65cf33d4b9a0dcf5514352fb8633a362ba0211fd8abfd3dfe038cb6daf544", "exact_match": 0.0}
{"doc_id": 42, "doc": {"question": "who is the coach for the ottawa senators", "answer": ["Guy Boucher"]}, "target": "['Guy Boucher']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the coach for the ottawa senators?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" D"]], "filtered_resps": ["D"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db9f0a3b1324ded0a8d1938e762324d18e042e9f073b88a1bead56e1fb6d3a1d", "prompt_hash": "297384f4ada41994d0728e24e66a033b047640ae57bec9000a53ee7161ba58c7", "target_hash": "28842ff60bad2734ee928385e664138c885e21695fd5d6aca0db81ac5bdf9730", "exact_match": 0.0}
{"doc_id": 43, "doc": {"question": "last episode of what happens to my family", "answer": ["53"]}, "target": "['53']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: last episode of what happens to my family?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" November 11"]], "filtered_resps": ["November 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3040c6c1e3ed884b442350fc92691f13284493a80465ab5d7564685922ea15cc", "prompt_hash": "2fc4911e97fbe6b8ea06b54022c5cde2e2d5629977bd0ea999033b62ea38a554", "target_hash": "994cf478b60a22500f61af47edcb519715ea3c7250c4deacc3eaa6386353d072", "exact_match": 0.0}
{"doc_id": 44, "doc": {"question": "when is the last time the vikings were in the nfc championship", "answer": ["1976", "2017/18"]}, "target": "['1976', '2017/18']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the last time the vikings were in the nfc championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ebd29d35dfdd4f4b054207a9566d12c6aba64b3856542f3cf3e5c66500d1448", "prompt_hash": "dad782a813ee0bfdf39594e3e246c2b1f30bec979b9b6dc114842f653c0f5e77", "target_hash": "e0dde953c8dbde9bcc300709cffd95bda495968a7375a5cfc9945991ad400da2", "exact_match": 0.0}
{"doc_id": 45, "doc": {"question": "what are the colors of the netherlands flag", "answer": ["blue", "white", "red"]}, "target": "['blue', 'white', 'red']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the colors of the netherlands flag?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" red and white"]], "filtered_resps": ["red and white"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "85afd08ec43cef836601531d183e00faed6b17dbcf015e3fc8d71e4c2e83a5c3", "prompt_hash": "fcd7aaad09ba792ad9b9720f85a5228ad0bee3710b838ff3c54831bfcfd7fc4f", "target_hash": "2025bc5688d17deac5fbfef4242ad8f3b04e6933b0b89c417433730bd7cfab93", "exact_match": 0.0}
{"doc_id": 46, "doc": {"question": "where is lord's prayer found in bible", "answer": ["in the Gospel of Luke"]}, "target": "['in the Gospel of Luke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is lord's prayer found in bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew"]], "filtered_resps": ["Matthew"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a3fb70e4c04cfc319b939dfcc929bb20823f960fc0e1a1532b8f33d914de40e3", "prompt_hash": "034d0d129d6fc00c2c02e199495b4c69cb45da7e1fec92b6fb28efc338899082", "target_hash": "a9ef16f3173170a3abd8366ba3540a985d473f814de6f4d50f12e52e77a25f01", "exact_match": 0.0}
{"doc_id": 47, "doc": {"question": "when was the last time oklahoma won a national championship in football", "answer": ["2003", "2000"]}, "target": "['2003', '2000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time oklahoma won a national championship in football?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2000"]], "filtered_resps": ["2000"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fc6ed9ce6dc6623b3b3fb3ffe3b317d62748d99feba7c8c8f9f9cb4261b827c1", "prompt_hash": "9a15591296a7b9f4bb8b56fe2e09585b2150777855ff94dcc5247b8144108e64", "target_hash": "5f55cb4608efcb651ad794b23f13c7603e674951f018200cf9cea71d60aad93d", "exact_match": 1.0}
{"doc_id": 48, "doc": {"question": "location of the ten commandments in the bible", "answer": ["Exodus", "Deuteronomy"]}, "target": "['Exodus', 'Deuteronomy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: location of the ten commandments in the bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Exodus"]], "filtered_resps": ["Exodus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c7b4d188ed1d43c30fd064f257eb664d9c9e0f06f6ca2e4cbe5a91cd716a3f2", "prompt_hash": "c5eb52b29eda2f79dfd9e9d1fa12c06997468062e349c19a70930b1548aa0d15", "target_hash": "117e67036f5407135cc9b00ffeab7554b6eb54019de3ef486aa7c673815744da", "exact_match": 1.0}
{"doc_id": 49, "doc": {"question": "where did the allies go after north africa", "answer": ["Italy", "the Italian Campaign"]}, "target": "['Italy', 'the Italian Campaign']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the allies go after north africa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sicily"]], "filtered_resps": ["Sicily"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "870e0f940a2370f90cf9325aecec3b5ca7b886e7a8a240cf1b76e45cec826437", "prompt_hash": "08d3a9351001c09a46cf2bd15a46ac91650c961d004bdf2b6ba437503426d427", "target_hash": "f7ea866ad3b46a3dccd35805b6a6eac791da28aa11297ea9efd5848828e860f8", "exact_match": 0.0}
{"doc_id": 50, "doc": {"question": "what is the meaning of veronica in english", "answer": ["she who brings victory"]}, "target": "['she who brings victory']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of veronica in english?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" true A: virgin"]], "filtered_resps": ["true A: virgin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5a85128a35705bf3d3c3f218997d4286ab7fa83ebe15f26a112f090374ea12a3", "prompt_hash": "a2b3b38019bdad3d0a003d47df416a6625d0f12885e565b9db855f140c2d1669", "target_hash": "3292b056a37dd065cca70c5d915e3538cc5efa7e912cf8b9c1e47e1c189a07ca", "exact_match": 0.0}
{"doc_id": 51, "doc": {"question": "what is the name of the main artery which takes blood from the heart to the body", "answer": ["The aorta", "aorta"]}, "target": "['The aorta', 'aorta']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the main artery which takes blood from the heart to the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pulmonary artery"]], "filtered_resps": ["pulmonary artery"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58de416ab62e10e18e351539c80738a18df8e8f0336b5cf4ea982ebad56e6568", "prompt_hash": "246f1d09467fabda37f73ccf97f3fc7743d538e9b0f814aa75cc23477b4fbd08", "target_hash": "d9483709e04727e52ce16d3df5a23a40dfc7baf1cb1f121ce7eba3ddb4e7b127", "exact_match": 0.0}
{"doc_id": 52, "doc": {"question": "who played solomon in little house on the prairie", "answer": ["Todd Bridges"]}, "target": "['Todd Bridges']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played solomon in little house on the prairie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Lerner"]], "filtered_resps": ["Michael Lerner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2eda08af90992bca386983cdb5d15efef63d5dd3bf4a221dc6fc806b12e5a90", "prompt_hash": "b033c167a3a4d0104c27c483994edf60c33926b19055545be2695f5ee21ff5d1", "target_hash": "962af2837e65d0863cc762dbdcf946c4c7bd8ad4ce8780d143e8107fc6b4d699", "exact_match": 0.0}
{"doc_id": 53, "doc": {"question": "who does eric end up with in that 70s show", "answer": ["Donna"]}, "target": "['Donna']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does eric end up with in that 70s show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Donna D'Errico"]], "filtered_resps": ["Donna D'Errico"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e13ff2d39795fb8bbf2449e14c0a1f6a6d2858a6813eba586f1892715fb6bb06", "prompt_hash": "11c2f75474c0081850e3c94e0cc2732bf8e7556fdd1b9de8807fd25266fb4b62", "target_hash": "9fa5e214dafbe51f4517859e8c5ca9da4664ed91b04720fcd85f04053fecdd89", "exact_match": 0.0}
{"doc_id": 54, "doc": {"question": "an object that moves around an external axis is said to be", "answer": ["revolution or orbital revolution", "orbit"]}, "target": "['revolution or orbital revolution', 'orbit']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: an object that moves around an external axis is said to be?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" rotating"]], "filtered_resps": ["rotating"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "835d1894e330ba6a04d0e821b7838d237919f72620a20aa3a8e91f73b69693cc", "prompt_hash": "6c73a5698929d41cf03986aac97ea1a3618058c311241d019931f6fd6caf3282", "target_hash": "f1cc27fa86b65ea94fc2049723e7954c9c001509903e6b7d4911ca71235c8ee0", "exact_match": 0.0}
{"doc_id": 55, "doc": {"question": "architectural elements forming rib vaults eg wells cathedral", "answer": ["an armature of piped masonry", "barrel vaults", "two to three barrel vaults"]}, "target": "['an armature of piped masonry', 'barrel vaults', 'two to three barrel vaults']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: architectural elements forming rib vaults eg wells cathedral?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wells Cathedral"]], "filtered_resps": ["Wells Cathedral"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9e8cabcdc1f9743f822ebeca5d1f4a5ec677b7ee3e6c24a43cda570e7e8b7b3", "prompt_hash": "fce538214ca6f4e936ca1cc33d2f4976fd5ad73cf0e606ca7356367d747e9648", "target_hash": "f34cd0216e1f3cac1dd8bb78be4f35c2582673bfd77177e37f1851f224023f68", "exact_match": 0.0}
{"doc_id": 56, "doc": {"question": "who sang the song good morning good morning", "answer": ["Gene Kelly", "Donald O'Connor", "Judy Garland", "Debbie Reynolds", "Mickey Rooney"]}, "target": "['Gene Kelly', \"Donald O'Connor\", 'Judy Garland', 'Debbie Reynolds', 'Mickey Rooney']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song good morning good morning?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Monkees"]], "filtered_resps": ["The Monkees"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "781fb1b030848217e4813b161d7a388e91d3b0fbdcaf9941d4bd6d48ebe230ae", "prompt_hash": "971f327edbfb74a0865555c6004db5295fb98734af2c78f1e71fe0fa4e22d96f", "target_hash": "e1682b7d624283b8c8194f9b8879719f499fd1cf54260d4cc71e84e69fa7517a", "exact_match": 0.0}
{"doc_id": 57, "doc": {"question": "who wrote papa got a brand new bag", "answer": ["James Brown"]}, "target": "['James Brown']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote papa got a brand new bag?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Brown"]], "filtered_resps": ["James Brown"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "998a14c514e2a44af3cc49918ef42d1f25f9b64404f2af2f0fc939cf2aac1494", "prompt_hash": "f5aaf0d326a506d2215b58509a7e070278b1bdde30b41798ccb79a700589597e", "target_hash": "e7eb324ab0c20ca12be12b7adef99ddfb5136ffac556dc6b726f1420602a2728", "exact_match": 1.0}
{"doc_id": 58, "doc": {"question": "where does a roadrunner live in the desert", "answer": ["mountainous shrubland", "arid lowland"]}, "target": "['mountainous shrubland', 'arid lowland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does a roadrunner live in the desert?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" southwestern United States"]], "filtered_resps": ["southwestern United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a129efbdb36f1066179a8b57f50054f93836e0d0f1e0a88870bfe962e3fc6ea8", "prompt_hash": "e465eabf62755c65004e767abf4b25083712b4514869638fc72fadc9bbe75889", "target_hash": "faee0f50d2f7956a5a207e3dc25834544c51606cf6b78559d931297ff023566d", "exact_match": 0.0}
{"doc_id": 59, "doc": {"question": "who wrote it's a long long way to pasadena", "answer": ["John Young", "Harry Vanda", "David Hemmings", "George Young"]}, "target": "['John Young', 'Harry Vanda', 'David Hemmings', 'George Young']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote it's a long long way to pasadena?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tom Snow"]], "filtered_resps": ["Tom Snow"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7bf12f966458db6854925c6607c25c5d72d21ad4ff38725a65406e10534f27c", "prompt_hash": "df74a1e88a8cf13f408733be9f07b8f51a18a59789370038b7daba7abf804936", "target_hash": "13ece7406ade40e1bbf4ba5cbd5383c1b1626b7c0bb2171cd51ab11a752a3d54", "exact_match": 0.0}
{"doc_id": 60, "doc": {"question": "what's the dog's name on tom and jerry", "answer": ["Spike"]}, "target": "['Spike']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the dog's name on tom and jerry?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sylvester"]], "filtered_resps": ["Sylvester"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "53d6058b5c5922309538fa326159261872b0e89c3065a7df3c1cb1556edcd791", "prompt_hash": "43196f3bc8033a49632860dd0191f57f5aa4d9df17d8be6af25bb995d3e16143", "target_hash": "e18a4a46b8e494783663e0ed8b60f98d8a6d9ed02a1f97cff25d2818977f7a6c", "exact_match": 0.0}
{"doc_id": 61, "doc": {"question": "who are nominated for president of india 2017", "answer": ["Meira Kumar", "Ram Nath Kovind"]}, "target": "['Meira Kumar', 'Ram Nath Kovind']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are nominated for president of india 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sushil Kumar Shinde"]], "filtered_resps": ["Sushil Kumar Shinde"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5bb05ccf0c3596c4de2c8fb9bcf683d4e87900adec0ad5c4c7972de3ec9cfcb", "prompt_hash": "57fba833f1b9628bdc82e1df245a7f0196ccc9ecd2da7e1a0760a583fa67b7d3", "target_hash": "51fe6e23ef5096952427e5309ddedbfdc378b6faac6ac85651cfa0fa6406bccf", "exact_match": 0.0}
{"doc_id": 62, "doc": {"question": "where does the path train stop in newark", "answer": ["Newark Penn Station"]}, "target": "['Newark Penn Station']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the path train stop in newark?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Newark Penn Station"]], "filtered_resps": ["Newark Penn Station"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ce18945c46d5cdfa9fe210ecd39d05769d0cb3004f53de509b0ceed54a104488", "prompt_hash": "348ee529f1aaed804f08506d68f01fb9a124a11ed1dcc800e0432eb78d606d22", "target_hash": "1313267f64aa9e9019e79f305c04084d29aefe5faee9444a9d8a3569320c65bf", "exact_match": 1.0}
{"doc_id": 63, "doc": {"question": "where does route 66 start on the west coast", "answer": ["in Santa Monica"]}, "target": "['in Santa Monica']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does route 66 start on the west coast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Santa Monica"]], "filtered_resps": ["Santa Monica"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "72e14d464c213715c368ff4e7d6f7470deff25971855962e134d2fb923237182", "prompt_hash": "616016c2e7eb3fa5969bf7613a9c9ca7b13378cb0a981f0a8bb6e174032a95fb", "target_hash": "ae1fd60d3ae0c8af2c7e22be3f7135f613f4e4345d846ea8521d64f55c8596fe", "exact_match": 0.0}
{"doc_id": 64, "doc": {"question": "who is the first wife on sister wives", "answer": ["Meri"]}, "target": "['Meri']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the first wife on sister wives?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kody Boren"]], "filtered_resps": ["Kody Boren"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ecb9928bb918152a5631e965a53a5f244450e9e61576dde8df13c94910314601", "prompt_hash": "48c1025be38f4ad97243706812058b92ad67eb2a5bc0a5b765e84637297346cc", "target_hash": "3e4614d7b699519370e9a6fbba9a66946bea26061315f3892d5f4eedaae7d49b", "exact_match": 0.0}
{"doc_id": 65, "doc": {"question": "when did houston go to the american league", "answer": ["2013"]}, "target": "['2013']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did houston go to the american league?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2013"]], "filtered_resps": ["2013"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e9339e2c8465569b3d95d65dfa5fcb693f1ab27dba3f2fbf3268c74ecbbcb289", "prompt_hash": "55d5856105b97ed733e3c92cb08cea0d91e367f1aacd7514fe7317ad271916be", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "exact_match": 1.0}
{"doc_id": 66, "doc": {"question": "when did they stop cigarette advertising on television", "answer": ["January 2, 1971", "1970"]}, "target": "['January 2, 1971', '1970']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they stop cigarette advertising on television?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1971"]], "filtered_resps": ["1971"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b058cd7870c8d964eb3a548229878e3d721e6d731f6b768bf7b220d13818c8fe", "prompt_hash": "5fbefd5bc65fdb154e835b721c55591cfae261b470fb499d98270d2fb285b618", "target_hash": "5bdec343e73c5dc395188aa46a3ad69a27b245ceac461400275b6af3492c9538", "exact_match": 0.0}
{"doc_id": 67, "doc": {"question": "coldplay song i will try to fix you", "answer": ["\"Fix You\""]}, "target": "['\"Fix You\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: coldplay song i will try to fix you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Parachutes"]], "filtered_resps": ["Parachutes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "835ab3d3743b68dfa69aa7425686612049cdff39b9f6bbf7254c134273322382", "prompt_hash": "dc5a5f44de5aa046de65b0b90d85cb80f21b2aa52482155211fc8d00362e1b37", "target_hash": "f181c00d54a3d64a5cd3d91ec545dbe489512454c0c4d72d7b569bf1d5eae3b1", "exact_match": 0.0}
{"doc_id": 68, "doc": {"question": "points on a sphere or angles in a circle are measured in units called", "answer": ["radians"]}, "target": "['radians']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: points on a sphere or angles in a circle are measured in units called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" radians"]], "filtered_resps": ["radians"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9094f300ab946a7b96704ce9566429d4ab58ffdee0063410de903179b89971d", "prompt_hash": "b21c9ffd77bb6c21ce4cb7a414b6c212db2687b6da5a6a002ea4c33996f334c8", "target_hash": "c79fac7e7c927cc9e62b0cc197ddf74b78240e4b1051469a0f6a6016f404b849", "exact_match": 1.0}
{"doc_id": 69, "doc": {"question": "the oligodynamic effect is a phenomenon that describes", "answer": ["a biocidal effect of metals"]}, "target": "['a biocidal effect of metals']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the oligodynamic effect is a phenomenon that describes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the toxic effect of heavy metals"]], "filtered_resps": ["the toxic effect of heavy metals"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eae8f32d8517087d2ac92928359323efa9c7546c082612edb8a07cbd29112432", "prompt_hash": "51c2e0be63b44ad148757924a0569c04ebf67297a92646bedb75fa979cd545c9", "target_hash": "e1498186ba7659313389daa0de49f400e50ac9669bb6d889310e788fbfe81914", "exact_match": 0.0}
{"doc_id": 70, "doc": {"question": "where does the white witch live in narnia", "answer": ["her castle"]}, "target": "['her castle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the white witch live in narnia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the stone chair"]], "filtered_resps": ["the stone chair"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "41f364d425c022a9674cc3760008f67fa55151333760965cdba6a51db96fdede", "prompt_hash": "047616804448321aad79a2a3c2afd2f11dee8f5c6af8d4b3ea55a975c9be62ff", "target_hash": "45774d7aa9a6215a17dfb6578bcf2c2283d75755bc03652cf43eb77b0c51955d", "exact_match": 0.0}
{"doc_id": 71, "doc": {"question": "how many games in a row have the uconn women's basketball team won", "answer": ["111 straight wins", "111", "90"]}, "target": "['111 straight wins', '111', '90']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many games in a row have the uconn women's basketball team won?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 88"]], "filtered_resps": ["88"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1f85dcf684b162659f2a92f829cb1ed7b40142ed698e53a6d1fa60a4a4ad03a1", "prompt_hash": "8bb33c12e3e094bb4011e7b38a099c9fcab3f7fa0d0d2b8385ac5327648e2208", "target_hash": "546d4d39b360b1a21b01cd70f6b7d593d2f962778621e1d4d2aa4dc093a52b17", "exact_match": 0.0}
{"doc_id": 72, "doc": {"question": "is there a name for the at symbol", "answer": ["commercial at", "at symbol", "at sign"]}, "target": "['commercial at', 'at symbol', 'at sign']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: is there a name for the at symbol?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" at sign"]], "filtered_resps": ["at sign"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8edf7e1f71eceaa9514f50f89960339db132f696289a2019cf0010405bbe22f0", "prompt_hash": "526fc1d5a3ee8e42bf4d01b6ff222007100a6bc29d8ff1b627c479ef5d5c0d97", "target_hash": "526e8ac5c02238b434783c983e4a8ddeab3bde5cc2a4515f6704dfef2e8cd9ca", "exact_match": 1.0}
{"doc_id": 73, "doc": {"question": "who sings the theme song for the tv show cops", "answer": ["Inner Circle", "Jamaican reggae band Inner Circle"]}, "target": "['Inner Circle', 'Jamaican reggae band Inner Circle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the theme song for the tv show cops?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Doug E"]], "filtered_resps": ["Doug E"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e49b108352bd863ffca13ccb84b91f24a2be467713de8268dd13b1ab5b06eac", "prompt_hash": "92d1b25951810267d84a708fe15da9d10c98994aac7ef07c73264d5df92c78d9", "target_hash": "906268e183a488e32ee5f3ea94995ad3df3169e790e099c0ee4b3a9a1463e909", "exact_match": 0.0}
{"doc_id": 74, "doc": {"question": "when was corporal punishment banned in south africa", "answer": ["1997"]}, "target": "['1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was corporal punishment banned in south africa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1995"]], "filtered_resps": ["1995"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "56d7df7fd032e6a33871f34b3ba85af11cebfa1d006acef81c32c94a7493f16f", "prompt_hash": "70b22aee0df126c92f277d5fdf1520f451dc9643718981d030ed80e9e4ac6640", "target_hash": "982df96f59043cb182196071792c2904ff376f72b15cb9b414721d5b74a296b0", "exact_match": 0.0}
{"doc_id": 75, "doc": {"question": "who plays captain phasma in star wars the force awakens", "answer": ["Gwendoline Christie"]}, "target": "['Gwendoline Christie']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays captain phasma in star wars the force awakens?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gwendoline Christie"]], "filtered_resps": ["Gwendoline Christie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aa57001b8c6a0aa919d96e27753cb63ed5b7f7a45e6f0dce03bcfb4ceb5aeb90", "prompt_hash": "c770aad7421bc5cf04a321343021459461e070ae915458db39734fb9116d61b2", "target_hash": "cbf50392ab353a5e291aed705556466b81c254141d07273eca63e75b05d2d28e", "exact_match": 1.0}
{"doc_id": 76, "doc": {"question": "who sings she's like the wind lyrics", "answer": ["Wendy Fraser", "Patrick Swayze"]}, "target": "['Wendy Fraser', 'Patrick Swayze']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings she's like the wind lyrics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alanis Morissette"]], "filtered_resps": ["Alanis Morissette"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d84e23603269e9a1f1b80deff6c9cfa30741a9b9f36eb7ed72bc719ed793f01", "prompt_hash": "34109e5af6b55ee5a653ff663fb224eecb3fdc691fdafc6a6671a53c995a60a4", "target_hash": "69ce52011c5cc064b1706f64339e244acc6113034e5f32f4bf3f7d3464e32931", "exact_match": 0.0}
{"doc_id": 77, "doc": {"question": "when did marathon change its name to snickers", "answer": ["19 July 1990"]}, "target": "['19 July 1990']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did marathon change its name to snickers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2011"]], "filtered_resps": ["2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb3c527643cccfac430113087612920b3a28a40e01e5308dbc3f220724b3c835", "prompt_hash": "8c157edef742340f902a8e997c5e4bc732d8340b669386533dd979ecce0c6c79", "target_hash": "65da034a5e3396f9de2b896155a6d6832a35ea389fafa105c71fd519764d7594", "exact_match": 0.0}
{"doc_id": 78, "doc": {"question": "who captained the first european ship to sail around the tip of africa", "answer": ["Bartolomeu Dias"]}, "target": "['Bartolomeu Dias']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who captained the first european ship to sail around the tip of africa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bartolomeu Dias"]], "filtered_resps": ["Bartolomeu Dias"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ffd1e278585857b77a0f2a87f3e227fbb3e826c4959a26ee149f4b761d3c36fe", "prompt_hash": "2666f8363605bd9ac729eaf4e0cd7df6361cf79423cbfd15c4b964334752faab", "target_hash": "64c9bcbffbf1010b641dc382dbddbaac96e39154342310df6a51988f6321e70b", "exact_match": 1.0}
{"doc_id": 79, "doc": {"question": "who ordered the forbidden city to be built", "answer": ["Zhu Di"]}, "target": "['Zhu Di']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who ordered the forbidden city to be built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Emperor Yongle"]], "filtered_resps": ["Emperor Yongle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2105979e42a4eae26d88b06d1d8938a04f9b9eaa86f6295bf801d221c238a793", "prompt_hash": "458efc4637a046664fe13eadeb836f0566196cbada4ebaf653fb133d67a611ee", "target_hash": "2b3e50b60e30c5c936838eff8c83db6516bf10de6e8b654a0ccaa8daee71ec46", "exact_match": 0.0}
{"doc_id": 80, "doc": {"question": "who sang let me tell you about the birds and the bees", "answer": ["Jewel Akens"]}, "target": "['Jewel Akens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang let me tell you about the birds and the bees?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Partridge Family"]], "filtered_resps": ["The Partridge Family"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5f1aa35b3954f65f04d37461b96dbae2e282fbfc6a60a2f403bb9d86e9f2fdf4", "prompt_hash": "41713237fec140a08c4f05e018109d0f59ad82d415af3d44d011cdd57d543fc3", "target_hash": "f7111695347701890bb2240093e2e35bab51fdbcba6790e1c4d8b74b63b80a56", "exact_match": 0.0}
{"doc_id": 81, "doc": {"question": "who has the power to approve or veto legislation constitution", "answer": ["the President"]}, "target": "['the President']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the power to approve or veto legislation constitution?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Congress"]], "filtered_resps": ["Congress"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "670fc5c675fe22e61fa8ef8b5f1c9fc09f22cfed4297ad1903fe5f72ebb849db", "prompt_hash": "5bdf7cb5d312daf8917f627d6f3d7f2a31d583f7ac8849ec209fafb9790f530c", "target_hash": "8411dd33a8292ec426cb7ef9a3a92c9a0ed1343f8827d3bfbdbb8e4db90bdf82", "exact_match": 0.0}
{"doc_id": 82, "doc": {"question": "how many seasons of the rugrats are there", "answer": ["9 seasons", "9"]}, "target": "['9 seasons', '9']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of the rugrats are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 8"]], "filtered_resps": ["8"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9960e7b576b78e67070f4e5d69fa2c56243ec0927f58dfbb02deb1c71701bd47", "prompt_hash": "74046fdc6f98f2a59126b1f53106b028938c14a460aa74a354e306108430721a", "target_hash": "5c5002f846b92ccf7cc94cfe6746b4f1a087154c40192ac01085d39853deb3b2", "exact_match": 0.0}
{"doc_id": 83, "doc": {"question": "who is the minister of local government in zimbabwe", "answer": ["Hon July Moyo"]}, "target": "['Hon July Moyo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the minister of local government in zimbabwe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Simba Makoni"]], "filtered_resps": ["Simba Makoni"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8fa96d9c01c3ca75579156abf169cf4c4de9191681cb7acdb7cf3bc425284d9", "prompt_hash": "8c01a9867cf630bd9650b058fe6c72e6026d2fef8b8c61226527f2a0839f6279", "target_hash": "e6d68e215bb5516e2a111913c47b9e28aed89584562d5b82a1983481d5c94c85", "exact_match": 0.0}
{"doc_id": 84, "doc": {"question": "what was the initial effect of the transition from command to market economies in eastern europe", "answer": ["Inequality of opportunity"]}, "target": "['Inequality of opportunity']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the initial effect of the transition from command to market economies in eastern europe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" poverty"]], "filtered_resps": ["poverty"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "add48c5e11d29dafe7037e2df9314d20c5771dafd5ca83aaea204b8b57ce98ea", "prompt_hash": "9557dc898d254084e0e9061dac0b2e54bc7ee9efc4fd51ce56641f81460143c1", "target_hash": "517811f41548b6b5a8e3aec001fd37d2f2ff7b7ece2b76557942f145972dc125", "exact_match": 0.0}
{"doc_id": 85, "doc": {"question": "when did toyota start making cars in the us", "answer": ["by the early 1980s", "1984"]}, "target": "['by the early 1980s', '1984']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did toyota start making cars in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1984"]], "filtered_resps": ["1984"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5fb5e7ca0e13e3822ad4d6a3ba39bd4b7b0bdb4621d2bc2c6b006a9ae171c5bd", "prompt_hash": "a96a3efa2aaa7923a6efec0adba68b4aff3968cdd2ddfcf51a70c106339564c7", "target_hash": "c642341c8b683c762e3e6d4fcdad4ac77641f307098e8270a2eef2fed11b4b96", "exact_match": 1.0}
{"doc_id": 86, "doc": {"question": "how many pieces in a terry's chocolate orange", "answer": ["six", "20"]}, "target": "['six', '20']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many pieces in a terry's chocolate orange?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "84b9c5b7af94b3b28c3624722389aef91d6cb77a1bed5ec35c8ed56ec5b8e6e0", "prompt_hash": "a0341983413a94c9c2bed0f3c0385291a23d860b69b721848c8b0a20acdde3ba", "target_hash": "f03821bc15692a38ddce961d12f1974f78157183d0e8d3b4891e4b201733b4e6", "exact_match": 0.0}
{"doc_id": 87, "doc": {"question": "who plays the saint of killers on preacher", "answer": ["Graham McTavish"]}, "target": "['Graham McTavish']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the saint of killers on preacher?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tom Berenger"]], "filtered_resps": ["Tom Berenger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d296a332e83b224d855f66fa41d4f91bf33ca57ff5ba733baf6090595ea9820", "prompt_hash": "41c2b237834982be3eaa10a51d924ed5f1733f863cb674c2bad9d6db697fe209", "target_hash": "ec508a2cad09a04adb650bff211e1e6cb61c369c69995496c17a45d248ae53d6", "exact_match": 0.0}
{"doc_id": 88, "doc": {"question": "when did the royal proclamation of 1763 end", "answer": ["the American Revolutionary War", "with the American Revolutionary War", "1783"]}, "target": "['the American Revolutionary War', 'with the American Revolutionary War', '1783']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the royal proclamation of 1763 end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1763"]], "filtered_resps": ["1763"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9770ddff618648bc23dd4718d83aa965088a0beef3c38fe1871339364ae4754d", "prompt_hash": "26c8340c6e345efddb887b5ac44c3df51717baed478bf41ee791a3ac4b167036", "target_hash": "36240f0f50f1f8067f815bd5d5d4fe569ad2e078c3bedf9a6a02f382184b9bc1", "exact_match": 0.0}
{"doc_id": 89, "doc": {"question": "who is playing the halftime show at super bowl 2016", "answer": ["Beyoncé", "Coldplay", "Bruno Mars"]}, "target": "['Beyoncé', 'Coldplay', 'Bruno Mars']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is playing the halftime show at super bowl 2016?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Beyoncé"]], "filtered_resps": ["Beyoncé"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eb02cab18be5b65febc06b87fe56a2c8e96a0fec4ca3f5172e9ca3fa5535b7f1", "prompt_hash": "32c9c019b0a93784cc19edb7a9cb1b5050d9e5ae1fb6c6500de1f0485d66adc1", "target_hash": "9760f26521f5efc03bf21e4c8093555945a52dcf386ec4d37f60fb3bc9928a8a", "exact_match": 1.0}
{"doc_id": 90, "doc": {"question": "jonny cash one piece at a time car", "answer": ["Cadillac"]}, "target": "['Cadillac']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: jonny cash one piece at a time car?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cadillac"]], "filtered_resps": ["Cadillac"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d62541b3053e717fb078333d9df1ac38ca4efc2036f5c8679d538f276652856c", "prompt_hash": "fd3f0a888aabad32944f7952e0beec569763fa428299a8652c0729be6d4caabd", "target_hash": "4e68107500159025d0b59d347d436a744b03208526362c87cbc42d5df558d2f0", "exact_match": 1.0}
{"doc_id": 91, "doc": {"question": "what is the minimum wage in france per hour", "answer": ["11.16", "€9.88 per hour."]}, "target": "['11.16', '€9.88 per hour.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the minimum wage in france per hour?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" €9"]], "filtered_resps": ["€9"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e46c2b1e10d48c7960f31cfc962cffc3fa625f6fb7d70d0f74d8521020c13bd4", "prompt_hash": "d379532f2674ac1350bcd838fd2b0016cfbccaf9b3cf6f9e17180daa998d91d0", "target_hash": "8d5a8125a9f8b4ba1ee72193b6637fdd6c30ec991b3b11507ace9a370368f9a2", "exact_match": 0.0}
{"doc_id": 92, "doc": {"question": "describe the three phases of the normal blood clotting process", "answer": ["blood coagulation", "vasoconstriction,"]}, "target": "['blood coagulation', 'vasoconstriction,']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: describe the three phases of the normal blood clotting process?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" coagulation"]], "filtered_resps": ["coagulation"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea3d60dea0dbb984a4d98674e52dce3c8838aefef85dc95c273a869714b3ca4f", "prompt_hash": "2c28afbddd6fb338c030ce699adbf2c76440e821e4f18ddc0f1f9a13486b63e3", "target_hash": "5d24d5dc04feab2d00d5d976c48ba2bcf815873b6996904178b71d7dcba6c3bd", "exact_match": 0.0}
{"doc_id": 93, "doc": {"question": "when did amnesia the dark descent come out", "answer": ["8 September 2010"]}, "target": "['8 September 2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did amnesia the dark descent come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2010"]], "filtered_resps": ["2010"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea381bd1c53879605d407efed683c8eca450c41ea83fd5a0c74e2b272e932433", "prompt_hash": "952359457e49ce9d9b087bf35ccff3ed4d043370b991bb30a2ab22c74dab6dc0", "target_hash": "0215b0d82163b05b54178171ea940df2d3bb594a60677e88bbd2414958d4388c", "exact_match": 0.0}
{"doc_id": 94, "doc": {"question": "what is the oldest street in the philippines", "answer": ["Cebu City", "Colon Street"]}, "target": "['Cebu City', 'Colon Street']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the oldest street in the philippines?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Quiapo"]], "filtered_resps": ["Quiapo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "131ee3b5ac4cfa4569ee9af3577b4ce40ba6471faae15bdf7a5b36e5ee825563", "prompt_hash": "72eb1811c81dfa4ebda0eb6b9d343be323a14d02c2b0b292adaf74135599cc83", "target_hash": "acf01aa532a34067de21640fc87bcb29f3057392e41c2c3957e80949f6f640ec", "exact_match": 0.0}
{"doc_id": 95, "doc": {"question": "where is the tibia and fibula bone located", "answer": ["leg"]}, "target": "['leg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the tibia and fibula bone located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" leg"]], "filtered_resps": ["leg"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a0af48738e60297fa5ffcc7ae18cf263a617cae614f5efc2ecb57039e9d3f8fa", "prompt_hash": "a4a1e951fe7fa1491e20a538976600b62f6b012285856d29caab292a375cb8f5", "target_hash": "ddfeb9d946a1af23527430b813e5ff138d2a58002274fd6d03b0282eb33fac51", "exact_match": 1.0}
{"doc_id": 96, "doc": {"question": "who played bat masterson in the tv series", "answer": ["Gene Barry"]}, "target": "['Gene Barry']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played bat masterson in the tv series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Arness"]], "filtered_resps": ["James Arness"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d076bf78219302c6b40cfad5387562ee0726a196aab3770a27bfd331373fe442", "prompt_hash": "d40a3b8c633191949a36e37d2af4541a4e6a1f2dc9db3076af130d7c89e8946d", "target_hash": "fdba9f95206ef9a2655aeb4184bc23c986b942e36fb28c12e416a79ea856698d", "exact_match": 0.0}
{"doc_id": 97, "doc": {"question": "who sings sugar sugar you are my candy girl", "answer": ["the Archies"]}, "target": "['the Archies']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings sugar sugar you are my candy girl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Archies"]], "filtered_resps": ["The Archies"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff52613be8e9faec63c068b9ebe41b4480719fe76c81cd783bb5417fcaaed5a8", "prompt_hash": "c27ec8f7db5b4d4910a0aa0e35fb23c43448912ce5c754a8326595bd23335a0e", "target_hash": "ad01b0f8cae6551e00d34f77bb8b6667bfc95f7275e2a89d412b4e2469793b27", "exact_match": 1.0}
{"doc_id": 98, "doc": {"question": "types of skiing in the winter olympics 2018", "answer": ["Slalom", "Downhill", "Super-G", "Giant slalom", "Slalom – (SC)"]}, "target": "['Slalom', 'Downhill', 'Super-G', 'Giant slalom', 'Slalom – (SC)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: types of skiing in the winter olympics 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alpine"]], "filtered_resps": ["Alpine"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "12597c33101505417e957b20fcab44089ff307474d85a2d7095f28afb73302cf", "prompt_hash": "e8053ec829e0787631787661b9cac917f2a916974f169924829c3cdaea3f5adf", "target_hash": "e6d37f526acf6e857f85f7c983c8ed80dcab4024d87235ec0128eee69b09e828", "exact_match": 0.0}
{"doc_id": 99, "doc": {"question": "who plays noah newman on the young and the restless", "answer": ["Robert Gillespie Adamson IV"]}, "target": "['Robert Gillespie Adamson IV']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays noah newman on the young and the restless?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William J"]], "filtered_resps": ["William J"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ab49a9f945af15989d916fc7f6f54380ba396b9adbc3d9d0d81586432cba06a2", "prompt_hash": "2327e787cca7417c7f765b8e62d66b9b61671d407a31b6d82fdce93595b76428", "target_hash": "bf411f98dd3d06c469b7cad600df5efa7cc02e5c8299d63e800c370176080d65", "exact_match": 0.0}
{"doc_id": 100, "doc": {"question": "who has won the most games in nfl 2017", "answer": ["Dallas Cowboys"]}, "target": "['Dallas Cowboys']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the most games in nfl 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dallas Cowboys"]], "filtered_resps": ["Dallas Cowboys"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3abbd82399e802eeb4a9dac4e86d74bb4eb88c385446a5b082d643df48da5238", "prompt_hash": "6efd3f1ab6d12180bbfacc2f92eab6cb9a67395e172040c706978f8b8f6f84a8", "target_hash": "df431f44c0110052e21f1e42852ba4075f0c24cc89c294152471ff589425aa4f", "exact_match": 1.0}
{"doc_id": 101, "doc": {"question": "under article 1 what is the minimum age required to serve in the house of representatives", "answer": ["25", "25 years old"]}, "target": "['25', '25 years old']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: under article 1 what is the minimum age required to serve in the house of representatives?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" twenty five"]], "filtered_resps": ["twenty five"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c7d37017216ca8133ea00876c650ee65d072fc0f08186019a56d7b6c31fa1c6b", "prompt_hash": "5fb20895ac78ad4f9be6b927aabc1a088f53431b195d7a3db0f70248453aae21", "target_hash": "5549865c69eebd63a1c7b189815c71e26b6b78495f627e31482dc5bb8314234e", "exact_match": 0.0}
{"doc_id": 102, "doc": {"question": "who has the most catches in nfl history", "answer": ["Jerry Rice"]}, "target": "['Jerry Rice']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most catches in nfl history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jerry Rice"]], "filtered_resps": ["Jerry Rice"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1b9e6fbd31d79911dd71f394788ed643d68e1a8784a6b0105809772192ca32f6", "prompt_hash": "3623932f519460a7c20d9f359bd43b2945868320f2d00ef7bbf64a8904423a68", "target_hash": "6c4ad64d33b8c6d1c82ca10fcc4597275147fc99bf6626240aa072bc826189f0", "exact_match": 1.0}
{"doc_id": 103, "doc": {"question": "when does the dlc for rainbow six siege come out", "answer": ["January 2018"]}, "target": "['January 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the dlc for rainbow six siege come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 2017"]], "filtered_resps": ["February 2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cd045ce9d32730544ad61222a608229f0111dc261c31deeb05761a766f790300", "prompt_hash": "52e0950657f7e6c0d5a442210decfb62d741e3697f8a58a0a441ab0a1a1767d5", "target_hash": "555366a826525fc6826eb52673034fc9f90594cf542dffba0e80a04898b03570", "exact_match": 0.0}
{"doc_id": 104, "doc": {"question": "who was the great wall of china built to defend against", "answer": ["nomads from Inner Asia", "nomads from Inner Asia."]}, "target": "['nomads from Inner Asia', 'nomads from Inner Asia.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the great wall of china built to defend against?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mongols"]], "filtered_resps": ["Mongols"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c9ecbff432a2eeeffa64789a8233336a89bbf980e06c4b0fbad85fb0f830b47", "prompt_hash": "f7eab8a2ee81a6dbda8a5c22ad0ea013b344b43ea08e0f1a873affd9d55438c6", "target_hash": "2681fec91e16b7b0c5246d25f331f5553207c2e2361439650fee4cf46d23e0ac", "exact_match": 0.0}
{"doc_id": 105, "doc": {"question": "where does the last name galvez come from", "answer": ["Spanish surname", "Spanish"]}, "target": "['Spanish surname', 'Spanish']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name galvez come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spanish"]], "filtered_resps": ["Spanish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "438ffeeff0135123942dca1ab8d9a1d9dc61396c70707873496b2047358d1bd3", "prompt_hash": "1d5609145e4874b1b0cc07afd7f0a7d9cd0f66e1d25f9c35457413432108475b", "target_hash": "0e2074f1a0890a2d84d4178796cccd729986a1aa75da3fc400bd56284dfff22e", "exact_match": 1.0}
{"doc_id": 106, "doc": {"question": "who opens the church of the holy sepulchre", "answer": ["the Sunni Muslim family", "the Nusaybah family"]}, "target": "['the Sunni Muslim family', 'the Nusaybah family']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who opens the church of the holy sepulchre?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Greek Orthodox"]], "filtered_resps": ["Greek Orthodox"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "813253ae7552843c232f30c15bf3971bca3c89502b77a33f0b7a761be596e3d3", "prompt_hash": "1f121a65b6e135697d57b0b1891f7627ef649e53c944b6929fdf3374eca21907", "target_hash": "98c92331b89cf15c010d7e7171ded69bebaf67de0572822c7f0b88cc019a2f55", "exact_match": 0.0}
{"doc_id": 107, "doc": {"question": "when does season 5 of the blacklist resume", "answer": ["January 31, 2018"]}, "target": "['January\\xa031,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 5 of the blacklist resume?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 10"]], "filtered_resps": ["January 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ce057983d1302fb8b51d0ff343aff7f27e98d7efe96a4440b87d53067606571b", "prompt_hash": "20ad652734a89f9b3f704cdc18af4fc837a5ea7cc07d1076c84a2c74140629b0", "target_hash": "f0e068bd7510015670b0ae5aac6d1756855afd1341beac7795d7e1af3e440f2e", "exact_match": 0.0}
{"doc_id": 108, "doc": {"question": "the chinese dragons are protectors of how many seas diggy", "answer": ["Four Seas", "Four"]}, "target": "['Four Seas', 'Four']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the chinese dragons are protectors of how many seas diggy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" five"]], "filtered_resps": ["five"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eabd879396a50b3ec69d8e565818293f6bc41e503c0d14747dcce9d15d0d67ec", "prompt_hash": "d3ff6c27e5e52c2ad8f6bdbbec53f26e75d740142e191a86af5f3be7f34fa533", "target_hash": "f5f11d86deca6df4cd4b2a137088db399f970b06a0fb24926838dbbfcece4729", "exact_match": 0.0}
{"doc_id": 109, "doc": {"question": "how much money did it cost to make gta v", "answer": ["137"]}, "target": "['137']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much money did it cost to make gta v?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $250 million"]], "filtered_resps": ["$250 million"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "360ac794dbb688c432c8f5c6f66f111c66c17e43e6b08448ab4238a74dc85d05", "prompt_hash": "0fd236fa88c7f2328e2cd11172ab3a08722eaf6333a5f433ee6521255e721632", "target_hash": "aea570b1acd6bbe59ebee0206ba139d139df004e45b5b3473f7282f58e910ff9", "exact_match": 0.0}
{"doc_id": 110, "doc": {"question": "what is the angle of the tower of pisa", "answer": ["about 3.99 degrees", "3.99 degrees", "at about 3.99 degrees"]}, "target": "['about 3.99 degrees', '3.99 degrees', 'at about 3.99 degrees']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the angle of the tower of pisa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3"]], "filtered_resps": ["3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aba96d53026f4706aaaa2631de352c79e06ca1d09d651626d02137467276e9de", "prompt_hash": "20a74b54acaf97c7280a2946f57cad18b1fe408e1d9f0107e842a0a3c344999e", "target_hash": "7cff86f27f5bead154c9d72ba096c6aa41eed4e60af138d9020bf66427b67b7d", "exact_match": 0.0}
{"doc_id": 111, "doc": {"question": "what is the share of agriculture in indian economy", "answer": ["17.32%", "23%"]}, "target": "['17.32%', '23%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the share of agriculture in indian economy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 17"]], "filtered_resps": ["17"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7fccb68bf5c13cef1db669bb6e66fa016823959ea53ee3b6c0258a1d54ae1a04", "prompt_hash": "0f6ce02c9e025fc8652efd7cf6c6c5576d20e67763eee7ecd721366fdf9346b8", "target_hash": "80feec237ee1ad6cb0df6c3e31598f9028f846c5537c339cb7f65ac144c217a0", "exact_match": 0.0}
{"doc_id": 112, "doc": {"question": "who does demetrius love in a midsummer night dream", "answer": ["Helena", "Hermia"]}, "target": "['Helena', 'Hermia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does demetrius love in a midsummer night dream?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Helena"]], "filtered_resps": ["Helena"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7987e35bc974806266c2c314250a21136708036531898630f28d368d58c9e0e9", "prompt_hash": "089f8ff2cacac022fcc40f900ff776603b0e4a92aa2263b36445aa95d1766c29", "target_hash": "c35d43c33e8aa3dc02624911a118415590d770dba870d6f824be59b85a19d936", "exact_match": 1.0}
{"doc_id": 113, "doc": {"question": "when did the movie varsity blues come out", "answer": ["1999", "January 15, 1999"]}, "target": "['1999', 'January\\xa015,\\xa01999']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the movie varsity blues come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1999"]], "filtered_resps": ["1999"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a40d175760f3b1949bfa3b4728a968a7a31b1a21ace4973a4e5c1ced1f604bf9", "prompt_hash": "f96d1561823df336da0b8069e2d97ef10bad7021a165a4a40cb73105ddac08e8", "target_hash": "ee1f8a0ea6a04d48d748223ba94bfbcacdd5c3cfbef4692d1c69845315b2314f", "exact_match": 1.0}
{"doc_id": 114, "doc": {"question": "when does the champions league quarter finals start", "answer": ["16 March 2018", "3–4 April 2018"]}, "target": "['16 March 2018', '3–4 April 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the champions league quarter finals start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10 March"]], "filtered_resps": ["10 March"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7abb0bde5aaa1e95031671a63f2f01f8b4fea9acb60139d7798d2f3bf92d34a2", "prompt_hash": "6bbc5a9e6dc023836a8bde5e931f43697177989cfcfbb27fa4241e6659a7d825", "target_hash": "527c0a27ed0e67e74ad28c70854ad847b13817d17dd77e5cba98208b05df8399", "exact_match": 0.0}
{"doc_id": 115, "doc": {"question": "who was the famous scientist that ran the research lab moseley went to in manchester", "answer": ["Sir Ernest Rutherford"]}, "target": "['Sir Ernest Rutherford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the famous scientist that ran the research lab moseley went to in manchester?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ernest Rutherford"]], "filtered_resps": ["Ernest Rutherford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "83c2c284443ed90f5dcf5a232e000fd3b39be07be7e7ec0135bd041a398a370d", "prompt_hash": "05dbac2581bf3f6955a57d7aa494752b0702ad888e016e2ffc89d92066b9aa97", "target_hash": "aa61689443d680761dcdb3eca35cc10db46148012a6ba1c32d72e36b6e965d0a", "exact_match": 0.0}
{"doc_id": 116, "doc": {"question": "where do you get a cashiers check from", "answer": ["a bank", "bank"]}, "target": "['a bank', 'bank']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do you get a cashiers check from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" bank"]], "filtered_resps": ["bank"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d19fbf2ca9683989094bf2fbb7fd675b68cbaab222c6263c9b5fa3d335451102", "prompt_hash": "aaf225d30d98356b384669b95e3ec6121ae570193673ee166264b54447291529", "target_hash": "be25c2f1b7716799277a6a89a8e9af0847a72c66cde8a36f3973c0bd6ca6605d", "exact_match": 1.0}
{"doc_id": 117, "doc": {"question": "who does stefan marry in the vampire diaries", "answer": ["Caroline Forbes"]}, "target": "['Caroline Forbes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does stefan marry in the vampire diaries?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Elena Gilbert"]], "filtered_resps": ["Elena Gilbert"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2268acefe30ec9aeba435e4bfb7430a81f90baaf9415d3d446f39a5900433119", "prompt_hash": "89ab0412ca45edc3fddc00efa1b489abd6449f33ef734c8e2b43abbfa469860b", "target_hash": "241bbf411795cb2a75b3e60906755d9c4396f002018c7e204fb7d598637a8067", "exact_match": 0.0}
{"doc_id": 118, "doc": {"question": "what is the meaning of cc and bcc", "answer": ["Carbon copy", "Carbon copy to secondary recipients", "Blind carbon copy"]}, "target": "['Carbon copy', 'Carbon copy to secondary recipients', 'Blind carbon copy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of cc and bcc?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" carbon copy"]], "filtered_resps": ["carbon copy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6ccf0feb23d49afdcb5a0ad0e7cbd0553b48e059762b7e3c5c57c440abd60d11", "prompt_hash": "e6272283e593674813c0682bf875b322c8131cb08b00259ad2e5f15bb0f84fa5", "target_hash": "49eb4a0a5d3b8d12759365b8af8af7cf15806d828be4ba12d6e24505af844ea7", "exact_match": 1.0}
{"doc_id": 119, "doc": {"question": "hazels boyfriend in the fault in our stars", "answer": ["Augustus Waters"]}, "target": "['Augustus Waters']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: hazels boyfriend in the fault in our stars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Augustus Waters"]], "filtered_resps": ["Augustus Waters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf84f415d061a25e85b6330e249c2c02e14d0ca6a892931e1b697659c1cce438", "prompt_hash": "a87ca33415da8ccd295c8249389f805022453cb572b5095c70d5a65f6a6f1b47", "target_hash": "6ff1a28ad5c0b831443df2dc0ba4acccad2d8f321007a03eac6840eddc0cc0d3", "exact_match": 1.0}
{"doc_id": 120, "doc": {"question": "when does the day of the dead end", "answer": ["November 2"]}, "target": "['November 2']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the day of the dead end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" November 2"]], "filtered_resps": ["November 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bec80835955667478bb127dce53f2628b3815bbb1e71e65816527eff4f0e5870", "prompt_hash": "aa7d5524bfb5082035f1090af813d6d45dfd4dfe314f9e87b463fe38e59eefd3", "target_hash": "76ae82cf8954b22cd2d11cb086efee46aad382c2df413410088c66e7ac499307", "exact_match": 1.0}
{"doc_id": 121, "doc": {"question": "when do you celebrate birthday if born on feb 29", "answer": ["February 29", "February 28", "March 1"]}, "target": "['February 29', 'February 28', 'March 1']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do you celebrate birthday if born on feb 29?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 28"]], "filtered_resps": ["February 28"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dfa0178cb6bac0108cf1498f4dea7d518b85db28cc0923594b865f24e73df38c", "prompt_hash": "0be1ead794d4d1e4ad8c6f1b9da4372f8a6f40010e1d1f7a2c403455a76268fd", "target_hash": "07ee7af00a0ed15862717c70568e78ecbd6b8d6a5275bcd0bc4c3028d6b37377", "exact_match": 1.0}
{"doc_id": 122, "doc": {"question": "where does new york drinking water come from", "answer": ["the eastern Catskill Mountains"]}, "target": "['the eastern Catskill Mountains']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does new york drinking water come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Catskill Mountains"]], "filtered_resps": ["Catskill Mountains"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "23abe2af02fef67525a36ca7cb052450c0853bab7c6c42780b8265036b119d6b", "prompt_hash": "faa473a031b615f4b3b36b8ce8b0f40398f82530826f0757b88961d9f4fd3320", "target_hash": "36610987b9b70c99104b2f662d054b078ff496f4e63635506de48e9b75b7a7c7", "exact_match": 0.0}
{"doc_id": 123, "doc": {"question": "where is the citrus bowl held this year", "answer": ["Camping World Stadium"]}, "target": "['Camping World Stadium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the citrus bowl held this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Orlando"]], "filtered_resps": ["Orlando"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ca55f62c08e39f890d1b1dc572fd49ae363b1b8ca76df44c0b8f1ecf7bc015f", "prompt_hash": "94376a5d53a5081ccdb3af99d2f7d862ee81e05cf7b08e28c99a63044e415bfb", "target_hash": "b8a5094b04f7fc1c1ec784f2cc668d1078dd265d71ab51d1b911dfc1fa5ea038", "exact_match": 0.0}
{"doc_id": 124, "doc": {"question": "what year does the quiet man take place", "answer": ["the 1920s", "In the 1920s"]}, "target": "['the 1920s', 'In the 1920s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what year does the quiet man take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1912"]], "filtered_resps": ["1912"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb7bd130c40d2ca6c2f4d46498da87d3ec2f397df94f08f8e8856e4e63dc4f68", "prompt_hash": "4442cd316c790610ebd34b263e0fd892b0862362440c6d3ab157ee596a255d19", "target_hash": "f759de6e5cb6213df1bbf49462a23e1e9ec0637025fa0f846fe0b30b666dc900", "exact_match": 0.0}
{"doc_id": 125, "doc": {"question": "how many seasons of rules of engagement is there", "answer": ["7", "seven"]}, "target": "['7', 'seven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of rules of engagement is there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 4"]], "filtered_resps": ["4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "15067b1aa861a291ba9fe714e7a162ded7e800a6cdd3711fd7df746b9f233123", "prompt_hash": "d93ff23eec25910429907d95e65c9064de2ef0dc88948048f853ac54b8aefc1f", "target_hash": "47f94e48a0b9d4503db57f1a49661eb2671f9fd3c109285b1dddf1e46f90fa36", "exact_match": 0.0}
{"doc_id": 126, "doc": {"question": "who plays alec ramsay in the black stallion", "answer": ["Kelly Reno"]}, "target": "['Kelly Reno']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays alec ramsay in the black stallion?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Frankie Faison"]], "filtered_resps": ["Frankie Faison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "27733f0ab9521abc652f71eb4d3091661a6208334af8765721bfd4d7da3a985b", "prompt_hash": "041e5445436090b8e6f25d5827143b8e60b26c99cf0b3f1415d4657f3fdb260c", "target_hash": "1cad2431af1aab70cb1c5fa7f33bb898b3e3521811d459b91972fecbd6d3bc95", "exact_match": 0.0}
{"doc_id": 127, "doc": {"question": "who plays dusty in the movie pure country", "answer": ["George Strait"]}, "target": "['George Strait']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays dusty in the movie pure country?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dustin Diamond"]], "filtered_resps": ["Dustin Diamond"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6bca9758b550dfa78f048c2267b0b8bb46948f7feb2fa6a599e2657a43380c70", "prompt_hash": "f9fa5a10e8b1874d6a08da7cad3bdc268c86b63c822ee4e9f48bb63d9cae04d0", "target_hash": "910d4fd970445c86fa3296f88f4728cc5484119a1ee2ff778182fd2659c7078f", "exact_match": 0.0}
{"doc_id": 128, "doc": {"question": "how many episodes of greys anatomy season 14", "answer": ["24", "24 episodes", "21"]}, "target": "['24', '24 episodes', '21']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes of greys anatomy season 14?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 24"]], "filtered_resps": ["24"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "394374a582fde680fc8ba70f5d6b2e5fcca0f632aee9468895694a929c9d233a", "prompt_hash": "5dad21fb9319e4beca67ad4a1524e603a77375a42cb9565c2827b05cb6e4990a", "target_hash": "ce80f32ef97ec25e7ba23a3ba9f4057e157c767875b6a87c8a00faa169f71eb3", "exact_match": 1.0}
{"doc_id": 129, "doc": {"question": "who is currently serving as president of the senate", "answer": ["Orrin Hatch", "Patrick Leahy"]}, "target": "['Orrin Hatch', 'Patrick Leahy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is currently serving as president of the senate?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John C"]], "filtered_resps": ["John C"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d6d0a06142d5ee5aeb7146164dbb8b197c09624a2a66f52fc8b7bae5f6c4d50e", "prompt_hash": "6569b9a2be8ee25da86f06cf0c433e0ab3c69757c4a395e238f811d71f635350", "target_hash": "4e39a695acef466f8e5e612b574a0fcaf3590b30337114079373021ef36a8cc2", "exact_match": 0.0}
{"doc_id": 130, "doc": {"question": "what percentage of sunlight is captured by plants to convert it into food energy", "answer": ["45%", "3 to 6%"]}, "target": "['45%', '3 to 6%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what percentage of sunlight is captured by plants to convert it into food energy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1%"]], "filtered_resps": ["1%"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "efc3d5b6df584a86d44d035d17291c2acb99f2c9a5a8646fa2de768533500023", "prompt_hash": "23e9f281595b93c4e83b0ce6f85ecedf11ccd9da771b596918708682a6ea8945", "target_hash": "1045fa2600e484cb093f5062be36b916e2d0df820dcaa78aad2b8f629a96450c", "exact_match": 0.0}
{"doc_id": 131, "doc": {"question": "what type of economic system was utilized in the soviet union", "answer": ["communism", "state ownership"]}, "target": "['communism', 'state ownership']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of economic system was utilized in the soviet union?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" command"]], "filtered_resps": ["command"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb1dcf3f5edaec5d94c08987fdbad4efbd8feb21b138c20b6285c30528272735", "prompt_hash": "e03653e6e59eab351db9ac0d8deff887b50b35bc44d374ad28476a424af0a547", "target_hash": "caf890fcbf83cc399700901d9a1807e7f1163627b751a171ea12aa8166a51d06", "exact_match": 0.0}
{"doc_id": 132, "doc": {"question": "how many gold medals did australia win in the 2000 olympics", "answer": ["16"]}, "target": "['16']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many gold medals did australia win in the 2000 olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cd79d359f38acd9f45db8c625efe49cc450e4062420345b6412d817a61d9339e", "prompt_hash": "b9f2f9a52613baf9f1942967a6bd5e0f438cd2488d08c5b0d93d0b8b90312b40", "target_hash": "424a22f5ba0f390d3484c7c87755c38fcebdb32337c3133a2723e6793b02f56d", "exact_match": 0.0}
{"doc_id": 133, "doc": {"question": "who started ww2 and how did it start", "answer": ["Nazi Germany"]}, "target": "['Nazi Germany']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who started ww2 and how did it start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Germany"]], "filtered_resps": ["Germany"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "07ff2bce4a68ca13cd367d0b15fc4ec5dfd244228c8ba6a2daf57b6381e57239", "prompt_hash": "f6ab834f9463631bd440d56697f53ab331a961e1b6b9461877b1d498c2e5838a", "target_hash": "dcc0ff350358051260a87dc5a162db954d0b037fdfaed9e80716d4c76b8f93aa", "exact_match": 0.0}
{"doc_id": 134, "doc": {"question": "when did the royal mint move to wales", "answer": ["1968", "the 1960s.", "17 December 1968", "the 1960s"]}, "target": "['1968', 'the 1960s.', '17 December 1968', 'the 1960s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the royal mint move to wales?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1968"]], "filtered_resps": ["1968"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb4d55fbe6c16be8caedea6e937bac91486c70d6a688c4b2735602e2423f0283", "prompt_hash": "8a5e945c573c83b0db1ebb075dc99796fd3d21f5ad46605e1cd7d688c1673521", "target_hash": "8eee97157342700e33997c0b097f7c50e51274ccfd48fdd972160bec2f879534", "exact_match": 1.0}
{"doc_id": 135, "doc": {"question": "when did bird is the word come out", "answer": ["1963"]}, "target": "['1963']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did bird is the word come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2013"]], "filtered_resps": ["2013"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dc3f453f25e66cb14bd231d5cbcb69b79afa2e3c914cc9928edc0d0ad6aabfd4", "prompt_hash": "957b6f5421999711d698e112e733221324ec35047348c038d663d2d6dd205a01", "target_hash": "1399b92614d21778f741baeb8120cec306d000e15745fe93c81f856309b362dc", "exact_match": 0.0}
{"doc_id": 136, "doc": {"question": "how tall is the actor who plays hagrid in harry potter", "answer": ["6ft 1in"]}, "target": "['6ft 1in']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how tall is the actor who plays hagrid in harry potter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 7 ft 2 in"]], "filtered_resps": ["7 ft 2 in"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d0ce67ff685b9570a5ba0ff83ffa04f6bf67bc46c6fdfaa4791c1c37fe316ba8", "prompt_hash": "86ba656004c3cae5dfbad3ee884322dcaecde3924220d8ce9bbc4a4b5a6aef99", "target_hash": "20440b27e3cd3029077a4908f96cd01a84a668dcab77d369f35828f8a79749dd", "exact_match": 0.0}
{"doc_id": 137, "doc": {"question": "who is opening for little mix glory days tour", "answer": ["Sheppard", "Zoe Badwi", "Louisa Johnson", "Bronnie", "The Vamps", "Ella Eyre", "Conor Maynard"]}, "target": "['Sheppard', 'Zoe Badwi', 'Louisa Johnson', 'Bronnie', 'The Vamps', 'Ella Eyre', 'Conor Maynard']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is opening for little mix glory days tour?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ariana Grande"]], "filtered_resps": ["Ariana Grande"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "43e5a0accf5d9fe687ab0f1807f50b6c6e038740587fb5e5aed9df82cfb99554", "prompt_hash": "bbf1a158218e813eaa91f884bfe5ed7f0422f9d24e6688f8a3867f593dab1a39", "target_hash": "6607595723a18e4097e5577039ba784f0972696ba2872f603a685bfafe7b3132", "exact_match": 0.0}
{"doc_id": 138, "doc": {"question": "who performed the first c section in 1794", "answer": ["Dr. Jesse Bennett"]}, "target": "['Dr. Jesse Bennett']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who performed the first c section in 1794?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Barry"]], "filtered_resps": ["James Barry"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "667030250991679cb35de52631d5f6cc6bb3907257899f837bc329d42f751a62", "prompt_hash": "9085150347b481bb47fc6018732368dc40c675df98381612f1e67390bb43f54f", "target_hash": "c4cee33c7dacba9fb120ee89951bb6a6621ccd27321834cc80af23d161d3be35", "exact_match": 0.0}
{"doc_id": 139, "doc": {"question": "who voices randy in f is for family", "answer": ["T.J. Miller"]}, "target": "['T.J. Miller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who voices randy in f is for family?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Ratzenberger"]], "filtered_resps": ["John Ratzenberger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6501725ad0e40b48fb306ed724d2fd3ff66d351c731c59556b67669fcf2d1c79", "prompt_hash": "4761dd157f179b3ba83cd5c662195f7f09b602d2a6af0f446355f0a1fe9bb811", "target_hash": "b357d52214ed96f69e6de63c3189369c1628345fb3474d98595159333a6d6afe", "exact_match": 0.0}
{"doc_id": 140, "doc": {"question": "bible verse taking the lord's name in vain", "answer": ["Exodus 20:7"]}, "target": "['Exodus 20:7']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: bible verse taking the lord's name in vain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Exodus 20:7"]], "filtered_resps": ["Exodus 20:7"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0a9a28775003b88585ff2fb12890ffc12d3ac106fe12605e36e5bc42f9659b41", "prompt_hash": "687ff771338b9b5f0514febb1ed8d9fee920f234e31c493b53bb1773a12dc2e4", "target_hash": "001875536651c1e2cb985596183df3d6b4e71286a7c12554b459cd9d21610a1c", "exact_match": 1.0}
{"doc_id": 141, "doc": {"question": "who controlled the house and the senate in 2012", "answer": ["Republican", "Democratic"]}, "target": "['Republican', 'Democratic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who controlled the house and the senate in 2012?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Republicans"]], "filtered_resps": ["Republicans"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0cc2a3ec623738719b1dbf20cfca1348b1a594216fb7cc4ec498b4c6f63ddb12", "prompt_hash": "b4fe331194dc5c8f98696cb0273843979a9099a158f726c161f79dc6e9f71586", "target_hash": "d26a8fe40f0308f3e9af31dd315e2eeb1bbece7a5d0b415de8cd2645753d31fc", "exact_match": 0.0}
{"doc_id": 142, "doc": {"question": "when's the last time army won the army navy game", "answer": ["2017", "Army"]}, "target": "['2017', 'Army']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when's the last time army won the army navy game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1947"]], "filtered_resps": ["1947"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91c9917eb12cf7e89d08a1f6fc2cc13760ae5a8f52763144d565a595ffee727c", "prompt_hash": "8fe14389442b8db4f70ef41ba24c75fe93bd13e05cc8aa225aa826324e6293b4", "target_hash": "a896608f2642937bf0860b3405fa4d2bbb33d65fa1de2817ad0ad20f3bef0c6d", "exact_match": 0.0}
{"doc_id": 143, "doc": {"question": "when does thomas rhett's new album come out", "answer": ["September 8, 2017"]}, "target": "['September 8, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does thomas rhett's new album come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" September 21"]], "filtered_resps": ["September 21"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0ca95df861445125e3e10694bfd92592860ab9305f61828629741117cece7a7c", "prompt_hash": "bfe4266103a17762690118f34e8302a749eaeff138b206ad9db89d2baa11957e", "target_hash": "e15aedcbf9ccbf3601114a41817dd8e86340ac20de80213bdd060b296d8b35da", "exact_match": 0.0}
{"doc_id": 144, "doc": {"question": "who won the champions league final in 2016", "answer": ["Real Madrid"]}, "target": "['Real Madrid']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the champions league final in 2016?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Real Madrid"]], "filtered_resps": ["Real Madrid"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d735d774bc73ae6b9a68e9f7dc723eff82b24a3098db0c002ccc1b8f49efb14e", "prompt_hash": "eabb2a901719c37f3be3cdd3c0f7cd3f95b7ef9ca9b6afac4c110097a3620e59", "target_hash": "a1f9675d65afe7eef9e2d095ee7541ff6c66883ef6d8f4aedc1bd3a0943b4418", "exact_match": 1.0}
{"doc_id": 145, "doc": {"question": "who is the real killer in basic instinct 2", "answer": ["Catherine Tramell"]}, "target": "['Catherine Tramell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the real killer in basic instinct 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nick Nightingale"]], "filtered_resps": ["Nick Nightingale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "740fb4d0c8b88ab4770bb21a15777ab517ca8b71cc841cc92af03d0427acf02b", "prompt_hash": "f6e030dc204cc19fa1b2dd9e8439011c804c4dbe5e057dc41090060bd0d94f17", "target_hash": "83a581eb642d68712521003dc72748a303810da7b34b31be47611582f0e53274", "exact_match": 0.0}
{"doc_id": 146, "doc": {"question": "who does the voice of mickey mouse on mickey mouse clubhouse", "answer": ["Bret Iwan", "Wayne Allwine"]}, "target": "['Bret Iwan', 'Wayne Allwine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of mickey mouse on mickey mouse clubhouse?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bret Iwan"]], "filtered_resps": ["Bret Iwan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8d3bb465342b0706cd69fab7b3eb6face9e8fbf1bced796089c868d41ee5561", "prompt_hash": "8c449f81ed2c507d4588984a956cfe7e2540ff50dce241b4b3c32473bb6de8d8", "target_hash": "1c81f50b2b39220981f693f0d41cc1a05617def1913701159e86ae74e84620a9", "exact_match": 1.0}
{"doc_id": 147, "doc": {"question": "when was the last episode of vampire diaries aired", "answer": ["March 10, 2017", "March 10, 2017"]}, "target": "['March\\xa010,\\xa02017', 'March 10, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last episode of vampire diaries aired?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" May 18"]], "filtered_resps": ["May 18"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b92aecf506fd76ab6dcb88b818c893509bcec7fab5462d42661c4c090582a6db", "prompt_hash": "908a5eda7d0dfaa9ec9dd7dedb6a7dc8b4561be6e3f7d6509dbd2266fe15daef", "target_hash": "92663fe1a2ab9c69d5833afd5466a5e5b1cc4b77283523e505b7f9254c4fdfeb", "exact_match": 0.0}
{"doc_id": 148, "doc": {"question": "where did the dewey decimal system come from", "answer": ["Melvil Dewey"]}, "target": "['Melvil Dewey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the dewey decimal system come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Melvil Dewey"]], "filtered_resps": ["Melvil Dewey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fce2d554be54d30c99ad3ccf6ea98429ea6b4e75143a4250a17402e69fffc5ff", "prompt_hash": "0f3bb7397f062dea1e4a27ba5c69736aec94f79ccc9ffa40d312a21b3c22fa5d", "target_hash": "02e9e6e274acabe1d865fc530386c40302c0bb68db559d2663a63fb4ecbdf78b", "exact_match": 1.0}
{"doc_id": 149, "doc": {"question": "where did the jungle book story come from", "answer": ["English author Rudyard Kipling"]}, "target": "['English author Rudyard Kipling']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the jungle book story come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rudyard Kipling"]], "filtered_resps": ["Rudyard Kipling"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "585634dccff34cb11ec9f725b9ad8deb730d0b508bebbdc96fbf677a6e35890f", "prompt_hash": "e4527db39374d91f029b9a160270c5c603f1c15a66f4a8f1fac45efd636dffd0", "target_hash": "47e81c320e4a8958bae00c5c309389ba7f53aea2a271887d7c20119ea7aac990", "exact_match": 0.0}
{"doc_id": 150, "doc": {"question": "derek and meredith get back together season 3", "answer": ["Staring at the Sun"]}, "target": "['Staring at the Sun']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: derek and meredith get back together season 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11"]], "filtered_resps": ["11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "469fa06b46deb3d8c1a163a01ed68a05cfa9464bb347f2f5f9dd53ed00566688", "prompt_hash": "93b64bb94d49c65a0ca7b72d73128d25febbd71c0468f5b6819a5ec9e2c428c7", "target_hash": "d81c43316b495a35c3c7e6709cd15b49ed52d17b999b14a00b4648a1b00546fb", "exact_match": 0.0}
{"doc_id": 151, "doc": {"question": "who did america declare war on in ww1", "answer": ["Austria-Hungary"]}, "target": "['Austria-Hungary']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did america declare war on in ww1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Germany"]], "filtered_resps": ["Germany"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c1239f81d80f0a2df9cc3ac75ec01eca8e0d4bb68f595b76eb21fd60f555847d", "prompt_hash": "3ad2ea55d2c280c120c40c7d6ff24f03e3d1aea6b6b5ee90eb6d8eccc91def27", "target_hash": "b6e06d92903ae87dc17a71c1a62eff7c30a2d7a32728167111fdaf7f4b0e7c2a", "exact_match": 0.0}
{"doc_id": 152, "doc": {"question": "who played zoe hart on hart of dixie", "answer": ["Rachel Sarah Bilson"]}, "target": "['Rachel Sarah Bilson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played zoe hart on hart of dixie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kristen Dalton"]], "filtered_resps": ["Kristen Dalton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1b5c20c8e2ceb5f481d379aed35d5b533e9a41ed719b8a8d144efb1d1188311d", "prompt_hash": "eb68be566101da972d16e162be8fb63db976903b5d4b2ad71e4a5a4842cc9953", "target_hash": "d104f4c9280ac8a1f20c2db29cf6abe3f47a1ac577f245c5a7bd910ac9e496f7", "exact_match": 0.0}
{"doc_id": 153, "doc": {"question": "what was tom hanks character name in castaway", "answer": ["Chuck Noland"]}, "target": "['Chuck Noland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was tom hanks character name in castaway?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chuck Noland"]], "filtered_resps": ["Chuck Noland"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c6d32133eb25bf8067dbcb749b6404caa38b0281887ad8ab52e41fb546bf6aa4", "prompt_hash": "ea47ed48c01a7aba85592e4b2515456ffd10866daa192b641fd8c96a4058501f", "target_hash": "7b0f2e671a205c68c484c77dec7297096ddab20ed8d21132add2acd542316f81", "exact_match": 1.0}
{"doc_id": 154, "doc": {"question": "what engine is in a holden v8 supercar", "answer": ["V8-engine"]}, "target": "['V8-engine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what engine is in a holden v8 supercar?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" LT4"]], "filtered_resps": ["LT4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9cfc6ed9530e58ba4a71c1fc759b491676739d8933421c1825a046f712f49e9d", "prompt_hash": "1b709e95ae508d698ff3312b95e71261f5f46721bd7d765d7f4e3742cac703d9", "target_hash": "c6665dc44d930b62c4b978b05d67aa4094ddafb55eaeb76b028546d862163f1a", "exact_match": 0.0}
{"doc_id": 155, "doc": {"question": "when did the sat become out of 1600", "answer": ["March 2016", "2014", "2016"]}, "target": "['March 2016', '2014', '2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the sat become out of 1600?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2004"]], "filtered_resps": ["2004"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9e38520e6ce391d3be6fbbb4a007f2b0263454b138dfc155267e4e4bb6571656", "prompt_hash": "5ba9602d51f547285aa8c0b9dd410d9ae27229be239ef675243ee9e41fb828ac", "target_hash": "423a1cb70a3d745996c8a1b6c267ed162441933bf85b2ef1e7f49b4d4e9520a7", "exact_match": 0.0}
{"doc_id": 156, "doc": {"question": "when was the minimum wage established in the united states", "answer": ["1938", "1933", "1912"]}, "target": "['1938', '1933', '1912']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the minimum wage established in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1938"]], "filtered_resps": ["1938"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aeb35e611e2ddcad45b2456d5fea9fbde093a219c7c177e027048a8296dae6c8", "prompt_hash": "2441424cd180abb297e3da3685e9773665b4054b8928cc33557213313fbfbee0", "target_hash": "dfc6f5c4d86afe21cf9a09c61a4190144c852f96ac0ea2bfe0d6e166c3e9e69f", "exact_match": 1.0}
{"doc_id": 157, "doc": {"question": "when did the rational dress society begin to work", "answer": ["1881"]}, "target": "['1881']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the rational dress society begin to work?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1881"]], "filtered_resps": ["1881"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "10aa62aaea4ede519a9e135cb413100f5d1aa3c183ce16ba1953f7c89ff15897", "prompt_hash": "4c6ee1e3063a325c1463aaecdcc4dc39e43c58f31607c19ea2db1ba60654ee1c", "target_hash": "7aa0cae52edf7609b3e6f3ab14f359f6f0027f96bb88da2840ec369cec3a1c2b", "exact_match": 1.0}
{"doc_id": 158, "doc": {"question": "what is the baby elephants name in jungle book", "answer": ["Hathi Jr."]}, "target": "['Hathi Jr.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the baby elephants name in jungle book?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mowgli"]], "filtered_resps": ["Mowgli"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea5892e8dca862374af0396bcdf1901674ece9071177a2ccf0f4b4e22051628a", "prompt_hash": "4795cfcc84b536a73c9c4ac8fe6ad4b9772368fb37860e06dfa662c18542fcde", "target_hash": "4135cda0a9753ded17ee7ba485ccd7661d02cc654ef602afdbf80aa2a817240a", "exact_match": 0.0}
{"doc_id": 159, "doc": {"question": "where is fe best absorbed in the body", "answer": ["in the duodenum", "the duodenum"]}, "target": "['in the duodenum', 'the duodenum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is fe best absorbed in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" stomach"]], "filtered_resps": ["stomach"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c0d52924d1b208e73191083a5095f0ebc37081c07eedc6624dbd9f6d748645b", "prompt_hash": "fd8220697f83093e7753df29d14897b645a33235ae4531a1826721ec6ab22589", "target_hash": "b348ba524c890e6f0ba0a1fe664b1d61c74751e0d9feed84078103b66aea099f", "exact_match": 0.0}
{"doc_id": 160, "doc": {"question": "who is the biggest selling female group of all time", "answer": ["Spice Girls"]}, "target": "['Spice Girls']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the biggest selling female group of all time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Supremes"]], "filtered_resps": ["The Supremes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "da2889372ed5a4047c32b4686fc105c3948a47b7cbf9f1979ab4d8766d4cd7f0", "prompt_hash": "bc885888536d03ff2bee0bb7c3d2b6429cd87d5842459a7188ae623dd0490dcd", "target_hash": "95709da20f4f239f22fef4484a0c5682797f66ee603d9542400a6987cded874b", "exact_match": 0.0}
{"doc_id": 161, "doc": {"question": "epidemiologists attempt to explain the link between health and variables such as", "answer": ["biological agents", "disease conditions in defined populations", "smoking", "stress", "chemicals", "alcohol"]}, "target": "['biological agents', 'disease conditions in defined populations', 'smoking', 'stress', 'chemicals', 'alcohol']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: epidemiologists attempt to explain the link between health and variables such as?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" environment"]], "filtered_resps": ["environment"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a8304178333c2e632eeaf1759b11d08b77ebc88c9a5ddf3a2b97f7c335e0ee8", "prompt_hash": "42aa92cde8ed19da4a39fced1caf0b18d36d3d698bd1a380fedf68c77c3dc2fd", "target_hash": "49bac0b0d748795a8854dd5872f0e174f6b1012dd5ad760962591b754e6e9df5", "exact_match": 0.0}
{"doc_id": 162, "doc": {"question": "who won the mens single ice skating 2018", "answer": ["Yuzuru Hanyu", "Javier Fernández", "Shoma Uno"]}, "target": "['Yuzuru Hanyu', 'Javier Fernández', 'Shoma Uno']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the mens single ice skating 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yuzuru Hanyu"]], "filtered_resps": ["Yuzuru Hanyu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b618b7c059a952ba30e6eceae3d84a30bde5738e807791f98e321a547fe42dd3", "prompt_hash": "04867e73f3aec37e4c3ecd8433befde1c0ddf5c944cdb1019f63a3f3b76fff62", "target_hash": "e8726f23b8265ee586fabc7e51cac6c372fbd8f1e8a79e0d98332df023187293", "exact_match": 1.0}
{"doc_id": 163, "doc": {"question": "where is the boy who played charlie in willy wonka", "answer": ["Peter Gardner[4] Ostrum", "Lowville, New York"]}, "target": "['Peter Gardner[4] Ostrum', 'Lowville, New York']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the boy who played charlie in willy wonka?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" England"]], "filtered_resps": ["England"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06590125c8e88362acdbe7d38d1892d3becbd13678208937d9dbf7aaff30f000", "prompt_hash": "6883c069fa62d474eaa167e53c7d100eec7c5dc3a8079ef6ba550b3aa283b820", "target_hash": "8c02368186ac32d61e14e74615e7d108249ecb121290da814d552924a9317327", "exact_match": 0.0}
{"doc_id": 164, "doc": {"question": "where was the first session of the assam association held in1905", "answer": ["Guwahati"]}, "target": "['Guwahati']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the first session of the assam association held in1905?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shillong"]], "filtered_resps": ["Shillong"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "246bfa9e2999f587f3c275943a48657f831421cdc68794cf15bd1979e76b14e6", "prompt_hash": "11963c8c29c629e47228d4579ddccca3cb516a1db183a626e59e1caf269c55c0", "target_hash": "e09bb1b35ae73509c8f6c5e9b10b0084ddc7f34718243b93ddd612e3a8dd0bc6", "exact_match": 0.0}
{"doc_id": 165, "doc": {"question": "who played ice queen in chronicles of narnia", "answer": ["Tilda Swinton", "Laura Brent"]}, "target": "['Tilda Swinton', 'Laura Brent']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played ice queen in chronicles of narnia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tilda Swinton"]], "filtered_resps": ["Tilda Swinton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2e8f4fea8341be91fd0084e82ad214e26108e12ad993b4d2c8994d29a5922bb4", "prompt_hash": "1bd0ed1d5c99b4722b7850b9925cab8ba2ab0adcbd5a88ec43fbb6d90b52a2a7", "target_hash": "985f8eed28e7aba56becac322a960b925cb910b4276aff614031c8ec16b1496f", "exact_match": 1.0}
{"doc_id": 166, "doc": {"question": "when is season 2 of jessica jones being released", "answer": ["March 8, 2018"]}, "target": "['March 8, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is season 2 of jessica jones being released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2016"]], "filtered_resps": ["2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "01b244223107e879e2b17bb7ad1b078dff68cfba9bd45f9675146bc107ee7560", "prompt_hash": "edb5b8fd13c484adb3f1f6c2cfe2c72a526ae8c036f8d404fb955baa0c91c1a4", "target_hash": "6c73b45caef50c343c8defcabb7a228445c49d1cfe48245cc4a5755d447d5f1a", "exact_match": 0.0}
{"doc_id": 167, "doc": {"question": "when was united nations convention on the rights of the child created", "answer": ["20 November 1989"]}, "target": "['20 November 1989']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was united nations convention on the rights of the child created?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1989"]], "filtered_resps": ["1989"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "69c4e07cece759d9c4402ac6ddd4cbc315936749dd6f6fcc3cdf61da1c110f07", "prompt_hash": "6a40968206f3d908c103518c237fd7b23d8d32b8fe5e1077957148a8117b4a88", "target_hash": "ceb67397e57dbb40aef559bfd47ac58bd4b0c96a3c8779be8426d3195fe0280a", "exact_match": 0.0}
{"doc_id": 168, "doc": {"question": "swan lake the sleeping beauty and the nutcracker are three famous ballets by", "answer": ["Pyotr Ilyich Tchaikovsky"]}, "target": "['Pyotr Ilyich Tchaikovsky']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: swan lake the sleeping beauty and the nutcracker are three famous ballets by?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pyotr Ilyich Tchaikovsky"]], "filtered_resps": ["Pyotr Ilyich Tchaikovsky"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "515a3ece3481538d94ca5ec560bb58be382ed155326fdbaec1aebf99596666b3", "prompt_hash": "fe88785060f5012815d22ae7dfbf234f817b45436066910c511d961e9ff5d1ee", "target_hash": "201b42cc29f2e84beb7a4d89ab2506a7ad48b6d6e9649473ff57c628786873d6", "exact_match": 1.0}
{"doc_id": 169, "doc": {"question": "what type of database is library literature and information science", "answer": ["bibliographic database", "bibliographic"]}, "target": "['bibliographic database', 'bibliographic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of database is library literature and information science?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Library Literature and Information Science"]], "filtered_resps": ["Library Literature and Information Science"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfb48525b30b4d862ee1beae363389f416a66ad06aab3f1aa972c66c73494dac", "prompt_hash": "9163a536cc4b3749885b116284b59823eaea5fabbc8cef810b6001b72fccdfd3", "target_hash": "5af01fc26f54f641fbd339ba29730477dced6ce3617b1541c565b35583292f6d", "exact_match": 0.0}
{"doc_id": 170, "doc": {"question": "when did the united states start using the death penalty", "answer": ["the beginning", "1608"]}, "target": "['the beginning', '1608']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the united states start using the death penalty?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1608"]], "filtered_resps": ["1608"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fcb86204adeb37bfec1580191642f5e1d46ebfc80c0d86d221f9ce750cf23d3b", "prompt_hash": "83e4f4f64c985b2e34500a680502476fd000f56a3684b214582dbaee3c3274d7", "target_hash": "0a17ac198ab47757c0f11c812c140b914e44e8ec55824ad56a94e115df0badd0", "exact_match": 1.0}
{"doc_id": 171, "doc": {"question": "when was coffee first made into a drink", "answer": ["15th century", "the 15th century"]}, "target": "['15th century', 'the 15th century']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was coffee first made into a drink?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 15th century"]], "filtered_resps": ["15th century"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0f20a2e3a31a9d1ab5f7ca65ae540fbe97c6b7db23f303332b81813d41369430", "prompt_hash": "e39746e4d596420c88b3710e7d145f2feb9a544fc8d24a4201a60178aae87266", "target_hash": "073bbf457f6060f91a8edc61f50ea4051083830c0f1c65effc40add849c39aa8", "exact_match": 1.0}
{"doc_id": 172, "doc": {"question": "who is jared on the bold and the beautiful", "answer": ["Andrew Collins"]}, "target": "['Andrew Collins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is jared on the bold and the beautiful?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Scott William Brady"]], "filtered_resps": ["Scott William Brady"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0cdeeb36b38c15ba59ae01040ab754bc37e2c9f9e2e427a984ed05421953d447", "prompt_hash": "89a85ffe92d3829d9586d05a31ce72f5d3a805829d43ea5e34ea9e559fd0aab0", "target_hash": "f2f138d34302e0ad6e38dd12570a6e6c339776541a66104de7d8eaa028dd13ca", "exact_match": 0.0}
{"doc_id": 173, "doc": {"question": "who wrote knock knock knocking on heavens door", "answer": ["Bob Dylan"]}, "target": "['Bob Dylan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote knock knock knocking on heavens door?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bob Dylan"]], "filtered_resps": ["Bob Dylan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f30d69bb0185f9d293e9ad99dc20222e348141c9ff9c37e7a4655fd61b3c9174", "prompt_hash": "244f6f2f7ee765784049860d65dce5ef97adbe7b044acf938500c83e3287cf8a", "target_hash": "183d2854d32090092acfd7e78bc5499b9b3fb04b27e2963a1269b70bf1812be7", "exact_match": 1.0}
{"doc_id": 174, "doc": {"question": "when did the movie napoleon dynamite come out", "answer": ["June 11, 2004", "2004"]}, "target": "['June\\xa011,\\xa02004', '2004']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the movie napoleon dynamite come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2004"]], "filtered_resps": ["2004"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b6882b657438ddadafc04716a78ac9ea63eee1795ef3dc828ca980c010be8024", "prompt_hash": "bce5a519ff91174035ff1a2cb815286102ef532e16ecaf2e3dabc2a0bf4c9206", "target_hash": "d91231bdac51277b41eca50578c13a2909ec7da9826a39b3e789d76c258389f1", "exact_match": 1.0}
{"doc_id": 175, "doc": {"question": "who starred in an officer and a gentleman", "answer": ["Richard Gere", "David Keith", "Louis Gossett Jr.", "Debra Winger", "Phillip J. Salmon"]}, "target": "['Richard Gere', 'David Keith', 'Louis Gossett Jr.', 'Debra Winger', 'Phillip J. Salmon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in an officer and a gentleman?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Patrick Swayze"]], "filtered_resps": ["Patrick Swayze"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55e80dcf05ecf445fe2a729114d16d2848dafa1e8d9e5d71bc9d89b2c1253794", "prompt_hash": "fad2bfca90aa2f39e8d2b007fce3611d50765865a0a012e711fb85357a8a146d", "target_hash": "db49666e4a1570075934b1a6c7c19dc189d507a629e0789f617a73c479bff7cd", "exact_match": 0.0}
{"doc_id": 176, "doc": {"question": "who plays chummy's mother in call the midwife", "answer": ["Cheryl Campbell"]}, "target": "['Cheryl Campbell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays chummy's mother in call the midwife?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sheridan Smith"]], "filtered_resps": ["Sheridan Smith"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b898405f74ef4ca9a88402d917a0859e000eeb5e5e43d71a803a8947ed3b38e", "prompt_hash": "87800b5cc3c4531231526e225a9fd1114430b4b4dd2aa579b2fabd1e5d22bdac", "target_hash": "e4771434cd3b9b27b85be202496823fa1a172ee9cc9041641b98cdb0ac566d35", "exact_match": 0.0}
{"doc_id": 177, "doc": {"question": "name two fibres which are made of proteins", "answer": ["feathers", "hair", "wool", "fur", "silk"]}, "target": "['feathers', 'hair', 'wool', 'fur', 'silk']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name two fibres which are made of proteins?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" wool"]], "filtered_resps": ["wool"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a0b75efd652253ea1acada96934dba86ad6846bf42300cfc9ce3f45d1faabbb6", "prompt_hash": "5e932cf855eac6754ba655b2091e607fafc050d35a6556536c0e017022575a68", "target_hash": "b783dae0f3e5e456d0c611c9c4d66818593f6f4e7a79992d3231238b9015ed17", "exact_match": 1.0}
{"doc_id": 178, "doc": {"question": "how many super bowl games has the patriots played in", "answer": ["10", "ten", "ten times"]}, "target": "['10', 'ten', 'ten times']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many super bowl games has the patriots played in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 5"]], "filtered_resps": ["5"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3276763e8b04e26fbbb9ec591c6f37d5658a97f70fd4bfc39cb20c6a69e6ddae", "prompt_hash": "e6f04c44a6f251c5302078087f26a84a85eacdc2e6d71ea78c452dcca606d082", "target_hash": "beb7c6548e7a6ba16673ae3758e7e98699d19c9a1fa25c60443d89b9ac7bd3b1", "exact_match": 0.0}
{"doc_id": 179, "doc": {"question": "where do they put the tomb vampires in order to burn them during founders day", "answer": ["the Gilbert building"]}, "target": "['the Gilbert building']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do they put the tomb vampires in order to burn them during founders day?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the pyre"]], "filtered_resps": ["the pyre"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb0ab81f1ad2fc7e59e43d4d38aca10e004037cb297db56b22a5cd2bcb01c31a", "prompt_hash": "feea89983630d9a84fce1c1714a8d9c71b747a5926bdcb5e216ed050bf40d47d", "target_hash": "27f1442c9552e2a5205844372cd5733db46ca9e707732ffa37cad2b4ee9d7780", "exact_match": 0.0}
{"doc_id": 180, "doc": {"question": "who is the king and queen of the netherlands", "answer": ["Queen Máxima of the Netherlands", "King Willem-Alexander"]}, "target": "['Queen Máxima of the Netherlands', 'King Willem-Alexander']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the king and queen of the netherlands?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Willem-Alexander"]], "filtered_resps": ["Willem-Alexander"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e49e3f4c5b0594525cc4807fc07205ef1a5d3f06105c50faee77e31fc2de462", "prompt_hash": "5196083e89ea4e826a25906292c5cca2ec32496db035a133e5d3af5cf4903f83", "target_hash": "dae0b3442bca6df734687a8a39b39937a4f86b4c03d8321ec82e42c0b6352a8e", "exact_match": 0.0}
{"doc_id": 181, "doc": {"question": "where is gall bladder situated in human body", "answer": ["beneath the liver"]}, "target": "['beneath the liver']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is gall bladder situated in human body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" abdomen"]], "filtered_resps": ["abdomen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c06e54c15f182113e4e58f797e990ec0b10cfde5fdf0be7a49bf8db24eccfba4", "prompt_hash": "cd2717850a9ff6da013ff1cdbf544212d4d359d041881ab002931eef4e011762", "target_hash": "cc070dbc1968922a396e224edc59bd5406be340dba23a798add23d945978d822", "exact_match": 0.0}
{"doc_id": 182, "doc": {"question": "what is the current population of bora bora", "answer": ["10,605"]}, "target": "['10,605']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the current population of bora bora?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 18"]], "filtered_resps": ["18"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "82555ec948a28e25c89e20d1a2b4b3735be13dcecfd039c21dae0fe0b8834c8a", "prompt_hash": "6b273599db6de5cf9171f16bb8ab6c7e4f727e80c1104f24d4fd309f92e860fd", "target_hash": "cafc7655d42ab0b8085f137ec8cc75fc0c1b99eb4bb47f41bebf8b27e899d7f7", "exact_match": 0.0}
{"doc_id": 183, "doc": {"question": "where does tropic of cancer pass in india", "answer": ["Chhattisgarh", "West Bengal", "Gujarat", "State of Tripura", "Jharkhand", "State of Mizoram", "Madhya Pradesh", "Rajasthan"]}, "target": "['Chhattisgarh', 'West Bengal', 'Gujarat', 'State of Tripura', 'Jharkhand', 'State of Mizoram', 'Madhya Pradesh', 'Rajasthan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does tropic of cancer pass in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gujarat"]], "filtered_resps": ["Gujarat"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4393cbde612efb8b877e2b6d1702648d7d557ab9e630d6a12d2ae71e4df0fd20", "prompt_hash": "200870a8aef899607c74e6d813c34be78f0b7a5a58b643f65e657114c9c02e7f", "target_hash": "caf5421ed06fcbd383fcd2b6b2cd13725ef1e76f25380a69261c60f504e8ef33", "exact_match": 1.0}
{"doc_id": 184, "doc": {"question": "who made the song falling in love with you", "answer": ["Hugo Peretti", "George David Weiss", "Elvis Presley", "Luigi Creatore"]}, "target": "['Hugo Peretti', 'George David Weiss', 'Elvis Presley', 'Luigi Creatore']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made the song falling in love with you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kenny Rogers"]], "filtered_resps": ["Kenny Rogers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad380aba0f9221a643a21e1ff26cc9c43d4c78269252160b0b5699bf144f48d4", "prompt_hash": "ad018ccd9fbb8d8e8e83a34f6db84abc86bc8cd3ca7507ff88d4d171b648198b", "target_hash": "ba4a9a2b508b969545d047170e425a346db85335e241aa8c48bc0d5d6e5e08c4", "exact_match": 0.0}
{"doc_id": 185, "doc": {"question": "which is produced in plants of narora kakrapar tarapur", "answer": ["Atomic Power"]}, "target": "['Atomic Power']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is produced in plants of narora kakrapar tarapur?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" uranium"]], "filtered_resps": ["uranium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ca11a6267640f6d3d3f8970ea245b23d1b37b1f400e34327efe9ee897a462ef1", "prompt_hash": "b2ab550c9120155f960bb3ca232cea404a211d483b89faa790839d68e0cf14a9", "target_hash": "50bd17867b7d3362a79811d10c638f94d4b654bcc2edd7c7f153784f61175ebd", "exact_match": 0.0}
{"doc_id": 186, "doc": {"question": "what is the first book of percy jackson", "answer": ["The Lightning Thief"]}, "target": "['The Lightning Thief']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the first book of percy jackson?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Lightning Thief"]], "filtered_resps": ["The Lightning Thief"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8b085642bdc6132b0843045bfb238b215d6262197b07b2e6330894126739c0f2", "prompt_hash": "fbd2518bd93dab2b768bba121042567de0386cce49a80e6bc54fea2debb59cc2", "target_hash": "7ffe6e2424cf3e6d2882a424224ea3dbca437f842f0169e94cbdbfa4bf4b2755", "exact_match": 1.0}
{"doc_id": 187, "doc": {"question": "who was originally cast to play indiana jones", "answer": ["Tom Selleck"]}, "target": "['Tom Selleck']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was originally cast to play indiana jones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Harrison Ford"]], "filtered_resps": ["Harrison Ford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e5d037fe924e87a661500b1bda8727af079882b6342324d4371dd01aaeaffa32", "prompt_hash": "3a1c56278f9c561a64f41e849b41eee46b46f9bc44cb6971a61711b7620c313c", "target_hash": "31841730771f89d08b982eb92c67bc25daf0baa3c83c05815035635ea145e974", "exact_match": 0.0}
{"doc_id": 188, "doc": {"question": "what proposition made the insurance commissioner an elected position", "answer": ["Proposition 103", "Proposition 103 in 1988"]}, "target": "['Proposition 103', 'Proposition 103 in 1988']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what proposition made the insurance commissioner an elected position?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Proposition 109"]], "filtered_resps": ["Proposition 109"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f7047f7d5ebccb37f984880ffc92f32e1a5b46901ae266c8f8630da9f0c28f81", "prompt_hash": "34290023a9d3ab0de9569036e4bf813f035f31920b8a94fd68dc2b3298a28fae", "target_hash": "ecec506ce0c70465e857586ae607ff8f3a526fd178c7db9fce493ff9e89201b6", "exact_match": 0.0}
{"doc_id": 189, "doc": {"question": "who played amy grant i i can only imagine", "answer": ["Nicole DuPort"]}, "target": "['Nicole DuPort']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played amy grant i i can only imagine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Glenys Lynne"]], "filtered_resps": ["Glenys Lynne"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "12a8ebea677814ac39e98c636ea5139cc37e61231f5689ddbe19b5268a0d00c6", "prompt_hash": "9a94b7c14872d173f6da9932eb9c81a4ff41e5175b6fc2cbe580ca8edc104aa4", "target_hash": "7fb157d13c346c62dfd15e8469a35ad4958db2434bbcdac18adb2d47e24107bc", "exact_match": 0.0}
{"doc_id": 190, "doc": {"question": "who sings the song only in my dreams", "answer": ["Debbie Gibson"]}, "target": "['Debbie Gibson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the song only in my dreams?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Beyoncé"]], "filtered_resps": ["Beyoncé"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3f5535b1f7552a9f7fa7853b89a22489cf72590ff8a282d327a8bb5c142d3c5d", "prompt_hash": "81265350951a48414e021286ebcb5a5e6dda1768cc6748a0657fe59249a3876d", "target_hash": "9ed19ed4391c4b633045586535f96887ab5b76f3ffc004ce61c45bb269c964ce", "exact_match": 0.0}
{"doc_id": 191, "doc": {"question": "who appoints the chair of the federal reserve system", "answer": ["President of the United States"]}, "target": "['President of the United States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who appoints the chair of the federal reserve system?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the president"]], "filtered_resps": ["the president"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a09506a09d83207048b3a0596ae76e67b686dca4f2f2e387ca08715ba0ddc9da", "prompt_hash": "5058bfe0352c7f93ec4f625953d49e53f5e22a9412a6ab32bec244b589ca595a", "target_hash": "735bf200f113466d550a52ff1102be2138f0be7b497bb0152bd4dd6060a2d8b4", "exact_match": 0.0}
{"doc_id": 192, "doc": {"question": "who wins the next iron chef super chefs", "answer": ["Zakarian", "Geoffrey Zakarian"]}, "target": "['Zakarian', 'Geoffrey Zakarian']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wins the next iron chef super chefs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Masaharu Morimoto"]], "filtered_resps": ["Masaharu Morimoto"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "309aab050a193bc58e0d5ff37ed81442d2a6e6d6fdca0fa391f733d743b34196", "prompt_hash": "f731954bc670ec8a2a4ca42e3e4171406d469474be9069d2eaf3e50312e98958", "target_hash": "ca9ecbec5bb1b434fa1dc95e8e631516b90fa0c62d56398aef569e21f1c9fb77", "exact_match": 0.0}
{"doc_id": 193, "doc": {"question": "when does body temperature tend to be lowest", "answer": ["11 p.m. to 3 a.m."]}, "target": "['11 p.m. to 3 a.m.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does body temperature tend to be lowest?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" morning"]], "filtered_resps": ["morning"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c51095a9fe56276c0a5b976e7d4aabcd972cce794042540d510ea825e18723aa", "prompt_hash": "a8f58547ae7fcfa6f74ea31b2c7bc88c5328b8980f93c57f8fdd71b8232adedf", "target_hash": "e83fe382b54d47c920f09af29a36e5a1351dc8cfd14bcaeedba7397018c79952", "exact_match": 0.0}
{"doc_id": 194, "doc": {"question": "who sings the whiskey ain't workin anymore", "answer": ["Dennis Locorriere", "Marty Stuart", "Travis Tritt", "Tim Passmore", "Travis Tritt and Marty Stuart"]}, "target": "['Dennis Locorriere', 'Marty Stuart', 'Travis Tritt', 'Tim Passmore', 'Travis Tritt and Marty Stuart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the whiskey ain't workin anymore?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dwight Yoakam"]], "filtered_resps": ["Dwight Yoakam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "20852d85f64ca36cffb66d3f3dfcf4d99e587f78a8742c9f99619c27c1b5e039", "prompt_hash": "4efc854241fd103a2d50989a0e24158e2d961745137b296c86b9b4163006974c", "target_hash": "8539aa107ea847e00aebeaac518b171585849f9f2e9ba2b65d4277ae083161f5", "exact_match": 0.0}
{"doc_id": 195, "doc": {"question": "what year is it for the jewish calendar", "answer": ["AM 5778", "5778"]}, "target": "['AM 5778', '5778']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what year is it for the jewish calendar?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 5777"]], "filtered_resps": ["5777"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df6b01cc98424238bc5b0c370542a8e42e6c101597f49c95144257d1dc19dba4", "prompt_hash": "aad6803cc98954ef8d9804d263d66d6c06ab77c1efc8bccb95d52e14b7a480ac", "target_hash": "10eb8a632d45f3b69a9a918159567e664712770c395e9c19b0c75f1d0119824e", "exact_match": 0.0}
{"doc_id": 196, "doc": {"question": "chief ministers of tamil nadu mentioned on wikipedia", "answer": ["Ramakrishna Ranga Rao", "Tanguturi Prakasam", "P. Subbarayan", "M. G. Ramachandran", "Janaki Ramachandran", "P. T. Rajan", "J. Jayalalithaa", "M. Karunanidhi", "A. Subbarayalu Reddiar", "C. N. Annadurai", "P. S. Kumaraswamy Raja", "K. Palaniswami", "O. Panneerselvam", "V.R. Nedunchezhiyan", "Raja of Panagal", "O. P. Ramaswamy Reddiyar", "Kurma Venkata Reddy Naidu", "B. Munuswamy Naidu", "C. Rajagopalachari"]}, "target": "['Ramakrishna Ranga Rao', 'Tanguturi Prakasam', 'P. Subbarayan', 'M. G. Ramachandran', 'Janaki Ramachandran', 'P. T. Rajan', 'J. Jayalalithaa', 'M. Karunanidhi', 'A. Subbarayalu Reddiar', 'C. N. Annadurai', 'P. S. Kumaraswamy Raja', 'K. Palaniswami', 'O. Panneerselvam', 'V.R. Nedunchezhiyan', 'Raja of Panagal', 'O. P. Ramaswamy Reddiyar', 'Kurma Venkata Reddy Naidu', 'B. Munuswamy Naidu', 'C. Rajagopalachari']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: chief ministers of tamil nadu mentioned on wikipedia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" M"]], "filtered_resps": ["M"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3533256ffde40aa2e2e1bc12ab392653f71b3eb60e9fcc9d7cd45e9e23a59c9", "prompt_hash": "2981b436e393c162ab10bc0ce98906ae8b5607603bc7182f69152f818918522e", "target_hash": "981fda1ff5a6c70c9582a8d2c4c41304f56e84f50891ed14cf964412a9ee9124", "exact_match": 0.0}
{"doc_id": 197, "doc": {"question": "what is the function of a political action committee (pac)", "answer": ["pools campaign contributions from members"]}, "target": "['pools campaign contributions from members']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the function of a political action committee (pac)?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to raise and spend money"]], "filtered_resps": ["to raise and spend money"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "915a0ed657f4ba8e7e9de0ac46972367b3d347f0d1f73b4cdcb0983494676907", "prompt_hash": "aaf13c9ec93050246ce367a2e5109b3204f199892d84c5432bf8b0882241bc48", "target_hash": "5d200e0f629b36f5f47ab863f968145e810d499d03a83b3f3be3bf4edb820763", "exact_match": 0.0}
{"doc_id": 198, "doc": {"question": "who has the most followers in the world on instagram", "answer": ["Instagram's own account", "Instagram"]}, "target": "[\"Instagram's own account\", 'Instagram']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most followers in the world on instagram?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cristiano Ronaldo"]], "filtered_resps": ["Cristiano Ronaldo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f060ccbc38a3e19d5bd8aa2325a8ade56d32843f88aa289b86c22db118f54cfb", "prompt_hash": "b955ff01360e8248008ca4b55f537be4df7db9b574552c3dde80b007856e1deb", "target_hash": "60cf1e492f01a0235dea4c61ad525f250ed571cb545eb455ffb7244f080381b0", "exact_match": 0.0}
{"doc_id": 199, "doc": {"question": "what type of political system does el salvador have", "answer": ["a presidential representative democratic republic", "\"flawed democracy\"", "presidential representative democratic republic"]}, "target": "['a presidential representative democratic republic', '\"flawed democracy\"', 'presidential representative democratic republic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of political system does el salvador have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Republic"]], "filtered_resps": ["Republic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "272dc8403c6ba43bcb618e87de1f1f7cbe9a860438fc973c38bd98f5c9b27a1a", "prompt_hash": "4608cd392647cf7f509aadddb4650fbe2412b80ccb09fdc7e95a353b965887f9", "target_hash": "91528c3315fad9d1e9437632d52974321e79dfbe90bac8d3f3138adf9bdf1b26", "exact_match": 0.0}
{"doc_id": 200, "doc": {"question": "where is simple squamous epithelium found in the body", "answer": ["alveoli", "outer layer of skin", "capillaries", "glomeruli"]}, "target": "['alveoli', 'outer layer of skin', 'capillaries', 'glomeruli']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is simple squamous epithelium found in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" blood vessels"]], "filtered_resps": ["blood vessels"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "516ba35fdcbca9ea9db5c5218c618c93c7d5f022843e2ef404e815be66f8029c", "prompt_hash": "eefba0e9a5c4ae5d6dddefdd3b9b3c5d276b10547deeeaeee252470f42eb4e4e", "target_hash": "294db7b7f8219f905a928aeb11c068d98016ea85046d94a0b831ba188b45e9c2", "exact_match": 0.0}
{"doc_id": 201, "doc": {"question": "who is president of india in present time", "answer": ["Ram Nath Kovind"]}, "target": "['Ram Nath Kovind']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is president of india in present time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pranab Mukherjee"]], "filtered_resps": ["Pranab Mukherjee"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b6e2382b3d079eaeeb17f3fab8bb2922344d73f04d3593df34889fb33899b9ba", "prompt_hash": "9f044cded5738b341f987e7775d9417a45438ee6f6802859af8ad0b2fdebd1b8", "target_hash": "d54c3beff9397946ea4872734ba6a282de36418d942fa66a6eba254618d7c393", "exact_match": 0.0}
{"doc_id": 202, "doc": {"question": "what role does the president play in foreign affairs", "answer": ["negotiates treaties with foreign nations"]}, "target": "['negotiates treaties with foreign nations']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what role does the president play in foreign affairs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" commander in chief"]], "filtered_resps": ["commander in chief"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "45916f061f702f2b45a5ca66bca8442a92ac212bd3b61e7802d14f5100dbb407", "prompt_hash": "9b23db72d5a66aaa356ceaf630d7470b9a96c04bff1bf3bb8aa06d8163eeefd0", "target_hash": "91afe7afc71d7101990f7a22cf122222b74d9f7426206fdbece9a560ac4cdbf7", "exact_match": 0.0}
{"doc_id": 203, "doc": {"question": "when did ole miss beat alabama in football", "answer": ["October 3, 1970", "September 11, 1976", "October 13, 2001", "October 27, 1894", "October 18, 2003", "November 5, 1910", "October 8, 1988", "October 4, 2014", "September 19, 2015", "October 5, 1968"]}, "target": "['October 3, 1970', 'September 11, 1976', 'October 13, 2001', 'October 27, 1894', 'October 18, 2003', 'November 5, 1910', 'October 8, 1988', 'October 4, 2014', 'September 19, 2015', 'October 5, 1968']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did ole miss beat alabama in football?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1985"]], "filtered_resps": ["1985"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "811cf132db97eafffc054f1fab4d099e05746890e00142a02aca20b69ca413c1", "prompt_hash": "4ac814db406183a21afa999cb5da5943c0acd244afbda89140a8357b911e8292", "target_hash": "0d164b1158896a308b17c6acc2aa13c4092f0a25251ea6ddc9e76c45ffcceb19", "exact_match": 0.0}
{"doc_id": 204, "doc": {"question": "what grade was arnold from hey arnold in", "answer": ["fourth"]}, "target": "['fourth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what grade was arnold from hey arnold in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fifth"]], "filtered_resps": ["fifth"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5fc04b9d223954aadba36514f032855a52fc0874c8b0ab1987a4a4162f0747be", "prompt_hash": "6b24c19e19a78e266c02ceed2728e11194e585c47d2471862e717dc7aff38d47", "target_hash": "5d4b8ac4c38aa14414bee3078af68d981756136946936f54ef97cbd4ade60082", "exact_match": 0.0}
{"doc_id": 205, "doc": {"question": "who translated the play neel darpan into english", "answer": ["Michael Madhusudan Dutta", "Reverend J.Long"]}, "target": "['Michael Madhusudan Dutta', 'Reverend J.Long']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who translated the play neel darpan into english?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Surendranath Mitra"]], "filtered_resps": ["Surendranath Mitra"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4a3c7588516fec588688e6fcb7be79e979a057c695a19a544720ad7017217ea", "prompt_hash": "6415a619c5db4b1e26c47a1592b266892bc44a87b0d7823f4cb315fb189573f5", "target_hash": "9ba09ce1edc97957f6416aa21dfdeca9dd60918d2d08c90b25d1544a0ff2b442", "exact_match": 0.0}
{"doc_id": 206, "doc": {"question": "when does the miz and maryse show start", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the miz and maryse show start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11:00 p"]], "filtered_resps": ["11:00 p"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2b1ce9c4bd070a68ccb6a6951974f060a32371289cb6c53baa02c45fba58f32", "prompt_hash": "16a3c334900d8f559a166003d77735f6f39cfd9934e6ed8d5e86349c686fd959", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 207, "doc": {"question": "what channel is celebrity big brother on in the usa", "answer": ["CBS", "on CBS"]}, "target": "['CBS', 'on CBS']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what channel is celebrity big brother on in the usa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" CBS"]], "filtered_resps": ["CBS"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d6b984c6c5954c21274ddcc28c52e55f13f6434b670802902dc16388cdabeaa5", "prompt_hash": "e07046cab9160fb4f46344a1c99bab709a4c497386b444874759f6da413aac2d", "target_hash": "02a1c44f0f3ea66b6683b997b4111d7a779737eb4d0c9354389d253ce5cd8283", "exact_match": 1.0}
{"doc_id": 208, "doc": {"question": "what is on top of us capitol building", "answer": ["The Statue of Freedom"]}, "target": "['The Statue of Freedom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is on top of us capitol building?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Statue of Freedom"]], "filtered_resps": ["Statue of Freedom"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4d50b3de8024b481f4f1c38db426ec11093e099162dbc89969983b5f5d29b95", "prompt_hash": "f89f3b8bbba78a677c85f5de8d3bfeaa9ab875f2be1e63732e10d0fbd20c8ece", "target_hash": "61b4e8af84ec61ed5967c3c2d6b0029cb2f79f9b5546ee185bb42a14c38fe8a4", "exact_match": 1.0}
{"doc_id": 209, "doc": {"question": "who made the most free throws in nba history", "answer": ["Karl Malone"]}, "target": "['Karl Malone']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made the most free throws in nba history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ray Allen"]], "filtered_resps": ["Ray Allen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "51423de14045ea7df200fa6fc951faa73108ab12d3c42b8df14016e16443bf0b", "prompt_hash": "adaff869e7e3414a675eaca517a7108e88403456c087c2e5428fced0c3f5e08b", "target_hash": "5b77f43c4fc0f304b48913e07f7cfa97c41c3c95f8483036c04f51c2685f3461", "exact_match": 0.0}
{"doc_id": 210, "doc": {"question": "who is the first president to be impeached", "answer": ["Andrew Johnson", "Johnson"]}, "target": "['Andrew Johnson', 'Johnson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the first president to be impeached?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Andrew Johnson"]], "filtered_resps": ["Andrew Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a80bb3c5f30385e52aaa9e61bde8e6b0776b51b96ae008b0fc540b10dfd16f51", "prompt_hash": "d779252cfc49ec1e0b1a1d4ac0b10f60aed5f4e66cd202662adf7bb11b6679c5", "target_hash": "c5d117a96dab0ae94b6a6538b22bbf3a2f80a670b5a1e37586d581d23e0aaf68", "exact_match": 1.0}
{"doc_id": 211, "doc": {"question": "why does kerosene oil rise up in the wick of lantern", "answer": ["capillary action"]}, "target": "['capillary action']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why does kerosene oil rise up in the wick of lantern?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" capillary action"]], "filtered_resps": ["capillary action"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c8da741a80f957299cb0265c30dc2b4e244af71799874ce477b453c66dacfb96", "prompt_hash": "34c34efa1e44c2ce723537f81433b64da25e36093b54e5919a7b068f308fa993", "target_hash": "05a6589d51d41a855066d256e8087e3ba8a2c9e8c6cdc6b7d4ed05534bef5b35", "exact_match": 1.0}
{"doc_id": 212, "doc": {"question": "when did the sims 4 toddlers come out", "answer": ["January 2017", "January 12, 2017", "the January 2017 patch"]}, "target": "['January 2017', 'January 12, 2017', 'the January 2017 patch']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the sims 4 toddlers come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" August 21"]], "filtered_resps": ["August 21"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6241fafe42e636459ba5efe7af1ff22f6cc24ce6fe4ee1bd24c53922900775e2", "prompt_hash": "a44b1f6d58d21c964faeda4bb98405d6d2f1f5820cc5bbc13302cbfbcddfc588", "target_hash": "4de007282aaeb22b5557596b9a622553a467d86d41f6028565d60c0687001bc5", "exact_match": 0.0}
{"doc_id": 213, "doc": {"question": "how many episodes of season 5 of curse of oak island", "answer": ["18"]}, "target": "['18']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes of season 5 of curse of oak island?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6aafcad0f940300cff932db773957d9f612635b13d0d9a748e3d0d205b4f8d2c", "prompt_hash": "91ee0dbb41cd11fc4a8b66a4aa7670410a2908e7e4e97d584470cb17dbaf52da", "target_hash": "fe29453a161fbb6965ccdbdca03593905c1e66cc520a49919b2d999ddf5c0c18", "exact_match": 0.0}
{"doc_id": 214, "doc": {"question": "how many episodes of touching evil are there", "answer": ["16"]}, "target": "['16']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes of touching evil are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eae3d3db5dd72ee176cd5b5b182ea63e1bcd087430cfe781b991afb5f85b9901", "prompt_hash": "738b370c2212f8b9e4a95a19040aeedd88b5edf0766a78b248ed5413faf3d472", "target_hash": "424a22f5ba0f390d3484c7c87755c38fcebdb32337c3133a2723e6793b02f56d", "exact_match": 0.0}
{"doc_id": 215, "doc": {"question": "hyori bed and breakfast season 2 air date", "answer": ["February 4, 2018"]}, "target": "['February 4, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: hyori bed and breakfast season 2 air date?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2016"]], "filtered_resps": ["2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42099198af9c7983c60c5a459bf9f61d9f6bd7645d928697f43cb184dae9eaa2", "prompt_hash": "6612d10ed66b7156ec7e8ac257797d517fa099bae5400af2110e29d4b0e29cca", "target_hash": "43e9549978c4ec250f0acca8ee574a095a5a832df9e72e93594d08ea0d48fa7e", "exact_match": 0.0}
{"doc_id": 216, "doc": {"question": "if a piece of music is perceived to have changed key then we say the piece has", "answer": ["transposed", "transposition"]}, "target": "['transposed', 'transposition']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: if a piece of music is perceived to have changed key then we say the piece has?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" modulated"]], "filtered_resps": ["modulated"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "468230c6fd37b9ca0a6e378bfee04930dbbc133d2442294a9c46f6eef8acb23b", "prompt_hash": "c16bbba16c57aa8696fa78443723341345939df6e7f295257dbd113b17bd9b4b", "target_hash": "cc6d7f0c3b5c194a970b1aec1e07d5898f07e30343b758c7d3b9da334d1db0ab", "exact_match": 0.0}
{"doc_id": 217, "doc": {"question": "who has the most gold medals in the winter olympics of all time", "answer": ["Norway"]}, "target": "['Norway']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most gold medals in the winter olympics of all time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Norway"]], "filtered_resps": ["Norway"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "46ffcdf6196c7f7acb53ec35e6b23e432d8c53d990bf25913625f896c91859fa", "prompt_hash": "e922a6992eeb84ca6eb24fb717d0876376cbf6ed6105102058fbd97c3f753b5b", "target_hash": "591af220b37bc7f77728189b7bed35d88db34e3243e62a46d9aa15883100846c", "exact_match": 1.0}
{"doc_id": 218, "doc": {"question": "who sings the song let me love you", "answer": ["DJ Snake featuring Justin Bieber", "Justin Bieber", "Singh H Spot"]}, "target": "['DJ Snake featuring Justin Bieber', 'Justin Bieber', 'Singh H Spot']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the song let me love you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lionel Richie"]], "filtered_resps": ["Lionel Richie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1f5433c4466cceb519e30e0dc57dff7e0b6ac70a9a25630e317d66a7c8da2b35", "prompt_hash": "b037fcc26715beeba320a701adcd9f99cf45acb3683dbd5e7ddfc0577772e4c3", "target_hash": "3c7f3dd7c8e421e59c0488cc0de01deb700df80a4d3e9a1e6735eda98b3753c1", "exact_match": 0.0}
{"doc_id": 219, "doc": {"question": "when is the fourth movie of the divergent series coming out", "answer": ["never made"]}, "target": "['never made']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the fourth movie of the divergent series coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2018"]], "filtered_resps": ["2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cd243a43bb27c3115b97f0400a7b5d16215c3d3e974f64099b37ea14314abd6b", "prompt_hash": "b7bad118c26c5ecf580ad1da30107d1311f83f9414fbfbb56b34e94bdba32345", "target_hash": "caf6639699a07db561596a77133633c80b2e654e8d03184fc49850b1bdf3a56c", "exact_match": 0.0}
{"doc_id": 220, "doc": {"question": "the cast of don't tell mom the babysitter's dead", "answer": ["Kimmy Robertson", "Jeff Bollow", "John Getz", "Keith Coogan", "Joanna Cassidy", "Concetta Tomei", "Robert Hy Gorman", "David Duchovny", "Jayne Brook", "Eda Reiss Merin", "Christopher Pettiet", "Dan Castellaneta (voice)", "Josh Charles", "Danielle Harris", "Michael Kopelow", "Christina Applegate"]}, "target": "['Kimmy Robertson', 'Jeff Bollow', 'John Getz', 'Keith Coogan', 'Joanna Cassidy', 'Concetta Tomei', 'Robert Hy Gorman', 'David Duchovny', 'Jayne Brook', 'Eda Reiss Merin', 'Christopher Pettiet', 'Dan Castellaneta (voice)', 'Josh Charles', 'Danielle Harris', 'Michael Kopelow', 'Christina Applegate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the cast of don't tell mom the babysitter's dead?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Coolidge"]], "filtered_resps": ["Jennifer Coolidge"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "600e821f3cdee5a51f2393118e519ff608c40a601ec92c3e2958beaf313ac266", "prompt_hash": "1707dc2d5fea6b416f4f72dbea11a4d622b575dd7c706901b212d9f293fa7b92", "target_hash": "2eb4cbd8bc7b50ad4396e8035e6b486a57138b611ee04bb5846b86a09a973c93", "exact_match": 0.0}
{"doc_id": 221, "doc": {"question": "the outer layer of the skin that contains no blood or nerve supply is the", "answer": ["epidermis"]}, "target": "['epidermis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the outer layer of the skin that contains no blood or nerve supply is the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" epidermis"]], "filtered_resps": ["epidermis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7dc2caa3f633b83ddb6541e8b1b60c438067d431bb356d627fdb6410f1b7a324", "prompt_hash": "350ade5e30a4467f4eb1b960c3c06d9dc156f23b08bf742d45fad091d465af0f", "target_hash": "6f35f501aeaed89ad7898c23ac33b41c70973d588a9c31f8a242c5d6e200a3fb", "exact_match": 1.0}
{"doc_id": 222, "doc": {"question": "who won the battle of the first battle of bull run", "answer": ["Confederate victory", "Confederate forces", "Confederate"]}, "target": "['Confederate victory', 'Confederate forces', 'Confederate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the battle of the first battle of bull run?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Confederates"]], "filtered_resps": ["Confederates"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3b5a7f39b7a1780941cada41b952bc68f45f781103cf934778e3361fd8f4d4c", "prompt_hash": "64bfecb953b5b88523e957a25400ff30b06edd7eb6eab6060f0f30b8213e9f6c", "target_hash": "31d847934266342c40c1741f38d09aaea4e2796f073b34d9e61a20fd8874256f", "exact_match": 0.0}
{"doc_id": 223, "doc": {"question": "which country has the most coastline in the world", "answer": ["Canada"]}, "target": "['Canada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which country has the most coastline in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Canada"]], "filtered_resps": ["Canada"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58d9ba56e70911d6e713cb0bbca1d9720b17811764b8708d88820e493f205aef", "prompt_hash": "11661fd05249694569acae1c397763958615c7fa1689bd0903516515311e185e", "target_hash": "b6331bd59b4f4c96bb93005fbd31f398481003ab682305e364aab6a46846811a", "exact_match": 1.0}
{"doc_id": 224, "doc": {"question": "who won battle of the sexes tennis game", "answer": ["Billie Jean King"]}, "target": "['Billie Jean King']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won battle of the sexes tennis game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bobby Riggs"]], "filtered_resps": ["Bobby Riggs"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a774b24242c9b7b966d1436a732798feca2c97931e04332bd77f2438eab92de8", "prompt_hash": "2c9e0bb543ef17dd1a07db002756029407d3c57a0ed4b80b7141f7ce36c3b9fa", "target_hash": "e39ffd806a00867085d1df6809ed75632ece72b132b2fd08f913a369e6caa8db", "exact_match": 0.0}
{"doc_id": 225, "doc": {"question": "who won in the brown vs board of education case", "answer": ["defendants"]}, "target": "['defendants']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won in the brown vs board of education case?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oliver Brown"]], "filtered_resps": ["Oliver Brown"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a12f5cc6a6ea7075bee7990f5ec13ba8936ddd642f1334b8701bfbf9318e6b1", "prompt_hash": "a9b2ed9787813c0031657c324e691704cc74c10898bf71cdf70dc7748ec85446", "target_hash": "57716b11b5f5f79bbd71308b77e906af6ce6b08092dd7e3d95e44cf5ea3ccd7d", "exact_match": 0.0}
{"doc_id": 226, "doc": {"question": "ganglion axons forming the optic nerve run to the", "answer": ["pretectal nucleus", "suprachiasmatic nucleus", "photoreceptor cells", "lateral geniculate nucleus", "optic chiasma"]}, "target": "['pretectal nucleus', 'suprachiasmatic nucleus', 'photoreceptor cells', 'lateral geniculate nucleus', 'optic chiasma']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: ganglion axons forming the optic nerve run to the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" retina"]], "filtered_resps": ["retina"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4170aeddcd0b7eae4aefa59d4d533c27ead73864115efed981cacee2e9abfd3f", "prompt_hash": "fe5fa4132b4f9bc0a240bc20bdc568e39f9d54f0de1e39d281407e37f19f9b32", "target_hash": "6cbe60291ed0f5db3a2010efe0f7924efa6a465fb28e2758689111abd2d291ca", "exact_match": 0.0}
{"doc_id": 227, "doc": {"question": "when did the word of wisdom become mandatory", "answer": ["February 1834"]}, "target": "['February 1834']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the word of wisdom become mandatory?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1833"]], "filtered_resps": ["1833"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "16252a717c62b1cf913280dcfadb9317918740819084c6402076889b0d2b5511", "prompt_hash": "46a57c68000f83fdd175241b6d59d326b43b789fb4ae420ecf15ea23c8f0e33e", "target_hash": "6e0d3cb4e72aef1733347727cfb69ce8da8f1a6e8b532c82a6d96f28845165fb", "exact_match": 0.0}
{"doc_id": 228, "doc": {"question": "element named after fictional planet from which superman came", "answer": ["Kryptonite"]}, "target": "['Kryptonite']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: element named after fictional planet from which superman came?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Krypton"]], "filtered_resps": ["Krypton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "948d0a1fcd110492dec73328cc4c7b22993b8174994156532bbf21827d8cf0b8", "prompt_hash": "e56507ac6249d46f9fe884ad001b2ec43f912308c1b84c4d1adfcce15fcc7609", "target_hash": "973b91db5ec3e663cba4786580780e40082677076b2ba04c1b285087ff45b07e", "exact_match": 0.0}
{"doc_id": 229, "doc": {"question": "who wrote and performed i can only imagine", "answer": ["Bart Millard", "Christian rock band MercyMe", "MercyMe"]}, "target": "['Bart Millard', 'Christian rock band MercyMe', 'MercyMe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote and performed i can only imagine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bart Millard"]], "filtered_resps": ["Bart Millard"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a5f974ee9f239700f68fa4ba5cc55d875a50968a53bd4409c9584bfdaec3c387", "prompt_hash": "b5deec89d2b70a5b5e468d6fb4ae7176167e84755d25ecadd422e38a44fae71f", "target_hash": "74e563f4d25b964c5a52003b0a3026d4a897c3d7342125b3e82afb6b699487c9", "exact_match": 1.0}
{"doc_id": 230, "doc": {"question": "what are the active materials of a lead acid battery", "answer": ["Lead", "sulfuric acid", "Lead and lead dioxide", "lead dioxide"]}, "target": "['Lead', 'sulfuric acid', 'Lead and lead dioxide', 'lead dioxide']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the active materials of a lead acid battery?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lead dioxide"]], "filtered_resps": ["Lead dioxide"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0f7c56731f343cb9d39c92d2a9e422f44ffc5372cc2284cd35f2aa7adfe07289", "prompt_hash": "5edc55d3c7e54f0fb7c79d1eeb5b3ea42bc6b591ed617c800c4f616dda426baf", "target_hash": "4a2403ca341019e92b4e3f7a4d2030861ebb2a0cfa88050f344d9e92add97e9b", "exact_match": 1.0}
{"doc_id": 231, "doc": {"question": "the roman name for a liberal arts education is known as", "answer": ["liberalia studia", "artes liberales"]}, "target": "['liberalia studia', 'artes liberales']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the roman name for a liberal arts education is known as?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" quadrivium"]], "filtered_resps": ["quadrivium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9491f0fea7782813e25c22ae8cbe53ecca777704830036b8a764b39ace70056e", "prompt_hash": "b1ca6ef8c77495cb825f91e08eaf0b1cfbe5f71c75f3d7f9c378bfb7d85374ae", "target_hash": "292fc85f6abb962446e8e88b45aa86c1d48245591aeb822f209e35d10bfdc00d", "exact_match": 0.0}
{"doc_id": 232, "doc": {"question": "who sings you're welcome in moana credits", "answer": ["Dwayne Johnson", "Lin-Manuel Miranda", "Jordan Fisher"]}, "target": "['Dwayne Johnson', 'Lin-Manuel Miranda', 'Jordan Fisher']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings you're welcome in moana credits?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Opetaia Foa'i"]], "filtered_resps": ["Opetaia Foa'i"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f8b3da9b0a3a1fcd7ba48df37004c471aad155354f2f88a0aefe10b37c109bcf", "prompt_hash": "3bcb472956fa3bdbcf8330fc6c45ef6bcaabfdd7853a475e3b3d48521c004f89", "target_hash": "e43fd8ed0eb2775b7e8c06211194889232c1c0481282ba4072e68b5418586fda", "exact_match": 0.0}
{"doc_id": 233, "doc": {"question": "who developed the central processing unit (cpu)", "answer": ["John von Neumann"]}, "target": "['John von Neumann']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who developed the central processing unit (cpu)?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Intel"]], "filtered_resps": ["Intel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "874ae6a549f39bfced8d863cfa9952e2493962a671be4f674ad4e6106f30e090", "prompt_hash": "25494afe8dbac18a0cdcb5ac5a727de57a81e90422e8c8786e364b9c3c667c47", "target_hash": "909d5f015a6eb3008594306cade6643b0653fee1e3a4851ac75c8e1525393a81", "exact_match": 0.0}
{"doc_id": 234, "doc": {"question": "who was kat slater's sisters in eastenders", "answer": ["Zoe", "Little Mo", "Lynne"]}, "target": "['Zoe', 'Little Mo', 'Lynne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was kat slater's sisters in eastenders?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tanya and Nicola"]], "filtered_resps": ["Tanya and Nicola"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "322bf9489c25db56e6256a23168d518d9ab08e0d1e4594025a7adcb1997179a0", "prompt_hash": "9a1a0a49def28046dab53b25f274c22a0581976a76b36c10090dbea7b4e36078", "target_hash": "dcae9ab283cc8724741da153de421abc318fc8ea0eb6313768e3a5c1625b9f8a", "exact_match": 0.0}
{"doc_id": 235, "doc": {"question": "what is katie running from in safe haven", "answer": ["her abusive husband"]}, "target": "['her abusive husband']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is katie running from in safe haven?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Reapers"]], "filtered_resps": ["the Reapers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f1a28afd071d10cc69dee111a24972bb12e1f35df052f92c5e5b039d47401d45", "prompt_hash": "7726abb2021dd5ce9eef2004a71bd71406e279e30ba69ac74c1bd80bea3c66da", "target_hash": "4fdfe1e64def61fe636083259e60d3b15cb1a75c1ecd70e7edc2c3c022eeed7d", "exact_match": 0.0}
{"doc_id": 236, "doc": {"question": "when did they replace lead with graphite in pencils", "answer": ["never contained the element lead"]}, "target": "['never contained the element lead']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they replace lead with graphite in pencils?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 18th century"]], "filtered_resps": ["18th century"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "30204d523b099d12b4cd2a01ee33edaf940ac46aa5bdf45f27b18ac372ce03e8", "prompt_hash": "8d5028bc10ce04d3dbf082eb1abde185c19ca4a139890645f6b7cd5c20d2e92f", "target_hash": "345df3494b48c14c7f0e5af057f67dc66a56b0a927b89216fd989fd05aec7ae8", "exact_match": 0.0}
{"doc_id": 237, "doc": {"question": "what started the war between us and japan", "answer": ["attack on Pearl Harbor"]}, "target": "['attack on Pearl Harbor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what started the war between us and japan?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pearl Harbor"]], "filtered_resps": ["Pearl Harbor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ef07874d45ce489502d7b6594b7744602fddeffda84f1fedda7e6c16ead6cad0", "prompt_hash": "4fb31cf9834c5e8f5a7159b0a61aad38eb52437f73e8183259535847a8a75b3f", "target_hash": "9878c821b6358659e2e13cd572c9fb6f011b18bc6dff34b9b151dd8e0f293fce", "exact_match": 0.0}
{"doc_id": 238, "doc": {"question": "how old is the actress who plays phyllis on y&r", "answer": ["Gina Tognoni", "age 44"]}, "target": "['Gina Tognoni', 'age\\xa044']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how old is the actress who plays phyllis on y&r?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 51"]], "filtered_resps": ["51"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5a89bb7ef09535b71ae24507985e6706ab9c7486e22f900b79af735e7223dd8e", "prompt_hash": "e310c4d35d7248295372648a02ce37a16725512afd09277e09c97c2c0a0adba1", "target_hash": "a11633f5bb7bc06399af63a27d7e26ebdb2e076015bb3ae3437e21f639eb85cd", "exact_match": 0.0}
{"doc_id": 239, "doc": {"question": "who wrote the french declaration of the rights of man and of the citizen", "answer": ["General Lafayette", "Honoré Mirabeau", "Lafayette", "Thomas Jefferson"]}, "target": "['General Lafayette', 'Honoré Mirabeau', 'Lafayette', 'Thomas Jefferson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the french declaration of the rights of man and of the citizen?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The French Declaration of the Rights of Man and of the Citizen"]], "filtered_resps": ["The French Declaration of the Rights of Man and of the Citizen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "599d6253729156fdde49e033148fc559a4c736da07d9331cc51fb04ee37638a1", "prompt_hash": "9d8c5f94bf46c958c8d43429f61c6bcc0c39570862bfbbdfbf9a6f5ce422f771", "target_hash": "b682276388c83ce7d6a09342744807bd10c05a9253c31facf170fe21cee5ef9f", "exact_match": 0.0}
{"doc_id": 240, "doc": {"question": "when does sam realize he is jim in ghost whisperer", "answer": ["Leap of Faith"]}, "target": "['Leap of Faith']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does sam realize he is jim in ghost whisperer?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 4"]], "filtered_resps": ["season 4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "98bb46f831eceb73fa2fa3722bfc36a0d33e2459af5220b4aced7ab157c1fa3d", "prompt_hash": "f55a2c1d273e54af71beaaceb0c2379f32e91fd9af1ba3c712011da7f70b8e4a", "target_hash": "3b99c3c3bf27837eb89a2c99e33583ca48f2b3e05516d102692f67df68b7c725", "exact_match": 0.0}
{"doc_id": 241, "doc": {"question": "what size engine does a 2005 honda civic have", "answer": ["1169 cc"]}, "target": "['1169\\xa0cc']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what size engine does a 2005 honda civic have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a1ac95a384efedac04d659efe4e8c811a51284c1ee75254b7f2528577839dde", "prompt_hash": "51a446ffea95e271e0819e0612a24fae606361b2f81b7a1584f8490132e2fc36", "target_hash": "3e0193c4983724ed47e2c4e1e803ee1369539c532e123b98713e5b78b7754f77", "exact_match": 0.0}
{"doc_id": 242, "doc": {"question": "who won college basketball player of the year", "answer": ["A'ja Wilson", "Jalen Brunson"]}, "target": "[\"A'ja Wilson\", 'Jalen Brunson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won college basketball player of the year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Karl-Anthony Towns"]], "filtered_resps": ["Karl-Anthony Towns"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3387c158eb16e4f94bb01e1cca9d28dc9ddb75862b6906d8525709a0d3b775a", "prompt_hash": "c59d215d13c75560b917e06f1f627b330b546b980bb439d1d9d10d4704d45d71", "target_hash": "ed20acbc9f6a66164d3844f73169edc92837d9fcd255c54df8a2322ccb155a89", "exact_match": 0.0}
{"doc_id": 243, "doc": {"question": "a single period of precession of earth's axis is completed in about", "answer": ["approximately 26,000 years", "26,000 years"]}, "target": "['approximately 26,000 years', '26,000 years']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a single period of precession of earth's axis is completed in about?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 26"]], "filtered_resps": ["26"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "12433d0604a198cf51bdcce6c5858c205488a3dd5487e3be763051a4036876b8", "prompt_hash": "41d0b7a9c89488aa03c20409e8f22ad8179ec1ba00be0f20315659b1746d7b30", "target_hash": "7be34eadc5c9ab34fdc20be6c4e1bc34c763d261ea664578ad2b1b1421fdc1d8", "exact_match": 0.0}
{"doc_id": 244, "doc": {"question": "locations for the film an englishman who went up a hill", "answer": ["Llanrhaeadr-ym-Mochnant", "Llansilin in Powys"]}, "target": "['Llanrhaeadr-ym-Mochnant', 'Llansilin in Powys']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: locations for the film an englishman who went up a hill?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" London"]], "filtered_resps": ["London"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "be3e6a55fff98985db7935b56978e124f396b0dc751e99466366bef72d3482fb", "prompt_hash": "8fc5889745f322ec05ecb179773bd23ff51335a7f30a00596e44a80904c3c839", "target_hash": "945e418a1bd5464032ac10094046440f18554389d808370dfa0103d82209b262", "exact_match": 0.0}
{"doc_id": 245, "doc": {"question": "rizal finished all the chapters of the novel noli me tangere in", "answer": ["December 1886", "Spanish"]}, "target": "['December 1886', 'Spanish']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: rizal finished all the chapters of the novel noli me tangere in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1886"]], "filtered_resps": ["1886"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d179e7269815977b75e054c12883b952e5bbb8d0ea79f6fb6bb0dc143e19d81", "prompt_hash": "188785925e3694a0372f7ae60d92f8e1bb5bc04e677ed504bac4aba6a15b6e84", "target_hash": "b775bd2470449976e9a5c93e97922c8e5bc20faa729bea8c3ffb03b30a8ec53c", "exact_match": 0.0}
{"doc_id": 246, "doc": {"question": "who sang the superbowl national anthem in 2017", "answer": ["Luke Bryan"]}, "target": "['Luke Bryan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the superbowl national anthem in 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lady Gaga"]], "filtered_resps": ["Lady Gaga"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e9993948d855de1704209231667f783d926264121297a2922974e1d4d3b8c08", "prompt_hash": "9f0b56eef8f639d41240349eb65ebdd16a1a164acf3f807d8eafd1c2b94b861c", "target_hash": "df8562f4fe6b7e2e997cf68b0020fe21d67494255ddd89381186c60849f59240", "exact_match": 0.0}
{"doc_id": 247, "doc": {"question": "who did the singing in into the woods", "answer": ["the cast", "the cast members"]}, "target": "['the cast', 'the cast members']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the singing in into the woods?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Lapine"]], "filtered_resps": ["James Lapine"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "886e37c47e6532c402615646d5285ffc36ce9d651edaacb3ebd0f8ade5fc5c1f", "prompt_hash": "3aefa5351162eb98ee99c9bca29d571c6f7f1a5a226fa0598398cd54a1706420", "target_hash": "ae99e04d41e0e54cc4f26787238d18c86315f1f52e8e60e8f13ead838c086e5e", "exact_match": 0.0}
{"doc_id": 248, "doc": {"question": "which country has won maximum number of gold medal in asian game 2014", "answer": ["China"]}, "target": "['China']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which country has won maximum number of gold medal in asian game 2014?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" China"]], "filtered_resps": ["China"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad0dcc2979ba35cc56841e34e0c5a5b7d0a957c7edc609a648948fb7b735204c", "prompt_hash": "03b10430d107e76239ed1d16cd6ce1ec0c548fe95b40616fe4346a9c70978487", "target_hash": "11548c21ae9d3fb44407ddaa2e48320ccef615891642b38b1a21470d38e07356", "exact_match": 1.0}
{"doc_id": 249, "doc": {"question": "which country is the last member of saarc", "answer": ["Afghanistan"]}, "target": "['Afghanistan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which country is the last member of saarc?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Maldives"]], "filtered_resps": ["Maldives"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b213ad0b6017bd641846066dfde66b71326b23a0d47effd7228d021e324b75a5", "prompt_hash": "38c7174546eab0c3e684213874afc41e7fc1341a5a59d881ebe9f0caca06cebe", "target_hash": "a323c7c283bf602803700a2c0cf678c124f7bfe8592f240fea41bb22529f14eb", "exact_match": 0.0}
{"doc_id": 250, "doc": {"question": "who plays timon in lion king on broadway", "answer": ["Max Casella"]}, "target": "['Max Casella']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays timon in lion king on broadway?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nathan Lane"]], "filtered_resps": ["Nathan Lane"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0ebc88b63419f6b7afffffaeb8649cc6fc7710576607e9d31a44abb4fed7d10a", "prompt_hash": "083aca1d9cec96484d4251b5b9149069408a16d7068a117f091f98f2b95fe011", "target_hash": "df28c92a90536e1917e25b8ecbe9d0da296b28e2044eb3c9647723c692533807", "exact_match": 0.0}
{"doc_id": 251, "doc": {"question": "what caused the breakup of the democratic republican party", "answer": ["the disputed 1824 presidential election"]}, "target": "['the disputed 1824 presidential election']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what caused the breakup of the democratic republican party?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the election of 1824"]], "filtered_resps": ["the election of 1824"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfe0a1f8384702ae718d9450f6945989a084c1aa0b90de937b0afdd09f75f4f1", "prompt_hash": "8347007106ab4a524a9def4006a3ff82da034550c57522d6694d0b0b96759ca9", "target_hash": "d7bbfc1cafbb6ae1ca22b4b4ff3e3ff1ee68bf905ea224db8840987e3ac4bdec", "exact_match": 0.0}
{"doc_id": 252, "doc": {"question": "most passing yards in nfl history in a game", "answer": ["Norm Van Brocklin", "554"]}, "target": "['Norm Van Brocklin', '554']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: most passing yards in nfl history in a game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 518"]], "filtered_resps": ["518"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "70bd96260d8d8e80bcb4473d7df8a4d3ff33150ad66b3ea3e2f8634457e281d5", "prompt_hash": "7196107e853bf4c054cb312d8a0182c7e2e37cd354a3066160edf6866b3b9b74", "target_hash": "35f1e16999f5b7f1b77a859a625a13a21cc44b98e5e1c2440114fd25e0a7c84f", "exact_match": 0.0}
{"doc_id": 253, "doc": {"question": "who has won the most college football national champions", "answer": ["Princeton"]}, "target": "['Princeton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the most college football national champions?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alabama"]], "filtered_resps": ["Alabama"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7553a54773c60bd10e0be56364d1fb4b1659d8a2f2d47e5dc0f319e0a06025ca", "prompt_hash": "e224af36d60ef8c17c75705325ea2564d8df2255f43e68c1929618bf61436498", "target_hash": "7f2d532fa32209d44a68dffd33430b6ab5a6736459dfa4050ef63cc8e8e2ffb2", "exact_match": 0.0}
{"doc_id": 254, "doc": {"question": "what languages are spoken in india the most", "answer": ["Bengali", "Telugu", "Marathi", "Hindi", "English"]}, "target": "['Bengali', 'Telugu', 'Marathi', 'Hindi', 'English']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what languages are spoken in india the most?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hindi"]], "filtered_resps": ["Hindi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e6d1276dc2cb1e7a4dc047e87673cba25e586c5624687d9affb4a99b7ff35ae5", "prompt_hash": "cba67bce710b9d5c1ee919cbae2d1929053ca0f8008cb364e11775a81fce2e46", "target_hash": "5bd95114341d6fdf99ad611685b6b6fd1baa0be174a3d1523e7a0713b81debf0", "exact_match": 1.0}
{"doc_id": 255, "doc": {"question": "what is the name of the hyena in lion king", "answer": ["Banzai", "Shenzi", "Ed"]}, "target": "['Banzai', 'Shenzi', 'Ed']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the hyena in lion king?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shenzi"]], "filtered_resps": ["Shenzi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e2ac5a09a7e8b7c789c58c248d927b1ecc148625585a91e3bd934a82db902454", "prompt_hash": "25af21b33cc78b825f30bece3fe13e18844f94f953d5f9efa4499dcc0045ccda", "target_hash": "6717f071163f9b2708313729d5b46b2bbce630da33a795a21624d4729d4c0128", "exact_match": 1.0}
{"doc_id": 256, "doc": {"question": "what is the meaning of the dragon boat festival", "answer": ["commemorating fealty and filial piety"]}, "target": "['commemorating fealty and filial piety']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of the dragon boat festival?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" commemorating the death of Qu Yuan"]], "filtered_resps": ["commemorating the death of Qu Yuan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18839c02fd1ca53a188f8cc8c3e6bf5c56ad13d39a5f7d199f88d55dff1f0e2c", "prompt_hash": "3f992c2533db6e66896b839f256f6eccbec44fef53562c1387440fa1377761ef", "target_hash": "9f07cf4ec463b38f6a8757d171e09ab1143852b491f2074a7d1b9b668359b8e0", "exact_match": 0.0}
{"doc_id": 257, "doc": {"question": "what was the real name of saudi arabia", "answer": ["the Saudi Arab kingdom"]}, "target": "['the Saudi Arab kingdom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the real name of saudi arabia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Saudi Arabia"]], "filtered_resps": ["Saudi Arabia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a851b697bc46bb96ee81bb2dc53b9b8c91d3736010e4c48355ecf0949d089613", "prompt_hash": "5d43614f76c58db410cdef200f7dd6e368f8c8a91584b58af7005f56bbc572ed", "target_hash": "64b079d61d6cf6ed93025249f77d8ced0cc7f6a8d743945573df85adc267bd0f", "exact_match": 0.0}
{"doc_id": 258, "doc": {"question": "what is the function of the pericardial sac", "answer": ["Lubricates the heart"]}, "target": "['Lubricates the heart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the function of the pericardial sac?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to protect the heart"]], "filtered_resps": ["to protect the heart"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "517a1c70b11ca7fb0e89c217612771ad4ecdd335f395ff6c036ae678e27dd644", "prompt_hash": "6b2c925e403eebb28692c230ce5cfbed95a0d8b60b36008c31f81f6d331e001f", "target_hash": "4a7efa47524cb02efa20c6a6dadef67ea357f4a205172e9927abf623f2f84074", "exact_match": 0.0}
{"doc_id": 259, "doc": {"question": "chris cornell she's going to change the world", "answer": ["Can't Change Me"]}, "target": "[\"Can't Change Me\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: chris cornell she's going to change the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yes"]], "filtered_resps": ["Yes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a7152005d0c0295bd8e28547c85451c7e1d70a1da7f39eacfd6c8449409e04e", "prompt_hash": "0e43f8dbc374d650f59c050591e0d4334a08db10863dc552636de1f190c0e6cb", "target_hash": "d9962b212cee12f8c57ce6b6429c31c72af36e8c2b43d938852f70225e290e2e", "exact_match": 0.0}
{"doc_id": 260, "doc": {"question": "who played the mad hatter in the batman tv show", "answer": ["Roddy McDowall", "David Wayne", "Benedict Samuel"]}, "target": "['Roddy McDowall', 'David Wayne', 'Benedict Samuel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the mad hatter in the batman tv show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Edward Woodward"]], "filtered_resps": ["Edward Woodward"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b0e663ddf94ccfbc44c239f019a8b2466ede955ae95703d824478fcedfd7bf1c", "prompt_hash": "b5133bf75c7720abdfd2fafa82f4167949eafbac2fdf489fb48e21c504479be0", "target_hash": "3b0afaba8e2d2fb3e0516a20563d74070b853c6dba02ac2dd99cc576ca4f5e63", "exact_match": 0.0}
{"doc_id": 261, "doc": {"question": "who played the colorado kid in rio bravo", "answer": ["Ricky Nelson"]}, "target": "['Ricky Nelson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the colorado kid in rio bravo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dean Martin"]], "filtered_resps": ["Dean Martin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "94c15c6fbfb6dd1a5fdb14b81a74f1d179b5f448b43b091d46842ef870a56796", "prompt_hash": "ffc0c3e4911d0cd5058e399749c8e0428662964f5183349a5e47b6c81200b51b", "target_hash": "31301c20b036ccda1112824920e05c41727ea2ef9d4b1247057bb1efdd64d86e", "exact_match": 0.0}
{"doc_id": 262, "doc": {"question": "other than water what else has hydrogen bonds", "answer": ["inorganic molecules such as water"]}, "target": "['inorganic molecules such as water']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: other than water what else has hydrogen bonds?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ice"]], "filtered_resps": ["ice"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7d950f0c007cb3634390658ef5d2c4b23c3bf5c15c7dbe2fc08f547e053721ff", "prompt_hash": "cdb2b50684179f169b83f1a5873e62601c7f1a6d8e2a6e7b82722947bf5f967c", "target_hash": "d300662e966578dd0c903bc3dacabbe2d812efae3b84632ced85e532f16c489e", "exact_match": 0.0}
{"doc_id": 263, "doc": {"question": "where was when we first met netflix filmed", "answer": ["New Orleans"]}, "target": "['New Orleans']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was when we first met netflix filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6192effa5471dd149eb3464120b3c426590c482bdf2ba34f3def5566e797ba9c", "prompt_hash": "35d6f4e76c67caa32fdec4403d6fc28909837cfe3c43b78587a08c1d1c14ac7e", "target_hash": "4ce658a62319e88ad94def31386c7c995046810b186d6c69a64093dd2356a8a9", "exact_match": 0.0}
{"doc_id": 264, "doc": {"question": "who was the viceroy when the simon commission visited india", "answer": ["Lord Irwin"]}, "target": "['Lord Irwin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the viceroy when the simon commission visited india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lord Chelmsford"]], "filtered_resps": ["Lord Chelmsford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dd63efe4ec563622d05abe6a660ff5878dadf846e2c23bc9aee3565c3023b37b", "prompt_hash": "285519bec5c06dc70b8ecce24d6f80602056adb0cb5c2240d2c4deb9d0915584", "target_hash": "f829d86747777d93bf2ce6d05b5f607a027ba20f88966436fb997a15efa4311b", "exact_match": 0.0}
{"doc_id": 265, "doc": {"question": "what category was hurricane charley when it hit florida", "answer": ["4", "Category 4", "Category 4"]}, "target": "['4', 'Category 4', 'Category\\xa04']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what category was hurricane charley when it hit florida?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Category 4"]], "filtered_resps": ["Category 4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2d765c96101f3cddd5b3ff385dd57b999a3848809e0146ad8ee12792fbc834db", "prompt_hash": "7e6adde4a34dc035bf51a451c1ef125b1fac9d19fe487e786c27e766453eca73", "target_hash": "6e9c6b71e20f6c003c6b71582babcda8cac42a24432693c0d639ca13df1ffe68", "exact_match": 1.0}
{"doc_id": 266, "doc": {"question": "when did the united states host the world cup", "answer": ["1994"]}, "target": "['1994']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the united states host the world cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1994"]], "filtered_resps": ["1994"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6688fbc8e46b31894adf5a028bf526125f94f17788f0a94df8ddb4ac6ba88b74", "prompt_hash": "26352dab3b0344ca6625b20102b22761209c9247776035dc63d1519aed12a569", "target_hash": "24b3e41cbaa8832929482068327eb5405c3e062251b127b509163466e611f5f4", "exact_match": 1.0}
{"doc_id": 267, "doc": {"question": "is parallax more pronounced with nearby stars or with distant stars", "answer": ["nearby objects", "nearby"]}, "target": "['nearby objects', 'nearby']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: is parallax more pronounced with nearby stars or with distant stars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" nearby stars"]], "filtered_resps": ["nearby stars"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a0c9c5ea8d915ae88289ced0b8364fa5ac2054411343c9b25cf47775a8cdc78", "prompt_hash": "32b2e86c3a9e1546e3adf24a74700db8acc1cb570f46f5501599cf781ce296c5", "target_hash": "d80b137031569542981ab85ebcca0eb4cc4300ee7e7be229a4638907e688af95", "exact_match": 0.0}
{"doc_id": 268, "doc": {"question": "when did the us not go to the olympics", "answer": ["The 1980 Summer Olympics", "1980"]}, "target": "['The 1980 Summer Olympics', '1980']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the us not go to the olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a65bd16092c1272636d3e0be8216d70fa8c1c4443b54fc92df71b9ca304b0707", "prompt_hash": "6152c43a04c1fcd31b09083a6832c7ebac56fc7eb7444a5cd0a2bf3d61094c07", "target_hash": "387dafa24f9773cc9eaefbbd93da2515a830eb17da3fb5e05579a57ff0579926", "exact_match": 1.0}
{"doc_id": 269, "doc": {"question": "number 4 in roman numerals on clock faces", "answer": ["IV"]}, "target": "['IV']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: number 4 in roman numerals on clock faces?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" IV"]], "filtered_resps": ["IV"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03a5131789f05c9ecfb8592e2cbd7924306a67812af3d793bc72198c3012cd5b", "prompt_hash": "beaa957b52214e6423a91018177203246cb71db03d28ac8f52f561c6d535c435", "target_hash": "3eae0f7cb617d24b6d745df07fa1f8909914c2d0484f17c530724f00ae6bc892", "exact_match": 1.0}
{"doc_id": 270, "doc": {"question": "when was the last year the eagles went to the superbowl", "answer": ["following the 2017 season", "2017"]}, "target": "['following the 2017 season', '2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last year the eagles went to the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fe6bbcb1d2196ad11acd1474f65fd88424735f7d4711486ae6ed7f25d1f4e603", "prompt_hash": "bb420781f0c7b925727ef0a24c4e977cbcb30cc960f067d552aba4e1f798ac3b", "target_hash": "0005d1a9495b288a498ec2b873f24f114fdd2c4558fac9def668f7467960a0a9", "exact_match": 1.0}
{"doc_id": 271, "doc": {"question": "who is the team that beat the eagles this season", "answer": ["Dallas Cowboys", "Seattle Seahawks", "Kansas City Chiefs"]}, "target": "['Dallas Cowboys', 'Seattle Seahawks', 'Kansas City Chiefs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the team that beat the eagles this season?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dallas Cowboys"]], "filtered_resps": ["Dallas Cowboys"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32e831d7326dd5fa18bdc7baf524eedc5ea9ca496586146d122b93fa5a96368a", "prompt_hash": "d223c1f1de5cb1063b32e91689884b557152406950ed9a7f69f0cb029f006e0a", "target_hash": "d9738be2d56439c18b6853dd7a406b318f9de6f797cbfa2904e252f34c9eb87a", "exact_match": 1.0}
{"doc_id": 272, "doc": {"question": "when did lionel messi play his first game for barcelona", "answer": ["2002", "2001", "October 2004"]}, "target": "['2002', '2001', 'October 2004']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did lionel messi play his first game for barcelona?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2004"]], "filtered_resps": ["2004"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "496bbdaf102d874a8b9a1aea3998baed2ed55e84f7d403f81dd8f56886780296", "prompt_hash": "2de09a80fcc68f79033aa84ef8c493cb865e521ff7dee89e119bd5d106aacbaa", "target_hash": "ec67bb05ec1aeb90d8537a8cfdc105af68fa012f6e9f14052917b81925836758", "exact_match": 0.0}
{"doc_id": 273, "doc": {"question": "who won the super heavyweight gold medal at the 2000 olympics", "answer": ["Audley Harrison"]}, "target": "['Audley Harrison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the super heavyweight gold medal at the 2000 olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Aleksandr Karelin"]], "filtered_resps": ["Aleksandr Karelin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cd2745418c35162aa5248ce2eb323f270767bdde033559534fb08e354f5bfed1", "prompt_hash": "444fc154a59e1e6c1437906fb915463d89f053885f96b2ae892f7779824f2aac", "target_hash": "bf3593aad0e2312f9b1cdd4ed85a86caf4fa08eba5c9f9292525c872c533621b", "exact_match": 0.0}
{"doc_id": 274, "doc": {"question": "who turns into a bear in the hobbit", "answer": ["Beorn"]}, "target": "['Beorn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who turns into a bear in the hobbit?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gollum"]], "filtered_resps": ["Gollum"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b511a6ed967b486aeb2d9c9c6edf3052eedd86f9e0a305bb350a65048a1e035b", "prompt_hash": "f6482413686a615ebdf255757de5622b4217af007f2998df90011fd628569c71", "target_hash": "d51dc7897ca715373ae9b6c7872ebaa3e5047236e938234fa93f540cddd88b25", "exact_match": 0.0}
{"doc_id": 275, "doc": {"question": "batman the enemy with episode 5 release date", "answer": ["March 27, 2018"]}, "target": "['March\\xa027,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: batman the enemy with episode 5 release date?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1999"]], "filtered_resps": ["1999"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "947390e881f9ed4a5c9db2bc36526eb2c1fb29643dfd7128608e6f6fc56210b5", "prompt_hash": "43d2a4c09a7f0a83d4a95bd1691d573e0494a4f7c4e2ee3995587a2cb05b995d", "target_hash": "fe223618aa4b9ad2628034b403931a49d544bfadccb77fd2fa77857288834a06", "exact_match": 0.0}
{"doc_id": 276, "doc": {"question": "yeh hai mohabbatein serial star cast real name", "answer": ["Divyanka Tripathi and Karan Patel"]}, "target": "['Divyanka Tripathi and Karan Patel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: yeh hai mohabbatein serial star cast real name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rajpal Yadav"]], "filtered_resps": ["Rajpal Yadav"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58db20343d16ed28a669bf290c85a85c9cec322e4f34b711fce86f4c81462a0a", "prompt_hash": "7965f58b3814f6dfaf42b9fc24ab02a6bbd7e1a00a630ce9eff12fff50964e2b", "target_hash": "9a3d99c6e1ac54479ac8ab7778f91f38743d2d78ee74ae56cee77f5b0ef9b4a1", "exact_match": 0.0}
{"doc_id": 277, "doc": {"question": "where is the slide placed on the microscope", "answer": ["stage", "on the microscope's stage"]}, "target": "['stage', \"on the microscope's stage\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the slide placed on the microscope?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" stage"]], "filtered_resps": ["stage"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0dd19034340fc61a2099a701636a4596c930fadfd933931c2625b67850b563fb", "prompt_hash": "42f763f02329d37cf58c6b804f01af88151fbe7b2ef97db5210392150dd4d0a7", "target_hash": "f7bf7f13889a417a8a9c1f0cb51b6b42e362f514ec685e018d15a738957bde15", "exact_match": 1.0}
{"doc_id": 278, "doc": {"question": "when did mcgee became a regular on ncis", "answer": ["in season two", "season two"]}, "target": "['in season two', 'season two']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did mcgee became a regular on ncis?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 2"]], "filtered_resps": ["season 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e450fd5b67026afe0f18e9bba92de6ca7a61ce73491957c481e37ed1f932a14", "prompt_hash": "4c5dbf32b0b4729f4fe1d6037a702b445c60da0aaf46d9e3011ebdc07c62c32c", "target_hash": "f1a90684784f81aba68867bf4f82d59290d678b49f47753cf63ba91893fdccb4", "exact_match": 0.0}
{"doc_id": 279, "doc": {"question": "who owns the crown plaza hotel in chicago illinois", "answer": ["InterContinental Hotels Group"]}, "target": "['InterContinental Hotels Group']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns the crown plaza hotel in chicago illinois?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Marriott International"]], "filtered_resps": ["Marriott International"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bde17acde633c1e689f8cca45dd12d8da22b6707ed65fb7a6d3a930cf366407f", "prompt_hash": "6a79d52ae4af6bec5e7d793243aa003d1f197c4d3f2cb27b20886698d86c77fc", "target_hash": "cd8c82cb3ce8c2486fb2b42dfae745cee4b4d7c8917ff658a4d515df0b8ad02b", "exact_match": 0.0}
{"doc_id": 280, "doc": {"question": "when was the last time the eu was audited", "answer": ["1994"]}, "target": "['1994']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the eu was audited?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2010"]], "filtered_resps": ["2010"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e686f868c3e70efd31b14891d4683b6de351f45775696309d05ea027beeac9a", "prompt_hash": "bb4506a2e91fa6ebcf39ea22224383e021f6c5e428dde2fba9e91743d1ff5bb9", "target_hash": "24b3e41cbaa8832929482068327eb5405c3e062251b127b509163466e611f5f4", "exact_match": 0.0}
{"doc_id": 281, "doc": {"question": "the legend of heroes trails in the sky the 3rd vita", "answer": ["July 14, 2016"]}, "target": "['July 14, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the legend of heroes trails in the sky the 3rd vita?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Trails in the Sky Crossroads"]], "filtered_resps": ["Trails in the Sky Crossroads"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4d5b417201b00c800c9942b62802fd391e9fd5ac39edb763f9daf5268db8a77", "prompt_hash": "a0cb2b3eee865b12f9fa2f33e4a2520b1d4bfb9522e609934d6fc9f6eeeeb367", "target_hash": "68f5692194a48008385321f5c6686a0dc2055cec0d8f65a4c51d20cfb6505cd1", "exact_match": 0.0}
{"doc_id": 282, "doc": {"question": "when was a series of unfortunate events published", "answer": ["September 1999"]}, "target": "['September 1999']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was a series of unfortunate events published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2013"]], "filtered_resps": ["2013"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "875a9a63556d04b8e51c095fbac85bb5fb67b814114f50494b38e13f2238a891", "prompt_hash": "bca2aa69049b9ec1ecce2374f616898639b686ba25646ad57de7f7dc83571a39", "target_hash": "d2517686ebe045dae0e3977cac18f04739a3addbbdb410a7ccb90ccc92336efc", "exact_match": 0.0}
{"doc_id": 283, "doc": {"question": "who will win 2018 election in sri lanka", "answer": ["Sri Lanka Podujana Peramuna"]}, "target": "['Sri Lanka Podujana Peramuna']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who will win 2018 election in sri lanka?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ranil Wickremesinghe"]], "filtered_resps": ["Ranil Wickremesinghe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b16b25b5afa5fced3d8c12a55cb2375f7b35efb06524abbc77a0091337ee2eb", "prompt_hash": "1dbee7ae805213f1ba4284f02c6c11e01596c6ae8cf614985367d0abe7efe4e1", "target_hash": "d6c11f179bc0fb5137533c90ffc1b9f61b1c2bdb26f81b7bea75ed9433a69a18", "exact_match": 0.0}
{"doc_id": 284, "doc": {"question": "what is the name for the ch3coo- ion", "answer": ["polyatomic anion"]}, "target": "['polyatomic anion']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name for the ch3coo- ion?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" acetate"]], "filtered_resps": ["acetate"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b522d31ac08f5f4b3cd1133b74ac057b24bb4be6506a667715948b5fa45459d5", "prompt_hash": "be694a6906c530331e11e012682ccbd3cdbb91151d0e9b0cfa221cc690524147", "target_hash": "00f149c11ec6c317f2377879f8be008077c9c86b89284f7307acfd18052dcdad", "exact_match": 0.0}
{"doc_id": 285, "doc": {"question": "what type of government did the ming dynasty have", "answer": ["imperial rule"]}, "target": "['imperial rule']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of government did the ming dynasty have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" monarchy"]], "filtered_resps": ["monarchy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "45459bf0d62c37e6a987f3f6bd21ed2be112d33bde54855bdace3a9488dad246", "prompt_hash": "f2c4fad1d30fde2f3faa3ed14771d04df858280720db1ce12061c19ff7f88a00", "target_hash": "0b4b8c96262dd9d155fd10fdc024225bd0245a83726a88e6ebbb01881fee39d7", "exact_match": 0.0}
{"doc_id": 286, "doc": {"question": "who did us fight in world war 1", "answer": ["Germany", "Austria-Hungary"]}, "target": "['Germany', 'Austria-Hungary']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did us fight in world war 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Germany"]], "filtered_resps": ["Germany"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e2cb2302e6caa3a44f291281278add11e062ad4a836b5d525c543d6160dc0413", "prompt_hash": "ff33f21ecff6fa8c157babe91844aaf80df3480616f74527d84f386b6e084e02", "target_hash": "4490e4fa6e773d75ab58c59ed986216cbf50dc9b7f233baf6bbbfacbceaf9214", "exact_match": 1.0}
{"doc_id": 287, "doc": {"question": "when was rosencrantz and guildenstern are dead written", "answer": ["1966"]}, "target": "['1966']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was rosencrantz and guildenstern are dead written?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1966"]], "filtered_resps": ["1966"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b95c2134512ab6a1b7f144397144c63a900e8d8b48bf91960224048e4c79b770", "prompt_hash": "b1749d89dc1a1962e7455b6c0ee5111b8b06c79dc68f17194b5a6523e8934218", "target_hash": "1ad11a2c3f62f6359bafd273cac75bcad0311b7a3f10bf93ff79f92e32e9074d", "exact_match": 1.0}
{"doc_id": 288, "doc": {"question": "nuclear power plant that blew up in russia", "answer": ["Chernobyl Nuclear Power Plant", "Chernobyl", "the Chernobyl Nuclear Power Plant"]}, "target": "['Chernobyl Nuclear Power Plant', 'Chernobyl', 'the Chernobyl Nuclear Power Plant']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: nuclear power plant that blew up in russia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chernobyl"]], "filtered_resps": ["Chernobyl"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d44730e1af492148f796ad49a7e6233f84a9956165b03e791455897fc095cc1", "prompt_hash": "1a09aae4930ee9afadc318fa46e7b5d0051aa4b8208600b2c71550a26e817a1f", "target_hash": "0bfb608fc34785eaa3b1c77e541f0cfafb19adb4fabecc3c6bbb3a18c0ca736d", "exact_match": 1.0}
{"doc_id": 289, "doc": {"question": "who sang picking up pebbles and throwing them into the sea", "answer": ["Matt Flinders"]}, "target": "['Matt Flinders']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang picking up pebbles and throwing them into the sea?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Beach Boys"]], "filtered_resps": ["The Beach Boys"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb8c780c1c37631b1eb209f6837ceddcd78cc60a7ef75a57815d31736fd82ba0", "prompt_hash": "495e7c926ae98b305eed37be01dc8cf89fd8920adcdb209df998098c85fdd2f1", "target_hash": "e05779e9180598b6eeb7b7755fe56086aa7e7f67ce1a825b94a5160e225c27e4", "exact_match": 0.0}
{"doc_id": 290, "doc": {"question": "how many breeds of pigs are there in the uk", "answer": ["---"]}, "target": "['---']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many breeds of pigs are there in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 17"]], "filtered_resps": ["17"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a0af3b66a0fdfd8e71e1658cc3a33695fb120e4a7b3d8bef4de5d88741d6ab8", "prompt_hash": "68440aca95cb5e25fd5be5c23ea36e0aee5ed450b346b96aef1b660c5f405f04", "target_hash": "e53a75f00b089f17067977af8f8a8147a9fd611cf403b4ab2b967ab1644a6c41", "exact_match": 0.0}
{"doc_id": 291, "doc": {"question": "where did the battle of corinth take place", "answer": ["in Corinth, Mississippi", "Corinth, Mississippi"]}, "target": "['in Corinth, Mississippi', 'Corinth, Mississippi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the battle of corinth take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mississippi"]], "filtered_resps": ["Mississippi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a52718883b623f30e8e8d5144444200d84cf350461b97e9ec1ae5ddd26d8401", "prompt_hash": "40ba616e3f45e51046d43948e54852d358a09affef9a8569899810ce1fed761e", "target_hash": "08334b5120ffeb1633bd6e3760db1a6ca0972ea9fb31394070bd509b85f398f2", "exact_match": 0.0}
{"doc_id": 292, "doc": {"question": "who sings the song i'll never forget you", "answer": ["Mariah Carey", "Zara Larsson and MNEK", "Noisettes"]}, "target": "['Mariah Carey', 'Zara Larsson and MNEK', 'Noisettes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the song i'll never forget you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Carpenters"]], "filtered_resps": ["The Carpenters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "996c702e1017d3cd2cc335642932aefa06f206c5a52816a37610cfa51ca0dbda", "prompt_hash": "54b9ad8e20204b0a36324ffbd2010e9f7221d4ac915fe9af6a14611d4f282a67", "target_hash": "b1f37483c48f4bdc0ead104443c0427f75ccef774582fc9d0d471e5704461514", "exact_match": 0.0}
{"doc_id": 293, "doc": {"question": "who played shmuel in the boy in the striped pyjamas", "answer": ["Jack Scanlon"]}, "target": "['Jack Scanlon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played shmuel in the boy in the striped pyjamas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Darius McCutchen"]], "filtered_resps": ["Darius McCutchen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fde3aa94e95afc1f3294483381e6cb7f8705f5db545e9ad7f7b2ad39228cb02a", "prompt_hash": "919a089e53cc9291511a83163338ec29386100e47265990bff44b72e73853c2a", "target_hash": "40a5b5d16cb0f09c4a49c705952fb7df8156ccd8dfaaa906363ccb5b06038188", "exact_match": 0.0}
{"doc_id": 294, "doc": {"question": "who sang rip it up and start again", "answer": ["Scottish post-punk band Orange Juice", "Orange Juice"]}, "target": "['Scottish post-punk band Orange Juice', 'Orange Juice']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang rip it up and start again?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rihanna"]], "filtered_resps": ["Rihanna"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cd8d1226ce81017fc4d5f4dbc12143377c8ca852227f8260733626cc1d317d59", "prompt_hash": "979eed3f9341c24adf2bf0d090154640f0a1b8c4a34f97f686f3a6fe06e41e1b", "target_hash": "8f28e863689254870050b31f08274145540f5a8d7187c78460629e79eacc49ac", "exact_match": 0.0}
{"doc_id": 295, "doc": {"question": "what are the 4 types of nitrogenous bases", "answer": ["cytosine (C)", "adenine (A)", "uracil (U)", "thymine (T)", "guanine (G)"]}, "target": "['cytosine (C)', 'adenine (A)', 'uracil (U)', 'thymine (T)', 'guanine (G)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the 4 types of nitrogenous bases?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" adenine"]], "filtered_resps": ["adenine"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1678932dfeed43e481addaf5be78586ef7a5b5b566587e1979a443b9096dba1d", "prompt_hash": "61327d77f67d77197175489d818fd3028ade452d6d27aac2882a5b445dadf77c", "target_hash": "ac6577cc8e2bdf07c95a923a740332252e1f096610fcc13b0fdb14c7df3d3598", "exact_match": 0.0}
{"doc_id": 296, "doc": {"question": "where are alkali metals located on the periodic table", "answer": ["in the s-block", "group 1"]}, "target": "['in the s-block', 'group 1']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are alkali metals located on the periodic table?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Group 1"]], "filtered_resps": ["Group 1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7fc491f7f5cfdc4acf2e2998b0c3cd8061a50bb90819f8f6a86fe4b889dbec2e", "prompt_hash": "a350eb541e36b044a467f5eb521392dd7b2ee3d4bf69e5ee4a1b914221a6910f", "target_hash": "da63d9f6b28bd095c420ce631d8df91afc2e43ac48f77086ed8386914ef9330c", "exact_match": 1.0}
{"doc_id": 297, "doc": {"question": "where is the capital city of alabama located", "answer": ["Montgomery"]}, "target": "['Montgomery']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the capital city of alabama located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Montgomery"]], "filtered_resps": ["Montgomery"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b6c946b0c07225ba3e8fc44a0b4ad191fbb11d9e3d1b2bed59059ff38cabed1c", "prompt_hash": "2d4fedb32f506a3e9168cdcec29edb3dae1512107203d8a715d5c47962e57b63", "target_hash": "f7f76d1c636c7068161c553e6021ad382807ae5e67347ab223934660c007e64a", "exact_match": 1.0}
{"doc_id": 298, "doc": {"question": "what year did the us hockey team won the olympics", "answer": ["1960", "1960 and 1980", "1980"]}, "target": "['1960', '1960 and 1980', '1980']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what year did the us hockey team won the olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f5599ac9ce11346bf6222e1912e855382fc5b3553ed0d16a5f97ed7ac4c85bd", "prompt_hash": "fab2846b9a7211df1a182749ab2dbe7547bed43516ab0fe8e28c50fbd6894469", "target_hash": "786a659531b0fa34790982e936b1ba3827a1148aae2ee0934ed0e2ddebc8cc4d", "exact_match": 1.0}
{"doc_id": 299, "doc": {"question": "what is the order of the netflix marvel shows", "answer": ["Marvel's Iron Fist", "Marvel's Daredevil", "Marvel's The Punisher", "Marvel's Jessica Jones", "Marvel's The Defenders", "Marvel's Luke Cage"]}, "target": "[\"Marvel's Iron Fist\", \"Marvel's Daredevil\", \"Marvel's The Punisher\", \"Marvel's Jessica Jones\", \"Marvel's The Defenders\", \"Marvel's Luke Cage\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the order of the netflix marvel shows?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Daredevil"]], "filtered_resps": ["Daredevil"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c7c2f0117f19fa104333243833d7b14ccfd0fc8da5ff71885147675821886ca", "prompt_hash": "fc2406a5bb522d227f05db465501ddaa2a36e92747e3976d8884f9577419c810", "target_hash": "0fc66cd7b42a4c4ddc08fb505e33fbfc3ede4fd3a0512df049e01c890766b4b3", "exact_match": 0.0}
{"doc_id": 300, "doc": {"question": "who did the broncos beat in the super bowl", "answer": ["Carolina Panthers", "Atlanta Falcons", "Green Bay Packers"]}, "target": "['Carolina Panthers', 'Atlanta Falcons', 'Green Bay Packers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the broncos beat in the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pittsburgh Steelers"]], "filtered_resps": ["Pittsburgh Steelers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "13e42240abcdb0e11ab1ad8916fd795bed9dcd16bcc1d1f21ce2c75ade3d74bf", "prompt_hash": "1ba7dc49d37e2335f80196a94b291897f204bf0735f54321699b71a74823dc71", "target_hash": "2fb641f536293dcdb0d21f89b4319400f83166fe9e0d179bfccd87b14625aa06", "exact_match": 0.0}
{"doc_id": 301, "doc": {"question": "who had created the second bank of the united states", "answer": ["President James Madison", "James Madison"]}, "target": "['President James Madison', 'James Madison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who had created the second bank of the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nicholas Biddle"]], "filtered_resps": ["Nicholas Biddle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "45be356d9ab54ecc67420cbbdc94017a492af7fd93f0b55059a43e788a32e103", "prompt_hash": "0a73b6fb815eb6d636707bb56588adb94f34e84fd8283c50ce6ccfdc0dd420fe", "target_hash": "c23107000b054cc9b4d1983de18ef0ba3fa1805816fc08abdb2295c02409b822", "exact_match": 0.0}
{"doc_id": 302, "doc": {"question": "where is the highest level of fluoride stored in the teeth", "answer": ["surface of the enamel"]}, "target": "['surface of the enamel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the highest level of fluoride stored in the teeth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dentin"]], "filtered_resps": ["dentin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c3b0a8f9524db2085c5d637c6781be228dd1a47fe1fed981ec66d27e501d7397", "prompt_hash": "854050812496943868e72bd24766a0049a28042631c4c32b0b4dfc7603d48347", "target_hash": "422454a6ae0152b66c1eece36ee0717fce149e1cef80a24df8808fdb00052468", "exact_match": 0.0}
{"doc_id": 303, "doc": {"question": "when was as you like it first performed", "answer": ["1603"]}, "target": "['1603']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was as you like it first performed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1599"]], "filtered_resps": ["1599"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f50c4bb10183cfba73ebcf543979729448aa5dd87fa9f98ea057473f66e98f5e", "prompt_hash": "e097143209e54d0e8096fa97ed21f1a30b60fdf76fbae053ffc8f56ad5f67f33", "target_hash": "73ad15ec145f7b0ad71d664a2039d1552bde486b97fb698ccc4b5da061c103eb", "exact_match": 0.0}
{"doc_id": 304, "doc": {"question": "when was the latest version of chrome released", "answer": ["2018-01-22"]}, "target": "['2018-01-22']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the latest version of chrome released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "871f3bb6a20858242aef7e7ce2d2e15f177416b2d7ec98c33262efae25a45b57", "prompt_hash": "cb7d255c3d15924ff994dd26e31d801ea84698c391b1e89c50dbb6e1fddc76fe", "target_hash": "2637e0c050248f68cea8cbe649bb0f246a747a66bb2d50886dd3407f952f7565", "exact_match": 0.0}
{"doc_id": 305, "doc": {"question": "who made delhi as capital for the first time", "answer": ["the Pandavas"]}, "target": "['the Pandavas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made delhi as capital for the first time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tomar"]], "filtered_resps": ["Tomar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fc5c10be83a88d12d70fcd5a9bad4d1dbedb14b6dd3be5c628f1977fc91c6bd0", "prompt_hash": "a86268ca89a18b638ca08961d8089c1bac15c0c4aa8184a4cb58556b84086a10", "target_hash": "9ba065ebb3a9675539074e9303788a04d60b19d9ef40d1932770029c328e90a3", "exact_match": 0.0}
{"doc_id": 306, "doc": {"question": "who is tinker air force base named after", "answer": ["Major General Clarence L. Tinker"]}, "target": "['Major General Clarence L. Tinker']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is tinker air force base named after?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tinker"]], "filtered_resps": ["Tinker"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cfcf2b0c66a1614296ee47d1ed9ede4a0cbdb5a01c405e2e3f2877875ff97060", "prompt_hash": "9b2507bef463bcc5d60b7f4f1d52aee6946584f90d22373e4be03c5f255b3789", "target_hash": "d378ef71c514c1efdb80e5a8efb7406f8cad528503d2e505dcc78939021a5c22", "exact_match": 0.0}
{"doc_id": 307, "doc": {"question": "when were the winnie the pooh books written", "answer": ["1924", "1926", "1927", "1928"]}, "target": "['1924', '1926', '1927', '1928']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when were the winnie the pooh books written?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1924"]], "filtered_resps": ["1924"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19a7b019a02945044b88a76bfe8bee1bd944942981c00143b61c22f8b48d8044", "prompt_hash": "ed94c924489d2dd5d4b1b6813c2bd94db426b031029c4865984f32ea9cbe8c7f", "target_hash": "f6639306ad358cae134aee8ac47b50eb5d15ee84bda11ee6a74c2b3fb1b3db10", "exact_match": 1.0}
{"doc_id": 308, "doc": {"question": "what is final season of game of thrones", "answer": ["The eighth", "eighth", "the eighth season"]}, "target": "['The eighth', 'eighth', 'the eighth season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is final season of game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2019"]], "filtered_resps": ["2019"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "78fbec5b644559fc487d5e103e25c154e4ce325cf4bac6a3ea9ba5d2c70c3671", "prompt_hash": "49a07cf745547b9b70ef282f48a08f04eafc2b5a820f3848a805292047f2468b", "target_hash": "38ae67b82cb166dfa6b18d1351f52ab466fdd25e21686ed5e518f80f5a4158df", "exact_match": 0.0}
{"doc_id": 309, "doc": {"question": "when do new episodes of riverdale season 2 come out", "answer": ["February 7, 2018", "April 18, 2018", "October 11, 2017", "TBA", "January 31, 2018"]}, "target": "['February\\xa07,\\xa02018', 'April\\xa018,\\xa02018', 'October\\xa011,\\xa02017', 'TBA', 'January\\xa031,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do new episodes of riverdale season 2 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 10"]], "filtered_resps": ["January 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "64c653a9f62b59a7fc51b2a0abf3a342a696d37c13351b4968ac7c561fa22c51", "prompt_hash": "df8fe4fd5b396af3ed4a9b01f1b04d0a7289fb0330afc0c3c1ec5ab67e32558f", "target_hash": "3e931824d0d80f4abe3d82680800284c1925f156815e2d8cbaf4fa767b641ea3", "exact_match": 0.0}
{"doc_id": 310, "doc": {"question": "where is arachidonic acid found in the body", "answer": ["brain", "muscles", "liver"]}, "target": "['brain', 'muscles', 'liver']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is arachidonic acid found in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" phospholipids"]], "filtered_resps": ["phospholipids"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b18583bda983823d3b3e7192c11f8ab326c71d4caff6996dd28902ee2bd0eba9", "prompt_hash": "f2a012c5b0502299ef4d549ba8637e37f4515ac6b52d72bdb223f2e8d54f1d37", "target_hash": "4deddc297982195a38415b4c1315797a3dc5b55970973a049dc312614b1c5d07", "exact_match": 0.0}
{"doc_id": 311, "doc": {"question": "who has the most all ireland hurling medals", "answer": ["Henry Shefflin"]}, "target": "['Henry Shefflin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most all ireland hurling medals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brian Canty"]], "filtered_resps": ["Brian Canty"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06b57b023bafb6d924c2c3504e30510ea8b254adb9c4a52d4648ca672679ed18", "prompt_hash": "1a011223705c73b5cdc909246ebc3a3e7043626b9912f4400d041b090bb9aab3", "target_hash": "a5ae9e3c9bb161ba14753399b96f69e3058139fc92dcd43ce5aad96920a8423f", "exact_match": 0.0}
{"doc_id": 312, "doc": {"question": "who hit the first home run in the houston astrodome", "answer": ["Mickey Mantle"]}, "target": "['Mickey Mantle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who hit the first home run in the houston astrodome?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Roger Craig"]], "filtered_resps": ["Roger Craig"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "666a3077d923fde0190df6aa09f9428300e95811835df569f253a722e87738a7", "prompt_hash": "3023bfae881df88445332b5b57b02d52ede695d1b607b07cd24e7fdc05d530bc", "target_hash": "eaf026c883a853380a3181de20f94aa52d7a5134c7094c5aab7efe5b9e2aec67", "exact_match": 0.0}
{"doc_id": 313, "doc": {"question": "when did the eagles play in the superbowl", "answer": ["February 6, 2005"]}, "target": "['February 6, 2005']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the eagles play in the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c681900fdeab0f0ac9dc5d009015d32d4bbede37a9b69c6c950300fb0e2d432", "prompt_hash": "4767747b9534493589026d648311f4a8a6353644b10fb2d4efdae4f6fa86f0d6", "target_hash": "87b08e1f3ce6e335a108fe4401d321126a9ef45b28c46a1798f61683eb2ef035", "exact_match": 0.0}
{"doc_id": 314, "doc": {"question": "when will fairy tail final season be released", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will fairy tail final season be released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2016"]], "filtered_resps": ["2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3361149cc67048dbb1e784f58877906550bf8605e723587457a91d83f992ba88", "prompt_hash": "c20b7f70bd87bc1481b478279b4b6646805b198af39d01861630da7ac2064573", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 315, "doc": {"question": "when did seat belts become law in ontario", "answer": ["January 1, 1976"]}, "target": "['January 1, 1976']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did seat belts become law in ontario?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1976"]], "filtered_resps": ["1976"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d22c6c36f7a9ae2d71c537c620b531dce17c5250a6c9d0068c71e69d6fe5b6c2", "prompt_hash": "9216b88bc71875ef581bf625e19bfd0d6bb7317a6686dd54e8f9e236049ac993", "target_hash": "cfc4562ece29371d65c67043bf8974cc64908cfc70a51afa478e86ea5e9cce35", "exact_match": 0.0}
{"doc_id": 316, "doc": {"question": "when did response to state of the union start", "answer": ["in 1966", "1966"]}, "target": "['in 1966', '1966']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did response to state of the union start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1966"]], "filtered_resps": ["1966"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a6f193b402cd1936252c0ce6cc215a5e11e39c0647ab211bdd361a684802305a", "prompt_hash": "eff5d00656e33700093aa0397b930179fa8bbffc55362761acec94328809c2b4", "target_hash": "50c60ae9524d6cd3cc112742172e944e408f652e3ab91051d4273924babc126a", "exact_match": 1.0}
{"doc_id": 317, "doc": {"question": "when does madea's family funeral come out", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does madea's family funeral come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2014"]], "filtered_resps": ["2014"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfa5f8acdc6fa5d9697b6c16931979bfcf3206a1c1eb94f385e28626b0f76ea6", "prompt_hash": "0e615c3507c14e060b8ac394aad943b41c99c35c3d2b0d9c77df17906ffdbfea", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 318, "doc": {"question": "when did canada get rid of the death penalty", "answer": ["July 14, 1976", "1976", "1998"]}, "target": "['July 14, 1976', '1976', '1998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did canada get rid of the death penalty?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1976"]], "filtered_resps": ["1976"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "946024d4d0199973d6795e1871679d6d1a62664aabd7ad277e5ac9d7c2180137", "prompt_hash": "bc418706b50a647a6cb3f61866181be3c3e3842ff3ad19f0305311e27d32156d", "target_hash": "b8c58cec97dce5243f2f670fff2d9b65b416f1685ed229f37d5766a8f0f1cf90", "exact_match": 1.0}
{"doc_id": 319, "doc": {"question": "when does the new gotham season come out", "answer": ["September 21, 2017", "September 21, 2017"]}, "target": "['September\\xa021,\\xa02017', 'September 21, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new gotham season come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 11"]], "filtered_resps": ["October 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6322ef4abd26167ccffd3741057cf24fe269e47bc2e01d97e0bd33c6a1019f84", "prompt_hash": "3fa6f4545982e58cc6be2b4701fd5ef6e4402cfbc17e2375aa003aecf187e1a8", "target_hash": "b905d744aced694ff82c00aca3bfe6ed39f6effd440bfb2958da7b8010b35f14", "exact_match": 0.0}
{"doc_id": 320, "doc": {"question": "who lasted the longest in the royal rumble", "answer": ["Rey Mysterio"]}, "target": "['Rey Mysterio']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who lasted the longest in the royal rumble?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sheamus"]], "filtered_resps": ["Sheamus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "165aef37994f63174e9622029e1991ba448d0b3710fd5d22126d0c9335c67933", "prompt_hash": "4bec4a03e25697b1861182b49c7c90402fc0e142940031ebef1e228a5f06c936", "target_hash": "77eb5371b39b1b61b763b3c0d2d6ab023ea9474511adf93eb79be5caaae8ea74", "exact_match": 0.0}
{"doc_id": 321, "doc": {"question": "star wars episode ii attack of the clones characters", "answer": ["Chancellor Palpatine / Darth Sidious", "Anakin Skywalker", "Mace Windu", "Padmé Amidala", "R2-D2", "Yoda", "Count Dooku / Darth Tyranus", "Obi-Wan Kenobi", "C-3PO"]}, "target": "['Chancellor Palpatine / Darth Sidious', 'Anakin Skywalker', 'Mace Windu', 'Padmé Amidala', 'R2-D2', 'Yoda', 'Count Dooku / Darth Tyranus', 'Obi-Wan Kenobi', 'C-3PO']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: star wars episode ii attack of the clones characters?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jar Jar Binks"]], "filtered_resps": ["Jar Jar Binks"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1f850e3827b408311d1077110c9dc76b5516fec59717938fcb8c9dc4845123c5", "prompt_hash": "4e5e807916325220be779136b3ee656bb2764e79d992f2d2885b313e431d3bfe", "target_hash": "edc642b02e7e64a79ff4c8bdf669c5e245b0e105d1a7fbf438910575902d0df4", "exact_match": 0.0}
{"doc_id": 322, "doc": {"question": "what does hp mean in war and order", "answer": ["hit points or health points"]}, "target": "['hit points or health points']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does hp mean in war and order?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" hit points"]], "filtered_resps": ["hit points"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c85f6e3ed7eedf469cca41466ee52b5df79d399ea1bc4fcada616b3a4f432e99", "prompt_hash": "072d8d52957e0ff822acb2691776bb6a42934e5ff75af4b8dfc4220b5cc0cf15", "target_hash": "e180a5db108f754c7d11e0061f45f3e0f5bf5d81b736aacd33dfcdbc7a495bbe", "exact_match": 0.0}
{"doc_id": 323, "doc": {"question": "who is the owner of reading football club", "answer": ["Xiu Li Dai", "Dai Xiuli", "Dai Yongge", "Yongge Dai"]}, "target": "['Xiu Li Dai', 'Dai Xiuli', 'Dai Yongge', 'Yongge Dai']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the owner of reading football club?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ian Ayre"]], "filtered_resps": ["Ian Ayre"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f6574c50238441eedd4bb74aba4b55789caa2f1339660773b86d6c4f80e0a3dc", "prompt_hash": "d7dc25281775054a52b3540e71f6dee0d17ddf54c13982d70eac35e1ee720d4e", "target_hash": "f7914ddf6b34c57a7a405bc4f9d6954390fa2ffa201e713eec60543ddd7605ee", "exact_match": 0.0}
{"doc_id": 324, "doc": {"question": "who is given credit for the gnu initiative", "answer": ["Richard Stallman"]}, "target": "['Richard Stallman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is given credit for the gnu initiative?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Richard Stallman"]], "filtered_resps": ["Richard Stallman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32d64055ee419f568e8e9633e6319fe09aa9460fde2166c3d73ed26060e3d259", "prompt_hash": "b5c60ebc62353c768c130204ba7b98f34564049be071970cdd78c66014f191c9", "target_hash": "ccd3f1335441aa3b2347803b8fb386d17a8e8cc032aa436abcd52767f5590ce3", "exact_match": 1.0}
{"doc_id": 325, "doc": {"question": "who is edmund on days of our lives", "answer": ["Adam Caine"]}, "target": "['Adam Caine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is edmund on days of our lives?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Alan Grier"]], "filtered_resps": ["David Alan Grier"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fa59b3c8e555f8f49f7fff61f4666092d7acee1f26c1cea968462e72a3d7633a", "prompt_hash": "610ef7a0e5c95fddd88225cc83de0c53d8d601effc087cc5032f3bbb7bb97644", "target_hash": "d230b2775c545bb09d281e28d53d4b602fcc758abd02f0cb736063b56a0bee63", "exact_match": 0.0}
{"doc_id": 326, "doc": {"question": "when did the nba create the 3 point line", "answer": ["the 1979–80 season", "1979–80 season"]}, "target": "['the 1979–80 season', '1979–80 season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the nba create the 3 point line?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1979"]], "filtered_resps": ["1979"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35e9284ea958db89c5b5aaab0814b8a4534a94879f8418650a095d14426beb38", "prompt_hash": "1c118a7eb0c6345835c0ef0bf5f241346d39e4be97e238598fea8e9a9e5cadba", "target_hash": "3b6ce31acaf1dda366344e0556857fc38c4e743a9e7b316aefc4b428986789d4", "exact_match": 0.0}
{"doc_id": 327, "doc": {"question": "what is an example of a tricyclic antidepressant", "answer": ["Amineptine"]}, "target": "['Amineptine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is an example of a tricyclic antidepressant?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" amitriptyline"]], "filtered_resps": ["amitriptyline"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2e990d7151c0f574d7525603f604f36b50fcc0037499a88fb783ef0bf076f536", "prompt_hash": "89bdd591f7ef732bdcd9c5eca674de4f1af59d72b93adce2e13a25e3de902c47", "target_hash": "50556b62d4d3aa06009a1e7073faa638e7a130cb6df508d5b660efa0eee6c435", "exact_match": 0.0}
{"doc_id": 328, "doc": {"question": "when was the canadian pacific railway started and finished", "answer": ["between 1881 and 1885"]}, "target": "['between 1881 and 1885']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the canadian pacific railway started and finished?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1881"]], "filtered_resps": ["1881"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "05613fb30c62a83f0539c437a48a1eec67c4d9a0e01e2a40c7b987d37e91293a", "prompt_hash": "001078f3eb0695f5be28aff2969542be072680b9bfc478e7952b74f2ab830f62", "target_hash": "a5909665eb5e2129e01447ae2433885abed89bc21d1a08f807d134a47977692e", "exact_match": 0.0}
{"doc_id": 329, "doc": {"question": "who played the creature on jeepers creepers 3", "answer": ["Jonathan Breck"]}, "target": "['Jonathan Breck']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the creature on jeepers creepers 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Boreanaz"]], "filtered_resps": ["David Boreanaz"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "454a9bf4478a25e2633a8491c955121bb31e7f1002a97f32450622d8e06a08d1", "prompt_hash": "be0caa4077ad9a311e4264e665b6daab3b680ae4e310668159ac58f11ca93ce2", "target_hash": "dde0d7baf086c62307f677c26ddb83c7c776a25b3100400445c7cdf194b82058", "exact_match": 0.0}
{"doc_id": 330, "doc": {"question": "who plays unis in she's the man", "answer": ["Emily Perkins"]}, "target": "['Emily Perkins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays unis in she's the man?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Amanda Bynes"]], "filtered_resps": ["Amanda Bynes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "714963e674d105ea6ecc253b9f79410747d13273a1f65a7d3b78ff1e5939558a", "prompt_hash": "c02f73782a0033de7a4754a40961ed782b3315346daefdd1dd47eef924b4c30c", "target_hash": "805f91a9fb6d463dfa04122a954af9b2220c239cd0448d2dade705ef01b143ca", "exact_match": 0.0}
{"doc_id": 331, "doc": {"question": "who played the mom on what's eating gilbert grape", "answer": ["Darlene Cates"]}, "target": "['Darlene Cates']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the mom on what's eating gilbert grape?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Darlene Cates"]], "filtered_resps": ["Darlene Cates"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "76668a8667f6046e7c9989a19a7514ed5f133307081a736e446e52bb5d244f32", "prompt_hash": "db7f3055c4ca2210741efad35c903d09ee0fe816b81629c4b73c1d25512d82b7", "target_hash": "1aa83d66ce03e740dee2ce33f2a4ba59b2c30787b8634bba8465305c80e7a978", "exact_match": 1.0}
{"doc_id": 332, "doc": {"question": "the fertile crescent is located between what two bodies of water", "answer": ["Tigris and Euphrates rivers", "Tigris", "Euphrates"]}, "target": "['Tigris and Euphrates rivers', 'Tigris', 'Euphrates']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the fertile crescent is located between what two bodies of water?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the mediterranean sea"]], "filtered_resps": ["the mediterranean sea"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a0ac176c673dbee63d95ddc4cff9d75f3c4768a1c669b7bf22c648c2808d8e36", "prompt_hash": "8d31c1c20df9a2fdeb5f2df410047c98a7c64d1495ff7b803c44cdc6cdf63ba4", "target_hash": "d017baa1ca587cf0544f4f83d9600eab8100430010d5a1668fdcee4ba2138432", "exact_match": 0.0}
{"doc_id": 333, "doc": {"question": "what episode does goku give up against cell", "answer": ["165", "180"]}, "target": "['165', '180']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what episode does goku give up against cell?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 28"]], "filtered_resps": ["28"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "812f6f2ce8cc3cf7a2b9a0fb4a70a4b74b672c8d26a43ca07325d7423ef2084b", "prompt_hash": "4e206f3915c0436756e0708ae26886203d528a0c41e23935abf8040d281390a8", "target_hash": "e5e8e0e2bb6fc2e43a15a882e259543ac96dca37caabe6d7b3d6eda154d57b17", "exact_match": 0.0}
{"doc_id": 334, "doc": {"question": "sri lanka belongs to which part of asia", "answer": ["South Asia"]}, "target": "['South Asia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: sri lanka belongs to which part of asia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" South Asia"]], "filtered_resps": ["South Asia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b7dc0642a0b06e4002fdecf25479606bc5878c1a83d8c7aaea643e20a53bc1f", "prompt_hash": "1379853fc0a7b833de56a4b9346c065305468e8fb8d98893b7216c85f6bb206b", "target_hash": "f053c5497a42d49df5af1ae923ce51c24c41f268a65976de91edf3c08da7b885", "exact_match": 1.0}
{"doc_id": 335, "doc": {"question": "when was the first nuclear power plant opened", "answer": ["December 20, 1951", "June 27, 1954", "On June 27, 1954", "the USSR", "December 2, 1942"]}, "target": "['December 20, 1951', 'June 27, 1954', 'On June 27, 1954', 'the USSR', 'December 2, 1942']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first nuclear power plant opened?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1951"]], "filtered_resps": ["1951"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1aca943f1af9b1926a5f42c6617ba029717b12ea001e43e0262fedc704d19558", "prompt_hash": "e2c47294b75931d5f4b6312ad985c08a3695c92c2af94355bb2d5d132e53b3ac", "target_hash": "49a91a1c153439e078cbc29f35e9eb213eed5fabca4a31e8fa3f474472f7d7a0", "exact_match": 0.0}
{"doc_id": 336, "doc": {"question": "who plays the dad in nanny mcphee and the big bang", "answer": ["Ewan McGregor"]}, "target": "['Ewan McGregor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the dad in nanny mcphee and the big bang?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jim Broadbent"]], "filtered_resps": ["Jim Broadbent"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e3468b9b264445a9b8219f359705f1aafcda2694c80b0c2d7aced21d5a994e39", "prompt_hash": "c59ee3d19371852a2c588d9ea5d4566b0305cf8fbc91eaf475f548bb15089597", "target_hash": "3ead49878ee5f9858d8e362c1525cf2e0430fe136d1594b6b5e27f47b7e809b4", "exact_match": 0.0}
{"doc_id": 337, "doc": {"question": "when was the taming ofthe shrew first performed", "answer": ["prior to June 1592"]}, "target": "['prior to June 1592']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the taming ofthe shrew first performed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1592"]], "filtered_resps": ["1592"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0c2c786766eb38c94fdfa54cfd96398862ebff992507ee0c9a384ce62e02cedd", "prompt_hash": "074246ee43e6d9fbab4b0abeff6cecb4c1274b8af6906b19ec7883245ecb5e1f", "target_hash": "270ea3f4e5c29db12bfd8030ae8899b2c315c2a631274f9819c48ce4620fb329", "exact_match": 0.0}
{"doc_id": 338, "doc": {"question": "how many languages in harry potter translated into", "answer": ["over 74", "over 74 languages"]}, "target": "['over 74', 'over 74 languages']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many languages in harry potter translated into?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 60"]], "filtered_resps": ["60"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "87146451ea2031794731e017d9ca93fa951acaa1cb1642cbc8300361ef441197", "prompt_hash": "3cda53d4025e609bad4bc96f90eb87026452f08a71c34b4c1898dfeaee75a0fd", "target_hash": "ad647a6c793312eaee7634107fc1b355e38f0dde890b6da85875ad149efa7d91", "exact_match": 0.0}
{"doc_id": 339, "doc": {"question": "when was the last time giants won superbowl", "answer": ["2011"]}, "target": "['2011']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time giants won superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1992"]], "filtered_resps": ["1992"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "238a0f4be5ee60d1897d00283179e4149ef71697b1142e057499eb561a4b4c65", "prompt_hash": "54b1fe8cddace9f1acaa6eb2b84ade755d25ba43253a63d3452ca68007b15da9", "target_hash": "3fd77e6a3c8aa1507fbee4fc02c1a1a3ef1013d169b015b0c71135a2990b6664", "exact_match": 0.0}
{"doc_id": 340, "doc": {"question": "who are the co hosts on the real", "answer": ["Tamar Braxton", "Loni Love", "Adrienne Houghton", "Tamera Mowry-Housley", "Jeannie Mai"]}, "target": "['Tamar Braxton', 'Loni Love', 'Adrienne Houghton', 'Tamera Mowry-Housley', 'Jeannie Mai']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the co hosts on the real?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kandi Burruss"]], "filtered_resps": ["Kandi Burruss"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7db4ee552ce49196349e1df311d5d7d3fab178fab75e4478ad9c5769b03aae5f", "prompt_hash": "35d8792bbdb64785fd4aa77e0d7a502f438f82c0e7d2c715991acb91882692cd", "target_hash": "66475ca733ad50fcb390c13fd8f9256ee7686e733eabec685b212a1f12fbe7ea", "exact_match": 0.0}
{"doc_id": 341, "doc": {"question": "to transplant a nucleus from one cell to another cell a scientist would use", "answer": ["somatic cell nuclear transfer"]}, "target": "['somatic cell nuclear transfer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: to transplant a nucleus from one cell to another cell a scientist would use?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cloning A: embryo splitting"]], "filtered_resps": ["cloning A: embryo splitting"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e9d4135c2e7f7e437e45a0197664d74f1f56dd1a460c420615543f4dcf727809", "prompt_hash": "1af05328e59f8a9cbdd1e608da3d7ae71f50b7bb84cd8b5c762c901cb46826e8", "target_hash": "0ad3b00294a2052644f5803055275d9a9ab99c7d4aafcf619e2f8a34aee97f62", "exact_match": 0.0}
{"doc_id": 342, "doc": {"question": "when did the golden state warriors win the finals", "answer": ["1947", "1975", "1956", "2015", "2017"]}, "target": "['1947', '1975', '1956', '2015', '2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the golden state warriors win the finals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aea6d2a71ad5876e324b8beda2b506e5a2b9d1257557be5c7a3e864b2a27472b", "prompt_hash": "7f50d8abab01494578ca0bfcf044a2605ab6c17423a74920e3a6b309c0394251", "target_hash": "3e6ca136ad38f6f6211f345828c4326f30e0318ccf5885132b45289d98acb306", "exact_match": 1.0}
{"doc_id": 343, "doc": {"question": "who plays general hux in the last jedi", "answer": ["Domhnall Gleeson"]}, "target": "['Domhnall Gleeson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays general hux in the last jedi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Domhnall Gleeson"]], "filtered_resps": ["Domhnall Gleeson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3e21bd01dd052a950a53ddfc0f56910170aad03464802a25da81908c796a432", "prompt_hash": "cacbc7162b73f6d2f741a93a8bbc18379563a33502aa8501122ff45ec6cdd765", "target_hash": "3f9d887600577f73dabfce40c27025f65ffcfe07f5be11fcdac27039b446c492", "exact_match": 1.0}
{"doc_id": 344, "doc": {"question": "who ran the fastest 40 yard dash in the nfl", "answer": ["Jakeem Grant", "John Ross"]}, "target": "['Jakeem Grant', 'John Ross']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who ran the fastest 40 yard dash in the nfl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ray Lewis"]], "filtered_resps": ["Ray Lewis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e8b510e330afe56e6c5c6982c30cffd62690a976ae7143fc44c40102b8bd31a4", "prompt_hash": "854821c60a82409dabb4d576a962fdf4eac7255256e24538967326fb0740fc58", "target_hash": "bc1059b1aa7b7c4dc46e6cb282c2c12897dfd607ef1fb1c7403a9c394b53c0c0", "exact_match": 0.0}
{"doc_id": 345, "doc": {"question": "when was the last time new zealand had an earthquake", "answer": ["11 Jul 2017"]}, "target": "['11 Jul 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time new zealand had an earthquake?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 22 February 2011"]], "filtered_resps": ["22 February 2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a26f1955aac44eb6da9100611b97dd967750f2ad090eb45fd9d8b21e5f06b806", "prompt_hash": "af03ad17d041311ff1bbd6ec48146e5ce8f784069a336aa19615b3d445672df2", "target_hash": "2aa67123ce47d6d10bc7d976c1ec1acd00a78773e7c0b3ffd41a1a807f6c88df", "exact_match": 0.0}
{"doc_id": 346, "doc": {"question": "who is the actor that plays dr. sean murphy", "answer": ["Freddie Highmore"]}, "target": "['Freddie Highmore']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the actor that plays dr. sean murphy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Benedict Cumberbatch"]], "filtered_resps": ["Benedict Cumberbatch"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e64b3cf753c6af2d2f60194b73c4778706dbca849d5b06e141330cf2f24ff0e6", "prompt_hash": "4e62c9a2788c0b218263466acd3899de67b8bb8a0424d5e2bfe26d5e91df5c31", "target_hash": "fbc722580e478568dd81cb45ab8714f4fb2b80b53667b8b59f6ed0ce6e8d26f1", "exact_match": 0.0}
{"doc_id": 347, "doc": {"question": "who played gino in a place to call home", "answer": ["Aldo Mignone"]}, "target": "['Aldo Mignone']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played gino in a place to call home?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew Daddario"]], "filtered_resps": ["Matthew Daddario"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8848b2fbddd9e43e4ae6ef4fa66d2e6c8ad0024f5b1147d807e6204379a27bbd", "prompt_hash": "75086982b2f63f811ffca7cb1db3cb3d57dcfc2af35e3c3412424d20bbeb35ca", "target_hash": "b0bf4ba7a185b8c3c4672d6017b49a527e18e10f9aab54f1fc3bb896954639ea", "exact_match": 0.0}
{"doc_id": 348, "doc": {"question": "what is the pirates of the caribbean in order", "answer": ["On Stranger Tides", "At World's End", "Dead Men Tell No Tales", "Dead Man's Chest"]}, "target": "['On Stranger Tides', \"At World's End\", 'Dead Men Tell No Tales', \"Dead Man's Chest\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the pirates of the caribbean in order?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dead Man's Chest"]], "filtered_resps": ["Dead Man's Chest"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a19420fdb517f4bdd8a9da175d370ec8e75fad1bee55ea6dfccb739385ccf68", "prompt_hash": "09447030242bba3feb674fb7af12c6978e8c40a891214c4bf87ee3b0f84a6f34", "target_hash": "e308cb80fb86b18edde4dbf6052845a9d89ac4ac28c3d838a27ab1e5e783c499", "exact_match": 1.0}
{"doc_id": 349, "doc": {"question": "what does the pearl symbolize in the bible", "answer": ["the Kingdom of Heaven"]}, "target": "['the Kingdom of Heaven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does the pearl symbolize in the bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sin"]], "filtered_resps": ["sin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d9dd3b51f779d8d1827237e31a00186900927c00c4b6dd295ebb92c42f41882", "prompt_hash": "aae6ddeb7f140f12554846a545b289dbf302c6ae4cd0c9ba57ad87aace568b28", "target_hash": "9a90f1fbf630ee7b07fdc6c642ee99f5cd1541c515d00eac966e60d58fe67cfe", "exact_match": 0.0}
{"doc_id": 350, "doc": {"question": "what album is sacrifice by elton john on", "answer": ["Sleeping with the Past.", "Sleeping with the Past"]}, "target": "['Sleeping with the Past.', 'Sleeping with the Past']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what album is sacrifice by elton john on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The very best of Elton John"]], "filtered_resps": ["The very best of Elton John"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "63f4c8f35deffd66ee435f33d0aaf02fe0a41117cc986fbd1333f036ae98116c", "prompt_hash": "2000dda1a2ad32723333cbd3dc4b4af79917c0e4d16380c2149bb89dbad145d4", "target_hash": "8128e2520a06d6795fd276e222363f321b7b0551234efbc9220bd098bbc1aaff", "exact_match": 0.0}
{"doc_id": 351, "doc": {"question": "who proved that cells come from other cells", "answer": ["Matthias Schleiden", "Robert Hooke", "Robert Remak", "Theodor Schwann", "Rudolf Virchow"]}, "target": "['Matthias Schleiden', 'Robert Hooke', 'Robert Remak', 'Theodor Schwann', 'Rudolf Virchow']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who proved that cells come from other cells?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rudolf Virchow"]], "filtered_resps": ["Rudolf Virchow"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e7b05a4cf00616b40332d5c6331ce6667d8d138930877970c47104509719c250", "prompt_hash": "18806a8bb579a73018540e22530d8ee24d8e63dc47f070f9d3639c73a2adac0d", "target_hash": "a7d367e96eb232163779abf6cfb1a3beae077acb793f6917a58af7f30bcddae9", "exact_match": 1.0}
{"doc_id": 352, "doc": {"question": "who holds the world record for the most world records", "answer": ["Ashrita Furman"]}, "target": "['Ashrita Furman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who holds the world record for the most world records?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rick Strassman"]], "filtered_resps": ["Rick Strassman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2bb1ea0a5b8c9ecca8e812e397dc0366a9fc993d92c2036d3f097757a4c3a795", "prompt_hash": "4f4899fa2efcff0ad3ff7aa69dae35422a76129728bdd0d115aa85c317bbc922", "target_hash": "212243c8689b4c2980981615eb80a60fea5ac186a68a740abeb6436a6184e260", "exact_match": 0.0}
{"doc_id": 353, "doc": {"question": "who is the designer in devil wears prada", "answer": ["Valentino Garavani"]}, "target": "['Valentino Garavani']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the designer in devil wears prada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Miranda Priestly"]], "filtered_resps": ["Miranda Priestly"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "573ca08ccc0b9d8fe4cfc79be5a253a2965babbaeb291b26acd89d12ea5072d4", "prompt_hash": "6c0062aed44cc8b36ae41ffe1bfce5830a6eb69e495850f24a8b4aeeaaf6473c", "target_hash": "d21f6c50c7b485619c10801e8542acb19ffaad7ac95a7a12ed5a8041be1e0c24", "exact_match": 0.0}
{"doc_id": 354, "doc": {"question": "where does dividends go on cash flow statement", "answer": ["the financing activities section"]}, "target": "['the financing activities section']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does dividends go on cash flow statement?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Financing Activities"]], "filtered_resps": ["Financing Activities"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c343471a99d9d45d06fc4fa982f2532ab96e94da96ff4769bdda2511a468349", "prompt_hash": "9195e268d3a4fa10e00f93ed22988249212dffbcf6177edd3357fe5930b7c579", "target_hash": "a7fb7fd7cc00393f2387b1f339d01cb555d5675e923d3d45e44465a7468cad74", "exact_match": 0.0}
{"doc_id": 355, "doc": {"question": "when was the 50th star added to the united states flag", "answer": ["1960"]}, "target": "['1960']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the 50th star added to the united states flag?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" July 4"]], "filtered_resps": ["July 4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "daac667cb612668d5eb10e9dae08e6ee5656f9096faafdf5171c6e7a3316f31f", "prompt_hash": "b9b5380a9b24abfa4ccacb0d80ba86b7bc5e8b1a1252314ccf91dbe6c6aeb322", "target_hash": "e6eb59589fc11b4a06dcdbd45c76c091d0b77679983b180d59f554396ab8d2a8", "exact_match": 0.0}
{"doc_id": 356, "doc": {"question": "what is the ethnic background of the shib sibs", "answer": ["Japanese"]}, "target": "['Japanese']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the ethnic background of the shib sibs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Asian"]], "filtered_resps": ["Asian"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e4ee8865b22efb227a86765c9afd5dab495e64ac9b998724d6a0de17ec395cd7", "prompt_hash": "d8ea1fdffcc3f7ea55a132eed63d901092353a010eadcd31ed5e737f69aea07f", "target_hash": "a246467e6280cadb27228e18c5252f36a7a851f613e193c7f9c99c4a772f0d70", "exact_match": 0.0}
{"doc_id": 357, "doc": {"question": "name the four major layers of the earth in order", "answer": ["a liquid outer core", "an outer silicate solid crust", "a highly viscous mantle", "a solid inner core"]}, "target": "['a liquid outer core', 'an outer silicate solid crust', 'a highly viscous mantle', 'a solid inner core']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name the four major layers of the earth in order?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" crust"]], "filtered_resps": ["crust"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d8952acd52b3d52d4c19c12c0c305a5de6a42e3ba8459d0acd595e9ba2dcec06", "prompt_hash": "70bd561001a2510ee66fb00678c8f7a0841fb43438bba7b213faee47e95f178e", "target_hash": "4ca54fadbeb81c3fcca7826df41e36a41ef3fcc66b48d61b62737ed65b9b600e", "exact_match": 0.0}
{"doc_id": 358, "doc": {"question": "how much for a passport in the philippines", "answer": ["$60 abroad", "₱950"]}, "target": "['$60 abroad', '₱950']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much for a passport in the philippines?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Php 2"]], "filtered_resps": ["Php 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ec5581da100c0e99aee9524ea5630ccad83543b7c35c0e1fa269fbd363661d41", "prompt_hash": "94167dfc10ce77ccabeba1f5f8027866233f53463000205cb55a90b159511b72", "target_hash": "86be4a8791ea9ea95bd94dfd4151576cae5968ce04fce2d388791804fed6d27f", "exact_match": 0.0}
{"doc_id": 359, "doc": {"question": "who was mr.owen in and then there were none", "answer": ["Justice Lawrence John Wargrave"]}, "target": "['Justice Lawrence John Wargrave']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was mr.owen in and then there were none?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Owen"]], "filtered_resps": ["William Owen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "203f93fe10615f3815cf3cbfea84385ffa602490d7f0541029cc3f387875adb6", "prompt_hash": "aecf29beebbfe227f62d0f1d0e7a056c3567d203863dcf74bc298a51cc6c18e9", "target_hash": "a63080bcee7b45bcfe0cb1b2888deb768b21a2cf3c159c9527a28e6a76425fa9", "exact_match": 0.0}
{"doc_id": 360, "doc": {"question": "when did gaurdians of the galaxy 2 come out", "answer": ["2017", "May 5, 2017"]}, "target": "['2017', 'May\\xa05,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did gaurdians of the galaxy 2 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" August 19"]], "filtered_resps": ["August 19"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2d945c74dd25d9c725fbb92043f515a82845ab26a497a3683dcebf24dbf993f4", "prompt_hash": "ef1838ade0792534d09d3335d10f53b37e75ad9b8f468c032176ea2242623625", "target_hash": "b3af694abb2fddeea4910f5bf8d31daae91a74ddb6a670e85e7dc75ae488a9f5", "exact_match": 0.0}
{"doc_id": 361, "doc": {"question": "kings and queens of england in the 1900s", "answer": ["George V", "George VI", "Edward VIII", "Elizabeth II", "Edward VII"]}, "target": "['George V', 'George VI', 'Edward VIII', 'Elizabeth II', 'Edward VII']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: kings and queens of england in the 1900s?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Edward VII"]], "filtered_resps": ["Edward VII"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9b3b3ebd8744e8ca4682b91d6ba18d68726b4a86dbc56646094fa81a789f0279", "prompt_hash": "a114fc838f37f7b744eae01c591fc1f9c0eeb09332b455ab4536e4ca07e13ddf", "target_hash": "d26973cfe2fbfa9605b75cd03f5dd0023d73c28aa067345a60762f25ca341227", "exact_match": 1.0}
{"doc_id": 362, "doc": {"question": "who made the poppies at tower of london", "answer": ["Paul Cummins", "Tom Piper", "stage designer Tom Piper"]}, "target": "['Paul Cummins', 'Tom Piper', 'stage designer Tom Piper']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made the poppies at tower of london?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Royal British Legion"]], "filtered_resps": ["Royal British Legion"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "89a563bdfaf2494899ad24ad382ebbb7a67e2a2b13b4cc89e0f3c29069831f3a", "prompt_hash": "84c901dff4c82c464ee1495df04024435f4cc1c99390e08d90c83b93069005d6", "target_hash": "b50bad4faa9a9964d5f5b34c9777543eb49c8ba4626d80fac80023aa7fcccb02", "exact_match": 0.0}
{"doc_id": 363, "doc": {"question": "what season does bart bass die in gossip girl", "answer": [")"]}, "target": "[')']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what season does bart bass die in gossip girl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 5"]], "filtered_resps": ["season 5"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ef33b0eccb9f3ffd45b03f0ebe5b0178e2b21b51389ef3ae8d17413d5d371f5b", "prompt_hash": "02361cd19fb39b97a44185decf14c18e6c71f9e7148a1d086fa9c3f788c6800d", "target_hash": "b411e9d7b98749894b1fb63e42ad7624be49dfee8ff18ee6df359fff3a3a0869", "exact_match": 0.0}
{"doc_id": 364, "doc": {"question": "who is the first indian woman to be canonized as a saint", "answer": ["Saint Alphonsa"]}, "target": "['Saint Alphonsa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the first indian woman to be canonized as a saint?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Teresa of Kolkata"]], "filtered_resps": ["Teresa of Kolkata"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "39946e548c0eb8c72a962eb8005c4745627dbc48647fc3e1beeba14f9aac513d", "prompt_hash": "706f1769ac2af427bd5ecf32eb5302042800c4a0b0339433f94934a9fc31a020", "target_hash": "b9ce9085584430aa7de0a4dbfb2fb58c70585d4f9ddd7794027a695052df57a6", "exact_match": 0.0}
{"doc_id": 365, "doc": {"question": "who is in charge of ratifying treaties in the us", "answer": ["the Executive branch", "the President", "Senate", "the Senate"]}, "target": "['the Executive branch', 'the President', 'Senate', 'the Senate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is in charge of ratifying treaties in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Senate"]], "filtered_resps": ["the Senate"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d1ed9df22d33b42c6b664d424dcc0d4bddbe968af46fc7c01d1cb238dfdff390", "prompt_hash": "fe1d2737fdee6a079ab4cdc049411fdd6106f225d4fbed795ef5cc45076408cd", "target_hash": "4ef42ce5c798eea3b4fe7e972e9f2cf7ef31d763676b205217168ade4146dc6f", "exact_match": 1.0}
{"doc_id": 366, "doc": {"question": "what is the name of the first earthquake early warning system", "answer": ["1991", "The Mexican Seismic Alert System"]}, "target": "['1991', 'The Mexican Seismic Alert System']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the first earthquake early warning system?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Japan's earthquake warning system"]], "filtered_resps": ["Japan's earthquake warning system"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a07030367e1a8681e55167c545b846a95b940a045241b3aa1ac4a9a9673b633", "prompt_hash": "3168b5536a3c3104420540879418de759ad11fad33d6cafb68f08884d531e3ae", "target_hash": "e6545ce31436eab15141efb8da037c187b46bfd1452667ca4df8e86e3e5afc08", "exact_match": 0.0}
{"doc_id": 367, "doc": {"question": "when was the last time the military drafted", "answer": ["1973", "1972", "December 1972"]}, "target": "['1973', '1972', 'December 1972']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the military drafted?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1973"]], "filtered_resps": ["1973"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "de032d14db69510bf54911374a75c6c3a54f543c06dda3f107fe2cdb732805a0", "prompt_hash": "cf0fe3c86ba1290e23ea6b1ac0ff27138100ff470b4f32e2f77d718ec7e50a16", "target_hash": "731ddd3d1dfea02dfbfecda3c5990c09e59c78acbd3da557e828fcecd810d297", "exact_match": 1.0}
{"doc_id": 368, "doc": {"question": "how many episodes of corrie has there been", "answer": ["9,436"]}, "target": "['9,436']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes of corrie has there been?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2000"]], "filtered_resps": ["2000"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5034086841200b6e7c27432d883b938b777487216329b8bf9192eaf5c05157df", "prompt_hash": "ade3b57937f21191d8984ad0f0c45017005d6cc7aaabbcf46384186ca992ffdd", "target_hash": "eb093b10835c3b36557591282d90177351b23c2f47ec2e1c32c701e1beaf69e6", "exact_match": 0.0}
{"doc_id": 369, "doc": {"question": "when do primary ossification centers appear in an embryo", "answer": ["prenatal development"]}, "target": "['prenatal development']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do primary ossification centers appear in an embryo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 8 weeks"]], "filtered_resps": ["8 weeks"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "160415749e08519c6bfd56f32e92ad3f00d63ff366251238c099ded1e5163b37", "prompt_hash": "95c0b8af473951a3999d6ace189f7578e72699fa3b4e5975bbe0b8fa40627e52", "target_hash": "82e7f7bac50c625e65a294d160f66234ceb54530a622df6e5b763139b94be293", "exact_match": 0.0}
{"doc_id": 370, "doc": {"question": "when was the biltmore house opened to the public", "answer": ["March 1930"]}, "target": "['March 1930']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the biltmore house opened to the public?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1930"]], "filtered_resps": ["1930"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bec8b79fd57faeaac35b81ff072f59c5f52f7d0e42fa41e01b7b14de10443534", "prompt_hash": "7fd5e4cf1dce5513090aa39a64228c669846504550c784962b072d3e341c9ff0", "target_hash": "358f640516e83a42f4260c8ee920bdfbc5d7b4936c4e0a0400a4d39189e936fd", "exact_match": 0.0}
{"doc_id": 371, "doc": {"question": "most of new zealands land supports which economic activity", "answer": ["agriculture", "Pastoral farming"]}, "target": "['agriculture', 'Pastoral farming']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: most of new zealands land supports which economic activity?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" agriculture"]], "filtered_resps": ["agriculture"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e3673809e632a7ba116bb9f5317500032bb9ebe5b8caeb35368f8b0e180a2256", "prompt_hash": "0037ab734a8ae6a10260c9462f5b4f1b29249b859cde0ff2cfe5938c1a90cd65", "target_hash": "43e4cca31bb88e11727819bd6fbf97bf571c01db7b76b9399596ad522dd7bc1c", "exact_match": 1.0}
{"doc_id": 372, "doc": {"question": "where does hydrogen peroxide come from in the body", "answer": ["nearly all living cells"]}, "target": "['nearly all living cells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does hydrogen peroxide come from in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" liver"]], "filtered_resps": ["liver"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "671c023fe5c9bc8b2a9a816026aaf4c4440aa8e577660b01113aa8d47d1bb172", "prompt_hash": "e0cebe88ff78d1686425614e266389c1092a82085855fb27edc4e577dddea9f2", "target_hash": "a7ac5d82453df536a7ca52433ade97b8f775ff0a03c5aee0cdacef9010cc7558", "exact_match": 0.0}
{"doc_id": 373, "doc": {"question": "who wrote old flames cant hold a candle to you", "answer": ["Pebe Sebert", "Pebe Sebert and Hugh Moffatt", "Patricia Rose Sebert", "Hugh Moffatt"]}, "target": "['Pebe Sebert', 'Pebe Sebert and Hugh Moffatt', 'Patricia Rose Sebert', 'Hugh Moffatt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote old flames cant hold a candle to you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul Anka"]], "filtered_resps": ["Paul Anka"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "83da6ae613ffd2152cc087d25f843be36aabbfde9d0057e96d80934a732ca0d4", "prompt_hash": "08dfc076244e8b779110f3fba1e149b09908ac3cd5253d685abd3eb8f5af4ca2", "target_hash": "f350c6bb254065e3f654a5222a1a228273868552938b6eeb93cfdac6f87187b4", "exact_match": 0.0}
{"doc_id": 374, "doc": {"question": "where was the killing of a sacred deer filmed", "answer": ["Cincinnati"]}, "target": "['Cincinnati']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the killing of a sacred deer filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Greece"]], "filtered_resps": ["Greece"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d8dc9a499ffe73a6e04305a2aaa512de57d9e1fe8d8c84dff35692e0814fbb4e", "prompt_hash": "37e4fc69929c39583c6240eed511d976536d11f55f6ab568b1093874eaa0c14a", "target_hash": "1bcc8ce189a8722b873c52b9763fa57177c34f6d830359fc78ce65f1015fe6a8", "exact_match": 0.0}
{"doc_id": 375, "doc": {"question": "who plays hannibal in silence of the lambs", "answer": ["Anthony Hopkins"]}, "target": "['Anthony Hopkins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays hannibal in silence of the lambs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Anthony Hopkins"]], "filtered_resps": ["Anthony Hopkins"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9e8ee379c1fada06174467fc0c8a9daf33c2a4265f1adc9872a09cb160631a7", "prompt_hash": "42898a9c4e26da21bcb4af5f08cc95ded0e5d53b8382491409769d0d20ecf38f", "target_hash": "4bc4ac87af692d4a30425ac043bdc7b3ed4e1ce7776d65ede54204688166ff10", "exact_match": 1.0}
{"doc_id": 376, "doc": {"question": "who plays the dragon queen from game of thrones", "answer": ["Emilia Clarke"]}, "target": "['Emilia Clarke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the dragon queen from game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lena Headey"]], "filtered_resps": ["Lena Headey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5fb9418b4dc3a1a6838b303b0cb8328d2e2e32d929ee04c5ae16996c619b4a4b", "prompt_hash": "25f8a4425cf9bd1c23f8bfcc1f0ea3e137f38552781d3291407872f9afe64479", "target_hash": "b1524385fa8697fffaeabd43db206e0ddfb1e30607378bee054fcda07c96b00c", "exact_match": 0.0}
{"doc_id": 377, "doc": {"question": "when did the first ice age come out", "answer": ["2002", "March 15, 2002"]}, "target": "['2002', 'March 15, 2002']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first ice age come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1992"]], "filtered_resps": ["1992"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7644933184e1338cee6801f22b00a2fd1b57e0d58ad0b0c7a4d64c62bc0849f7", "prompt_hash": "be13a049d2eff9f7c868522e46f52d89823f553f50f8bce400f33d2a18373b0f", "target_hash": "35734d4482a844f936cbaa5795d515f9e72ad8819168fabc83ab9eb2e2887b2b", "exact_match": 0.0}
{"doc_id": 378, "doc": {"question": "when was the term social justice first used", "answer": ["the 1840s", "1840s"]}, "target": "['the 1840s', '1840s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the term social justice first used?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1843"]], "filtered_resps": ["1843"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "af6d2ff7856dde72555c96f6493df04141e6d6530e14760f7d2b7183668057a4", "prompt_hash": "304db8935f6e83662080c550b57fc6436626cf7acf345381a941e778f34928c1", "target_hash": "f739f292bd04678728d1cd12242fe369ccf92c7f1e9e7bda426499bc23ffaf85", "exact_match": 0.0}
{"doc_id": 379, "doc": {"question": "when is the publishers clearing house sweepstakes drawing", "answer": ["just after the Super Bowl"]}, "target": "['just after the Super Bowl']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the publishers clearing house sweepstakes drawing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" December"]], "filtered_resps": ["December"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e774c5ca68ec55bbbc9ecd45189c40182ed4f620458bdc141ec20642f2c385ac", "prompt_hash": "86f11249e2c690f6777b60401cd15f4808f3cf0011d9132f095b9bd0fd30e398", "target_hash": "e47762642a3e7a72014ef53949c630ccc7cc8939ae5528e9a026cd76d813bf8b", "exact_match": 0.0}
{"doc_id": 380, "doc": {"question": "who played the elephant man in the film", "answer": ["John Hurt"]}, "target": "['John Hurt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the elephant man in the film?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Hurt"]], "filtered_resps": ["John Hurt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cc180dd8f8aa03010a8d2bec4e9a55f78454c76303ac1d86d1b62fe41c35a655", "prompt_hash": "2fff660b0606b26003a2b6ed716e4bea04cede710b4063ea5edc471188fb50e2", "target_hash": "710ccff9331cb123ccd45bea19f039cd1fa3a3e676ba74a51260312b1fc8dc3b", "exact_match": 1.0}
{"doc_id": 381, "doc": {"question": "when was the last time the ducks won the stanley cup", "answer": ["(2006–07)", "2006–07"]}, "target": "['(2006–07)', '2006–07']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the ducks won the stanley cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2006–07"]], "filtered_resps": ["2006–07"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a9ed200cbd7d7e6b329d252c5b83d5b80e54d28bb0121b3da8f936575eb26a3", "prompt_hash": "e89b660fa9bfd7221370b3d710c11e487916f7c54f2bb6d5a570a0ac6c3f21a9", "target_hash": "46cb6cdee76eb1a50ff9c937311d832fa4ce75e6c5a63453551147a3d2f6a1e6", "exact_match": 1.0}
{"doc_id": 382, "doc": {"question": "where do rory and paris go to college", "answer": ["Yale University", "Yale University,"]}, "target": "['Yale University', 'Yale University,']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do rory and paris go to college?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yale"]], "filtered_resps": ["Yale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5bd2e8d6e7637bbb090ebaa8abc03f43ec98d15bab1d74267d6b902f96332838", "prompt_hash": "b38a7bc707d6068da0dda959154ef2f788acdabbe6697a3cdf7b49faeecc0ae5", "target_hash": "0ba855d16765325bde0ca3c1fbe5f3f29a3c453eb402e0e8384ad35c32220b87", "exact_match": 0.0}
{"doc_id": 383, "doc": {"question": "who has won the eurovision song contest the most times", "answer": ["Ireland's Johnny Logan", "Ireland"]}, "target": "[\"Ireland's Johnny Logan\", 'Ireland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the eurovision song contest the most times?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ABBA"]], "filtered_resps": ["ABBA"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c5da76c669a48c500e6a81e9dfec1939340c86cd95edf0d4bc852ca87433b8cf", "prompt_hash": "f83b840a190ba37b706a2aa7423b3a1426d1724436407dad1d08f12ef6cd7276", "target_hash": "527e4df600441808205ca83145d6e450f07d7cc2f93fc07de403ffb1e9d98ae6", "exact_match": 0.0}
{"doc_id": 384, "doc": {"question": "who is the director of taarak mehta ka ooltah chashmah", "answer": ["Malav Suresh Rajda", "Harshad Joshi"]}, "target": "['Malav Suresh Rajda', 'Harshad Joshi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the director of taarak mehta ka ooltah chashmah?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rajkumar Hirani"]], "filtered_resps": ["Rajkumar Hirani"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "552033e2075203a20381040badafc3c261280288633fd8bdd19cbb0cadeff0d8", "prompt_hash": "8f32dd2b2f70d4ce81900682adad545857fbc5d0e31770ee8130d5205eae4f16", "target_hash": "add710bf23209bfe18d97d6e89681ece9e8d369da94b52eec7e6bbc20887a471", "exact_match": 0.0}
{"doc_id": 385, "doc": {"question": "genre of diary of a wimpy kid the getaway", "answer": ["Children's novel", "Graphic Novel"]}, "target": "[\"Children's novel\", 'Graphic Novel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: genre of diary of a wimpy kid the getaway?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" comedy"]], "filtered_resps": ["comedy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d8258e497c52d1916d86383d13476dc067efbea78a96083b1f6e965f0c6824d", "prompt_hash": "f074cae533172f74e21e05f0141e677b1c01169446d9121cdfc2db7be4de96bb", "target_hash": "5f2497beafd1c43faf5f84ba3782720bfb14e1897feb6449b9ed6d96dbec1343", "exact_match": 0.0}
{"doc_id": 386, "doc": {"question": "when is the met office leaving the bbc", "answer": ["31 March 2018"]}, "target": "['31 March 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the met office leaving the bbc?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2019"]], "filtered_resps": ["2019"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "84d63024dc70b71486aa46aed244275c1ae4c1a08aa52044c030869aac72b13c", "prompt_hash": "acf63b62195c7d34beb3b3f0fe8e358bd7876eef85124de3d1187f507da5faea", "target_hash": "f424def64f56488a197cbd90d7eae19bfd31e170ac86686fa475253b2cb82940", "exact_match": 0.0}
{"doc_id": 387, "doc": {"question": "what is billy last name in where the red fern grows", "answer": ["Colman", "Billy Colman"]}, "target": "['Colman', 'Billy Colman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is billy last name in where the red fern grows?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Granger"]], "filtered_resps": ["Granger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "317bcd7ba59d1d20e9c7707fbc6b6fd7867f27680f11eff3c6770024deaf04ee", "prompt_hash": "dc1de6b35c65264e62d4e15d163ca1d5ed43bef0efd277ddf78d41fcfe7ba77f", "target_hash": "b1250d3837ae3cf8b3f1e028c895235fdb6f50c68b87ee9669b85450d25c1272", "exact_match": 0.0}
{"doc_id": 388, "doc": {"question": "who did puerto rico belong to before the u.s", "answer": ["Taíno", "indigenous Taíno people", "Spain"]}, "target": "['Taíno', 'indigenous Taíno people', 'Spain']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did puerto rico belong to before the u.s?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spain"]], "filtered_resps": ["Spain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8eb28b19e3cad599750febd883719119e0af94cdd8e0d1a9cdda51f8c6c3394e", "prompt_hash": "5769613e5d081ed70158b22c7328fe9d9eb57f9adf79bf8a3545c80824777762", "target_hash": "cbe7dc4e1a2188c619bdab5f5044be8a2f2b0e7ae36609bb393f0214640f64f1", "exact_match": 1.0}
{"doc_id": 389, "doc": {"question": "what is the population of st petersburg fl", "answer": ["260,999", "257,083"]}, "target": "['260,999', '257,083']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the population of st petersburg fl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 244816"]], "filtered_resps": ["244816"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "926766e5dd53b25ea754b3c6a1612d363e3280692b592cf8c0065f2982db02ad", "prompt_hash": "f812b8e91b631bd8a02c7057a9e72795720e82cad34d85d2ecc585c2cfa03a56", "target_hash": "da98073003fbf6e5f31b40895f7a9dfab76a8c4de85f7e836b79be6c500df671", "exact_match": 0.0}
{"doc_id": 390, "doc": {"question": "when did the study of media effects begin", "answer": ["1919", "1975"]}, "target": "['1919', '1975']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the study of media effects begin?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1940"]], "filtered_resps": ["1940"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cb67883094b16a27340c93052509a661fb4af4855e4dac1fe5467e64225f3896", "prompt_hash": "b2a57e284aad25dd5907279fc697d7167904c6216392a9fefa909dad7d6a1a71", "target_hash": "0edf4bfd0251dbf4eb4714b44cd598bc9665a982f19221ec416656433ab50368", "exact_match": 0.0}
{"doc_id": 391, "doc": {"question": "who plays heather in beauty and the beast", "answer": ["Nicole Gale Anderson"]}, "target": "['Nicole Gale Anderson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays heather in beauty and the beast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paige O'Hara"]], "filtered_resps": ["Paige O'Hara"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9f25278dc74cdb175a6c3c7adfc291f6089dadc7ab09b487401fca74d002da0f", "prompt_hash": "364811de8433908a226cc703ec78dcc8d797c2223b85043ce2e10e067c8a1a9a", "target_hash": "1286dc7574bfae8c1cfec246e833b7cabee0b549cd444c4eb272e4a811c967a5", "exact_match": 0.0}
{"doc_id": 392, "doc": {"question": "how many customers does edf have in the uk", "answer": ["5.7 million customer accounts", "5.7 million"]}, "target": "['5.7 million customer accounts', '5.7 million']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many customers does edf have in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3"]], "filtered_resps": ["3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1fae348f33ce08812b4de3a4d4052982bce5f0d2814425f75a374efb83abe6c6", "prompt_hash": "bbbe8188d0f79aae430c88f27b57ba0cd8f111996dde6ce537e1ad4adb9b6598", "target_hash": "80d2fd8a1118f734fe6c3309d60c51e3263719c90e85353c46db8059c73844c7", "exact_match": 0.0}
{"doc_id": 393, "doc": {"question": "where does the formation of atp take place", "answer": ["plasma membrane in bacteria", "inner mitochondrial membrane", "thylakoid membrane", "mitochondrial membrane in eukaryotes"]}, "target": "['plasma membrane in bacteria', 'inner mitochondrial membrane', 'thylakoid membrane', 'mitochondrial membrane in eukaryotes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the formation of atp take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mitochondria"]], "filtered_resps": ["mitochondria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a898bd99b7146ee0d07bd6976f77dedff1b1005ced8d841e40bcf94e6f150b95", "prompt_hash": "873f689ee5e11b06a8744d9b3ad3f86be756453d5561a8d0b9c3d296a7f2cf27", "target_hash": "c2d1877047644418985805057eb63e7e98c82e5667e93d455cbf98136def797c", "exact_match": 0.0}
{"doc_id": 394, "doc": {"question": "who is jack's mother in when calls the heart", "answer": ["Charlotte Thornton"]}, "target": "['Charlotte Thornton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is jack's mother in when calls the heart?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Fiona McCall"]], "filtered_resps": ["Fiona McCall"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a8f4e5e8c378cd595e5f67144de3696e3bca4d13a39a4e34d3dcdfccf4805a49", "prompt_hash": "0f7d9fa41483c1ef42991c6ca9b2733dad662fd6af81c8036abc1fe430c60f5f", "target_hash": "e9d68ee9bbf7c43458d47a9ac634db0e5122ff3a807a80a0e715af9033e96616", "exact_match": 0.0}
{"doc_id": 395, "doc": {"question": "who was the head of the spanish inquisition", "answer": ["Grand Inquisitor", "Tomás de Torquemada", "Dominican Tomás de Torquemada"]}, "target": "['Grand Inquisitor', 'Tomás de Torquemada', 'Dominican Tomás de Torquemada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the head of the spanish inquisition?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tomás de Torquemada"]], "filtered_resps": ["Tomás de Torquemada"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b72609cd24b751cb1073ed309f8a0f83cf065f42e1ac65d338d11e0bf80bc78", "prompt_hash": "cf95173cb8a043471343a276cf674862f46e7d4ec5cb53211a27de972ca4f47e", "target_hash": "f4587ddc445fa5ed33e61fe839150823231188716e7060132e1563a6e08f9586", "exact_match": 1.0}
{"doc_id": 396, "doc": {"question": "what does istj mean in a personality test", "answer": ["Extroverted Thinking (Te)", "Extroverted Intuition (Ne)", "Introverted Sensing (Si)", "Introverted Feeling (Fi)"]}, "target": "['Extroverted Thinking (Te)', 'Extroverted Intuition (Ne)', 'Introverted Sensing (Si)', 'Introverted Feeling (Fi)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does istj mean in a personality test?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Introverted"]], "filtered_resps": ["Introverted"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8b2edc51014282ceeafcf2fc0a0141bdc8bc5bb913eda7a10917971a54720023", "prompt_hash": "39978cf123f3c96001a673227eba92f5447c9ecf965a1f82490d22fd6b6be0ba", "target_hash": "ee29942d2ccdc92d36c404b2fa5230fd56b3485b1fad9f0740a93953cb770276", "exact_match": 0.0}
{"doc_id": 397, "doc": {"question": "how many seasons are there for lost girl", "answer": ["5", "five"]}, "target": "['5', 'five']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons are there for lost girl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 4"]], "filtered_resps": ["4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "37989e6338fd691b3151c8cd110f5b56b3d30657b27479ee72e966417c342918", "prompt_hash": "c7497e937be1c7ba95576409f278bff7fc954abafdd4b8779bddcd3a382c84e1", "target_hash": "be3091206c49fc38e579554ab1cb3b60d3d57077a72c8e9b39a3b774a3460d78", "exact_match": 0.0}
{"doc_id": 398, "doc": {"question": "who plays artemisia in 300 rise of an empire", "answer": ["Caitlin Carmichael", "Eva Green", "Jade Chynoweth"]}, "target": "['Caitlin Carmichael', 'Eva Green', 'Jade Chynoweth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays artemisia in 300 rise of an empire?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lena Headey"]], "filtered_resps": ["Lena Headey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed820585195e6442e37f013b24764dbf9307cf9af7336e018be07624389c545a", "prompt_hash": "44299a8055a7145d927e9673756c2e91e8ebffd30c1315a025e77cfccec1c57a", "target_hash": "818e31a458ba7499ce4cc3be8c96fa919f540a5950eaf275558d54b04d12749f", "exact_match": 0.0}
{"doc_id": 399, "doc": {"question": "when did the the regulatory reform (fire safety) order 2005 first come into effect", "answer": ["1 October 2006"]}, "target": "['1 October 2006']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the the regulatory reform (fire safety) order 2005 first come into effect?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1 January 2006"]], "filtered_resps": ["1 January 2006"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b43bbede92f4d5d0215a317cde24e1ba7cba4566f4c3d2ebc75654b96dd319fe", "prompt_hash": "ae8e7f6005a707c4abdb193f8640d2fee646749ab5f0d471e3e41fdbad1a7056", "target_hash": "5f02d998d258c9c41ec7f689bf50eb461635dcd4a4bda50f859c6bee0ef701c0", "exact_match": 0.0}
{"doc_id": 400, "doc": {"question": "when did the bill of rights come out", "answer": ["1689", "16 December 1689"]}, "target": "['1689', '16 December 1689']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the bill of rights come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1791"]], "filtered_resps": ["1791"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e225c02075ab91a3d27d3941664c726fcfb9763a0b95eed0da15a1380aecbcc", "prompt_hash": "0183e93a06ce9057bfbbbe5c6b4941aaf2fbc11490fbb14994778ecf5e0999f8", "target_hash": "42d73aeda8e441fbfb0700542542a4b7ced99b4fefbbf8560c86425ff1c1bceb", "exact_match": 0.0}
{"doc_id": 401, "doc": {"question": "when did the east india company take control of india", "answer": ["in 1757", "1757", "1799", "1612"]}, "target": "['in 1757', '1757', '1799', '1612']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the east india company take control of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1757"]], "filtered_resps": ["1757"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d4a6a5dc4008f688f9c189118d86b175fdf0e43886965d2e54471e01dd6f748", "prompt_hash": "f053f480d8b3a414445cb6e097425f951541508ab29f0d68a0fc7003e0e6afbe", "target_hash": "89648fa8f9ed18f3f9f0e549f4dc6b32f319d7271bc8e8e3a3de53cdcf1ff9d7", "exact_match": 1.0}
{"doc_id": 402, "doc": {"question": "who was elected president in mexico in 2000", "answer": ["Vicente Fox"]}, "target": "['Vicente Fox']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was elected president in mexico in 2000?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vicente Fox"]], "filtered_resps": ["Vicente Fox"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ab22639f1180da5137beeb59bafca841b4c01783da69451f264e4fc5988b7020", "prompt_hash": "b6cfcc7f46c4b44c3b35ed565977847022164fc33e8fcab617dfa5702a98b284", "target_hash": "6b39abb18352080d7978ff6eeb4dbf24d1ffd8d916c2db4f9e94b8fcb6abe1c8", "exact_match": 1.0}
{"doc_id": 403, "doc": {"question": "when does season 6 of the next step start", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 6 of the next step start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c5ee76e770383571071550dbcb48546116cd5afca9168ca247f55e190da71aa", "prompt_hash": "ddbdc0be3f1f0010ae83187fbb6398221f814e02f5518db009941e4c2864496c", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 404, "doc": {"question": "where do the sharks play in san jose", "answer": ["the SAP Center", "SAP Center", "SAP Center at San Jose"]}, "target": "['the SAP Center', 'SAP Center', 'SAP Center at San Jose']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the sharks play in san jose?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" SAP Center"]], "filtered_resps": ["SAP Center"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed0c998b9a944595ce907f509f5f70b5108eacf8bb837208502bc4f4401d7185", "prompt_hash": "37d4d86d199a9ce55b2ac589c74e0253690888b17ea2d46b15e762bc47cab713", "target_hash": "f23e7ed4cc606af282ecd9f0817e423b3e693acab8ff213966e9ce31db4bd741", "exact_match": 1.0}
{"doc_id": 405, "doc": {"question": "abbreviated name of the highest peak in tasmania", "answer": ["Mount Ossa"]}, "target": "['Mount Ossa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: abbreviated name of the highest peak in tasmania?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mount Wellington"]], "filtered_resps": ["Mount Wellington"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bacb68fcaeae6d7d86a59b787fa9411d634666fd0a0dd9b2d6fff1944ce22134", "prompt_hash": "4f099efcec20bc708943f8cbe6b06a55ec14c26dab0a570e323268952680e1de", "target_hash": "8c21144bc107523b1563b0d162afd677b08fda76ce158a7e4d735f38fdfe82e0", "exact_match": 0.0}
{"doc_id": 406, "doc": {"question": "who played alex cross in along came a spider", "answer": ["Morgan Freeman"]}, "target": "['Morgan Freeman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played alex cross in along came a spider?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Will Smith"]], "filtered_resps": ["Will Smith"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a33b604f9dcf22101c241dcaf6548747d7489dbf82bf5c2cabac9b50e52e28da", "prompt_hash": "d32e93bbebba0286f4e718473777ae26664719503538a4390db92b5d86f29831", "target_hash": "3b85852946ebf4d36e7453643ee8e5cb419be73fe8f74493eba26b03107301a9", "exact_match": 0.0}
{"doc_id": 407, "doc": {"question": "who has the most wins on around the horn", "answer": ["Woody Paige"]}, "target": "['Woody Paige']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most wins on around the horn?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Curt Schilling"]], "filtered_resps": ["Curt Schilling"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "33bf66823dccd0b7f561db832627bba763771412c3b880d9187fc081603c2213", "prompt_hash": "873e0262973ce544b34b5c2be4f0b17142f94c19c8e200393579c726b1a867c9", "target_hash": "ab8901238d6a11426587d9480f7779fae797ec169a5acf211ab79cb13f4cfee1", "exact_match": 0.0}
{"doc_id": 408, "doc": {"question": "what are the main sources of economic growth in the us currently", "answer": ["pharmaceuticals", "heavy machinery", "telecommunications", "aircraft"]}, "target": "['pharmaceuticals', 'heavy machinery', 'telecommunications', 'aircraft']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the main sources of economic growth in the us currently?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" productivity"]], "filtered_resps": ["productivity"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d211ad714a9e4568a9d0136e5c364dc25619d77a136598eacc4a347a0600f541", "prompt_hash": "3d4d35b4408fb0a7f9d5cb1a7574510550c42ce303317f0566ba4dc7e0d744e2", "target_hash": "9b179ab9064feeb6a04a89cce6de0306e4c7ae042bb39ba9cbbfe587617a6f86", "exact_match": 0.0}
{"doc_id": 409, "doc": {"question": "who are the australia's got talent judges", "answer": ["Kelly Osbourne", "Ian Dickson", "Ian \"Dicko\" Dickson", "Eddie Perfect", "Sophie Monk"]}, "target": "['Kelly Osbourne', 'Ian Dickson', 'Ian \"Dicko\" Dickson', 'Eddie Perfect', 'Sophie Monk']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the australia's got talent judges?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nigel Lythgoe"]], "filtered_resps": ["Nigel Lythgoe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cb38ee389fdce1b07917aae92c00594d1ed1356ae014486814d5ecb378d04c00", "prompt_hash": "0c7d5feb62212a864667a327ca058496f39a55b890ec9d052db778d00dc9b0ba", "target_hash": "7eba775fe73bc22e3f7dad5d18c38a4c1be210eedb4f325ecd066d192373e5bc", "exact_match": 0.0}
{"doc_id": 410, "doc": {"question": "who sang the song i wanna be sedated", "answer": ["the Ramones"]}, "target": "['the Ramones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song i wanna be sedated?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Siouxsie and the Banshees"]], "filtered_resps": ["Siouxsie and the Banshees"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42e657f5fbddc9684d9887d8c01c22b0dedc1365eb2a71a14c6cfc0e5e89f5ba", "prompt_hash": "81085960ca916289638be64231c7eea1ce951f2e39473d4ab06ff8d055757436", "target_hash": "ac7553b80e2d3ea7055d0e6fa65442878e2203b3cdb053de395efa684308e22c", "exact_match": 0.0}
{"doc_id": 411, "doc": {"question": "what is the setting of the book hoot", "answer": ["Florida"]}, "target": "['Florida']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the setting of the book hoot?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Florida"]], "filtered_resps": ["Florida"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5c0e0a6df1c7fbcb4ecdfa65d1db86e85ba698a20bd9afce2e3969d5edb1b4b7", "prompt_hash": "b34470660b54759b42bc41e18a8539a28841d652ee9fa2b69f76f6e14878f61c", "target_hash": "3a10045e60753262513a93b91b133b58cb1ba7f091839dc5fb133bfd7baa4547", "exact_match": 1.0}
{"doc_id": 412, "doc": {"question": "when is season 3 of grace and frankie being released", "answer": ["March 24, 2017"]}, "target": "['March\\xa024,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is season 3 of grace and frankie being released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 11"]], "filtered_resps": ["January 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ddf23d496a8dea1367bae55d8c9138794bf22d74a11a4eb2df5fcd91ca38022", "prompt_hash": "a01bec0b9067c25a6ce64ca315aca42a67193aeba79c42f88fff7967953e8819", "target_hash": "d156e1068736a71b9269a910f0d65c637fcc616aa446fc6fded30a271b5a19f1", "exact_match": 0.0}
{"doc_id": 413, "doc": {"question": "what does the sun represent on the uruguay flag", "answer": ["the May Revolution of 1810"]}, "target": "['the May Revolution of 1810']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does the sun represent on the uruguay flag?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sun of May"]], "filtered_resps": ["Sun of May"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0dbe1fdaddb2563ed4252a609493bd13f46c5f6419242283ed2c4e4bc2c12cd3", "prompt_hash": "3956b91b2774043e1414cc9e0f1620bdd5e401dce11f19ac12518731a29d983e", "target_hash": "9d5035b2e26b8b1e92c10b5711541dac994b9bf04cf6e845123b5b811bb0d74d", "exact_match": 0.0}
{"doc_id": 414, "doc": {"question": "who was in charge of the revolutionary war", "answer": ["General George Washington", "the king"]}, "target": "['General George Washington', 'the king']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was in charge of the revolutionary war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Washington"]], "filtered_resps": ["George Washington"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f2779b1253e05dc894b5e199cacd06c180f64c053e6679cace56484360adfe02", "prompt_hash": "c41d9a027e03c41447786e2e162eca6a5ad59eac2313066748ee19c8369a4d94", "target_hash": "926a226fee8ce0c16c48a06e3f299fc38f9ecc148854c82b05acbf58cee306af", "exact_match": 0.0}
{"doc_id": 415, "doc": {"question": "why was hong kong important to the british empire", "answer": ["a centre for international trade"]}, "target": "['a centre for international trade']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why was hong kong important to the british empire?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" trade"]], "filtered_resps": ["trade"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "53140538e8a63af9dbc37fee1b4392022e363d6903ac7c370279c7b02f129ef1", "prompt_hash": "b7f509eb109925018a084ee759606357272cbb279e00d1f63cd9bc8a246be588", "target_hash": "d720dd27278a60e94bb974f4d436b74c7decf9e2261fa2bcbab6bb5dc1042eae", "exact_match": 0.0}
{"doc_id": 416, "doc": {"question": "what position did doug peterson play in the nfl", "answer": ["holder on placekicks", "quarterback"]}, "target": "['holder on placekicks', 'quarterback']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what position did doug peterson play in the nfl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" quarterback"]], "filtered_resps": ["quarterback"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9880e5cb7cb32cfbee1b70e91b40002aa503bbb03c7da6a58889392eeaf12cee", "prompt_hash": "58e15207d0f2e204b41e8d7bd58bacb6449efba38f2015925326d7e88f223ccc", "target_hash": "960d837b6e0620cf47c3d33546f4718949b25601bcb0cb765aed643c2bbc6db2", "exact_match": 1.0}
{"doc_id": 417, "doc": {"question": "where is creatine phosphate found in the body", "answer": ["brain", "pancreas", "skeletal muscle and the brain", "muscle cells", "heart"]}, "target": "['brain', 'pancreas', 'skeletal muscle and the brain', 'muscle cells', 'heart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is creatine phosphate found in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" muscle"]], "filtered_resps": ["muscle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fcfb82add6ffea2568f32a05bbd185b0cb154c6d5ec8fa367e9a05430f720a33", "prompt_hash": "18313019b2c14a26e96925541e24b89be38f4fee6c15823348c8fbe83ddcc19e", "target_hash": "ec3eb1487451213fb949bdea16bcd5fd830c602d4fe3131104c9a9ed0da6e91b", "exact_match": 0.0}
{"doc_id": 418, "doc": {"question": "criminal minds what episode does jj find out she pregnant", "answer": ["in April 2011", "The Crossing"]}, "target": "['in April 2011', 'The Crossing']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: criminal minds what episode does jj find out she pregnant?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 101"]], "filtered_resps": ["101"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6ba3e8119f72fe3c85c124a3659ecec5d8f0ded60d9e0834b647b70afbb44c08", "prompt_hash": "b5f2698aab5366251b71e66d1cb78f18722d2c0234374afed904522c68d93dbc", "target_hash": "e14a942f689bf987673d09f057a7cef751520ecd1011cd45b8682a7badff399e", "exact_match": 0.0}
{"doc_id": 419, "doc": {"question": "when was harry potter and the philosophers stone published", "answer": ["in 1997", "1997"]}, "target": "['in 1997', '1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was harry potter and the philosophers stone published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2850342fc03ea171db32b2b542f909c741cf9d112d45f5296b225b7d15ed4504", "prompt_hash": "1a411a207a16db483037dca3533a65d63716a6b1f534cf92cc93f64356a7874c", "target_hash": "fd1b229fcb77eb80dd6bfc5bb433b83e69da06bea4315c8a7b518385bcad34f5", "exact_match": 1.0}
{"doc_id": 420, "doc": {"question": "who played the twins in darling buds of may", "answer": ["Christina Giles", "Katherine Giles"]}, "target": "['Christina Giles', 'Katherine Giles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the twins in darling buds of may?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David and Timothy Harewood"]], "filtered_resps": ["David and Timothy Harewood"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0951a58ecb7602fbd79f3008b55f39c15b86dd8997f46a39cfbbef48a16308ab", "prompt_hash": "357a58a398607dc6ee1925c5c89446d80803c6d49b195b098f5f3d5c9f07267a", "target_hash": "1f1a6c79e5291a318dcf20813f0eb35f141098f36c36806db89efce9c191b79b", "exact_match": 0.0}
{"doc_id": 421, "doc": {"question": "when did skiing halfpipe become an olympic event", "answer": ["2014"]}, "target": "['2014']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did skiing halfpipe become an olympic event?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "88a59807ba7dfcb81ff9dcceea00ffdfd4d48e78493e930516333c9ef4440430", "prompt_hash": "e18115f41f4f324a59d05081e3e567a9765e067e643d68336be51ab2e82be461", "target_hash": "942ffbb00e6e9519f078a09a7536e830dac4445486c8971c4c3d743eeb2c8e7e", "exact_match": 0.0}
{"doc_id": 422, "doc": {"question": "who plays at the prudential center in newark", "answer": ["Seton Hall Pirates", "New Jersey Devils"]}, "target": "['Seton Hall Pirates', 'New Jersey Devils']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays at the prudential center in newark?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New Jersey Devils"]], "filtered_resps": ["New Jersey Devils"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1faa059c54c80019a28860f5369e879248d9f0f37a67655a9ea61b24f4da034e", "prompt_hash": "f1cfb6f2fd742117d30e1b459b6c5bea434eb1925d84eaaa0740baf198332b2a", "target_hash": "8bab9e0140c1cc01243da9d103dd9d4df6462adb0062f7bf994893c9371465fd", "exact_match": 1.0}
{"doc_id": 423, "doc": {"question": "where was the first colony in north america located", "answer": ["Virginia"]}, "target": "['Virginia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the first colony in north america located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jamestown"]], "filtered_resps": ["Jamestown"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b30d295a757b05aa79b0bb816e4690192feac6698d93e62b765d33716b0329c", "prompt_hash": "91c599f9165e133c1b879fd86c894afef6f91887167e363f2ab2a90a5621feca", "target_hash": "8b605053708c94d2f8829601fdf3d6121e791a22e056104f877b2266253da5bc", "exact_match": 0.0}
{"doc_id": 424, "doc": {"question": "during the first world war against which country germany did not fight", "answer": ["Austria-Hungary"]}, "target": "['Austria-Hungary']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: during the first world war against which country germany did not fight?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Russia"]], "filtered_resps": ["Russia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4e855d5f9b110b7073eba260a5a02640579a1819421604d0e14be9502f2b62e", "prompt_hash": "45ce20d1e647ba24359d985956b3e4a1f2660f5708ff67ff38bff396b97be511", "target_hash": "b6e06d92903ae87dc17a71c1a62eff7c30a2d7a32728167111fdaf7f4b0e7c2a", "exact_match": 0.0}
{"doc_id": 425, "doc": {"question": "who did the minnesota vikings lose to in the super bowl", "answer": ["Pittsburgh Steelers", "Oakland Raiders", "Miami Dolphins", "Kansas City Chiefs"]}, "target": "['Pittsburgh Steelers', 'Oakland Raiders', 'Miami Dolphins', 'Kansas City Chiefs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the minnesota vikings lose to in the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Denver Broncos"]], "filtered_resps": ["Denver Broncos"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e282fa3f8eec48ef37bdc54f3a5e856db69d0ac61490df5daf9f22982a5bfc0b", "prompt_hash": "29ff3aaa744ec6bcb827bc7a4a8ab3ca9884d1435a540559a881d0cf5ef41b6e", "target_hash": "0cf3b7a9bfe1856c0f024011bd1524e4b8da655282a2f11490888432d8b3c56a", "exact_match": 0.0}
{"doc_id": 426, "doc": {"question": "who was the aztec ruler when the spanish arrived", "answer": ["Moctezuma II", "emperor Cuauhtemoc"]}, "target": "['Moctezuma II', 'emperor Cuauhtemoc']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the aztec ruler when the spanish arrived?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Moctezuma II"]], "filtered_resps": ["Moctezuma II"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "caa609128956b0ccc9b2f1bcbc873e64d752b53198a99e9b127a3ab592552816", "prompt_hash": "2960502e1933078dc647d06705f45d75b0b2df8f819c3e997cebf20e1204a94d", "target_hash": "51529d8a3a01d257d3a5b3754a46f3d2b172083e91df517218c0349d80c4a99a", "exact_match": 1.0}
{"doc_id": 427, "doc": {"question": "product-market fit means being in a good market with a product that can satisfy that market", "answer": ["Mark Andreessen"]}, "target": "['Mark Andreessen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: product-market fit means being in a good market with a product that can satisfy that market?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" yes"]], "filtered_resps": ["yes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6bc77f91581bfd9451c9af2f77166d76d7dffc99e40471311a5817425ce49361", "prompt_hash": "f18f5d049d1dff24ebec9cade4e72ff601fd006ea525e9e9121ac2abf71f12b2", "target_hash": "869ab57931a5688e4305fffc733298c05ff017804653ff74ee1af10f5460c3e1", "exact_match": 0.0}
{"doc_id": 428, "doc": {"question": "who does bryce dallas howard play in the grinch", "answer": ["Surprised Who"]}, "target": "['Surprised Who']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does bryce dallas howard play in the grinch?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cindy Lou Who"]], "filtered_resps": ["Cindy Lou Who"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a914311c59c5e83fd8f5be287564d344ff3e3aabcbdd2a6c7c224ab9361344d9", "prompt_hash": "8ddebaf291c98faab45a3c81fc7ffd46b5fd25f4ab08d1ce0b41e3eefefd9fdf", "target_hash": "3bb7f7ae6940f8940b8071e8aac7af896c4ff967105aae61c1150d0c7b00197a", "exact_match": 0.0}
{"doc_id": 429, "doc": {"question": "who plays the robot on the orville show", "answer": ["Mark Jackson"]}, "target": "['Mark Jackson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the robot on the orville show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wil Wheaton"]], "filtered_resps": ["Wil Wheaton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "593e62cd93f6700b0858422d0de9412a18f677731c14698f277af1243498bf8d", "prompt_hash": "b1fae8e0c1ba47df8874acbd1718950274fecf4bde30c86868762489552316a4", "target_hash": "127c017fdecd350f536c8483b61da43863c60fbd5264c193865bcc9d7b6fc1cf", "exact_match": 0.0}
{"doc_id": 430, "doc": {"question": "the group that officially elects the president of the united states is called", "answer": ["the U.S. Electoral College", "U.S. Electoral College"]}, "target": "['the U.S. Electoral College', 'U.S. Electoral College']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the group that officially elects the president of the united states is called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Electoral College"]], "filtered_resps": ["the Electoral College"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb4c04072a11fdd5cbe7afafdecd4f9012a6047037a77b6ba7ce2085c995c73f", "prompt_hash": "9b7b10c54645cc367cc52004ebae22393cc500ce66c7425c7143aac41b27e631", "target_hash": "014c15cde133b936104ce1f9a801b687f7ad6025e5766ed9beb6006434b7dea2", "exact_match": 0.0}
{"doc_id": 431, "doc": {"question": "how many wars held between india and pakistan", "answer": ["four"]}, "target": "['four']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many wars held between india and pakistan?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" three"]], "filtered_resps": ["three"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c7e0fb718b7dd0fea8023385f09dc03277de2a373be5eb56fd6e93d47de939fb", "prompt_hash": "2a8f69dad4c9d9b2eec424f0fc818486614fc8d8ec3c077383d434a9065f5cf4", "target_hash": "c0b12eaa48bad2eab0833b330f1f9a7a1caf1ef07c30f4fc2a1bd97e38e98618", "exact_match": 0.0}
{"doc_id": 432, "doc": {"question": "when was the young and the restless first aired", "answer": ["March 26, 1973"]}, "target": "['March 26, 1973']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the young and the restless first aired?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ac7fceba66549f67ed6a7a192250d805859f8288ee4f077498040ec7149c549", "prompt_hash": "0075e1df401648213a71dde74156f38c52d6fa39a739fe0da74b8503fabfaf47", "target_hash": "c9228328184d4abbc1d90895b88a7b8c39e31f30513bd678e1e5546563e754d0", "exact_match": 0.0}
{"doc_id": 433, "doc": {"question": "what was the emperor name in star wars", "answer": ["Darth Sidious", "Sheev Palpatine"]}, "target": "['Darth Sidious', 'Sheev Palpatine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the emperor name in star wars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Palpatine"]], "filtered_resps": ["Palpatine"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5dfa5a348cfd70bb397a3f43557a481c97a58ce60c5206092f8950f153ba8dd9", "prompt_hash": "450c94c84e56d77b7a0da05ef29dc5fc55eb0f0f356ef3e0c053bd6173e9bdf5", "target_hash": "30c47ae1b8046cd6ca00ae914a0d8d7804831ac45eb9016d2d33065c17636df6", "exact_match": 0.0}
{"doc_id": 434, "doc": {"question": "who has won 2017 women's singles korean open series badminton championship", "answer": ["P. V. Sindhu"]}, "target": "['P. V. Sindhu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won 2017 women's singles korean open series badminton championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chen Qingchen"]], "filtered_resps": ["Chen Qingchen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42d29686d1d7dded51891ef71b6cf6d73f79f432afd3c523400814387efa3a6a", "prompt_hash": "e29b112212747acc950e850600bec1a2255208b2bf5c4096cca82074d03f419e", "target_hash": "d5d6f150f3bc6267cba2135177266a727e4ec26b13cb5f77f6ab455715ca18c9", "exact_match": 0.0}
{"doc_id": 435, "doc": {"question": "what is the full form of cfc's", "answer": ["Chlorofluorocarbons"]}, "target": "['Chlorofluorocarbons']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the full form of cfc's?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" chlorofluorocarbons"]], "filtered_resps": ["chlorofluorocarbons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03266c10fab868ddeb547b86676ef5a2d7598b45de3ef3bd6acc4d521d9917be", "prompt_hash": "adb0715349542612fb894c7c2d30ec19cd8de9f66958f328c6ef47d7fadbb387", "target_hash": "0cab53dac5a0cd8e92aa0144224ad3275d4d1bb3f606a4fa373b53ec1fc1160d", "exact_match": 1.0}
{"doc_id": 436, "doc": {"question": "the most important battles in the revolutionary war", "answer": ["Philadelphia campaign", "Yorktown campaign", "Saratoga campaign", "Boston campaign", "Invasion of Quebec"]}, "target": "['Philadelphia campaign', 'Yorktown campaign', 'Saratoga campaign', 'Boston campaign', 'Invasion of Quebec']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the most important battles in the revolutionary war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Saratoga"]], "filtered_resps": ["Saratoga"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "089d94b079346727503b6252bdc7f4ca8f4490d6271719a8b8b6c7af1bb39cfa", "prompt_hash": "8de1b6d9b31a9d5c7d91edd0ba1bb182a0f93feb4410aeb4b2856767d22182ed", "target_hash": "348689f6f10f98325b651ef000167dc73247756b83f5cd9738672c43b1d79d59", "exact_match": 0.0}
{"doc_id": 437, "doc": {"question": "who does eric end up with in gossip girl", "answer": ["Jenny"]}, "target": "['Jenny']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does eric end up with in gossip girl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Blair Waldorf"]], "filtered_resps": ["Blair Waldorf"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2adf9b2fa99a189e8df9c2dd1d8e57f8f566c9c1e7687e991f466460a9df6e6c", "prompt_hash": "5f1865c936e1f3f3bdc52a087390b1de658b5d0e7e8e54a7bafefcb6e40b733b", "target_hash": "1f33e1fde4d2254ef6b58879d47af816160e14044ab74d4296f99161464d917e", "exact_match": 0.0}
{"doc_id": 438, "doc": {"question": "when was the first temple built in jerusalem", "answer": ["the mid-10th century BCE"]}, "target": "['the mid-10th century BCE']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first temple built in jerusalem?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1000 BCE"]], "filtered_resps": ["1000 BCE"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c7937a7c3399c4ae91c637139ad84bbf68f9fd370ad143d13cd6912995cba54", "prompt_hash": "e37a4634276091606a55228ba16dd4c1450edc2d8360a1fa013a00055c0112e2", "target_hash": "9df55da282500daba9edf41fd50127d83ebdad38cb0a96278345ece3b3ed8352", "exact_match": 0.0}
{"doc_id": 439, "doc": {"question": "when did the steel mills closed in youngstown ohio", "answer": ["September 19, 1977"]}, "target": "['September 19, 1977']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the steel mills closed in youngstown ohio?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1977"]], "filtered_resps": ["1977"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "050573d9175365dea60757f10feeaabd3ed3a524a17bc80c165cfa233449bb1b", "prompt_hash": "8b35457664c5380e47daeda98e6c7d919184bfb4d19dbde4b026025c2fbb2bf0", "target_hash": "4709458ca383996a87d8fabb7094723672be36b46e068c73477a41a915c51a75", "exact_match": 0.0}
{"doc_id": 440, "doc": {"question": "when was the last time the jets won a playoff game", "answer": ["2010"]}, "target": "['2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the jets won a playoff game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1968"]], "filtered_resps": ["1968"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0f35366f45ae0a613b90184e2310b03fa949879869802d75d0b0ab221d7abc7b", "prompt_hash": "4f359b37fd8bfabc6020151fc6c976224051bd6e475039421ed0eefc064ef7cb", "target_hash": "2ee1672483fc554adda35e9fdae9ee5db286e7d25dde7bd484842b76ac64c784", "exact_match": 0.0}
{"doc_id": 441, "doc": {"question": "who sings the song it ain't me", "answer": ["Selena Gomez", "American singer Selena Gomez"]}, "target": "['Selena Gomez', 'American singer Selena Gomez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the song it ain't me?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Beatles"]], "filtered_resps": ["The Beatles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eabba717cd116f8f2abdacd93324ace59e50e776a5d80f2eeaf24035bac6ca58", "prompt_hash": "6312891df1d4705d14be6d4e0b7c934d6ef297f68a7cd1eb06a861f1f80b07e6", "target_hash": "9a8d3541c51f6d7ca3ba1762db70c0880c556e6f89b0b18db995415cdd645bb1", "exact_match": 0.0}
{"doc_id": 442, "doc": {"question": "is it marley and me or marley and i", "answer": ["Marley & Me"]}, "target": "['Marley & Me']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: is it marley and me or marley and i?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Marley and Me"]], "filtered_resps": ["Marley and Me"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9ef862e3d93de345c62513224a3ad3f692dc308d54df4e4869f262692099a8a0", "prompt_hash": "8b80db5dfebbb25de471d55364a3131ebb430ced4fa796252335d45ed20772de", "target_hash": "6ca8b01f133b793d923fd2a52c16ec88085d09d9d2923de7bf0927d14e54015b", "exact_match": 0.0}
{"doc_id": 443, "doc": {"question": "when did vat change from 17.5 to 20", "answer": ["4 January 2011"]}, "target": "['4 January 2011']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did vat change from 17.5 to 20?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1 January 2011"]], "filtered_resps": ["1 January 2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2b34ba7111fc287f978c53b8f2d1f4ee50bcf29a884a9c3a91b0a59f7a40566", "prompt_hash": "0e4f34aab7619926ab0c02974b169a41f08fd0769731916cf4a0e4648a1670c8", "target_hash": "0038cdbfe98c0f4cc2fb0440a98198827fcfc6529d1a491807ae54cc955128f5", "exact_match": 0.0}
{"doc_id": 444, "doc": {"question": "when did seattle slew win the triple crown", "answer": ["1977", "in 1977"]}, "target": "['1977', 'in 1977']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did seattle slew win the triple crown?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1977"]], "filtered_resps": ["1977"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f63cfd3a564f5a4a1af7566982e8646f3e76116d139770ec498b70131f28c7f", "prompt_hash": "a363229f7df924c4c1243286087d24ac19f49cbad336c4984d31a4a8a84c34d8", "target_hash": "dd5dea99a8f7fc5e93d48cf4f26a21a57c575f3d4918404b8eb8dc0d915a8c97", "exact_match": 1.0}
{"doc_id": 445, "doc": {"question": "who sings too much time on my hands lyrics", "answer": ["Tommy Shaw"]}, "target": "['Tommy Shaw']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings too much time on my hands lyrics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Carpenters"]], "filtered_resps": ["The Carpenters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9e554d0146ddaaca3f84334b65d89d4a09316e7db76e78f3541eaa5cd143de76", "prompt_hash": "729a7c11ebb49e3138be0c7727c6dc3b0dacc9d785dc5a13770a401868146a73", "target_hash": "946500653df5e51af084a5e29de299e6b962e4a85a60e19e016fc922765f4685", "exact_match": 0.0}
{"doc_id": 446, "doc": {"question": "nba record for most double doubles in a season", "answer": ["Tim Duncan"]}, "target": "['Tim Duncan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: nba record for most double doubles in a season?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 41"]], "filtered_resps": ["41"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f80cb4d3acfa4b168dc3d1083d8780c90f6a48bcb58a461a5bc5e8770bf6ef3e", "prompt_hash": "2dd15af513e501b63f6058598ea1bc57e9dccd1c74638516ef4a3519e47afbb3", "target_hash": "241e3e2ca32486b9a7bb58ab58266ae905692a42f1cc63236a137862c035991d", "exact_match": 0.0}
{"doc_id": 447, "doc": {"question": "who is super bowl 2018 half time show", "answer": ["Justin Timberlake"]}, "target": "['Justin Timberlake']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is super bowl 2018 half time show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a14a44e320389e288ee507a868201069af0a07ff0643fbfd159bf93342a661c0", "prompt_hash": "40720ae949b8d461f8ada63565afcf99d092482e1d4abb9bf65fdc6d8d4acf64", "target_hash": "5d3c152075c166190023989a2621c5a25323ea4325675ced16b03032cec5a254", "exact_match": 1.0}
{"doc_id": 448, "doc": {"question": "all the motor neurons that control the skeletal muscles are", "answer": ["efferent nerves", "Somatic motor neurons", "Somatic"]}, "target": "['efferent nerves', 'Somatic motor neurons', 'Somatic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: all the motor neurons that control the skeletal muscles are?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" somatic nervous system"]], "filtered_resps": ["somatic nervous system"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "da1346703482e0e6537937b38447cb3f6c973da4c98a3ae71a1a9c03de6526e0", "prompt_hash": "e061dd7de089520b0a6d882a12412dde6ce8d55a36264c73a2dea598fb4c2895", "target_hash": "231df7fd7e2da6fa4e72ed820f452ef818c4edeedcc4dd4a7329d946056349dc", "exact_match": 0.0}
{"doc_id": 449, "doc": {"question": "who became the king of ayodhya after ram", "answer": ["Kusha"]}, "target": "['Kusha']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who became the king of ayodhya after ram?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bharata"]], "filtered_resps": ["Bharata"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d85acad37d59f55003d05863c0705f09ab16b4381757b9e84d4768c58f4ce69c", "prompt_hash": "5631c689c2d21f916bc6edd946befd3c11818aa8017f5cb9293e1bb09bd7e5ef", "target_hash": "641150be628e5cabc0a4b095222c7aa02562d9907be27a71dadfd92e4cd625cf", "exact_match": 0.0}
{"doc_id": 450, "doc": {"question": "who wrote lyrics for phantom of the opera", "answer": ["Charles Hart", "Charles Hart and Richard Stilgoe", "Richard Stilgoe"]}, "target": "['Charles Hart', 'Charles Hart and Richard Stilgoe', 'Richard Stilgoe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote lyrics for phantom of the opera?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Andrew Lloyd Webber"]], "filtered_resps": ["Andrew Lloyd Webber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b921a8b1222c8bc21cdd841ed8870060cad332f5ec47fa04b3435b076fb229a4", "prompt_hash": "7a559e849d82de951b4a8880252ed7d778988416a95ace4d84badbdc848b918f", "target_hash": "85be4ba72cce6d3c9225519468b0ef6b8eab95de05607b11e45932d94bd84e3a", "exact_match": 0.0}
{"doc_id": 451, "doc": {"question": "what album is help by the beatles on", "answer": ["Help!"]}, "target": "['Help!']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what album is help by the beatles on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Revolver"]], "filtered_resps": ["Revolver"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f6597005f823704bf47c37b68ff0729464fd99748217d784d3ef583f1fe11282", "prompt_hash": "b90e874dd17f2fc92e5070a05867fea0c3bcf496c98665b572da8e9cf59c5c29", "target_hash": "f2769e7cd87b3a5a82153a9df41f24b6a5043ec95a5e6e479955150a96091f83", "exact_match": 0.0}
{"doc_id": 452, "doc": {"question": "who was the king of england in 1756", "answer": ["George II"]}, "target": "['George II']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the king of england in 1756?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George II"]], "filtered_resps": ["George II"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "563a44c8a76e761c216b9682c9f6b471508259ade2e92438e64c6255bea1e7ef", "prompt_hash": "2091dcbae341bde2db50ed02c94b65840f50afbfb11d3af7ec6306a45b2e294f", "target_hash": "c305ae40d276daf755ca5b977b27ffb89f6d109b4c94e2a295dfaa91422b219f", "exact_match": 1.0}
{"doc_id": 453, "doc": {"question": "where is the left anterior descending artery located", "answer": ["the left coronary artery"]}, "target": "['the left coronary artery']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the left anterior descending artery located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" heart"]], "filtered_resps": ["heart"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8c6a8497d41aadf3cdcff2141e71f0aabfd891cfa39518d43ad24743b8f67bd7", "prompt_hash": "2ee1387ea8b8ac45cf73fb0ba2b367001d0304ac210a06fddc0df5482e99ea9c", "target_hash": "eec7c0315085f0d1cb6557d15ba4097610ecf3fddea55ea4cf8c5e7d3650b6aa", "exact_match": 0.0}
{"doc_id": 454, "doc": {"question": "who does the head of the fbi report to", "answer": ["the Director of National Intelligence", "the Attorney General"]}, "target": "['the Director of National Intelligence', 'the Attorney General']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the head of the fbi report to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the President"]], "filtered_resps": ["the President"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "056bd6324e38fa8693392f0ec950a6b4ab287f2dd4787584323f06c12c371f4a", "prompt_hash": "92d1066226a9825b064e9fa5a3cc297737b41736ac7a221e69713081913ec686", "target_hash": "f3e1803902ab8358a2e9237e673e0c1f9e30c04bc1c393d0e23bbd19615bd861", "exact_match": 0.0}
{"doc_id": 455, "doc": {"question": "who sings every light in the house is on", "answer": ["Trace Adkins"]}, "target": "['Trace Adkins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings every light in the house is on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2eb72eb157e17fdff27ab7613c04c819c8df7d9d2c2d8f28981444bb5aefe97d", "prompt_hash": "53489a38123fb4dd60b0559d3e5d5640a7fbe18a4a8b7f114fb5b436cc3aa7da", "target_hash": "73e445670ddd0bd5c640f53570820a554b1d67463ff33f7445758fb9472d9fac", "exact_match": 0.0}
{"doc_id": 456, "doc": {"question": "what is the meaning of the harp in ireland", "answer": ["the arms of Ireland"]}, "target": "['the arms of Ireland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of the harp in ireland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" national symbol"]], "filtered_resps": ["national symbol"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfe3a87b85076203b222c5e452f3511125670db0409502b50a116f9e606ee1d5", "prompt_hash": "f47a5fe8dffa085be1773cd80d9644f595b345454de79daa123c474ef88c4deb", "target_hash": "423afc4507e9bcd369965f8637ffd12e3239242a36e6cc6279fa9b0b0d682b55", "exact_match": 0.0}
{"doc_id": 457, "doc": {"question": "who wrote got this feeling in my body", "answer": ["Max Martin", "Johan Schuster", "Justin Timberlake"]}, "target": "['Max Martin', 'Johan Schuster', 'Justin Timberlake']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote got this feeling in my body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Jackson 5"]], "filtered_resps": ["The Jackson 5"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "99b3f99cf08e6638597e86705b0844f557442464284566b9b4728c6eb1876bd5", "prompt_hash": "9489c0318c481ff5ca4c98f93ea0b7ace2e405c0d1b048135cc1e863f1f512d3", "target_hash": "7574c3d4f1dc84a9212117d7971852a9b79c67bd781af49e0b3f63724f8dabba", "exact_match": 0.0}
{"doc_id": 458, "doc": {"question": "when does like cage season 2 come out", "answer": ["June 22, 2018"]}, "target": "['June 22, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does like cage season 2 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 11"]], "filtered_resps": ["January 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9ac3b50e4b4ac68551e2dead3017ba0671412cc7288afbb3629c4ce1a4508c60", "prompt_hash": "0e971e487b144a348992296380879e5a26fc7675c3d40216763318b54ce1c024", "target_hash": "8b978bc73344d79bc2f5b96849b5e3dc9133130a0dd73a5fdfe226a8f0f75270", "exact_match": 0.0}
{"doc_id": 459, "doc": {"question": "when did frank sinatra first sing new york new york", "answer": ["1980", "1979"]}, "target": "['1980', '1979']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did frank sinatra first sing new york new york?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1973"]], "filtered_resps": ["1973"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "264d3799abd8b1873dabf54fa1286efa5189c7ca9ae7712ab4c3a5852925f938", "prompt_hash": "395d660c8e7fecd10711e53d09f478d47b9d992ee8d86be0dd5c8fcb265d2d21", "target_hash": "cdb8dfe4cd43e9e6e94885d60b86692048ddb89509c59d6bfca918bd3a493b9c", "exact_match": 0.0}
{"doc_id": 460, "doc": {"question": "where was the salvation army's christmas collection kettle first introduced", "answer": ["San Francisco", "in San Francisco"]}, "target": "['San Francisco', 'in San Francisco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the salvation army's christmas collection kettle first introduced?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York City"]], "filtered_resps": ["New York City"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "89209fa8298ddb2572c3759cc3a778795803b4fd77c2dfadaa2ba84d9e527508", "prompt_hash": "d45c26b98d8a800df482c82f7e0f4b299d7175a68407cacda8857ed6fc62a4d6", "target_hash": "98b41a382b0aaf2dbcbcc8d5d23578f581637dd27b36a01702ffdf3843121952", "exact_match": 0.0}
{"doc_id": 461, "doc": {"question": "who is jojo in horton hears a who", "answer": ["Jesse McCartney"]}, "target": "['Jesse McCartney']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is jojo in horton hears a who?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mayzie"]], "filtered_resps": ["Mayzie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a5147f31937f502b4b1176577bd7a8c9ec971d7fda8e83053d15f62d7729ba5c", "prompt_hash": "0241f5b964fc024058a4830f7e8bd1cba121b6a2915dc5a611dbad495bb37938", "target_hash": "8e28337f72264894ec1fb98697d1611fcbe01b614e156e60ce12ade5d1bc5025", "exact_match": 0.0}
{"doc_id": 462, "doc": {"question": "what song is played while raising the american flag", "answer": ["Reveille", "\"Reveille\""]}, "target": "['Reveille', '\"Reveille\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what song is played while raising the american flag?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Star-Spangled Banner"]], "filtered_resps": ["The Star-Spangled Banner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "88665cf0ddaa5395e1f242a1edf6b4d45ef1823d6c2ff6c79164791248ba708e", "prompt_hash": "c81eab4998a4d711ac5da8e8cfb5ee2ee578bfd6354b32a87f3181db3e2794ed", "target_hash": "844812a93139b94ec1588f10e80a474147c8faca96776b2a93df253433f78640", "exact_match": 0.0}
{"doc_id": 463, "doc": {"question": "this poster was created to increase support for the war effort.the poster links food rationing to", "answer": ["aiding the war effort"]}, "target": "['aiding the war effort']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: this poster was created to increase support for the war effort.the poster links food rationing to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" patriotism"]], "filtered_resps": ["patriotism"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7dec1e62e51224dc46f3e35a9351429e046b5cae70d54c3103119080184b074a", "prompt_hash": "882bc6b94cedb29e921cb5b63d822605491d742fa95ad2a7f95bcf1e7b9c2ecf", "target_hash": "8b80fd355dbddfcd71a4d31588406edb862aa20c4df3b155c1b35ea9b7cc886e", "exact_match": 0.0}
{"doc_id": 464, "doc": {"question": "how many countries does cadbury sell its products", "answer": ["more than 50 countries worldwide", "more than 50"]}, "target": "['more than 50 countries worldwide', 'more than 50']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many countries does cadbury sell its products?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 100"]], "filtered_resps": ["100"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "323d2ab63f3b46e19b7b41a3341cb710278a6cbd547a82971d1ed60c1492e016", "prompt_hash": "747f4cd7a7c560ca66e3d9aa9dbbce75e62ab22eb6daabafdbc9c4031bef7b4f", "target_hash": "6f2848c97e87556c89181661889b2b128aa06aebfec738cced90436de20ad865", "exact_match": 0.0}
{"doc_id": 465, "doc": {"question": "when did mcdonald's sell 1 million burgers", "answer": ["By 1965"]}, "target": "['By 1965']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did mcdonald's sell 1 million burgers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1975"]], "filtered_resps": ["1975"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d0308ef5bd988f641a5206d3adeb1d0ff52d14baea402114d98056cca5c455e", "prompt_hash": "d47e6665a59c88591321ed864060e974ea0602a736a3026a81d464902a8c5cd7", "target_hash": "f29ae80991018145561e7fee72f639e4db8329c777b71bb2ca1e21be29f67570", "exact_match": 0.0}
{"doc_id": 466, "doc": {"question": "what are the power of supreme court in the philippines", "answer": ["Justices"]}, "target": "['Justices']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the power of supreme court in the philippines?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" judicial review"]], "filtered_resps": ["judicial review"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4afe8cc6b4c1e8c108afac12d423548e2628ca29034eac951a9213a82d671a23", "prompt_hash": "65a14f71433764c0b57ee9113779cb0f41110d0cf89b0a4fca87bb1a118bdc12", "target_hash": "fea93190a39cccf429460b50e525692fc34fb70f36ed0feab75764c5dc3770f2", "exact_match": 0.0}
{"doc_id": 467, "doc": {"question": "when does the last episode of adventure time air", "answer": ["TBA"]}, "target": "['TBA']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the last episode of adventure time air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" November 21"]], "filtered_resps": ["November 21"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "112ec1d984c6dfcf63e7560d5eca060cb6d8a47c8ed1664f3379b7b95ada511d", "prompt_hash": "d6eb260041d893144800a7a9a9051bc946fefa4ae537f61d36c55b575fdfad3c", "target_hash": "f8d86866cae6da1292290b219611a77b715295b97684052714fa7bafdce522e9", "exact_match": 0.0}
{"doc_id": 468, "doc": {"question": "who are the cast members of ncis new orleans", "answer": ["Zoe McLellan", "Lucas Black", "Daryl \"Chill\" Mitchell", "Shalita Grant", "Rob Kerkovich", "Vanessa Ferlito", "Daryl Mitchell", "Scott Bakula", "CCH Pounder"]}, "target": "['Zoe McLellan', 'Lucas Black', 'Daryl \"Chill\" Mitchell', 'Shalita Grant', 'Rob Kerkovich', 'Vanessa Ferlito', 'Daryl Mitchell', 'Scott Bakula', 'CCH Pounder']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the cast members of ncis new orleans?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David McIlwraith"]], "filtered_resps": ["David McIlwraith"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9185e1247b7b988d02733199bef70194e13d5b4a3bf8a3aea5953b2008ceb728", "prompt_hash": "815e1460f9b5a89ee0e4d428bbd031b25c59e382437243f1eba8d70fa2c6e1f3", "target_hash": "9c0fad848c5afd4fa0a45b3f981519459ba310cb956c38b9bcbdd1c33bf75a9f", "exact_match": 0.0}
{"doc_id": 469, "doc": {"question": "what word is used to describe knowledge about the universe and method of obtaining that knowledge", "answer": ["epistemology", "Empiricism"]}, "target": "['epistemology', 'Empiricism']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what word is used to describe knowledge about the universe and method of obtaining that knowledge?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" science"]], "filtered_resps": ["science"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a58a4d11373040f9ce8e371dedcfd3c0e1131741fe55d1015a552e0d93ffe49", "prompt_hash": "1f5827e2017e0b89d7f249a37c6b35e9093ea10a71ff85738f670c350f673666", "target_hash": "bb73fbc1121cd16a58982acea471d9aeca3ee747e1c9c3cc0ef4245f8d059658", "exact_match": 0.0}
{"doc_id": 470, "doc": {"question": "how many paintings of sunflowers did van gogh paint", "answer": ["two"]}, "target": "['two']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many paintings of sunflowers did van gogh paint?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 37"]], "filtered_resps": ["37"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b31ae8738d3ffa9265dee45bf8177fc38d7494b1bc74f5b3ff6ee2d81b638c6f", "prompt_hash": "bfb3129ac71bc9480c52d2e84e46347f96b3bb0d774291823fa13cdd574e8c6c", "target_hash": "e5308d30c3c259f8b22b83f586470d404a7a19aa1351fe2216e81cb611e4af57", "exact_match": 0.0}
{"doc_id": 471, "doc": {"question": "when was the $1 000 bill discontinued", "answer": ["1969", "December 27, 1945", "July 14, 1969"]}, "target": "['1969', 'December 27, 1945', 'July 14, 1969']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the $1 000 bill discontinued?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1969"]], "filtered_resps": ["1969"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b162054ded05a621f66f0affc14e6266b5e2927c6905b603ff4ac115a7701056", "prompt_hash": "1b6208a4af06e1d11f2894b7193d1ed3fd279837609aa00a78aa1eee888d07b3", "target_hash": "e5a9e56b763706bc9f1811269e21e6f208ef3a1d49a8b50cc618d5a487d3e048", "exact_match": 1.0}
{"doc_id": 472, "doc": {"question": "what's the population of prince edward island", "answer": ["142,907 residents", "142,907"]}, "target": "['142,907 residents', '142,907']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the population of prince edward island?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 158"]], "filtered_resps": ["158"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c7ec83f6cdf5a694002fc816344a7a324785949bc4441117d7b7bc0f8dfcccf6", "prompt_hash": "9dfc2bccbbd211f8dd110913335513637a5c9530014dfdad67d7bc06c47e5eb3", "target_hash": "b3b34ffdf97bc03d0c54a1d227cf0c941bd5e51445e8881cee950aca25a43872", "exact_match": 0.0}
{"doc_id": 473, "doc": {"question": "how many ceos of fortune 500 companies are female", "answer": ["32"]}, "target": "['32']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many ceos of fortune 500 companies are female?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6022fe112688267ddcd2dc4d05e4edcf11fc50a0ea6fd4d56ffc14a5364a1d7d", "prompt_hash": "64ece69dc160c26a01d6cca796581052bfb28374d68f1f231b05fa380785cfb1", "target_hash": "d131175028cad293189b2f275716bcf81d712ca30419972cad7e981fb82c65b6", "exact_match": 0.0}
{"doc_id": 474, "doc": {"question": "where is the orinoco river located on a map", "answer": ["in South America"]}, "target": "['in South America']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the orinoco river located on a map?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Venezuela"]], "filtered_resps": ["Venezuela"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4f75e2f3c1e2ecc0ac951ff41f39b73cbbdd38e6cf7ef8847f7f669af8ce621", "prompt_hash": "ac991e671ec5a3f1bc65a995306d64b80022b2f27f60e1262014a2663fde9a20", "target_hash": "2daeade3d866caf4e6aa105410ce1d050aeb075f7a0aae0bdb4461a66a2091ab", "exact_match": 0.0}
{"doc_id": 475, "doc": {"question": "when was the debating club established in almora", "answer": ["1871 A.D.", "1871"]}, "target": "['1871 A.D.', '1871']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the debating club established in almora?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1897"]], "filtered_resps": ["1897"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ec3bbd2c38e28e086a0356d7736239a6bf1ea810a8a7584d9927e586a4bb7c0b", "prompt_hash": "101c0756c9896151aeb0710a47c4082be7d37dbd99f78cb16607a6f6d3ae47f3", "target_hash": "92ea8ad8e4990e187e3e01c9f12922bf3ecfa8aeb0f8e6b5c531d19544961272", "exact_match": 0.0}
{"doc_id": 476, "doc": {"question": "when was the last time the boston red sox pitched a no-hitter", "answer": ["May 19, 2008"]}, "target": "['May 19, 2008']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the boston red sox pitched a no-hitter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1917"]], "filtered_resps": ["1917"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "457035bca2e6bd3ace2f49dc4670a0b00bd09d443e1c12a2937293acb8d87b0c", "prompt_hash": "659ff9e81ddd0ba63ee7f339a9ff2aad0dbd09cc8330159a6c5afc522b2c20b5", "target_hash": "e86d4acb80be9fa40074fd34e041b292494702db2ba043af5da0a2290203ae58", "exact_match": 0.0}
{"doc_id": 477, "doc": {"question": "who was the french chef given credit for developing the classic kitchen​ brigade", "answer": ["Georges Auguste Escoffier"]}, "target": "['Georges Auguste Escoffier']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the french chef given credit for developing the classic kitchen​ brigade?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Marie-Antoine Carême"]], "filtered_resps": ["Marie-Antoine Carême"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "23394661e4747e0efe6647a1760b87621557c10058307c2dbdbdc77ef03e7a0b", "prompt_hash": "d8ae65196204a4637a81353459805bd1b590b5fb353630cd872650f127271776", "target_hash": "92291fe1f2b82a18fbf60690759a3107a95083a71de358a926fe6a7091e438a1", "exact_match": 0.0}
{"doc_id": 478, "doc": {"question": "when does the cannes film festival take place", "answer": ["Cannes, France", "usually in May"]}, "target": "['Cannes, France', 'usually in May']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the cannes film festival take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" May"]], "filtered_resps": ["May"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bac3075e3b2be3145ec2c94f78439fe49aa49b5b57f9f3280625f73377f7c33f", "prompt_hash": "f3f8434b8ad41c51a957e4485a7d66d495140778ec2ebf71fd7bc3c9c035cc52", "target_hash": "3ffce4c6d98a8f9c80f73c41aa318ba1ca7641b91159d78fe4223c7752982cf0", "exact_match": 0.0}
{"doc_id": 479, "doc": {"question": "who plays the dad in pretty in pink", "answer": ["Harry Dean Stanton"]}, "target": "['Harry Dean Stanton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the dad in pretty in pink?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Schoeffling"]], "filtered_resps": ["Michael Schoeffling"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ccaff4c9fdfeff668a5285e8ef8eec432931c6e756ea1b2c14a9153282354b1", "prompt_hash": "798f55f83571c607ddac581f2590e20092dcd361759437539ca7344ecd40f450", "target_hash": "704092fc291af2b0294ad97ea96a6fb62859c05c9ad6cb942f3a7650cc207171", "exact_match": 0.0}
{"doc_id": 480, "doc": {"question": "unsaturated fats are comprised of lipids that contain", "answer": ["double bond", "at least one double bond"]}, "target": "['double bond', 'at least one double bond']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: unsaturated fats are comprised of lipids that contain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" double bonds"]], "filtered_resps": ["double bonds"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b6c9b433a9f158434698c3eedccabea56b83ad768fee93fa1d1962d4b2d84880", "prompt_hash": "58c2f6bd850394b2071f664b8ec07758f722226ea00034a4d8a755db3e51a5ee", "target_hash": "571466e4333fe8e03a45adaf36d33d12d085d16974529deea31908641b065ecf", "exact_match": 0.0}
{"doc_id": 481, "doc": {"question": "when do the new episodes of supernatural start", "answer": ["May 10, 2018"]}, "target": "['May\\xa010,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do the new episodes of supernatural start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 11"]], "filtered_resps": ["October 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "434faec85401586786c13e0fa94dfb7e7a4e04259c9d33a33000f8dee1a64592", "prompt_hash": "86040127e93238939ba143c9c0487c2da578ac221c6d4013e3a98ebd468f02b6", "target_hash": "1f5d857af85edc38a9ce20d984ef28d539dc3be800cbcf2adb7a7e9beabe290a", "exact_match": 0.0}
{"doc_id": 482, "doc": {"question": "who won the award for best goalkeeper in football world cup 2006", "answer": ["Gianluigi Buffon"]}, "target": "['Gianluigi Buffon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the award for best goalkeeper in football world cup 2006?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oliver Kahn"]], "filtered_resps": ["Oliver Kahn"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b15bd61fb5691653f295e7b622d543f1fa6301b54d968ad1d9b7d23e10f64ee3", "prompt_hash": "1492947aff0663792a9b6905182b82279cec912bcc161872183fb3e6978539aa", "target_hash": "9e487d06821ead069b5361a7c242551b1b5d212cf7925e0a3ac2a01a8a5f1e05", "exact_match": 0.0}
{"doc_id": 483, "doc": {"question": "what is upstream project in oil and gas", "answer": ["drilling exploratory wells"]}, "target": "['drilling exploratory wells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is upstream project in oil and gas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" exploration"]], "filtered_resps": ["exploration"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f8e13223f800206b9a3afda7efd6a9739a2b96979f43d198034eefcbaaaaa1a", "prompt_hash": "0669827d5825127ea6bd62800e5cd6d139b5e8c0224cf9751bce9f7268d306ca", "target_hash": "0996056dbbec05aeafc973a97aa2b1c886de8df66c9b896f167690d4bef8ebb8", "exact_match": 0.0}
{"doc_id": 484, "doc": {"question": "is a network connection device that can build tables that identify addresses on each network", "answer": ["routing table", "a router"]}, "target": "['routing table', 'a router']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: is a network connection device that can build tables that identify addresses on each network?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" router"]], "filtered_resps": ["router"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d3f84fa6616e1357becf8bc58a9c7e446dbe99a1aa615007a2ef1aff3d5295aa", "prompt_hash": "603cb8fab0e37cf1f9dad4b9f728a14bc031b0304e20f79d9917eb019911370d", "target_hash": "1e24399830d5f88949d5aaea8f116077ab1f101a3069ec3924aa280e24cb4e41", "exact_match": 1.0}
{"doc_id": 485, "doc": {"question": "the era of the great mughals began with the accession of", "answer": ["Akbar the Great", "Babur"]}, "target": "['Akbar the Great', 'Babur']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the era of the great mughals began with the accession of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Babur"]], "filtered_resps": ["Babur"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "74a72ee4dfa10fa894e2f60a545ff3a5ecbe79cad774d14b677db6215e165766", "prompt_hash": "39683b5544d088ecde4d66374f78da1cc833004e00ff2a3730afb92993630495", "target_hash": "3b1cf02bb6f4f29593c99e668145f5ebfa554436558a9362dc3b8ca7232435f5", "exact_match": 1.0}
{"doc_id": 486, "doc": {"question": "what was the final episode of quantum leap", "answer": ["\"Mirror Image\""]}, "target": "['\"Mirror Image\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the final episode of quantum leap?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Leap of Faith"]], "filtered_resps": ["Leap of Faith"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "36b522688319855c1faa4ac23c8ffdbbb3de966af7ce0528528ba4deb8cab1da", "prompt_hash": "9dfb676c6631ee29cec01d092247428f45f465486ea9a8cdac707a827acd7a68", "target_hash": "d387e4ac7571c5b3edbd80596d07e8bc4fb5a7e32dd58064e1caf5db9a693916", "exact_match": 0.0}
{"doc_id": 487, "doc": {"question": "when was the first election held in india", "answer": ["1951–52"]}, "target": "['1951–52']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first election held in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1951"]], "filtered_resps": ["1951"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d59f6c090484c246e92a6ff13acecf7da0ff5d07eccf14d2836fcc2db5d77b80", "prompt_hash": "d06192272000625389546edfdee381cfec1f2af6ffcd7bdf6e6e59fae41264bc", "target_hash": "6fee232f1c1397131a1333bb0d28fa3b3e648d980eb923fdae371680dc3c38cf", "exact_match": 0.0}
{"doc_id": 488, "doc": {"question": "who has been appointed as the election commissioner of india", "answer": ["Om Prakash Rawat"]}, "target": "['Om Prakash Rawat']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has been appointed as the election commissioner of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sunil Arora"]], "filtered_resps": ["Sunil Arora"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b2a1b7e2183f18c8078fdec738bad5c576fe2e6e189f505a6b8feed043b687ac", "prompt_hash": "4dbcca8b0abff6b5e55c844096b618ab76bac759e9170a91feb86ec848a1d23f", "target_hash": "4b2c04c0efd0c7ea1bfc9b526d32e4f945d81eea512a1a47679c3f6c9b747cd3", "exact_match": 0.0}
{"doc_id": 489, "doc": {"question": "who discovered cells divide to make new cells", "answer": ["Hugo von Mohl", "German botanist Hugo von Mohl"]}, "target": "['Hugo von Mohl', 'German botanist Hugo von Mohl']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who discovered cells divide to make new cells?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Robert Hooke"]], "filtered_resps": ["Robert Hooke"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c17c3e78d984bb27e5bc7651a0e9fd449d58f6e8d5233fab62ca0dc07238540", "prompt_hash": "8b4f14dcd2f44241d1f77a0be8a036ad561a6c9a97b455be90e7ac404db40969", "target_hash": "12b99f604aeb5ef53e75d29c1e01831f0eb9acf225c915f977375de43cb747c9", "exact_match": 0.0}
{"doc_id": 490, "doc": {"question": "where does a brisket come from on a cow", "answer": ["the breast or lower chest", "breast or lower chest"]}, "target": "['the breast or lower chest', 'breast or lower chest']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does a brisket come from on a cow?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" brisket"]], "filtered_resps": ["brisket"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "acf4d5838adc41ac508553045630045a4a89feb0fe4c76af8487086af57083ba", "prompt_hash": "1a8b863797d43e9c35ef07284dedf339deef38b55da110c2d816d44078cd143b", "target_hash": "f5471597babda2493f648fa1b5b6d1f1cec5dda931c23e1c0032c7325c307fa7", "exact_match": 0.0}
{"doc_id": 491, "doc": {"question": "what are the parts of a domain name called", "answer": ["subdomain", "top-level domain", "hostname"]}, "target": "['subdomain', 'top-level domain', 'hostname']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the parts of a domain name called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" TLD"]], "filtered_resps": ["TLD"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d1112fbe4418036eb3163765a698df6ae10c3f3aa7415a24a4caa4909c3d3b7e", "prompt_hash": "d86a0d971ed4861ad29e3429cee675571d09665ca7ffbd8a1fe3d898e954302d", "target_hash": "5712deddeacfd2c17f112cbbd473619c724927339aff40693f38e1e96ba1def4", "exact_match": 0.0}
{"doc_id": 492, "doc": {"question": "who does april end up with on parks and rec", "answer": ["Andy", "Andy Dwyer"]}, "target": "['Andy', 'Andy Dwyer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does april end up with on parks and rec?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Andy Bernard"]], "filtered_resps": ["Andy Bernard"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b57f8c260a77d3b4f474a1a7da3216bbf86c1bdd07048da71905d2efa72cfef", "prompt_hash": "e8d5e442d6a4589022d22691d0afe60afc18f6931047157ef80e120dd44b3fe2", "target_hash": "f2c3636a7b34d41b145d795de57ed2c40ac1983f2b52ad335a734e19372462e3", "exact_match": 0.0}
{"doc_id": 493, "doc": {"question": "who played tom in four weddings and a funeral", "answer": ["James Fleet"]}, "target": "['James Fleet']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played tom in four weddings and a funeral?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hugh Grant"]], "filtered_resps": ["Hugh Grant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35302ad9a46de4fc5180af438dfdc2e9e05728e3f62b0f54880cbfadf1cea741", "prompt_hash": "42d8d8fe69b1264b47c7ad30e9d8038df19d757db68dbcce908e337a0af9d09a", "target_hash": "f1a547686d01d1254e77f4c909b1892cce8783208e251eadb32579713ebd01fb", "exact_match": 0.0}
{"doc_id": 494, "doc": {"question": "who sings don't take your guns to town", "answer": ["U2", "Johnny Cash"]}, "target": "['U2', 'Johnny Cash']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings don't take your guns to town?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dwight Yoakam"]], "filtered_resps": ["Dwight Yoakam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "66c5fb0a977d0cdbdd0cc7dbbca67a294dd7eb59ffed71722da7ec8e1ab28f81", "prompt_hash": "2bc0d52ba8fcf535f04477096fb8774c68658304b23f8be37bfe65c4df0f2860", "target_hash": "e0a06545f5b6b4001ac12129678fd89d71b16590d4f8ba5520d05c6688739ee8", "exact_match": 0.0}
{"doc_id": 495, "doc": {"question": "where did huntington's disease get its name", "answer": ["the physician George Huntington"]}, "target": "['the physician George Huntington']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did huntington's disease get its name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Huntington"]], "filtered_resps": ["George Huntington"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d311efd795c3e965433a2df671139bfa21308a760a206468949536e29a7f2dbd", "prompt_hash": "686359c666e7499f7047f993ea93e2d4ac1d5582c272e1e43be84350cf63888d", "target_hash": "f707408ffd04a6b85faa246da4595f19244d51c29080035cdb65dfbb04563efc", "exact_match": 0.0}
{"doc_id": 496, "doc": {"question": "when was the last time michigan won the championship", "answer": ["1989"]}, "target": "['1989']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time michigan won the championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b2ee54a507fb42acfb9d24c3dedb45b0a7714c78028b86a32dd98d7b1f89f17b", "prompt_hash": "a95bf7f8e7988e6360a1223722f5d5ff175b8bb1ff77f1ce8bdaa86575062a4b", "target_hash": "f29cbf85fafcf80ace6273501b873d82d249a9b85142c12822207ef416c8728f", "exact_match": 0.0}
{"doc_id": 497, "doc": {"question": "who did deion sanders go in the hall of fame as", "answer": ["Cornerback"]}, "target": "['Cornerback']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did deion sanders go in the hall of fame as?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" wide receiver"]], "filtered_resps": ["wide receiver"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae9ab5a5771aaaf8385b40fbdce20e3a04858def79414a08feefd990e270d75b", "prompt_hash": "a137f059555517860a451ff1bd76f69886e719fce28b73a5b1e507c0d4a0dd63", "target_hash": "3c85a51a81e1b599d69b82b3ed563fec4a460b1908a2a67455cfcb5113d8900e", "exact_match": 0.0}
{"doc_id": 498, "doc": {"question": "who are the leaders of the jehovah witnesses", "answer": ["Gerrit Lösch", "Kenneth Cook", "Mark Stephen Lett", "Mark Sanderson", "Anthony Morris III", "Samuel F. Herd", "David H. Splane", "Geoffrey Jackson"]}, "target": "['Gerrit Lösch', 'Kenneth Cook', 'Mark Stephen Lett', 'Mark Sanderson', 'Anthony Morris III', 'Samuel F. Herd', 'David H. Splane', 'Geoffrey Jackson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the leaders of the jehovah witnesses?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joseph Franklin Rutherford"]], "filtered_resps": ["Joseph Franklin Rutherford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9afb93e02326ae9501ca2d2094b6f3f2f64c48897d3ac9cd27ada7230244eae8", "prompt_hash": "67e66366844b5b3386bbb129a67f7112e84fac912f701567d01c49cce943dc4e", "target_hash": "7f13eb62000e6b075115d376c66b4959cc2badd801dd31b9c895acb1768538d2", "exact_match": 0.0}
{"doc_id": 499, "doc": {"question": "who was originally cast as phoebe on charmed", "answer": ["Lori Rom"]}, "target": "['Lori Rom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was originally cast as phoebe on charmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alyssa Milano"]], "filtered_resps": ["Alyssa Milano"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aed9a2cad8ad9aef5067eb0ceb925c012036ec26090b1837773556da4190acbd", "prompt_hash": "1ad4aca868fe98b2a9cc0c15eff874027c2137fa8d2a79a89379e126ddffaba0", "target_hash": "435612713810c611ee4d3241403ce447275a821ce0c875951089a0a495f818ad", "exact_match": 0.0}
{"doc_id": 500, "doc": {"question": "who turned out to be the mother on how i met your mother", "answer": ["Tracy McConnell"]}, "target": "['Tracy McConnell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who turned out to be the mother on how i met your mother?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Robin Scherbatsky"]], "filtered_resps": ["Robin Scherbatsky"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "588a84dc4e9bde82d3ed2086f7447e29a33d7365e00076cbe1241f279d8ce748", "prompt_hash": "96356441c92f7c43bb0db65f9af4e1eee818b4080dc3190326d666f6e67dbddd", "target_hash": "42a97dd11c4dd5dca2fd825a1a36f956d2a782330af88c6c0dfcae02cb50d105", "exact_match": 0.0}
{"doc_id": 501, "doc": {"question": "animals that are active at dawn and dusk", "answer": ["Crepuscular", "crepuscular", "Crepuscular animals"]}, "target": "['Crepuscular', 'crepuscular', 'Crepuscular animals']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: animals that are active at dawn and dusk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" nocturnal"]], "filtered_resps": ["nocturnal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "244d2056f789d3b84b7d8d6a78ff4011b1dac2b96c05731448b6a2497c1418f8", "prompt_hash": "43043d8a14008f59d7d421732a2e8c236e8995d4aa0b5ce5062766dcdea9273e", "target_hash": "6a2a11ee0633d7f6bafae2dc78c3802a08165df2bdd15cd9b5dcde1d258a0e0b", "exact_match": 0.0}
{"doc_id": 502, "doc": {"question": "whos jack the ripper in gotham by gaslight", "answer": ["Jacob Packer"]}, "target": "['Jacob Packer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: whos jack the ripper in gotham by gaslight?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jack the Ripper"]], "filtered_resps": ["Jack the Ripper"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aed04b8183ee5b158807a0d1f6f84e218b05f49a20b66f9ac55e072b946ae5fb", "prompt_hash": "b0b57af8a9f09112c130ace0a1b07c49656cee9aaec0e430519956e6fefcce8a", "target_hash": "2edc31b927af899dffaf3c77dffe3c937601e0032dc189fa0507a624879f5375", "exact_match": 0.0}
{"doc_id": 503, "doc": {"question": "where do secondary xylem and phloem cells arise from", "answer": ["The vascular cambium", "the vascular cambium"]}, "target": "['The vascular cambium', 'the vascular cambium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do secondary xylem and phloem cells arise from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" vascular cambium"]], "filtered_resps": ["vascular cambium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "255fa96f9e63ad1f4adbc34499043be8fc0bb8b94b3396ee2740df259a70253f", "prompt_hash": "43fbbe0a27a3be812ee62ee05cbbe0fc6e31503e5924a9274e9fcdaad6e6e3c8", "target_hash": "cf70fee5df3ac64382ee18a3014e94bdd0a1e19444feb979f3dfefe8445fc57b", "exact_match": 1.0}
{"doc_id": 504, "doc": {"question": "who did johnny manziel play college football for", "answer": ["Texas A&M", "Texas A&M University", "Texas A&M Aggies"]}, "target": "['Texas A&M', 'Texas A&M University', 'Texas A&M Aggies']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did johnny manziel play college football for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Texas A&M"]], "filtered_resps": ["Texas A&M"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c088767aaeeb518f46f5e80e664a891f211445d9e3f9bf49b7f2b2ae5aff75e", "prompt_hash": "dea5295007ada6ed420f68a115b2fad75fd7f321fb11a1167542f383a34ba1d0", "target_hash": "013db993b2df453bb64ff51c34e63caf75c6fb401b23c9e977bcc4d94a16e14d", "exact_match": 1.0}
{"doc_id": 505, "doc": {"question": "who sings the christmas song mary did you know", "answer": ["Michael English"]}, "target": "['Michael English']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the christmas song mary did you know?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael W"]], "filtered_resps": ["Michael W"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8c0548844fc25e2e8684392b97ed40803a87176d7202456fe633cfb96df3d419", "prompt_hash": "cc39a15d6fe17719b6c8d03cb0337bfc260d379427ff3fa3474bc29d827413b6", "target_hash": "da992ee62ecaaa70627cb55296e0fe92f177159f1bf8b6b7d5ee247fc6884c5e", "exact_match": 0.0}
{"doc_id": 506, "doc": {"question": "who sang gonna sit right down and write myself a letter", "answer": ["Fats Waller"]}, "target": "['Fats Waller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang gonna sit right down and write myself a letter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "45a91bad0c8a5118263790c4a29f353c59c0a0ac49ff50b5b8cbdee81ed8a148", "prompt_hash": "55aea804d1ea975f6c543ad66d080ad7e7914adfe9fd34efc59419e7b196af15", "target_hash": "c609802148055c418368e9f1b7499ddca5d4414e4d563348540c44c3abb49d09", "exact_match": 0.0}
{"doc_id": 507, "doc": {"question": "who picks the players in the nfl draft", "answer": ["each team"]}, "target": "['each team']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who picks the players in the nfl draft?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" NFL teams"]], "filtered_resps": ["NFL teams"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d0c94591ba5f33748046bca42842e506d6dc362994665c1cb82996b9e6ebf0ea", "prompt_hash": "2a7f5baf7faa72c9d26d753042e99c82dc7b7131a67df21cdf21f206c7f3a208", "target_hash": "0f2be59a7c85a7ba0a701bd00c4ff610ee017a1ece64bd1369b2ba3890bb48cc", "exact_match": 0.0}
{"doc_id": 508, "doc": {"question": "who sings got my mind set on you", "answer": ["James Ray", "George Harrison"]}, "target": "['James Ray', 'George Harrison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings got my mind set on you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lionel Richie"]], "filtered_resps": ["Lionel Richie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f54769d46d386fb3fe560cbffed0218b14e359f345d8da818d8d8279dc7fd3e", "prompt_hash": "918d8cf085b44b5d51e257dd0ea619ad51e19838cefb49deb31803f74d16421e", "target_hash": "53a8a95c263296ba2ea7d8fbe7888bb76c221f6a89697af84f4e2eb2dc5e54e4", "exact_match": 0.0}
{"doc_id": 509, "doc": {"question": "when did the song here comes the boom come out", "answer": ["May 2002", "May 14, 2002"]}, "target": "['May 2002', 'May\\xa014,\\xa02002']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the song here comes the boom come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "01fe2367d83daa9eb768f4b8ccc85754766fe8c66b6ba4de855821ce475748a2", "prompt_hash": "b5c369e15e44d398d30ce1fb8e64305d42dfa3edb01d84afc9118b9165f99078", "target_hash": "efd1da8be9ca3e0943f169b6a72da1328dcd7ddb86d6ed728f94f58d6bea4465", "exact_match": 0.0}
{"doc_id": 510, "doc": {"question": "who sang the song i think we're alone now", "answer": ["Tiffany", "Tommy James and the Shondells"]}, "target": "['Tiffany', 'Tommy James and the Shondells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song i think we're alone now?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shania Twain"]], "filtered_resps": ["Shania Twain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e53537edf131bc2cfd0010f06955fdee1fcea052b5b145a36875c977bbc9b43d", "prompt_hash": "32c25744b5a7a607df6419af78f8dae70b2199eec1a400639db6e0dad41327cf", "target_hash": "e6930397702dd94ebd09b4ab88a5b1133d4e81a419fc776a7a9047287d0f7fd3", "exact_match": 0.0}
{"doc_id": 511, "doc": {"question": "when did we first put a rover on mars", "answer": ["January 2004"]}, "target": "['January 2004']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did we first put a rover on mars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3441578fe9744118b1072f373e149cc0f9d6af292584c9902493048b568e19f", "prompt_hash": "57a16d30ca0588bc8d078e40d30e144b04e213e77b30828b628059a4b126d98f", "target_hash": "08a66e81d4e9b0fc94736242aa51985cfa08efd6e91a2ccf02fd97492acaf209", "exact_match": 0.0}
{"doc_id": 512, "doc": {"question": "who were the members of the traveling wilburys group", "answer": ["Roy Orbison", "Jeff Lynne", "George Harrison", "Bob Dylan", "Tom Petty"]}, "target": "['Roy Orbison', 'Jeff Lynne', 'George Harrison', 'Bob Dylan', 'Tom Petty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the members of the traveling wilburys group?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Harrison"]], "filtered_resps": ["George Harrison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ceff51a5b8f7bd67ec50d34fcc159a5d6ecfbc18bf1db6fbad5d390b8db030d2", "prompt_hash": "ab6c0f7a65ded2075a5a3412fb9246a92cf19410ff65c5b4c4a3f7d774eb73c5", "target_hash": "c83f74ad14b080af8f99fb76cc2ce8f613f4bd662b91024d8392454969768d92", "exact_match": 1.0}
{"doc_id": 513, "doc": {"question": "who won game 4 of the 2000 nba finals", "answer": ["Los Angeles Lakers"]}, "target": "['Los Angeles Lakers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won game 4 of the 2000 nba finals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Los Angeles Lakers"]], "filtered_resps": ["Los Angeles Lakers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "225c2ede67fee31d63e38a61230894d8c8899cbb3b79244401d9b89b2e68e9bb", "prompt_hash": "eaebdfd88b5568e6ae4990a2395a6f796b7ba5c58009d20384e98867e717b88e", "target_hash": "d6630190cf9693ea3d7c60379bf072ab39ed3493ee93caaf9d0fdffa20b9c12c", "exact_match": 1.0}
{"doc_id": 514, "doc": {"question": "what is the highest peak in the ozarks", "answer": ["Lookout", "Buffalo Lookout"]}, "target": "['Lookout', 'Buffalo Lookout']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the highest peak in the ozarks?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mount Magazine"]], "filtered_resps": ["Mount Magazine"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1caffc535652010d3146156f0ccf20db23ad275ccddf02aeab31f2d477d79624", "prompt_hash": "22f29c6a59bf9a8b7603fdf127489693f45516213854b09f631b5766a9eb3193", "target_hash": "3f375fe265a25ec53d5663a4f4689b117b04b6b4d83964ea631849eb7cbbd205", "exact_match": 0.0}
{"doc_id": 515, "doc": {"question": "what is the name of the last episode of spongebob", "answer": ["\"Bottle Burglars\"", "The String"]}, "target": "['\"Bottle Burglars\"', 'The String']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the last episode of spongebob?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bubble Trouble"]], "filtered_resps": ["Bubble Trouble"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5aec54f7689f1e9978b429ca4b1e8ecfd750ca94968e4c02351cb713eb6ea0c", "prompt_hash": "cab8580f13f3ad4f282e4ef35a0f32b45df10ad068f6938c9ee367d95e6828da", "target_hash": "48628d807db0a4dd228667fde1d9e67534c99aca7786efa690a4b971a9a3c18c", "exact_match": 0.0}
{"doc_id": 516, "doc": {"question": "when was the last grand slam hit in the world series", "answer": ["2016"]}, "target": "['2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last grand slam hit in the world series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c3ccb1a5307783a217f8047442f30201e13fa0362e73e79a2705d3d621667e4c", "prompt_hash": "1959a58920eec24fa9a757be089f11ced147000ac0d5aa04e4032d2e7bc7a928", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "exact_match": 0.0}
{"doc_id": 517, "doc": {"question": "when does episode 29 of boruto come out", "answer": ["October 18, 2017"]}, "target": "['October 18, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does episode 29 of boruto come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" July 15"]], "filtered_resps": ["July 15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9e7d49bada390a3d18672202e7e29c0af2382e04a5dfc0f3a3385437a4c642ab", "prompt_hash": "3f3f8ea601354bd2481779367d10243ffa8aeba0dc3f5fb8c3b3025c386b96da", "target_hash": "05f8e4a552906ce5fa6785d26d08e5c68e243a51ec8df27155b5310798aea268", "exact_match": 0.0}
{"doc_id": 518, "doc": {"question": "where was the tv show in the heat of the night filmed", "answer": ["Decatur in Dekalb County", "Atlanta", "Covington, Georgia", "Hammond, Louisiana"]}, "target": "['Decatur in Dekalb County', 'Atlanta', 'Covington, Georgia', 'Hammond, Louisiana']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the tv show in the heat of the night filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mississippi"]], "filtered_resps": ["Mississippi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f52bdce26f4879126223b4a298752993b9b7eaf28cb864e8b217c524070f0fbb", "prompt_hash": "a6dbf85cec98f3dbf2566e0928924180ff17cd7886323b685814a94a42d36c27", "target_hash": "b46b6a1c300662dd288b2490cc52f3f4fe684a817eeaa60d04a2b479ca49a71e", "exact_match": 0.0}
{"doc_id": 519, "doc": {"question": "who are the judges on do you think you can dance", "answer": ["Nigel Lythgoe", "Mia Michaels", "Adam Shankman"]}, "target": "['Nigel Lythgoe', 'Mia Michaels', 'Adam Shankman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the judges on do you think you can dance?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carrie Ann Inaba"]], "filtered_resps": ["Carrie Ann Inaba"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bdbe9cd6ae90f3a22362ecabc6da7354ca9e4cdb2e8be081f88e7a42b468b9b8", "prompt_hash": "4c356adc73a43a83bbddc4c73e76e1f1c4f5b892c9fddeb5b82de7d3d3fc53c2", "target_hash": "1846c19d8db10912866ad4f5b6c7774b2ee32a00ed6d7d351f07b71a5d3106e8", "exact_match": 0.0}
{"doc_id": 520, "doc": {"question": "the most common form of megalithic architecture in europe is", "answer": ["the portal tomb", "portal tomb"]}, "target": "['the portal tomb', 'portal tomb']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the most common form of megalithic architecture in europe is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dolmen"]], "filtered_resps": ["dolmen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3a7ec7472f9c921b90a92f6e6e363a8a60e071f434c30fc2398ca16dbaaa0b9", "prompt_hash": "56f03e2a6bed4d16219a43f97ee5eb75ebd2db5170a362b68440be21c613ac9b", "target_hash": "c3c6570b62ba83450537c15a42b3033260be87b1abda8c539060d63e16964df1", "exact_match": 0.0}
{"doc_id": 521, "doc": {"question": "how long was dark side of the moon number 1", "answer": ["a week", "42:49"]}, "target": "['a week', '42:49']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long was dark side of the moon number 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 20 weeks"]], "filtered_resps": ["20 weeks"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "323c0d2a26c71738a963075d222c25bfa51bb0136ebe13423ed6c71f20db3fc3", "prompt_hash": "60162ea069e5ce7162c54994c430fc92dfade080656c41607d2c9e3e73ca5d7a", "target_hash": "ae73b805c9b1f405478e785831fd37ed32c780d9047cbe7a3a8e1986bf6aa78d", "exact_match": 0.0}
{"doc_id": 522, "doc": {"question": "the golden age of india took place during the rule of the", "answer": ["Chandragupta II", "Samudragupta", "Chandragupta I", "Gupta Empire", "Vishnu Gupta", "the Guptas", "Sri-Gupta"]}, "target": "['Chandragupta II', 'Samudragupta', 'Chandragupta I', 'Gupta Empire', 'Vishnu Gupta', 'the Guptas', 'Sri-Gupta']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the golden age of india took place during the rule of the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mughal Empire"]], "filtered_resps": ["Mughal Empire"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a3ca0544679c2d337fa31bdb4e089d9d63df5eddf37c77c573d9a2dc62b7226f", "prompt_hash": "c4f6dcff75e9f741c066d70d0163a531345729b556a639a449cd7f8406a87231", "target_hash": "128d887890339dc179d3255c098dd011a513cd97d9c39f898fca458e133baccd", "exact_match": 0.0}
{"doc_id": 523, "doc": {"question": "what is the origin of the name robinson", "answer": ["English language patronymic surname"]}, "target": "['English language patronymic surname']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the origin of the name robinson?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Robinson"]], "filtered_resps": ["Robinson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dc132c90fae3455d116d437954e2175f03b285fd399cf7c2187585eac45f1d7c", "prompt_hash": "f7725fed7ee92e66fe9866cb01c71879c5d57e191fe534bb8fbef42ca815a89b", "target_hash": "cb204650ce25fe06feb772c915fe51f023d7626316874f17388beb5566e105b3", "exact_match": 0.0}
{"doc_id": 524, "doc": {"question": "when was the first super bowl the eagles ever won", "answer": ["2017", "1948"]}, "target": "['2017', '1948']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first super bowl the eagles ever won?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3ed0e92cae748bc548e4a61868e491dc37947b97ab1e7ff5335daa23c90b3985", "prompt_hash": "5e3c480b6941140b64319451a087f5d9b0f20d89d853e83cb9c00198aa7d5758", "target_hash": "518477ddf2729074f6d6d5e5b7a33fd2df3708ec45f3c1d00326e35ba5b196d6", "exact_match": 0.0}
{"doc_id": 525, "doc": {"question": "who has the best batting average in test cricket", "answer": ["Australia's Sir Donald Bradman", "Don Bradman", "Sir Donald Bradman"]}, "target": "[\"Australia's Sir Donald Bradman\", 'Don Bradman', 'Sir Donald Bradman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the best batting average in test cricket?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sachin Tendulkar"]], "filtered_resps": ["Sachin Tendulkar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92d28cf5af7552dec0eafe4b384e597299ab909ade68803e366120ae27d608d1", "prompt_hash": "4247e5b945dd9c4f845bef05cf141aa662cdec4ba72dca636bd1518431ee39b3", "target_hash": "c69c8b4159391c5bcb55d2233c887f4ec066fada61f4d05c9f9158a862303d3b", "exact_match": 0.0}
{"doc_id": 526, "doc": {"question": "how long has it been since eagles went to super bowl", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long has it been since eagles went to super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32a50978306aecaafd22d7b99cc5a5e768157c9518311092291a3b689220162b", "prompt_hash": "58916f6debf86aa11df2a82e71a680ae7165ee5130b69acc1c6c94c468e2ea53", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 527, "doc": {"question": "who built the tower of london in 1066", "answer": ["William the Conqueror"]}, "target": "['William the Conqueror']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who built the tower of london in 1066?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William the Conqueror"]], "filtered_resps": ["William the Conqueror"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c160e73925191849837bfdcc8e77af0d02c80bc4727d5dd19a6ea92dc4167e9b", "prompt_hash": "361548314ac8c4034992a36240f20c25e2f8b7a545d0bff90d6905725127de2e", "target_hash": "0b9b1314467a8939ea5e267be52412e675d0f6711d9d922941e2788bfadd7657", "exact_match": 1.0}
{"doc_id": 528, "doc": {"question": "where is fulda and what is its significance", "answer": ["Hesse, Germany", "on the river Fulda"]}, "target": "['Hesse, Germany', 'on the river Fulda']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is fulda and what is its significance?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Fulda"]], "filtered_resps": ["Fulda"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db6058d397e497126e7138d36130e207fcfe7fe3418e5856dd034485974a0806", "prompt_hash": "cee85496abf8d5bed481724e10777a9496894925f8644dae14b70c77786afac9", "target_hash": "6f4a85baff69a914bc122c404b25876a8acbf76bd16fb111490e3b10f2226b73", "exact_match": 0.0}
{"doc_id": 529, "doc": {"question": "who overthrew the mongols and established the ming dynasty", "answer": ["Zhu Yuanzhang"]}, "target": "['Zhu Yuanzhang']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who overthrew the mongols and established the ming dynasty?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Zhu Yuanzhang"]], "filtered_resps": ["Zhu Yuanzhang"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fbeb1501d38b929f542ab09a3781179338419da34f291be1000dfeea8b616755", "prompt_hash": "82efaa9f69776976260c073ef0d9cc9acdb179ac943f310044d2423eae19efb7", "target_hash": "b9069db500cac552dd46eae7eda67734f9a4476fb2ce1f286f293ccbd70ceb52", "exact_match": 1.0}
{"doc_id": 530, "doc": {"question": "when was the first book made into a movie", "answer": ["1924"]}, "target": "['1924']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first book made into a movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1915"]], "filtered_resps": ["1915"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3ba73f42c83fef7e9223347e36ead97c952e0efac438875c6e0184b25c84295b", "prompt_hash": "28e32d9952ef64482ab650e6d8a8c8ce1c20bb8a421bee10fa962484c620e48f", "target_hash": "5a32a22495b5db8838ee77d9290b4a353b26cf488d4082279189078229daa58a", "exact_match": 0.0}
{"doc_id": 531, "doc": {"question": "when did tim hortons open in the usa", "answer": ["1981"]}, "target": "['1981']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did tim hortons open in the usa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1964"]], "filtered_resps": ["1964"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "180e361d9aef3288f461a5b5771ac6695606616b9f817b0ef3e9ba9f25dd85e5", "prompt_hash": "4f14dab312c5034c0065160ec9bd1fab459a5ca44abfd18c002fc1d79e49280b", "target_hash": "957d512e662d9fcb3974a8776c0b0a6d5ff8134c98ca9d129f2941b247a8d498", "exact_match": 0.0}
{"doc_id": 532, "doc": {"question": "when was the reaper invented by cyrus mccormick", "answer": ["1837", "1834"]}, "target": "['1837', '1834']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the reaper invented by cyrus mccormick?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1840"]], "filtered_resps": ["1840"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d6347809d70cf2da6c7d2a434f3c3fcbf0fb6c83dc8753ffcbcd88add78f52c", "prompt_hash": "333750b9a80d7e7c996eb539e85d33f4a3e3a20c07340989e1f3b372b146aeae", "target_hash": "2be5469ebe9566e2d3f848b7cb03dc6eed5cadd076cc2c2fb49a172603649f0f", "exact_match": 0.0}
{"doc_id": 533, "doc": {"question": "when is the next scandal episode coming out", "answer": ["February 1, 2018"]}, "target": "['February\\xa01,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the next scandal episode coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 17"]], "filtered_resps": ["January 17"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3225a8aa96d992059b73695f80b774213101c8e0a0145571b50c74d3513766f", "prompt_hash": "563926fa2c73beaefdf2c99c8f55201b3604da8c9c5bc3f61bdfdac839768aea", "target_hash": "681e25f9bc4dfd976d9d9f6c0b5b775c6452424ef13557c52a57f7c16caefbd5", "exact_match": 0.0}
{"doc_id": 534, "doc": {"question": "who wrote antigone and what are the dates of his birth and death", "answer": ["winter 406/5 BC", "c. 497/6", "Sophocles"]}, "target": "['winter 406/5 BC', 'c. 497/6', 'Sophocles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote antigone and what are the dates of his birth and death?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sophocles"]], "filtered_resps": ["Sophocles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d83f7ecbfe1a15be5792346508afb2870ae0184bca52aeba4e8bf59f16e2e3dc", "prompt_hash": "356295866fb84510505a63189ec16c12368266cc1b78bd430ea857e6ff49e465", "target_hash": "0e1dab8923ed2e16332d1a6ec9677d117358c2a961964d216c155979c4b2759f", "exact_match": 1.0}
{"doc_id": 535, "doc": {"question": "what goes on the bottom of shepherd's pie", "answer": ["minced meat", "mashed potato"]}, "target": "['minced meat', 'mashed potato']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what goes on the bottom of shepherd's pie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mashed potatoes"]], "filtered_resps": ["mashed potatoes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1097a9274f00a19c1924574523e1a848c8ad23055d4526eae86f64b6f1ea8cc7", "prompt_hash": "6dc8a840a8477c18872b200e86b7bc2231817fdd849dad481dc31da913bfb50e", "target_hash": "16d48579ec368e1f0a05db81af17d38b00acb615a8f9f3a2825e85d292e587b0", "exact_match": 0.0}
{"doc_id": 536, "doc": {"question": "where does the ohio river and the mississippi river meet", "answer": ["Cairo, Illinois"]}, "target": "['Cairo, Illinois']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the ohio river and the mississippi river meet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cairo"]], "filtered_resps": ["Cairo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5723623e10deadf19b6bfd75f1e8f74de1712c423da016d0f8e8ad96c0b7f3a4", "prompt_hash": "2d32fe7ad150d1d3df2da947d089c3359bd91e5a0e5020d7fd0b35dcb170ba14", "target_hash": "525f4cdff369250551eb2d4f92eaaf534d4284d28ccd09b0cb81c45e0cb5c42d", "exact_match": 0.0}
{"doc_id": 537, "doc": {"question": "how many kilometers of great wall of china", "answer": ["8,850 km", "21,196 km"]}, "target": "['8,850\\xa0km', '21,196\\xa0km']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many kilometers of great wall of china?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 21"]], "filtered_resps": ["21"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e7b6b7fcdfbe5b4d23c553ac88958cd35cc10fb71245db93b711dcddd19441f", "prompt_hash": "77d11948f4a9b012eefe2c0f2b55f7ef46f93e32e017b98e443819d68a3045c4", "target_hash": "0ce364a07285254c85647e98d00b365d046dd81288d169f84ed030a3947906f6", "exact_match": 0.0}
{"doc_id": 538, "doc": {"question": "when did they start assigning social security numbers at birth", "answer": ["1990"]}, "target": "['1990']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they start assigning social security numbers at birth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1935"]], "filtered_resps": ["1935"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3000567f65199e4506c43fd18d9639af57e696ced8295f344ac198277f6f7769", "prompt_hash": "1ed34eabab6699570f405c4c8fd7ecac40419af361580b3cc833b5f4c6a19a96", "target_hash": "e0b02261abe31dfd5543c214570eaa30cdb8fabfb3f4f4830a9f11581c824889", "exact_match": 0.0}
{"doc_id": 539, "doc": {"question": "under the federal unemployment tax act which party pays unemployment taxes", "answer": ["Employers", "the employer"]}, "target": "['Employers', 'the employer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: under the federal unemployment tax act which party pays unemployment taxes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" employers"]], "filtered_resps": ["employers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c61ae32f73ec0c72ef4f42599fc85fdefde9a3379996770adb18ad14e87af3a9", "prompt_hash": "b8ac776dfe5d19ea210710f9a054b629f32b49ea3d2d96c7f45b04371d976e41", "target_hash": "10168dc9240daffa8e1b3ed8f63fb0679b0298ccbb76a8ccf7b7f29d1a8987de", "exact_match": 1.0}
{"doc_id": 540, "doc": {"question": "what are the names of the hunger games", "answer": ["The Hunger Games"]}, "target": "['The Hunger Games']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the names of the hunger games?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Suzanne Collins"]], "filtered_resps": ["Suzanne Collins"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d445e8bcac430b5b462e69be09bc7b86d34bcd4c4f07126763c8059a9fb1e65e", "prompt_hash": "56eeefe105a4e423543ad5612ee04e56a58881aec7f3e619535b35d4c1bf6307", "target_hash": "360b135956a42e58f1bc66b5e69359fe057446b23f32d2a8becab074c870e88b", "exact_match": 0.0}
{"doc_id": 541, "doc": {"question": "where does safe haven take place in the book", "answer": ["Southport, North Carolina"]}, "target": "['Southport, North Carolina']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does safe haven take place in the book?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michigan"]], "filtered_resps": ["Michigan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0a72df6114d00e07a068b3d346301988a3bddccce3f8606e65da46b526fdd34f", "prompt_hash": "bc791cbc9922cd7db00295a915bb9408273f8c5d71da3f16c96211edc1139a3b", "target_hash": "3d0d55933059fe9869f6edd776b9e46144a277b40f7d0b1073c2b6a3f146dd08", "exact_match": 0.0}
{"doc_id": 542, "doc": {"question": "who played jason in friday the 13th the final chapter", "answer": ["Ted White"]}, "target": "['Ted White']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played jason in friday the 13th the final chapter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jason Alexander"]], "filtered_resps": ["Jason Alexander"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0f8abb316bc54c195120afb7d498b27415aa84c7add450f9db4cb588f12670d4", "prompt_hash": "c3f656062f7547fe6c5ba0195af5e20644a2779335bc5ae5610bd9dd502f30cd", "target_hash": "be828da79ccdf11e9471bf6f8e183cd66275f43f1865cd74b253df02fcda2bd6", "exact_match": 0.0}
{"doc_id": 543, "doc": {"question": "state the position of india in terms of population size", "answer": ["second"]}, "target": "['second']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: state the position of india in terms of population size?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" second"]], "filtered_resps": ["second"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "94623a4313a32f0b503542eaafdd506daac522836b69502ca10e080f8615419b", "prompt_hash": "3a2dbbb8dca0d9dac0a160ff6dc0bb11015b77ea5abc16cbdaabb02a39adadce", "target_hash": "5978731ebdd7774befbb1b6e1cbf50718707fa71d0d20e8fc5659d6e9243992a", "exact_match": 1.0}
{"doc_id": 544, "doc": {"question": "real name of daya in taarak mehta ka ooltah chashmah", "answer": ["Disha Vakani"]}, "target": "['Disha Vakani']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: real name of daya in taarak mehta ka ooltah chashmah?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Daya Rai"]], "filtered_resps": ["Daya Rai"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dd3cb2a29396b38575d51d8cc2504f9a3c6399a196f07be7ad80a8ebd51708f5", "prompt_hash": "f7317d8abc984ceddccdee5df41fd0171f4c26aa3e63a2f8786e563fa2a3c8e7", "target_hash": "13242bda6dee109c829cfb31032a8375339a2721d74cde08d8f8fcf3847939a7", "exact_match": 0.0}
{"doc_id": 545, "doc": {"question": "when does the shape of water book come out", "answer": ["March 6, 2018"]}, "target": "['March 6, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the shape of water book come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1b4d40d7411d35670869f0838a84fe7ec64db9905015dd3876faccc67997a695", "prompt_hash": "6136b73cd0fe824ec61708637c17d9de9a8dfe5eba69c7aa688b009873ea2d1e", "target_hash": "137bc02726cc16f9d640913195d01ef0df117ee5a5de80074a3e9aa6f374ec60", "exact_match": 0.0}
{"doc_id": 546, "doc": {"question": "who discovered that neural communication between cells occurs through chemicals", "answer": ["Charles Sherrington"]}, "target": "['Charles Sherrington']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who discovered that neural communication between cells occurs through chemicals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Claude Bernard"]], "filtered_resps": ["Claude Bernard"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d5cb777d926b71eb65a3772560c85b2a6c34ea9bfe21e4715febe94b29c2455b", "prompt_hash": "d8ccfc4a93c8aeddb106e18592f12f5daf8ce6397676a2bbcb44ffcdd569cefc", "target_hash": "552f035374169ba71c999633fcb64268842bb6ae466c2a85b6993a59c392d14b", "exact_match": 0.0}
{"doc_id": 547, "doc": {"question": "who may be affected by monetary penalties imposed by the irs", "answer": ["Taxpayers in the United States"]}, "target": "['Taxpayers in the United States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who may be affected by monetary penalties imposed by the irs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" individuals"]], "filtered_resps": ["individuals"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "78bbf8c7ecfa471631b3295cb9baf924ebf2f6e676f25ff595f840a63345922b", "prompt_hash": "553cef0a391e63aeb1cebadbbb12f19925e2128760fa1bf06a5fda6a3f382eb5", "target_hash": "1672d19579ea0d45b54210551986e2ea68ebb90fec105486f1acf822280814ff", "exact_match": 0.0}
{"doc_id": 548, "doc": {"question": "where did they film ghosts of girlfriends past", "answer": ["Massachusetts"]}, "target": "['Massachusetts']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they film ghosts of girlfriends past?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Atlanta"]], "filtered_resps": ["Atlanta"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d0c675fe1adce002d5f03c8b6c09de36194e3e2dbd747063e86392f10998f672", "prompt_hash": "b36a29c664bef6212a55c0a3965c1eeb946b0586600f7477e8ce05cd5943a046", "target_hash": "89d52666ddb5962ba1f4ac72138c5913efac9a77edff3159a78a318cd90be1ca", "exact_match": 0.0}
{"doc_id": 549, "doc": {"question": "aphasia is a term defining difficulty or loss of ability to", "answer": ["comprehend and formulate language", "formulate language"]}, "target": "['comprehend and formulate language', 'formulate language']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: aphasia is a term defining difficulty or loss of ability to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" speak"]], "filtered_resps": ["speak"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a728ab19870287497ad67cec0c6174ffb54a0fa873e203b6b466938e105526e4", "prompt_hash": "4cbd006044f04109536499152fd315dfd86a0e3802d06023d40b9204c678a97a", "target_hash": "f51c43915773c722b343dd40e43fb64e16ae03f86b255458aea7c608dfb27e91", "exact_match": 0.0}
{"doc_id": 550, "doc": {"question": "what was icq as related to instant messaging", "answer": ["an instant messaging client"]}, "target": "['an instant messaging client']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was icq as related to instant messaging?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" instant messaging"]], "filtered_resps": ["instant messaging"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "66f82141ce64024f76257e2c51304f9e70b0d5280e417e95a2235f0a6d5b3cad", "prompt_hash": "3a73b47002787f3adfa430d607fa4263df71aa25abc3a09a153abc67d7b814ca", "target_hash": "035261a14ed42b536ef5585fc7e5ee9a4b1788f8d11ec6efa8415ad5d60091a0", "exact_match": 0.0}
{"doc_id": 551, "doc": {"question": "how many episodes in adventure time season 1", "answer": ["26"]}, "target": "['26']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes in adventure time season 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 26"]], "filtered_resps": ["26"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f839152107cbc2c2fb429ed16b88e1ed1e009e913e88e7169133c366c5057097", "prompt_hash": "592298914a3afa944c81308bbf5d188dce93199c3b88ab8e096747d2c6c0822f", "target_hash": "36f103bfa180cab4646a275ad7c212b50941b1693063fde5912f0447227479f1", "exact_match": 1.0}
{"doc_id": 552, "doc": {"question": "when did the us stop drafting for the vietnam war", "answer": ["1973"]}, "target": "['1973']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the us stop drafting for the vietnam war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1973"]], "filtered_resps": ["1973"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "adef034829c0b62299cf3df2ed3c07a31cbde52277e3088f883ca8f873adf541", "prompt_hash": "c060dff633f5f2c74c6c12f8b3556fdb9e789c9ec0bd2f2ce634ea7ee56f2a18", "target_hash": "387b1fb00107960d85ded2b00e1864020065df1bd6ac810dc0fabc026ebd0e54", "exact_match": 1.0}
{"doc_id": 553, "doc": {"question": "who were the bands in pitch perfect 3", "answer": ["Evermoist", "Saddle Up", "the Bellas"]}, "target": "['Evermoist', 'Saddle Up', 'the Bellas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the bands in pitch perfect 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Fifth Harmony"]], "filtered_resps": ["Fifth Harmony"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7233089ea8bdbce7b110d5834a207774b5ac27c7e722b47b08f6328923aad1b5", "prompt_hash": "e3d105666c8c530b0c2628a38945d7a838392a9a953530531a47f8bb13dc52e3", "target_hash": "8adaf7befd70c7eb9b8cc06d513830316b7231e775b185d5da0fe30709fa9fdd", "exact_match": 0.0}
{"doc_id": 554, "doc": {"question": "when does walking dead season 8 second half start", "answer": ["October 22, 2017", "February 25, 2018"]}, "target": "['October 22, 2017', 'February\\xa025,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does walking dead season 8 second half start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 7"]], "filtered_resps": ["October 7"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bae8f458fad4c691454d5a194f2c4b64676eb8cef1214b90c056e7898d6cf5bb", "prompt_hash": "f357b9f4b18b968cbf4af24c49d979d915bbb62f747ab31f6a1c5d1a22d64da7", "target_hash": "9fb7fbf48e1ddc5d5ae41e339bc014e5caa9253db76d187339fa14aa0023bf82", "exact_match": 0.0}
{"doc_id": 555, "doc": {"question": "who plays whitey bulger's girlfriend in black mass", "answer": ["actress Dakota Johnson", "Dakota Johnson"]}, "target": "['actress Dakota Johnson', 'Dakota Johnson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays whitey bulger's girlfriend in black mass?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jessica Chastain"]], "filtered_resps": ["Jessica Chastain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0ec55a81564ba64a6a07975b7019f9c84ab2ee25180bde474cd4f5a42120c48c", "prompt_hash": "e4bf538a97bbc65a7478c6b70d15d4dbff7d39f084ab2190f53f1a970105cf1b", "target_hash": "f1e416dc4edab7ebc92d9b1b55fb3f39bf895b5710c4bc1591212d78e4bba1b6", "exact_match": 0.0}
{"doc_id": 556, "doc": {"question": "when will the next episode of flash be aired", "answer": ["May 15, 2018"]}, "target": "['May\\xa015,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will the next episode of flash be aired?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 10"]], "filtered_resps": ["October 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "347ec90c93002df3f7e40230c52b1df4d474fd7c7a494e0e179f9787baf9bf0b", "prompt_hash": "44cfcc31fa6152c8e8316ab9d0314f450bcbe77f63da8f56f0dcaf02c0d64e29", "target_hash": "1587eaf4b96c3d2980b264be61a0023af9dadeb4a75f890137158583546d578b", "exact_match": 0.0}
{"doc_id": 557, "doc": {"question": "who plays izabella in transformers the last knight", "answer": ["Isabela Moner"]}, "target": "['Isabela Moner']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays izabella in transformers the last knight?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rosie Huntington-Whiteley"]], "filtered_resps": ["Rosie Huntington-Whiteley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e2dc422bbf31031af4288125ae0c64ad9be3461d189c5e9b3180cbd7ff787912", "prompt_hash": "47b297bd263b341c71f8e790e8468c4ed66205d211e8a5b1ff49ed76e83da4e4", "target_hash": "10ce9833376a266d933e2abc26abaee1541b4f07661e9eac2beeb72ec70efda9", "exact_match": 0.0}
{"doc_id": 558, "doc": {"question": "who has the most yards per carry in nfl history", "answer": ["Emmitt Smith", "Jim Brown"]}, "target": "['Emmitt Smith', 'Jim Brown']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most yards per carry in nfl history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walter Payton"]], "filtered_resps": ["Walter Payton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d93761da9600f44538e04542f03b3ba8cbdd830a980fd3cd3c8d60f67b18058d", "prompt_hash": "fd4e2bf5c5bf6fbee113eb3c3409f024a5fdb305eb75989655394715eeac881f", "target_hash": "051f7907647330906dc72f594a6a6b9bc3bda60c98ba554610e97ef7c7efe92f", "exact_match": 0.0}
{"doc_id": 559, "doc": {"question": "sweet leavened bread prepared for easter in romania", "answer": ["Kozunak", "Cozonac"]}, "target": "['Kozunak', 'Cozonac']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: sweet leavened bread prepared for easter in romania?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pârcâi de pasărtie"]], "filtered_resps": ["pârcâi de pasărtie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7317b3c879a1cbb9e39ee208508a8e574ec787b08703f1128282d1615e0dca12", "prompt_hash": "298748e965a91ba878b7d9c8cb3dc223960c4b968ba5bee090d7576b4f45f2c8", "target_hash": "9916fac544832dbb92583dc3694f02b7b90d0424dbd06e540f4751e899254ec6", "exact_match": 0.0}
{"doc_id": 560, "doc": {"question": "when were birth certificates first issued in the united states", "answer": ["1902", "1900", "From 1900 to 1946"]}, "target": "['1902', '1900', 'From 1900 to 1946']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when were birth certificates first issued in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1890"]], "filtered_resps": ["1890"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e8df849b01b615c2cd776cf0309637ee7d6536cccd98f112948deaf083ea7b01", "prompt_hash": "15de59791362603ef86dd3e9f77830245523ca2dafb7ccdce1a4ca25f661b250", "target_hash": "bb251cc8a21fcbfc8a7ec92cd35fccfb4b48fd2204f64cd09f0ea9a1c897e1dc", "exact_match": 0.0}
{"doc_id": 561, "doc": {"question": "where was because i could not stop for death published", "answer": ["1890", "Poems: Series 1"]}, "target": "['1890', 'Poems: Series 1']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was because i could not stop for death published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Poems"]], "filtered_resps": ["Poems"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "24b479bed765535b78c03a5defc5e89cd3827f0f404096fa0c4c54a379d301eb", "prompt_hash": "f0b6c7dff2074b1f4a8ca68fe45e2ad72d1974c01e44e63b0f0e409fe9678765", "target_hash": "78c7364970c906dd0cd221d61584017848758eba25b086b243f4fb299e559eac", "exact_match": 0.0}
{"doc_id": 562, "doc": {"question": "oppo is sponsor of which country's national cricket team", "answer": ["the Indian national cricket team", "India"]}, "target": "['the Indian national cricket team', 'India']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: oppo is sponsor of which country's national cricket team?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" India"]], "filtered_resps": ["India"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ec10d5f55d8a2958b1978ff2d32cfda41d75bd5b42ca7b081a96ad6057338f8", "prompt_hash": "344793883c1474887dd09f68c228c23a6c1ad66ed98f5fbdb87fe08b4052253d", "target_hash": "fee8d3eb3d476cbcbaae7f4250985243d6ed532d7435d63c0d87a8044a2816e9", "exact_match": 1.0}
{"doc_id": 563, "doc": {"question": "when did fender start making amps in mexico", "answer": ["1996"]}, "target": "['1996']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did fender start making amps in mexico?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1965"]], "filtered_resps": ["1965"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fc1009f9f4d4ad06071d74fe96489cda61dd81e6fc4464e7a44aeeb82848dc0b", "prompt_hash": "8fa2702e34c0534d097a16ffc262dd4a9739f2dacf99ab4f6c05919c29789736", "target_hash": "4884bad91c23e17f70a396eecdc381a658567383863ece0c7474966e045c229b", "exact_match": 0.0}
{"doc_id": 564, "doc": {"question": "where will be the next olympics be held", "answer": ["Tokyo"]}, "target": "['Tokyo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where will be the next olympics be held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rio de Janeiro"]], "filtered_resps": ["Rio de Janeiro"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7cb8720b24870324e7515477d115216e151a49e20fe719a80377ab4dc42f0460", "prompt_hash": "956fc58ab53553be7069e93fb2f923a5868c41eca04da33b0a33b16a9415b353", "target_hash": "caf7379a49bf5d66024d194c2125a150c1d2753600181004e3d58431023f147f", "exact_match": 0.0}
{"doc_id": 565, "doc": {"question": "what is the money called in south korea", "answer": ["won", "The won"]}, "target": "['won', 'The won']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the money called in south korea?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" won"]], "filtered_resps": ["won"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "53c0b445fa186f70fc1975a182e3c1f145d2254d8b5ed871a322a3a6f2bb8668", "prompt_hash": "663d814cea2ccdd0902b9feb4f5f5239f1eecfbd78c29c1e654ee21b84abc726", "target_hash": "f26f251659d1bbfefa0fac54915c603b56fd467a7da0d8aff9316f21b5b0b306", "exact_match": 1.0}
{"doc_id": 566, "doc": {"question": "name of the boundary line between india and bangladesh", "answer": ["International Border (IB)"]}, "target": "['International Border (IB)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name of the boundary line between india and bangladesh?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" International Border"]], "filtered_resps": ["International Border"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a9e877ce8b5cb557b5469b40287c073caab9e0fb4469a5133a028955c8b1b10", "prompt_hash": "4842a2871d4fed79827663267c5cd737ed763fd517656e6bd4d5ed3eaf384027", "target_hash": "8eee841b95c752a64509d227edb9c451ade79126b8fbb68e8d2c077db582798c", "exact_match": 0.0}
{"doc_id": 567, "doc": {"question": "what tectonic setting is responsible for the folded mountains of pennsylvania and the high himalaya", "answer": ["a convergent plate boundary"]}, "target": "['a convergent plate boundary']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what tectonic setting is responsible for the folded mountains of pennsylvania and the high himalaya?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" collision of tectonic plates"]], "filtered_resps": ["collision of tectonic plates"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9d352a78e3d43d050305e02b9a2ad5a1a9364bba006d007f6b218a3c651e88da", "prompt_hash": "b5bae2a02a862b99ea5680ed540dbd7af76d6491aba73a83e5c518218eceef9d", "target_hash": "9ba89b16bda013ff37536aeadbc0d91717dbf27b346e80f5f250cf348b0a24db", "exact_match": 0.0}
{"doc_id": 568, "doc": {"question": "when does dragon ball super episode 130 comes out", "answer": ["March 18, 2018"]}, "target": "['March 18, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does dragon ball super episode 130 comes out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 14"]], "filtered_resps": ["January 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9cabaaecc0d849faacf4e2422a0ce71c242e368fcad0a639f5781fdc8cb6f044", "prompt_hash": "aceef0f2c1d04cd4a5937e676f1506c7d362cd9af7007c0d0acd2f15cb9418b1", "target_hash": "a9d0a6a6f04a2b338bc54956cf87cc898ac19b4dfaf92e4b81462c38ee3efd17", "exact_match": 0.0}
{"doc_id": 569, "doc": {"question": "what percentage of the population is naturally blonde", "answer": ["2%"]}, "target": "['2%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what percentage of the population is naturally blonde?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77ce5eefd5e8f51ec1268d7909c832c76b4adbcc62d40bccd0bc2642e3d68509", "prompt_hash": "7bc7055ae70f09a8e907d7845aa7aedd0a8e9967bb3d8678969c7cdef7228b1b", "target_hash": "71dd5a4e43b6683d951c42bf6d47f5105a5530eed6aee4d96f3248070b567c9a", "exact_match": 1.0}
{"doc_id": 570, "doc": {"question": "how long did it take to build the great pyramid of egypt", "answer": ["20-year period"]}, "target": "['20-year period']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long did it take to build the great pyramid of egypt?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 20 years"]], "filtered_resps": ["20 years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "acebbdfd08369a6b40bd4c0dd33b3761b1832251fbeecf9d08c7db0409a7957e", "prompt_hash": "f0644536522ba6751b473c66fba14c198b1298e3f63e67809696417a56f2996f", "target_hash": "29939be36ebb0ff03b1a3c87b9ac531c87dedd84e2f7f2ceb2817e2f02a98a50", "exact_match": 0.0}
{"doc_id": 571, "doc": {"question": "who was the president of pakistan during 1971 war", "answer": ["President Yahya Khan", "Yahya Khan"]}, "target": "['President Yahya Khan', 'Yahya Khan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the president of pakistan during 1971 war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yahya Khan"]], "filtered_resps": ["Yahya Khan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "318322fc1af336241d97b41a62219a4e6cd160c4c640d4cbe17233274d73b3de", "prompt_hash": "035f3b5fe4aaf230f23ca73339f4557206b291ea2ee859b3bb315361189b779d", "target_hash": "08724aadae3c759a39d07826342f75022ab14d03edcb21687396063f86914577", "exact_match": 1.0}
{"doc_id": 572, "doc": {"question": "what is the scientific name for the calf muscle", "answer": ["gastrocnemius", "soleus", "gastrocnemius muscle"]}, "target": "['gastrocnemius', 'soleus', 'gastrocnemius muscle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the scientific name for the calf muscle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" gastrocnemius"]], "filtered_resps": ["gastrocnemius"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2cdc0010e3d23b73981cf398461d5ba902b53822c1c54214e3482e7a37e858c9", "prompt_hash": "26b47d34a1f029b2ed6443c5f3dc4b9efeda9b36f2426e2fdaaa73355ba4e777", "target_hash": "3701e017188bc3329ad1bd5639363a72126ce66e96f3b2fde79ff39cca579d7f", "exact_match": 1.0}
{"doc_id": 573, "doc": {"question": "where in the constitution is the executive branch referenced", "answer": ["Article Two"]}, "target": "['Article Two']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where in the constitution is the executive branch referenced?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Article II"]], "filtered_resps": ["Article II"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ffccca1422533453fc1d7d9be7bb3f14596325e8b687e86187bf06fd8a111133", "prompt_hash": "aaa0da0afb081c1560f6fee9728f0dd12bd64fd8dbf02e07caacd8a87b27b78c", "target_hash": "3a76306156582ba4b1b07c17e24df5b2e8a3a21408448bce7f8703eac6e60ec7", "exact_match": 0.0}
{"doc_id": 574, "doc": {"question": "what emperor took over france after the reign of terror", "answer": ["Napoleon", "Napoleon Bonaparte"]}, "target": "['Napoleon', 'Napoleon Bonaparte']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what emperor took over france after the reign of terror?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Napoleon Bonaparte"]], "filtered_resps": ["Napoleon Bonaparte"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0950a3fbc792be884f57238a1ed457a3acfa48fddc389fdb607ff93d65d94874", "prompt_hash": "d9ea1cfbc0a3e4a629f00f102103a145975b885e2dc35d55e233fa88e6dafbb3", "target_hash": "15acce9a0ba428e6a8d89983ec79b3c459072b3e3f0473cafd0d50a68fa72c6f", "exact_match": 1.0}
{"doc_id": 575, "doc": {"question": "who plays mr wilson in dennis the menace", "answer": ["Joseph Sherrard Kearns"]}, "target": "['Joseph Sherrard Kearns']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays mr wilson in dennis the menace?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Astin"]], "filtered_resps": ["John Astin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "47b09411cb6cb56f53eaee8d3770dd94f730d82db229be12f753c63867395429", "prompt_hash": "d41b34e4af63c314315af36c6b0aaaa52b0dbdbe687be953dc214346f40fb9eb", "target_hash": "0253f0f3179b49925f0c33730e6909ffd16179da8229c7df59d394405c1a57e5", "exact_match": 0.0}
{"doc_id": 576, "doc": {"question": "who are the judges on the fisa court", "answer": ["Thomas Russell", "Claire Eagan", "Dennis Saylor", "Anne Conway", "Raymond Dearie", "Robert Kugler", "James Jones", "Rudolph Contreras", "Jeb Boasberg", "Michael Mosman"]}, "target": "['Thomas Russell', 'Claire Eagan', 'Dennis Saylor', 'Anne Conway', 'Raymond Dearie', 'Robert Kugler', 'James Jones', 'Rudolph Contreras', 'Jeb Boasberg', 'Michael Mosman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the judges on the fisa court?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Robertson"]], "filtered_resps": ["James Robertson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fcf13bf27defb87420bb4591304fccebe851205fcf50d33734b5fce17f35d588", "prompt_hash": "61f32ca4343eb3970f095558e280f818f7b6c9393183c1ae82d9c5dc868e6276", "target_hash": "f2eb85d8219b2efbce9bbdef39a5cfefacf4f7d525295d4c3a7016b695dd3df0", "exact_match": 0.0}
{"doc_id": 577, "doc": {"question": "when did rob dyrdek's fantasy factory end", "answer": ["March 5, 2015"]}, "target": "['March 5, 2015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did rob dyrdek's fantasy factory end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2011"]], "filtered_resps": ["2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0e5ec2edc9bba8f86240d51970aed18a07fcfccfc9ec57b2bdb1630457d91e4f", "prompt_hash": "70797810461c862227feee7ec169a4d2f664845f5d9a53ec7475901ac3e86a85", "target_hash": "a98050ce2af0c57b40b8a604b4ceec7f254d70fa8f814b17bb907937be7f569f", "exact_match": 0.0}
{"doc_id": 578, "doc": {"question": "who was the girl in the video brenda got a baby", "answer": ["Ethel \"Edy\" Proctor"]}, "target": "['Ethel \"Edy\" Proctor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the girl in the video brenda got a baby?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Britney Spears"]], "filtered_resps": ["Britney Spears"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "97e6999df91453aaa5837e1b599fe322dff305492654df4f661f5cd82bac032a", "prompt_hash": "e2364629b5b3ab71378360b0bee78ca373e8bef4471b40259ce242413a7f4783", "target_hash": "c6e0a224fd8b8ecc4b6253b0a240f5e724ac7aa4ee46df9e31c0ac1496bf023b", "exact_match": 0.0}
{"doc_id": 579, "doc": {"question": "who discovered that plants were made of cells", "answer": ["Parashara"]}, "target": "['Parashara']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who discovered that plants were made of cells?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Robert Hooke"]], "filtered_resps": ["Robert Hooke"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "012c0b3297981e8541dfb020570cc6aa71d88d79afbd3fbf2ba187fb4077193e", "prompt_hash": "5354054e5af3004369f65840a57a0d4fc2d36861b8919853151e609a21d2deb6", "target_hash": "38c563e3c3235559b4915596c193893d62430d405d53db937bded61983949c81", "exact_match": 0.0}
{"doc_id": 580, "doc": {"question": "when did disney art of animation resort open", "answer": ["May 31, 2012"]}, "target": "['May 31, 2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did disney art of animation resort open?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b7dd979cb150060823a68056678828d5d53b504ec03c4861f5edc045d01d594", "prompt_hash": "31f458d245d575ed224bd41224cc3f8078f3b9181e0f9e47538fe1af07c79dc3", "target_hash": "6e1fe080ed00352583c785276602b6928fdbd706af2e6067ddc1975c1a23c719", "exact_match": 0.0}
{"doc_id": 581, "doc": {"question": "do you have to have a gun permit to shoot at a range", "answer": ["Typically, no"]}, "target": "['Typically, no']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: do you have to have a gun permit to shoot at a range?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" no"]], "filtered_resps": ["no"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "985d8ccd3e7d924ab6c7f1a8b1590cbe0780ac684fd8630aa82d61f0c324e347", "prompt_hash": "b1c62ac2e68e4fc57dfb82f65464f738892656fae58bb033fc9220d56ca9f30b", "target_hash": "78623bb1ef1da2d4e5cc598b28a1051d4d6e745644e3c1293918c26aa1f79694", "exact_match": 0.0}
{"doc_id": 582, "doc": {"question": "the area enclosed by hysteresis loop is a measure of", "answer": ["energy loss"]}, "target": "['energy loss']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the area enclosed by hysteresis loop is a measure of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" energy loss"]], "filtered_resps": ["energy loss"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "69cefaa8206057a6b6d47248834bfba21b87cd6ef56a94cd31bd0a9111ea71ad", "prompt_hash": "7992c82c4ba404748bca5dec96414e64a1b82e710c9b1f25d64a3f4097433180", "target_hash": "bd652604531bb011f47f02a7948d14b93c6c12d475584e85ccb04413a042ed13", "exact_match": 1.0}
{"doc_id": 583, "doc": {"question": "when did rachel have her baby on friends", "answer": ["May 16, 2002"]}, "target": "['May\\xa016,\\xa02002']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did rachel have her baby on friends?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rachel's baby was conceived in the episode \"The One with Rachel's Baby\""]], "filtered_resps": ["Rachel's baby was conceived in the episode \"The One with Rachel's Baby\""], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "13061f156f757992d2a590ebb79ddb88c27b0aa9c312a7989625ede1e85c4997", "prompt_hash": "81b1285ddb0222fcd90b51e0a8c62ec9e961e3a1973cf6df288867acc868a15f", "target_hash": "e38e0389285c7f93fcc87d5d42e8e1b5c45bee16e377b2e314f8245bb8bd6450", "exact_match": 0.0}
{"doc_id": 584, "doc": {"question": "list of rights protected by the first amendment", "answer": ["the free exercise of religion", "right to peaceably assemble", "freedom of the press", "the freedom of the press", "freedom of speech", "free exercise of religion", "the freedom of speech", "the right to peaceably assemble"]}, "target": "['the free exercise of religion', 'right to peaceably assemble', 'freedom of the press', 'the freedom of the press', 'freedom of speech', 'free exercise of religion', 'the freedom of speech', 'the right to peaceably assemble']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: list of rights protected by the first amendment?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" freedom of speech"]], "filtered_resps": ["freedom of speech"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7203e1540f0e84aa6dcba5b39c1c01a858826d4fb1a288f35f3bc7a91c612dbc", "prompt_hash": "390a48e221e23c36a969b60c4774a886c1911807fd62a76e91a2541e92e242b5", "target_hash": "4839b99430e53b6b1d2abb087012aa6696dc3c4f9467a0a64026ca70384c77b3", "exact_match": 1.0}
{"doc_id": 585, "doc": {"question": "when did the last state became a state", "answer": ["Hawaii", "August 21, 1959"]}, "target": "['Hawaii', 'August 21, 1959']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the last state became a state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1959"]], "filtered_resps": ["1959"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b9db71d76d597512ee9e52cfc3e9868ec70dbfe477402556f13c2667338ccc3", "prompt_hash": "961bc64d33a0318da73fff47edb95f19738a96c69f70b01274e40f708940b281", "target_hash": "e4d31a83dffe04cb9e0a0220379c017ce0b7ee727e81bdea302a27f8814bbb87", "exact_match": 0.0}
{"doc_id": 586, "doc": {"question": "different ways to say bless you in french", "answer": ["à tes / vos souhaits", "Santé"]}, "target": "['à tes / vos souhaits', 'Santé']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: different ways to say bless you in french?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dieudonné !"]], "filtered_resps": ["Dieudonné !"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d8f0b32d167bde0c885ae24d71c281bcb2c486ea98f042ef7a858d564dabd37", "prompt_hash": "ad66e63df2d6ae78a31b7c9633559c1457af14e716b64b1a239560409dd19593", "target_hash": "e9cb85e19f44b47afb8ae802e48d2a9a1f2df80edd326868eb54f80ce539fb11", "exact_match": 0.0}
{"doc_id": 587, "doc": {"question": "who has been designed as the new finance secretary of india", "answer": ["Hasmukh Adhia"]}, "target": "['Hasmukh Adhia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has been designed as the new finance secretary of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nirmala Sitharaman"]], "filtered_resps": ["Nirmala Sitharaman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b17c2adb405ee50b007a51c807ab4de8aa3d36df22e26b726f026d10489cd0df", "prompt_hash": "973462783e3326ac8813c1086660f860a44189166fd916b6689d8726424efba7", "target_hash": "31e98d5516b0fba58fddb15e737529b209455a9eb198fb30318dc0c5974b8d2d", "exact_match": 0.0}
{"doc_id": 588, "doc": {"question": "who sings angel of the morning in deadpool", "answer": ["Juice Newton", "Juice Newton's"]}, "target": "['Juice Newton', \"Juice Newton's\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings angel of the morning in deadpool?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Halsey"]], "filtered_resps": ["Halsey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d39ce29f01df5d675920d53a56cb78837f2b82d948c62355b753182fe1d0a4bc", "prompt_hash": "c8343a40de5d914379d82724dd89cbfd253c1a1de8627f64647d65ff08943832", "target_hash": "3e09070f71ada1c14761beab4adb8ab38147f52ee98e95a845cd37d2cd6a1535", "exact_match": 0.0}
{"doc_id": 589, "doc": {"question": "when did last podcast on the left start", "answer": ["2011"]}, "target": "['2011']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did last podcast on the left start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2007"]], "filtered_resps": ["2007"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3ddb4228f1595db76d6e0ad44115f287b9a6eb2253d85b4339c205de6cc51a9f", "prompt_hash": "b60067f7d817ead9dd2f172a3ff9b304518af46afb163c1725ba72e15a8250ff", "target_hash": "3fd77e6a3c8aa1507fbee4fc02c1a1a3ef1013d169b015b0c71135a2990b6664", "exact_match": 0.0}
{"doc_id": 590, "doc": {"question": "who won first ancient olympic 200 yard race", "answer": ["Coroebus of Elis"]}, "target": "['Coroebus of Elis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won first ancient olympic 200 yard race?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jesse Owens"]], "filtered_resps": ["Jesse Owens"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "603005390f9b7a4565823562139d7b577c33fcc06339bf646ce54947ed55adc8", "prompt_hash": "136f90ab8f82c9fe582e5baa9b061eeadbafbdacdcdf46293b9df0a43b7ed6f8", "target_hash": "bdd5446541b0c367ffb93acb97b336255fcd139a984ba7ea0e235f7ac35946f2", "exact_match": 0.0}
{"doc_id": 591, "doc": {"question": "what are the spices in chinese 5 spice powder", "answer": ["Sichuan pepper", "Cloves (dingxiang 丁香)", "Star anise", "Cloves", "Chinese cinnamon", "Fennel seeds"]}, "target": "['Sichuan pepper', 'Cloves (dingxiang 丁香)', 'Star anise', 'Cloves', 'Chinese cinnamon', 'Fennel seeds']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the spices in chinese 5 spice powder?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cinnamon"]], "filtered_resps": ["cinnamon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "72229e8d6bd7bdc60976b0482058eb49e8fb99f26c401c1b2b38f18436010369", "prompt_hash": "e02db8e6e76d45cf0fa417631a7eec33bdb3a88ff322344475339fc3859bccd5", "target_hash": "ca122e04c38280941aea01638e98f7f3aee8c1f4709a6f9fdec1a2dfc8fab40e", "exact_match": 0.0}
{"doc_id": 592, "doc": {"question": "who played major hochstetter in hogan's heroes", "answer": ["Howard Caine"]}, "target": "['Howard Caine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played major hochstetter in hogan's heroes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Werner Klemperer"]], "filtered_resps": ["Werner Klemperer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "acb96a50c87303ba09921fc2110bc61521c70881a299bd602d3ca2d19699ceed", "prompt_hash": "c240790f962ffe86a847bafb7c2e8e1a039de5d0056cca323499f9113c506544", "target_hash": "11f194a1ce9d7002cebad2fb757ae192f86bf78ce0fe6e06b514fb95dcdc4d0b", "exact_match": 0.0}
{"doc_id": 593, "doc": {"question": "when was beowulf most likely composed when did it's events probably take place", "answer": ["in the 7th century", "700–1000 AD"]}, "target": "['in the 7th century', '700–1000 AD']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was beowulf most likely composed when did it's events probably take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 771"]], "filtered_resps": ["771"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2fa92fcad3a3263489712c08c59e57e07370e449b9b42914509b6e4ce16eb35c", "prompt_hash": "10b1f2ee584c2ceb017f3cdbc559dc4ce07423fd453154cb1a0e5eaa84107deb", "target_hash": "020dbdc803810ebb64590d20d84e6d3f0e1145287d54642e5145b3e6f7c567ab", "exact_match": 0.0}
{"doc_id": 594, "doc": {"question": "who played nicky on the drew carey show", "answer": ["Kate Walsh"]}, "target": "['Kate Walsh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played nicky on the drew carey show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Ratzenberger"]], "filtered_resps": ["John Ratzenberger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "278d442fa5d9b031feeb5b5aa437d7aa1833f6db77fa95fd5b920e1d2ec29883", "prompt_hash": "2ebb828fbd493c7bee7842b1e11acfdc9feee0b5e5aee9d41edbc12282c2a63b", "target_hash": "f246dfa8df254fbc941ad2c693b75a42e44f132d81c5d66b36cd05690a80e94c", "exact_match": 0.0}
{"doc_id": 595, "doc": {"question": "who had the most governmental power under the articles of confederation", "answer": ["the states"]}, "target": "['the states']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who had the most governmental power under the articles of confederation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Congress"]], "filtered_resps": ["Congress"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "966b7ad75d6718e512258bd4d21199d4a7513cb575ee969a391879fa922199ca", "prompt_hash": "df07f40927ca049404144def31800cf9d84ec65a4d82dc98a640f9d3fbd70ef2", "target_hash": "25b09b29172bdca4e349da062e283db3e1ea7251842d601fd781b57d73cfe4fc", "exact_match": 0.0}
{"doc_id": 596, "doc": {"question": "yo la tengo theres a riot going on release date", "answer": ["March 16, 2018"]}, "target": "['March 16, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: yo la tengo theres a riot going on release date?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1993"]], "filtered_resps": ["1993"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35f838333a31f3ed8308be7c45d31eb61d974226cb40e16486d51628166e9628", "prompt_hash": "e68f27eb25cb1074612a01d29540044976ea03d36fd86a20dbd6e4412531a392", "target_hash": "639c7514cc16f734bcb3764a511c765b410be13fa605c8cb5414d851837bc0a3", "exact_match": 0.0}
{"doc_id": 597, "doc": {"question": "when did athens emerges as wealthiest greek city state", "answer": ["the late 6th century BCE"]}, "target": "['the late 6th century BCE']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did athens emerges as wealthiest greek city state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 5th century BCE"]], "filtered_resps": ["5th century BCE"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6cac0fe055f0634323b6cd5bdb3d99dcd8e609c13431c77f4309380faf748958", "prompt_hash": "f862b37545839a53883626bd55c4288e4775d3aa69e25916d00e1740e5472fb7", "target_hash": "78f28bb58dd27fe553ecc4f6c6f102660aa6d51d6e41821e8edb8b86cdf0a3f9", "exact_match": 0.0}
{"doc_id": 598, "doc": {"question": "when was national service abolished in the uk", "answer": ["1960", "November 1960", "31 December 1960"]}, "target": "['1960', 'November 1960', '31 December 1960']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was national service abolished in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1993"]], "filtered_resps": ["1993"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d07ebd78b918ab47fe1262defd11c7e47efdc9e6fb114473d52f66ee3fc6b932", "prompt_hash": "a875f8f03414e4fd9f402ce96fc280464e26f9b900b259101e0affe728f1b204", "target_hash": "3b145b55d97d67be0a846e1ab58513180b995f34a9ff4ec26ce333a21cc81b66", "exact_match": 0.0}
{"doc_id": 599, "doc": {"question": "bruce springsteen we shall overcome the seeger sessions songs", "answer": ["\"Old Dan Tucker\"", "\"Erie Canal\"", "\"Jacob's Ladder\"", "\"Shenandoah\"", "\"Mrs. McGrath\"", "\"We Shall Overcome\"", "\"My Oklahoma Home\"", "\"Jesse James\""]}, "target": "['\"Old Dan Tucker\"', '\"Erie Canal\"', '\"Jacob\\'s Ladder\"', '\"Shenandoah\"', '\"Mrs. McGrath\"', '\"We Shall Overcome\"', '\"My Oklahoma Home\"', '\"Jesse James\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: bruce springsteen we shall overcome the seeger sessions songs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Born in the USA"]], "filtered_resps": ["Born in the USA"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "38136a0f1b1e9ba6021c4f075cc59ee66a3e68354344c115958a3858f98b853c", "prompt_hash": "0f78443adbac949f52d8e1d718a718c6aae101034f4dde2df9ae9f3e135b8cd6", "target_hash": "37b6dbcbc410836cd9153c2813adbba729e050744fb1fbbab022d6adcb048ad3", "exact_match": 0.0}
{"doc_id": 600, "doc": {"question": "what us president is the only president to become an eagle scout", "answer": ["Ford"]}, "target": "['Ford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what us president is the only president to become an eagle scout?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gerald Ford"]], "filtered_resps": ["Gerald Ford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91bc705d41e3ec42ab9f1343ac06e85d67775d6a3f1b003f02544f3bf10f14cf", "prompt_hash": "37493accaa643c339833d4ca9972dfe35f24438d9d979b4e260e82c5c2fe41e6", "target_hash": "1af406abdaf6fa1251a5f55844fe2394254748d2a6c097d6930d5ae4406e0253", "exact_match": 0.0}
{"doc_id": 601, "doc": {"question": "who is regarded as the founder of psychoanalysis", "answer": ["Austrian neurologist Sigmund Freud", "Sigmund Freud"]}, "target": "['Austrian neurologist Sigmund Freud', 'Sigmund Freud']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is regarded as the founder of psychoanalysis?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sigmund Freud"]], "filtered_resps": ["Sigmund Freud"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d29a348ebbaf1720e9cc47521b4a4c9d9d48d04d8f730ddd1ad2ced7711b094", "prompt_hash": "865a77fb4a35eb9dc6d12caf6e2fca206159301087740e486e544bfb97e6f5fb", "target_hash": "25bc203c84b932de5e2b95ad38f23937f2b265b4493717d17f1546eac8ebb478", "exact_match": 1.0}
{"doc_id": 602, "doc": {"question": "how many episodes are in chicago fire season 4", "answer": ["23"]}, "target": "['23']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes are in chicago fire season 4?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 22"]], "filtered_resps": ["22"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "33c457dd93e6020f701b2c1d9f9cdb6a95064ffba4dca6d0772ead63fbed18be", "prompt_hash": "984c0c1bbbd9a8efdfe3342ccdbb3c8fd28a18ca7335cbeda3987e7982bd6658", "target_hash": "d6400f20b20b074bbeb94d4d7a7fa59dae6733e3302b507cd1d9757b53ecfb00", "exact_match": 0.0}
{"doc_id": 603, "doc": {"question": "who played the daughters in last man standing", "answer": ["Alexandra Krosney", "Molly Ephraim", "Kaitlyn Dever", "Amanda Fuller"]}, "target": "['Alexandra Krosney', 'Molly Ephraim', 'Kaitlyn Dever', 'Amanda Fuller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the daughters in last man standing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Aubrey Anderson-Emmons"]], "filtered_resps": ["Aubrey Anderson-Emmons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c8708cc03687917fff21c7596a7bb428550a804fafb3d4271f8000f002c990b3", "prompt_hash": "8207453fa8ebd840486f98cff4b9eb470fc5db351fd544d41908cfab0edff8f9", "target_hash": "ffa3439b4e92db6133a70013d8095fc5c8d824fd1997da2864d2bb890aabc554", "exact_match": 0.0}
{"doc_id": 604, "doc": {"question": "who played maudie in the movie el dorado", "answer": ["Charlene Holt"]}, "target": "['Charlene Holt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played maudie in the movie el dorado?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kim Novak"]], "filtered_resps": ["Kim Novak"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a155211b20d581e6e3854d07ca79158175e584432179783167c432aaa746a78", "prompt_hash": "8a8bc8829594dafa8199c7740c88b11ad5bc59ed0fd9fb19f53be0fa30d64351", "target_hash": "aa3f99d8aff9af202aa7de05efa21cd52215e70c16d4b06eb0eaecceb0d5f0c3", "exact_match": 0.0}
{"doc_id": 605, "doc": {"question": "when does isle of dogs come out in us", "answer": ["April 13, 2018", "March 23, 2018", "March 23, 2018"]}, "target": "['April 13, 2018', 'March 23, 2018', 'March\\xa023,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does isle of dogs come out in us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 19"]], "filtered_resps": ["January 19"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ceb94e88cf76f3c68bb7f09b729290edf44e34b36467686f06d357bbefbb3f0d", "prompt_hash": "0a8175b36355cc4316bdbb2a30dacc7bc64c45e472a73a9b6696ac46770f7cc9", "target_hash": "a883667259db500a08e958d1e76779d82258840ea0d1df3d45cedf01c835e9e1", "exact_match": 0.0}
{"doc_id": 606, "doc": {"question": "who does marge's voice on the simpsons", "answer": ["Julie Deborah Kavner"]}, "target": "['Julie Deborah Kavner']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does marge's voice on the simpsons?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Julie Kavner"]], "filtered_resps": ["Julie Kavner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "087a9eae5cb773ce127c7e7b9d917d64f1132eeee717af81277b85572693e75c", "prompt_hash": "0e754296d018c60fd02de3e841a1be0d3671db60f3eee56db3f2ad4fbaaba591", "target_hash": "dc6b2cf15babef939186754bfb35113e26d06f8fa774cb55916c170fc295c6d7", "exact_match": 0.0}
{"doc_id": 607, "doc": {"question": "who is the drummer for guns and roses", "answer": ["Frank Ferrer"]}, "target": "['Frank Ferrer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the drummer for guns and roses?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Steven Adler"]], "filtered_resps": ["Steven Adler"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b26d0fe48edf49fdece78c1ba909b9d1fe6e249af0fc1555af4f484e0c1ab9b0", "prompt_hash": "bbd25099fd5150728c0785a8ff31c1a7294cf40986223c2d4a748d0ecccd46d9", "target_hash": "6a032c9b8f97208b255854ee5a047ad71f7f920c0581b0309564da423d6418f3", "exact_match": 0.0}
{"doc_id": 608, "doc": {"question": "how do you say evil eye in greek", "answer": ["matiasma", "vaskania (βασκανία)"]}, "target": "['matiasma', 'vaskania (βασκανία)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how do you say evil eye in greek?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ommagio"]], "filtered_resps": ["ommagio"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a1f33b96fdce57e9711fd0cfc8f9407d0a0e83c8b14d17a6b804be1c0a2f8a65", "prompt_hash": "f820a162c8e2405480a0267fe96ac49d05d72ae210b5c8ecf408d11dd19797af", "target_hash": "dbddda2e4c7bb1da9a5d042b1043e7719ed3b825c7b1f91b2555415be52ca82b", "exact_match": 0.0}
{"doc_id": 609, "doc": {"question": "who won the battle of stirling bridge 1297", "answer": ["Scottish victory", "William Wallace", "Andrew Moray"]}, "target": "['Scottish victory', 'William Wallace', 'Andrew Moray']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the battle of stirling bridge 1297?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Wallace"]], "filtered_resps": ["William Wallace"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92244387e3e62054c162a92950c01d22054779be9cd0006853b777c283583168", "prompt_hash": "ae39eeee2fd936e97b3f8f138b6a3789a6a05e612f2b47d6a2731808e30ca76d", "target_hash": "23e72e88be49df29889e83de88bc4a4dd3a1319cceffca5cfe1fb595cc88a728", "exact_match": 1.0}
{"doc_id": 610, "doc": {"question": "when was the book pride and prejudice written", "answer": ["28 January 1813", "1813"]}, "target": "['28 January 1813', '1813']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the book pride and prejudice written?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1813"]], "filtered_resps": ["1813"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5352c750dbe664a405c68157841b0ed2d98b0d62ec0bc230ab91e0142db0ca41", "prompt_hash": "92e360ab4845ac5d491033b89463216a32f4f4512bf0fceb9c88961ce995fb58", "target_hash": "170e4fbccc1d1cb9848777a0c4a28c041284c8c152e088b849df1ae3190807a9", "exact_match": 1.0}
{"doc_id": 611, "doc": {"question": "when did the first junie b jones book come out", "answer": ["1992"]}, "target": "['1992']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first junie b jones book come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2001"]], "filtered_resps": ["2001"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "85e15df7fb7f02b1e9d65df3eac0692e14f1ce8ba2bcfcf3e9c70ed08204d5d6", "prompt_hash": "7cfa48f2d8399750420a752e30c2682adc18139d2e4fd44feeba4bc67bb3b0fb", "target_hash": "5e56bf386b12a9e6abeaa107866348aa8879a9334b5f4c1e1cb16df85ac73ec6", "exact_match": 0.0}
{"doc_id": 612, "doc": {"question": "how long have we been celebrating pi day", "answer": ["1988", "2009"]}, "target": "['1988', '2009']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long have we been celebrating pi day?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 28 years"]], "filtered_resps": ["28 years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "95d3d7d064868fa49d419127043285ce1f33c2cc9c3c7f0e77edb79b0a051642", "prompt_hash": "bae6d5a2def27cccd16d0721587b0c683e00e8eb3af3ade151b42c2ad69b0a39", "target_hash": "57b0fb960ea633e9e04b73cf4b870c39a10e4c686b73c25fbf316d7aa548373a", "exact_match": 0.0}
{"doc_id": 613, "doc": {"question": "who is the number one ranked golfer in the world right now", "answer": ["Dustin Johnson"]}, "target": "['Dustin Johnson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the number one ranked golfer in the world right now?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dustin Johnson"]], "filtered_resps": ["Dustin Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "858767141640f274af8696e91062feb3f9663acce134a7801c81ae1e39443278", "prompt_hash": "f5d92ea0d87c936707011ffaa8fc479f8875f4b8e7b262e9583dce123894b892", "target_hash": "d65fb012b78ebaf32ac2e73f51a6957ce39a28f7737511b99105394f61b80e67", "exact_match": 1.0}
{"doc_id": 614, "doc": {"question": "who plays the mouse in alice in wonderland", "answer": ["Barbara Windsor"]}, "target": "['Barbara Windsor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the mouse in alice in wonderland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stephen Fry"]], "filtered_resps": ["Stephen Fry"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "76b362b0031db7dc087383c40f0fde2c904088d0056fb4cd0a77e2e3e45f0b61", "prompt_hash": "3718fb776fab7cd696caca3402c4961db5a4245c026100ea727c17d7bbf7b3c6", "target_hash": "401079e1e0341442b7c1c66306342439f5f1fd258143d7bdf6a51e882b976f3e", "exact_match": 0.0}
{"doc_id": 615, "doc": {"question": "which organelle is the site of atp production and storage", "answer": ["mitochondrion"]}, "target": "['mitochondrion']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which organelle is the site of atp production and storage?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mitochondria"]], "filtered_resps": ["mitochondria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "758e25063a8ba6340ea88f593644e71bfd7b36b647f152d1f8ab4c4a0c9c6219", "prompt_hash": "071f7908c473ab127453ae3b88761bafae26e6e227c3a34544cb26bbac572ab6", "target_hash": "a7ee75c40b7fae2f3739d83de19d6d6a6e4bebdbcc5c6f63432567ba5bd0eadb", "exact_match": 0.0}
{"doc_id": 616, "doc": {"question": "which songs did liam write as you were", "answer": ["I've All I Need", "For What It's Worth", "When I'm in Need", "You Better Run", "Bold", "Greedy Soul", "I Get By", "Wall of Glass", "Universal Gleam", "Come Back to Me"]}, "target": "[\"I've All I Need\", \"For What It's Worth\", \"When I'm in Need\", 'You Better Run', 'Bold', 'Greedy Soul', 'I Get By', 'Wall of Glass', 'Universal Gleam', 'Come Back to Me']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which songs did liam write as you were?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" \"Love Me Again\""]], "filtered_resps": ["\"Love Me Again\""], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "563abc7317ca96dd2831d2d17c57b43facb95892434be4c35654d467ba53c11a", "prompt_hash": "73ec6985dd2544ebdf6ee82b300ad0e1f31027a146fb076863d7f211fe6176e4", "target_hash": "88c6c7e0da364be51241d842320ad6fbb14259c447600a7974b6a03529eb98db", "exact_match": 0.0}
{"doc_id": 617, "doc": {"question": "who played gareth in four weddings and a funeral", "answer": ["Simon Callow"]}, "target": "['Simon Callow']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played gareth in four weddings and a funeral?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hugh Grant"]], "filtered_resps": ["Hugh Grant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d13d20e1758d84fdb1cc7e7652667526e0af9bdd71af6e086445379deed9c3b", "prompt_hash": "9ca3c875b4e7f69cafc3d5268adbf0dad10b8d4a9ff913f505160820da971281", "target_hash": "065e87283554d69f24d77fa26b472c5d9510a67567116166422b62a1196c98f5", "exact_match": 0.0}
{"doc_id": 618, "doc": {"question": "what is the final season of downton abbey", "answer": ["the sixth", "sixth", "six", "the sixth series", "Six"]}, "target": "['the sixth', 'sixth', 'six', 'the sixth series', 'Six']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the final season of downton abbey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f275b03ceb5cfe9d56d13a59f5cec459b2df9b5ff0553762e706320329a95d5", "prompt_hash": "5b17976e52dd45e8504b6ce2429946bbe37aa8ab4343a4c0e813f78876c15141", "target_hash": "67aa2b42e01fa898528bf48a2d68e4b2d96cd227fe452555d2cc0c06406c780c", "exact_match": 0.0}
{"doc_id": 619, "doc": {"question": "where did the term hot rod come from", "answer": ["southern California"]}, "target": "['southern California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the term hot rod come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" automobile"]], "filtered_resps": ["automobile"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a966cb23fcfb98f7d9603f5955c1f8ec99441a7e57ce49c28a2104c0776c3134", "prompt_hash": "7d7563539612f36fe94a2cd7bcf37dfad2bc9313c02aad6bbf36f4377da6ea1b", "target_hash": "8ae2e9e13203a0c00fd0bf732537293b810ed302fcb20e9df3734e3cf1d8a9cb", "exact_match": 0.0}
{"doc_id": 620, "doc": {"question": "which church was given a letter in the book of revelation", "answer": ["Philadelphia", "Ephesus", "Sardis", "Smyrna", "Pergamum", "Thyatira", "Laodicea"]}, "target": "['Philadelphia', 'Ephesus', 'Sardis', 'Smyrna', 'Pergamum', 'Thyatira', 'Laodicea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which church was given a letter in the book of revelation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Laodicean"]], "filtered_resps": ["Laodicean"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9cf93fb376f35ddec365b594dd23c70963454aca7abc0d92bf5d84f9f2221a71", "prompt_hash": "3b8e799b4285fbe1145fe6cb2e06ce2a02aab2eed54f28b25fe688beadf0150f", "target_hash": "e3c69f7ee701ccc691e3cf659b0b377f152f331494daaf2e8ccec365efa94c91", "exact_match": 0.0}
{"doc_id": 621, "doc": {"question": "cast of law & order special victim unit", "answer": ["Kelli Giddish", "Richard Belzer", "Stephanie March", "Diane Neal", "Ice-T", "Danny Pino", "Dann Florek", "Tamara Tunie", "Michaela McManus", "Mariska Hargitay", "Adam Beach", "B. D. Wong", "Christopher Meloni", "Raúl Esparza", "Michelle Hurd", "Peter Scanavino"]}, "target": "['Kelli Giddish', 'Richard Belzer', 'Stephanie March', 'Diane Neal', 'Ice-T', 'Danny Pino', 'Dann Florek', 'Tamara Tunie', 'Michaela McManus', 'Mariska Hargitay', 'Adam Beach', 'B. D. Wong', 'Christopher Meloni', 'Raúl Esparza', 'Michelle Hurd', 'Peter Scanavino']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: cast of law & order special victim unit?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mariska Hargitay"]], "filtered_resps": ["Mariska Hargitay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c0d57634a5f05986b8d0f0e27ac167c49fe6ee5d6fff89d079a3b907f60b1584", "prompt_hash": "f87ae1150047f27a6e3e6b3ca229a3df5274633465845175df8b3394bf2139f5", "target_hash": "cd7dab92b6d335fd7449d37571e3375626dabe967a87a8a2a88fed1246aecf99", "exact_match": 1.0}
{"doc_id": 622, "doc": {"question": "what book of the bible is the song of solomon in", "answer": ["Song of Solomon", "Song of Songs"]}, "target": "['Song of Solomon', 'Song of Songs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what book of the bible is the song of solomon in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Song of Solomon"]], "filtered_resps": ["Song of Solomon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "030d1f2a9ee96dd5dce69510f84b6cffaa4d3133ad81e81393b5c122584e212d", "prompt_hash": "e3b3981df5ab2d561104555d035eafaa901c47af0c647d95596a7ea68d8f25d9", "target_hash": "4149e79e129fb964ed7f2c120e1db75a87b4a639af1b2998bba8690a924511dd", "exact_match": 1.0}
{"doc_id": 623, "doc": {"question": "when did the military plane crash in mississippi", "answer": ["July 10, 2017"]}, "target": "['July 10, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the military plane crash in mississippi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2014"]], "filtered_resps": ["2014"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a13d60b67cff491178b0b389352be2d9979189c1b64e66e6cbd815a53a289fa", "prompt_hash": "69bdf12f03730fc773d76a8d1612f108f88744050259937aaea63a46c74de39b", "target_hash": "a675410474024cbb502cefc46e51f54965e6ae387b26bd68666a17ef9e1cca3f", "exact_match": 0.0}
{"doc_id": 624, "doc": {"question": "who has the most international goals of all time", "answer": ["Ali Daei"]}, "target": "['Ali Daei']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most international goals of all time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Miroslav Klose"]], "filtered_resps": ["Miroslav Klose"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e62d9c24c73e9effc058eebaf7ae29ad639b4296efc49db9630c217f717710d9", "prompt_hash": "3a2b440f2ea84b5a7d91eff8f0daa9398001e24fc5bb5deaa37906d3c842ce22", "target_hash": "b65de07092c1c2e8d869eaff1f3655a9ac85abf32c49a04658f8441ef583c3d6", "exact_match": 0.0}
{"doc_id": 625, "doc": {"question": "who played bat masterson in the movie tombstone", "answer": ["Mason Alan Dinehart"]}, "target": "['Mason Alan Dinehart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played bat masterson in the movie tombstone?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kurt Russell"]], "filtered_resps": ["Kurt Russell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e00d25daa49fab514ea18b4b58c4df21d3d4829a965d5de9d1369e8d49f3bb0a", "prompt_hash": "3566811986c4fbce1c4230634f307ba59c126e725a78185687ae3ee5b88970ef", "target_hash": "77052ba84cbc7cc75aed0ee0ff5b8a5b2a771ad4db39ae64563094439789bbca", "exact_match": 0.0}
{"doc_id": 626, "doc": {"question": "tools made from high-speed tool steel are generally used for what type of machining operations", "answer": ["milling"]}, "target": "['milling']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: tools made from high-speed tool steel are generally used for what type of machining operations?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" high-speed cutting"]], "filtered_resps": ["high-speed cutting"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6c02c0dc1495c98cf0bce1bb4e11c936bceefb121f1a18c005a2a2615d93abbd", "prompt_hash": "3054cdf9f4a4ce25ca33ce481f21c7581d47425d96b51ab256d50e17e5147f19", "target_hash": "df897b39ce8ca1e2a3d3aff5df44c6617a1c3f0763748f3f634e9ad2e2775472", "exact_match": 0.0}
{"doc_id": 627, "doc": {"question": "where does the this is us family live", "answer": ["Pittsburgh"]}, "target": "['Pittsburgh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the this is us family live?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "267bdf3ea2c4a83bdbf28c4ba97c3b8163f6dda1bb9094b464e8b010103866f4", "prompt_hash": "384b00b166a7ee106d2d8bf555a04065174c863a3fc1b977cd6f45017ae0abe2", "target_hash": "4e1731b9275013e7eeefff426ce6ac1dfd29991d575082113034dd2bea19dc8b", "exact_match": 0.0}
{"doc_id": 628, "doc": {"question": "how many pennsylvania house of representatives are there", "answer": ["203", "203 members"]}, "target": "['203', '203 members']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many pennsylvania house of representatives are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 203"]], "filtered_resps": ["203"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "09b05f2c0dc1eb37cada0cb32f0adb7958fcef01ff402fd8956461bbc2820c18", "prompt_hash": "f37f18d0d8a0cb7e669083f6b37eb28df400daea65287f4ce06c5620f7fcd5c7", "target_hash": "aa903c18ff68f1a01c6e65c9be6c0479f08415b2899905ec6aca0e0ec108d79f", "exact_match": 1.0}
{"doc_id": 629, "doc": {"question": "when did fortnite battle royal come out on xbox", "answer": ["September 2017"]}, "target": "['September 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did fortnite battle royal come out on xbox?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" September 26"]], "filtered_resps": ["September 26"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dd7a5550a19bdbea267be6a912bf3db0dd07fe0f613dc0ba3681c4ce8f33ecb2", "prompt_hash": "0b41373aa9e616bf00df102e7f9b9661a6c76d18cd9108141b7279be9a063c39", "target_hash": "d19a62aa2c85e927a3c13a19308298458a7033d8d6885a0a2e72ada88ca65b38", "exact_match": 0.0}
{"doc_id": 630, "doc": {"question": "two examples where low voltage transformers are used", "answer": ["doorbell installations", "toys"]}, "target": "['doorbell installations', 'toys']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: two examples where low voltage transformers are used?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" street lights"]], "filtered_resps": ["street lights"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "427818a63449bbb3f138d27861c859019cf70a5707c0ad2d5e27f4399c4485e2", "prompt_hash": "eeefbc31a52249e392d08fcfead8bc82d82684e961d8782b245f1f40236920fe", "target_hash": "1d65448d886d8081846cb452c849a23741efc3eb9f92a340d986dae58cc4f770", "exact_match": 0.0}
{"doc_id": 631, "doc": {"question": "what is the center of heredity in a cell", "answer": ["nucleus", "Cell nuclei", "the nucleus", "chromosome"]}, "target": "['nucleus', 'Cell nuclei', 'the nucleus', 'chromosome']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the center of heredity in a cell?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" nucleus"]], "filtered_resps": ["nucleus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6a8247b4e52a253e27bb79524e44995d1814365c4266912a156769c785ae29cb", "prompt_hash": "bc513fc6eb70ec95b3c6cb169a3e82c12604600902686cfd969dfadb6bbc422a", "target_hash": "8291b5f1d62e622d631a8f1933eb630bf1f2db115403c1933f3f3d365a3d977b", "exact_match": 1.0}
{"doc_id": 632, "doc": {"question": "when was the first dark tower book published", "answer": ["1982", "1998"]}, "target": "['1982', '1998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first dark tower book published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1982"]], "filtered_resps": ["1982"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d48f3f01c97fdbf426cc49aefef841c420f27c1df555b6ade69ad96e75a50aaf", "prompt_hash": "e538f439f9e29886a950fe0a4a98465c6effd88bb5f0e6f91851cb02d24af2ef", "target_hash": "bf7d4dc3f740fee1f732dabe373a98b359241b5ce469c1c1a182320329f36c0e", "exact_match": 1.0}
{"doc_id": 633, "doc": {"question": "when do you get your white coat in pharmacy school", "answer": ["before the first year begins"]}, "target": "['before the first year begins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do you get your white coat in pharmacy school?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" first year"]], "filtered_resps": ["first year"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e4ca4beaf72ee43a89fe879f37afae561292d2355df51d89848f8cc50b533a55", "prompt_hash": "d9fb392c8aa554e7c97b9505cfb5be25d93bc910bb3e1150d2d9fe3cc7a5e8a5", "target_hash": "d3b0f0e4275ae4d40f96aeea129a70d7b4218795cedda3b004ea9937a7136665", "exact_match": 0.0}
{"doc_id": 634, "doc": {"question": "when did the ouija board game come out", "answer": ["July 1, 1890"]}, "target": "['July 1, 1890']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the ouija board game come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1966"]], "filtered_resps": ["1966"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7d64c5f5c632d3a41bc9b006ebbaafe967d5b0dc6d7fdf93e7503ca695d7a7ac", "prompt_hash": "2d4f4738997169817d059ade4a8dfe3f7a4eb20843daa863d1fca74785f02458", "target_hash": "23c86d708b1dff7fd51299a3251ffccd98eb58a30720e2a2f9112dceb85158d2", "exact_match": 0.0}
{"doc_id": 635, "doc": {"question": "who warned europe to stay out of the americas", "answer": ["Monroe Doctrine", "Roosevelt Corollary"]}, "target": "['Monroe Doctrine', 'Roosevelt Corollary']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who warned europe to stay out of the americas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spain"]], "filtered_resps": ["Spain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a229659d05750555a44d59bf0dd2f9919bbf40f7b123d031d47e9d41eae9ee74", "prompt_hash": "fcaaa23af0a5ec97a3790f44e9b4a671328e3ff0a6b2897a5552d45efdbc2572", "target_hash": "01fed9a2dbed9852fc33ccb190d3df5282f2085356357cf150458540c34f9493", "exact_match": 0.0}
{"doc_id": 636, "doc": {"question": "who played desmond doss father in hacksaw ridge", "answer": ["Hugo Weaving"]}, "target": "['Hugo Weaving']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played desmond doss father in hacksaw ridge?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hal Holbrook"]], "filtered_resps": ["Hal Holbrook"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b695116fd27113d40089b95ed442cf34580c471b0459a77075b62a1902a54851", "prompt_hash": "1ace470bbd87e282892919565704da45813ef2bc09c7b2e3568bbf6cc10dad83", "target_hash": "dfbe401ad588a9943a695d93dc98594b7e8460c4e8479f0606414fbf85a9f200", "exact_match": 0.0}
{"doc_id": 637, "doc": {"question": "who did carolina lose to in the super bowl", "answer": ["Denver Broncos", "New England Patriots"]}, "target": "['Denver Broncos', 'New England Patriots']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did carolina lose to in the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Denver Broncos"]], "filtered_resps": ["Denver Broncos"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a47ba6e63e724c1556396a8935407db25212d06066683c3ff08b023617d098bc", "prompt_hash": "07c8ed14631a4c2fb199faef29e9a6748ef422f0da20608789e746f3d6c7162b", "target_hash": "254300a9586c208403aca5981ca71f90901e5d43fa4ce8b4591331cde0335969", "exact_match": 1.0}
{"doc_id": 638, "doc": {"question": "who is the founder of the ubuntu project", "answer": ["Mark Richard Shuttleworth"]}, "target": "['Mark Richard Shuttleworth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the founder of the ubuntu project?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mark Shuttleworth"]], "filtered_resps": ["Mark Shuttleworth"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "29e5b950e8128997928f94cda9cb631bb75cc17c024a586ab8d73752e0cbd395", "prompt_hash": "33ecd97bba604cdf97a0425027cd81dfcd6c6ca2428586d1829175437e4ba1e5", "target_hash": "599ec80c4b7286d9b80791132ff3ba5cba6b8c6bb26fc2ca7a774dc7ff1c3c70", "exact_match": 0.0}
{"doc_id": 639, "doc": {"question": "for complaints of sudden chest pain patients should take a", "answer": ["Aspirin"]}, "target": "['Aspirin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: for complaints of sudden chest pain patients should take a?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" aspirin"]], "filtered_resps": ["aspirin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f381e40f1e87c39007ae5291fc91a1a6cd7bd4803c19dae7f43c731fb49e898", "prompt_hash": "dfd24b42bb3b045d151b2db75541515f126f2af96d21afe393c40586ee2f5bc6", "target_hash": "9aa4bf42a4092a472d3db16af2e51c59906ab026c7be5be748692f2bc2299607", "exact_match": 1.0}
{"doc_id": 640, "doc": {"question": "who won the most medals at the 2014 winter olympics", "answer": ["Russia (", "Russia"]}, "target": "['Russia\\xa0(', 'Russia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the most medals at the 2014 winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Norway"]], "filtered_resps": ["Norway"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2482b04290bf1028657e74101cdded69ead1425b7a8b78a850f3d151281e2259", "prompt_hash": "baa01d811b9311976643c42ba096d9ae7899e4ea89df40050d36f0fb1036f2e6", "target_hash": "185e251c9c438465ee7f5d8b9bfb6b311db9f06743809312cb680ad4dfc93ef3", "exact_match": 0.0}
{"doc_id": 641, "doc": {"question": "where is urinary bladder located in human body", "answer": ["on the pelvic floor", "pelvic floor"]}, "target": "['on the pelvic floor', 'pelvic floor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is urinary bladder located in human body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" abdomen"]], "filtered_resps": ["abdomen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7f97e77fb77b61990f9a6a108b6d60228c937b3d9583eca2490317a7caa93f75", "prompt_hash": "ddb6aa8f0001c83c132c6814d449c1b13e962cd64829d4be2ee44c8fe8d1e6b5", "target_hash": "033d3449fb2af0b8af52a87f14a3ff3f5fc42e9db5338f4b481e6f420ea83fed", "exact_match": 0.0}
{"doc_id": 642, "doc": {"question": "when was kudzu introduced to the united states", "answer": ["1876", "in 1876"]}, "target": "['1876', 'in 1876']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was kudzu introduced to the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1876"]], "filtered_resps": ["1876"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6cad55904cfc8bfa5ebd0e7614fc0b8d0e8e833f0a336934f249c6ec096f595a", "prompt_hash": "853cc3db20b65d9778a69410656343237e20525f3c4ad5fa3340386eeaf4c49c", "target_hash": "fbae9f703b52a6333ab045021afb7384166a576c1c859db8d3387151f74c80f4", "exact_match": 1.0}
{"doc_id": 643, "doc": {"question": "name the cell which is attached to the lateral side of sieve tube", "answer": ["sieve tube elements"]}, "target": "['sieve tube elements']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name the cell which is attached to the lateral side of sieve tube?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" companion cell"]], "filtered_resps": ["companion cell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dde3eb8dd2a9297332c08144d5a7402b0dc00cf3ed3c51e954332de66d48e218", "prompt_hash": "756bc0f0816cfdb91f28cfa7f9dc63ee2f3d33d35ad3f2659534686b0c873a2f", "target_hash": "91b5a54f84c9ead5f0855278df276f9f628e742811ccf3a6f63431b1bd0fd0a3", "exact_match": 0.0}
{"doc_id": 644, "doc": {"question": "where was the flag placed on the moon", "answer": ["Hadley–Apennine", "Sea of Tranquility", "Fra Mauro formation", "Descartes Highlands", "Ocean of Storms", "Taurus–Littrow"]}, "target": "['Hadley–Apennine', 'Sea of Tranquility', 'Fra Mauro formation', 'Descartes Highlands', 'Ocean of Storms', 'Taurus–Littrow']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the flag placed on the moon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tranquility Base"]], "filtered_resps": ["Tranquility Base"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed2374a1197b1ad8a42e3ed1449f6b8806bdff73f16ea9d605034164d42d0c36", "prompt_hash": "00130e3ad9d3f866161f87ec08c6466eda57ae9edc3ff535947ececb7562a8f4", "target_hash": "eb7d4289d0510e44fe6a332b5a85ee874ad7d27b39d04b150a42b4a7bf806e25", "exact_match": 0.0}
{"doc_id": 645, "doc": {"question": "who won the first celebrity big brother on channel 5", "answer": ["Jack Dee"]}, "target": "['Jack Dee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the first celebrity big brother on channel 5?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nicole Appleton"]], "filtered_resps": ["Nicole Appleton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7c52c0a617131807ceb9610905279f3cf42e35e54d37dc065b83a4702ea77b6", "prompt_hash": "9c81fe2b6b55f9d74feecd1d1ba2622019b1d37ba42ea110a2b9baaf875c046b", "target_hash": "baf53dceeb23cad9be77ece1a1e5085fc338e8016f32bba74f44340d9eb3d82b", "exact_match": 0.0}
{"doc_id": 646, "doc": {"question": "where was the movie strangers prey at night filmed", "answer": ["Covington, Kentucky"]}, "target": "['Covington, Kentucky']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the movie strangers prey at night filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New Mexico"]], "filtered_resps": ["New Mexico"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b703b5a953f55338670af14ed115045b4ff88fe7442fd25296cdbae8c79cea31", "prompt_hash": "a553862e82538793959d44be444c934f7a4099a663f53a7b656f40de6709fb32", "target_hash": "3e19bd649e4753ac3433b4221d13784c586bb39f048192750700c6624506e21a", "exact_match": 0.0}
{"doc_id": 647, "doc": {"question": "how did the dominican republic get its name", "answer": ["patron saint, Saint Dominic"]}, "target": "['patron saint, Saint Dominic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how did the dominican republic get its name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spanish"]], "filtered_resps": ["Spanish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f36e3c20ea205cae2a69f5e76d2b2f7b65a1b41b15e3bf81e7b25761641820d7", "prompt_hash": "11ac9a2eaac58c76289544f873a6287e8b81e0201430663d416112af46ab6059", "target_hash": "aa423bdeb508c9c4ca5e3937f757a7fd867c9d0ab032a2a438aa8afe73cf6b2c", "exact_match": 0.0}
{"doc_id": 648, "doc": {"question": "who played the mother in the black stallion", "answer": ["Teri Garr"]}, "target": "['Teri Garr']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the mother in the black stallion?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sharon Stone"]], "filtered_resps": ["Sharon Stone"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "36b119574d8b6f8baba54a0424e2a5418e54d66b6863eb91180e8d053076aded", "prompt_hash": "0bb98b034a2957990dd5c3273c4296b60c1be02efaa22c5f8c8574bd8c9b00cf", "target_hash": "3f02ede09b49738176833bb27aa8e314090ff48e661ca4e87445322c445add5c", "exact_match": 0.0}
{"doc_id": 649, "doc": {"question": "when is the end of the football season", "answer": ["February 4, 2018"]}, "target": "['February 4, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the end of the football season?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January"]], "filtered_resps": ["January"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c07f9ef721cd7a9886d9b3d6f3fc48f6fb5279d3d73924a6d98b664c2772fd36", "prompt_hash": "dc4a87cf8aedff9eb3a782d473218816135b9ec300bc7bbe7f3be77cc326108e", "target_hash": "43e9549978c4ec250f0acca8ee574a095a5a832df9e72e93594d08ea0d48fa7e", "exact_match": 0.0}
{"doc_id": 650, "doc": {"question": "when did michael jordan return to the nba", "answer": ["2001", "1995", "March 1995", "in March 1995"]}, "target": "['2001', '1995', 'March 1995', 'in March 1995']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did michael jordan return to the nba?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1995"]], "filtered_resps": ["1995"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f297c7e409826f605938887a0b8e069c07fc14e27092c884850e84c636416c75", "prompt_hash": "3941d56fd43e8c3fff9085d8d72cbeadce83523445ea47a4308db3e550b658a8", "target_hash": "26cb4a6b3d13f0038f2bad19fd2f89bcdb3c6daf548e4f0bf7f3823196cabf95", "exact_match": 1.0}
{"doc_id": 651, "doc": {"question": "where is geothermal energy used the most in the us", "answer": ["California"]}, "target": "['California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is geothermal energy used the most in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" California"]], "filtered_resps": ["California"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6082b24bdd5b672debeb312c8af36ab3795b422469dfffcf736746006d23a045", "prompt_hash": "ccda6c469a7879a7bc596b250d62ffd2a222cb7445b5a82ddd67973553d597d6", "target_hash": "06cf4562a4f6489654c6ea5952a99bbceb81c4e34e8de327d8bb4734b81035c5", "exact_match": 1.0}
{"doc_id": 652, "doc": {"question": "when did jack mccoy join law and order", "answer": ["1994"]}, "target": "['1994']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did jack mccoy join law and order?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1999"]], "filtered_resps": ["1999"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "22bf2681c5356d8bbf46a6986c8d13d6b2485534eb55d80b0ee19c02a23d6c28", "prompt_hash": "7b72a9965b81af9a5669cbc47c26bd36985230926002346a55a9a84de149660b", "target_hash": "24b3e41cbaa8832929482068327eb5405c3e062251b127b509163466e611f5f4", "exact_match": 0.0}
{"doc_id": 653, "doc": {"question": "where was the u.s.s maine when it exploded in 1898", "answer": ["Havana Harbor"]}, "target": "['Havana Harbor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the u.s.s maine when it exploded in 1898?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Havana"]], "filtered_resps": ["Havana"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a64dc3ead124ce842b48cd9c4511044ad5acdaeeb7e09dcd6e601f210d2bee6", "prompt_hash": "358286e8dd351a5da25da43b8bd2a371be0644ae3456a876950261ffe27cb5b4", "target_hash": "c1aa834984d316cc130ffb0988dbdb3197b730b2bfdf5297b2faae6126a41b3c", "exact_match": 0.0}
{"doc_id": 654, "doc": {"question": "who sings the original windmills of your mind", "answer": ["Noel Harrison"]}, "target": "['Noel Harrison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the original windmills of your mind?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Henry Mancini"]], "filtered_resps": ["Henry Mancini"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b6ffa9600f1bf4ae449ceb9a1c7b0387f62c9124aa72c2f68ffe7969fa93ad2b", "prompt_hash": "06757c96e9d0d8409dc4a042437acc594da373d8b5c5c259afd8ebc565eae028", "target_hash": "8c1a0ce82986c02b711d7bb14282fd8ba331f97f19fb4938580f1294aeee9603", "exact_match": 0.0}
{"doc_id": 655, "doc": {"question": "what color pages in the emergency response guidebook (erg) list compounds by name", "answer": ["blue"]}, "target": "['blue']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what color pages in the emergency response guidebook (erg) list compounds by name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" red"]], "filtered_resps": ["red"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f563c76126b2c04be8139a29f3a6c12f1359a012bbe3877489677d9b8c3b72f6", "prompt_hash": "fd4c5fe46e4d354088866c01f39ef67db494d4549b39e735fbf2ee6b92df58ac", "target_hash": "31aa98f6841a174c056df8793acb5241c050added0c5e185b4c5a8673cde4632", "exact_match": 0.0}
{"doc_id": 656, "doc": {"question": "who won three stanley cups in a row", "answer": ["Toronto Maple Leafs", "New York Islanders", "Montreal Canadiens"]}, "target": "['Toronto Maple Leafs', 'New York Islanders', 'Montreal Canadiens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won three stanley cups in a row?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Detroit Red Wings"]], "filtered_resps": ["Detroit Red Wings"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96b645ac89ca504f3c141d1cbc126ed7d61449db5db21b8b0416a54342f5994e", "prompt_hash": "226011baf7a6de784b88a47eca9f1a4fc624e804d0bd5bc48c30356db340338d", "target_hash": "856967b6acfc124c54f79be0a5b4edfd9b13bebe9656f263846385beeff18a23", "exact_match": 0.0}
{"doc_id": 657, "doc": {"question": "what is alpha centauri's approximate distance from earth", "answer": ["4.37 light-years"]}, "target": "['4.37 light-years']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is alpha centauri's approximate distance from earth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 4"]], "filtered_resps": ["4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a33b0e8a034a79e2329e888fed5ed702dbcbbee73994d6ff8ef5984d7fc06716", "prompt_hash": "2e582241b8b923bf4a78d39051855d6b80a923afd8676648db5fada259bbcfaf", "target_hash": "1c7c9c28cfd9afd98555756b457711b9cc085d978287608b29bc6c83d0b0c0ed", "exact_match": 0.0}
{"doc_id": 658, "doc": {"question": "what is the weight of a honda fit", "answer": ["1,020–1,080 kg"]}, "target": "['1,020–1,080\\xa0kg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the weight of a honda fit?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0408fc644d6681a777fe0df350369599b9fce75413a888e26f3ac28a2613ec05", "prompt_hash": "77d26e9f3635608792041834c41d9bbb72aaeaff31521c620e87bafdc5da3e6e", "target_hash": "855f0cf2b39b37d3f57e5b1c974d93c0ea015e226cd5d951c689bdadb3ab42e3", "exact_match": 0.0}
{"doc_id": 659, "doc": {"question": "when will the 2018 oscar nominations be announced", "answer": ["January 23, 2018"]}, "target": "['January 23, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will the 2018 oscar nominations be announced?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 23"]], "filtered_resps": ["January 23"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "22a97ba5965521ab1b5d0d81efaba6acbfe1f4f141439fd9cad676f46334aafa", "prompt_hash": "ecca57febc1fc28737a1953f1eeb402de5a54041abbec1dcf5f1d3c0db609a68", "target_hash": "a144ac9e256af111738562ea65ee6244e2b5ce3e61335d17020beb0162078b85", "exact_match": 0.0}
{"doc_id": 660, "doc": {"question": "who is the father of lucius in gladiator", "answer": ["Lucius Verus"]}, "target": "['Lucius Verus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the father of lucius in gladiator?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Marcus Aurelius"]], "filtered_resps": ["Marcus Aurelius"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77ee75d3a35bee7c1ead8d27f6294d8b5f81e0351ff4a32e1cf4c37b8bd9f412", "prompt_hash": "4ffaa99961847b04c3f9505c1bd449ff95bb98b7c70abb7a2e2c6cf32f793e6a", "target_hash": "aaf3e3648e54cbae57ed74efba630aeb36158ffa357f76da4602b6fd6f700dfe", "exact_match": 0.0}
{"doc_id": 661, "doc": {"question": "guardians of the galaxy blue guy in prison", "answer": ["Michael Rooker"]}, "target": "['Michael Rooker']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: guardians of the galaxy blue guy in prison?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nebula"]], "filtered_resps": ["Nebula"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6bab28c89c5cbf48e7e458c2941a57bc7c02c151d6577b686357fc46177bb345", "prompt_hash": "946ace84a21c051ad7aa4439e7bfd4fc81b4ef002ff454c5574b77054e3ff3b5", "target_hash": "1fc2f58621e38ed59ed3f64725e1f5e0a3b3ebb25815bd32acf3d171b36c3388", "exact_match": 0.0}
{"doc_id": 662, "doc": {"question": "of which country is sofia the capital city", "answer": ["Bulgaria"]}, "target": "['Bulgaria']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: of which country is sofia the capital city?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bulgaria"]], "filtered_resps": ["Bulgaria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed8698b6237b41f513ac20da6ae90ecf24f439442447176dc16086d70277a2d1", "prompt_hash": "8b6c02c091d79baf913bb9062b6819f8a2fcbf724645333f96a1b839f88c7bda", "target_hash": "b6af57c012b778464c5386f13b96e1d75fb643e7921134adf77b5b197373266c", "exact_match": 1.0}
{"doc_id": 663, "doc": {"question": "what is the legal age for marriage in australia", "answer": ["18"]}, "target": "['18']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the legal age for marriage in australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 18"]], "filtered_resps": ["18"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d551c958da056bc857b7c7e0bf7e7cfc75478b2018fed6695a5448dafa57b9f", "prompt_hash": "991a00deebe5b3c815516e4837f05c7fb4367fe88f1adf21808cc2858c9f4e8c", "target_hash": "fe29453a161fbb6965ccdbdca03593905c1e66cc520a49919b2d999ddf5c0c18", "exact_match": 1.0}
{"doc_id": 664, "doc": {"question": "what percentage of the us population lives below the poverty line", "answer": ["13.5%", "In 2015, 13.5%"]}, "target": "['13.5%', 'In 2015, 13.5%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what percentage of the us population lives below the poverty line?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "754b745f0ebf5efe1435f51a83a9864868ec199c60a53dc2c71b307e43b37d75", "prompt_hash": "79ad30c5848b8f92c51e94a8e2e8f8fdffa8c3305acc41fd803f10d77a5dae35", "target_hash": "6f979e12fae8a0500c61e081c54c2cc60157f7253852b12d869810ec9c26a64f", "exact_match": 0.0}
{"doc_id": 665, "doc": {"question": "where can you find dna in the body", "answer": ["chromosomes in a cell", "inside the cell nucleus"]}, "target": "['chromosomes in a cell', 'inside the cell nucleus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where can you find dna in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cells"]], "filtered_resps": ["cells"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a7ad4eaaf70bb5236d37e4e8ac10374f00d9dac6b2447a0b0cf25c14f3a7ad3", "prompt_hash": "6c3d625824a90a5922806efed46f96fed9c16b9ce38a9e381decd9ea0582b620", "target_hash": "0b961cf541c3b18edc1cc353e1317790baea73521665386f63fb60a76380e6fd", "exact_match": 0.0}
{"doc_id": 666, "doc": {"question": "who played in last train from gun hill", "answer": ["Bing Russell", "Earl Holliman", "Val Avery", "Brad Dexter", "Kirk Douglas", "Brian G. Hutton", "Walter Sande", "Carolyn Jones", "Anthony Quinn", "Ziva Rodann"]}, "target": "['Bing Russell', 'Earl Holliman', 'Val Avery', 'Brad Dexter', 'Kirk Douglas', 'Brian G. Hutton', 'Walter Sande', 'Carolyn Jones', 'Anthony Quinn', 'Ziva Rodann']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played in last train from gun hill?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Earl Jones"]], "filtered_resps": ["James Earl Jones"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "043feedfab6572e0991e88732e75daaf1f14a660a96264e24ca9b9fb763391ea", "prompt_hash": "1d3290430749b6127ee5af29599abf15949e063dd3a50507f3b8797073db5a4d", "target_hash": "d44b0e8382d02f6b79a1c08f5598c8bf1649a1c55fb7a1090e7593d3c4c05f4e", "exact_match": 0.0}
{"doc_id": 667, "doc": {"question": "who won women's singles us open 2017", "answer": ["Sloane Stephens"]}, "target": "['Sloane Stephens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won women's singles us open 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Angelique Kerber"]], "filtered_resps": ["Angelique Kerber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "53f4e2cccb6b0bd925f46b8e368c6ecad4ec1da1d44f4d070105bf64bedcf7d8", "prompt_hash": "0144444cbf1d88c4af3c411db71faab5d771926c1abe2cea1e4b4fcd70dbca2d", "target_hash": "e1228be6c6d23048cba0fb9aee51f80d1ba7399e9100213149add5e7ffc8e8d6", "exact_match": 0.0}
{"doc_id": 668, "doc": {"question": "who played the female lead in the 1942 film casablanca", "answer": ["romantic drama", "Ingrid Bergman"]}, "target": "['romantic drama', 'Ingrid Bergman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the female lead in the 1942 film casablanca?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ingrid Bergman"]], "filtered_resps": ["Ingrid Bergman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bce78a7d49b420e4bdd20fbdada5df3b794583f2aa429c7c8fe27ee15ad40727", "prompt_hash": "20b8c774e046cfad317a7e3fec0c33ee5109e998f714847d44f168f1663d9430", "target_hash": "99519f6da67fb4eac8d75bfb5aa41201881b810404b7f4b7abd10e42e7ccf03c", "exact_match": 1.0}
{"doc_id": 669, "doc": {"question": "which government had more power under the articles of confederation", "answer": ["the states", "states"]}, "target": "['the states', 'states']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which government had more power under the articles of confederation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the state governments"]], "filtered_resps": ["the state governments"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3fc94f4610dfbafaddd84ab54d5533c3ef6552a69351db39b9df3e42e5676d00", "prompt_hash": "468007911a30d9945568ac860b9f42ee320549c20d98d4da176a5e313db175aa", "target_hash": "25c786e90ad88040129aeb808890f4dad86070ee854532ff340c50a1083785a9", "exact_match": 0.0}
{"doc_id": 670, "doc": {"question": "3 candidates for the democratic nomination in 1960", "answer": ["George H. McLain", "Pat Brown", "John F. Kennedy"]}, "target": "['George H. McLain', 'Pat Brown', 'John F. Kennedy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: 3 candidates for the democratic nomination in 1960?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Adlai Stevenson"]], "filtered_resps": ["Adlai Stevenson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "79f8f2ffde9e70240d781592c5783bd80e89e76688d045dedfc04ff7a286961b", "prompt_hash": "600e78d9b7a214fd5a4a9d282782bb4327fc1314ec7f7d432a8e5a6da1422d9f", "target_hash": "f0e491ba292598fddc27eba9f5cb4bf74f7747742953ed0975e488690ed63403", "exact_match": 0.0}
{"doc_id": 671, "doc": {"question": "when did the astros change from the national league to the american league", "answer": ["2013", "March 31, 2013"]}, "target": "['2013', 'March 31, 2013']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the astros change from the national league to the american league?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2013"]], "filtered_resps": ["2013"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fa6d54faccab840cba6773fa27ed22eeea0f82733ca2223ceea892f20399ccef", "prompt_hash": "b8b01571044ef4e9ce5f881c3b7113c9d7ac3d19ec97ff292bd316bf3fcce739", "target_hash": "4ed34c1b80931f9870a9a5dfb7c637cdd13579efb2cd6a460ba19c68c6c2671a", "exact_match": 1.0}
{"doc_id": 672, "doc": {"question": "who wrote the song would i lie to you", "answer": ["Peter Vale", "Mike Leeson"]}, "target": "['Peter Vale', 'Mike Leeson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song would i lie to you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e4f96e951220551559a4d07d00c7a058661c5faf0ebcfec50351ade76891cb21", "prompt_hash": "4b98a1fa81590dbc6b558d53095fbb9f769f5a61be2b3ca5b28f77cf0dda836c", "target_hash": "1c2467f56e2c8b2f2b72772961b845b165afad0349228bf61ade73152cb873ef", "exact_match": 0.0}
{"doc_id": 673, "doc": {"question": "when is the last time the philadelphia won the superbowl", "answer": ["Super Bowl LII,", "2017"]}, "target": "['Super Bowl LII,', '2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the last time the philadelphia won the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1982"]], "filtered_resps": ["1982"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "467f83e9104f1140a4b2ec1c1fc2bd104e2b61db3cfd554ec06a78c931d70406", "prompt_hash": "bc8118882d6f09c401220a2ffaf7b61c58a92fc57290c2de1da62cbc4218c099", "target_hash": "6acd98517968375bb246f67578d8b66f7a2b96aa3c8f8b460e8c589b8bdbf274", "exact_match": 0.0}
{"doc_id": 674, "doc": {"question": "who is the actor that plays jt on the young and the restless", "answer": ["Thaddeus Rowe Luckinbill"]}, "target": "['Thaddeus Rowe Luckinbill']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the actor that plays jt on the young and the restless?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William J"]], "filtered_resps": ["William J"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3e26d70527fb26a13ec5b38b97347a9db5dfd3a749dd44becdfc30e690b99c0c", "prompt_hash": "a2dcd6c218c196a8070e74035de914fee3b427a1ee1688a87e036b1723906d24", "target_hash": "9d594789035ab3a131e967176140bb806791bed227b4099a4029c1145204fc45", "exact_match": 0.0}
{"doc_id": 675, "doc": {"question": "who did the mavericks beat in the finals", "answer": ["Miami Heat"]}, "target": "['Miami Heat']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the mavericks beat in the finals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dallas Mavericks"]], "filtered_resps": ["Dallas Mavericks"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5dc50afe75821ddbc03178de4f5cc5e62ecc4a0735954f5650dd421eec6e419b", "prompt_hash": "31b0b3b0280cb094279c8843a8a3a43ac5f580fc278745cae2ff345c22a9b610", "target_hash": "01015c0c0c6c69a24952db7f6ff30a5b132c10507a7b52fb3559347b79b4fc0c", "exact_match": 0.0}
{"doc_id": 676, "doc": {"question": "who built pedestrian bridge at florida international university", "answer": ["Munilla Construction Management", "FIGG Bridge Engineers"]}, "target": "['Munilla Construction Management', 'FIGG Bridge Engineers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who built pedestrian bridge at florida international university?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Miami"]], "filtered_resps": ["Miami"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e330486220e465c62608ca77c61659b7221cff403b6cfdce0276f92f6ccdee73", "prompt_hash": "fb4e9b62303cb90510112df11f99491e80c8dc06543d0d51d878e1255e52f7e6", "target_hash": "5a9e4068475638939af8614e8633cba5b51a2742986f7ae74a3a90c20be07ace", "exact_match": 0.0}
{"doc_id": 677, "doc": {"question": "mark who went to golf majors in 1998", "answer": ["Mark O'Meara", "O'Meara"]}, "target": "[\"Mark O'Meara\", \"O'Meara\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: mark who went to golf majors in 1998?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tiger Woods"]], "filtered_resps": ["Tiger Woods"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3e43377ba066010c491f57074492312a045fde7fbc5039af855c9d2085a4e948", "prompt_hash": "a89ab5136f1d69122b4dedfd618aa1453f56cb8dab181054c96de8f11b782d2e", "target_hash": "75b814ac95c476a13ff32a62bd0b2448e4a604113450b516483ba55693aa072c", "exact_match": 0.0}
{"doc_id": 678, "doc": {"question": "is coca cola a public or private company", "answer": ["Public"]}, "target": "['Public']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: is coca cola a public or private company?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" public"]], "filtered_resps": ["public"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad5127b7d84e8b7cb2c6923d61053d549231389340e7b8ee3cbdca3f5851965d", "prompt_hash": "6eaf3c63806655b674e7145d9e0c2fedf2daf7f835c23e10c49ea75998a3d4be", "target_hash": "a6368217f883a0c6d4c4fc4da8c6e0009342fb9cbd276715904c9be5158a98bf", "exact_match": 1.0}
{"doc_id": 679, "doc": {"question": "host of the late show who was once a correspondent for the daily show", "answer": ["Stephen Colbert", "Craig Kilborn"]}, "target": "['Stephen Colbert', 'Craig Kilborn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: host of the late show who was once a correspondent for the daily show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Trevor Noah"]], "filtered_resps": ["Trevor Noah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "874bed6f3f4f5943e23da80f6cf2ca706d8b897545abc3852a69bb66e4d214ab", "prompt_hash": "bef5120501a989d2a2ee5ed7e8848fbe5b442705017995db13f7d2253592e49a", "target_hash": "ac4d7c768e960721cc77edc691fd4f885a9804de195d627c166d0fe8a8be4c5f", "exact_match": 0.0}
{"doc_id": 680, "doc": {"question": "where is fort myers located at in florida", "answer": ["Lee County", "the Southwest"]}, "target": "['Lee County', 'the Southwest']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is fort myers located at in florida?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lee County"]], "filtered_resps": ["Lee County"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ea80e3b540e4c94276f62fed7dc88910a53d25f832fffd215aaa753352958a8", "prompt_hash": "920d02fd686ab0cd204bbf9c6a167c8f6719b76c63cca6e34b7dfe8d7c64f1dc", "target_hash": "ac6e26ae1db1f7944ee5574f9434736e846d626e52d16fc32dfaa7120a0d7d06", "exact_match": 1.0}
{"doc_id": 681, "doc": {"question": "who plays jack in battle of the sexes", "answer": ["Bill Pullman", "(Bill Pullman"]}, "target": "['Bill Pullman', '(Bill Pullman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays jack in battle of the sexes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Russell Crowe"]], "filtered_resps": ["Russell Crowe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6274aec7f8b483ffcdfa2a3d54e06b74e6c9924da0e822bc7c76448a04e7e376", "prompt_hash": "1ddf53b3f6d0624c912447b033aeb9ea09d5ed03a6914f67906eed7945d551ce", "target_hash": "f92e04ec3844779677bd86cdfc7dce15308430c9da797e04088601ed84435f6b", "exact_match": 0.0}
{"doc_id": 682, "doc": {"question": "was star wars a book or a movie first", "answer": ["film"]}, "target": "['film']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: was star wars a book or a movie first?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" movie"]], "filtered_resps": ["movie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bac09771f0fad7475f286b9879d00ad6be3a496a4c39aa822f39cf4cef04c92b", "prompt_hash": "f20a7945d5b8b59a414002940702e294138eb8cc6828a4f2a2044d4f7521a7b4", "target_hash": "1714e83a1422002b9ef64145000f0a0802df286fedca0beb69b9201826a0c882", "exact_match": 0.0}
{"doc_id": 683, "doc": {"question": "when did beds become popular in france and germany", "answer": ["the 12th century"]}, "target": "['the 12th century']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did beds become popular in france and germany?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 16th century"]], "filtered_resps": ["16th century"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1cad13e287e6cae1acdd61ae06843ea6894eb01a78ef035da0e709cb0ff4af92", "prompt_hash": "89fbf92b365d27acc9d64ec1908ed41a8dd8dfb4fa29e9a9a9d5113ba05d77e0", "target_hash": "25cb72805315827873a3de00d96b8341200198c4cacc4746b56e3c218fe60807", "exact_match": 0.0}
{"doc_id": 684, "doc": {"question": "who owns the golden nugget casino in atlantic city", "answer": ["Landry's, Inc."]}, "target": "[\"Landry's, Inc.\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns the golden nugget casino in atlantic city?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" MGM Resorts International"]], "filtered_resps": ["MGM Resorts International"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9dba55ddfba932339a51cba0129708236b79ca730d27f797d528add6b5882551", "prompt_hash": "372e77a605753ec691180d9a0a27c52871ec890a52650d40bb7c27895e463a3d", "target_hash": "ce4a2033ae03ef2408717e8453f9ec10015ec35e1cf407906d3c9bb5b07d622a", "exact_match": 0.0}
{"doc_id": 685, "doc": {"question": "where was the tv show friday night lights filmed", "answer": ["Austin", "Pflugerville"]}, "target": "['Austin', 'Pflugerville']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the tv show friday night lights filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Texas"]], "filtered_resps": ["Texas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cbc510e336b5bff372076a18cc50c98a8cadf7c3015b1c61305bfa5e931f8d7a", "prompt_hash": "c4565212cc15916f51f8a33cc63bfef68e4066f4d023473087ebf3b5fb3166bb", "target_hash": "1f6ee5e8838ebd0141d3494e2646fa9cf0693a55bb713d598e0a978e5c6081c3", "exact_match": 0.0}
{"doc_id": 686, "doc": {"question": "who came out first batman or spider man", "answer": ["Batman", "Superman"]}, "target": "['Batman', 'Superman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who came out first batman or spider man?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Batman"]], "filtered_resps": ["Batman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8260f76eab09598c4e08ee76f6c09fdb2e80c895c8323d3180279e21a1b9d68a", "prompt_hash": "7e426873fe5fde7945f5f759a86efbcc91bde1dcce75f196fa08cac1dea5a9f1", "target_hash": "0b73efce901e112322adf386c61824677aa615d5871e6359f264f9c7a98114e8", "exact_match": 1.0}
{"doc_id": 687, "doc": {"question": "where did they film the book of eli", "answer": ["New Mexico", "in New Mexico"]}, "target": "['New Mexico', 'in New Mexico']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they film the book of eli?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Utah"]], "filtered_resps": ["Utah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e1fa4d3cecac2eeedd0d73883b17f5d5990b51b134806fb2023a1ab3a5dcd733", "prompt_hash": "bb206c440c9941391c17b112df96144d8e46c1926e7ec144519760793de9882f", "target_hash": "9b21ed93faec53eff29cdf8d13033148f0ab28b0675e0bf372977c27f7baa611", "exact_match": 0.0}
{"doc_id": 688, "doc": {"question": "who kills barry's mom in the flash", "answer": ["Professor Eobard Thawne", "Thawne"]}, "target": "['Professor Eobard Thawne', 'Thawne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who kills barry's mom in the flash?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Reverse-Flash"]], "filtered_resps": ["Reverse-Flash"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c32368bb91114f0eb76f8f26d55cb83b2f2f3fbeeb78a98cdb216e58886ae4c5", "prompt_hash": "ddbf751ee4d71b1b0ff6661bdc17d141735a7e0d0d2a042f9e015d3bf40f36ed", "target_hash": "d9569305b77fdd9d7005dda31761a5701e6a7f360a73e9c1c067989026d63604", "exact_match": 0.0}
{"doc_id": 689, "doc": {"question": "where are the spanish steps located in italy", "answer": ["Rome"]}, "target": "['Rome']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the spanish steps located in italy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rome"]], "filtered_resps": ["Rome"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aab813d2ef05647f7bd80a0d1df3a12c60c5e79aebfa2ee079bd916f646f1a48", "prompt_hash": "a348e1b5cad621e991f2df6519809b5d12af5c35babfdda56c228ed23ecb2b4c", "target_hash": "ded15dde978e356aa7259ba062b1c7ce7cda762dd21e578e68b0c993154923ec", "exact_match": 1.0}
{"doc_id": 690, "doc": {"question": "who is mostly responsible for writing the declaration of independence", "answer": ["Thomas Jefferson"]}, "target": "['Thomas Jefferson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is mostly responsible for writing the declaration of independence?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thomas Jefferson"]], "filtered_resps": ["Thomas Jefferson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "02fc47ea25febfa1ee56384ad56ccc51d6942e4d8fe2ad59a2acaa372675094f", "prompt_hash": "91afae708377e4f62673562a4e7be1857aae16b642eff3c075d7431cf0c1bcfb", "target_hash": "42475f6d3851560323eedeea820f3a8225fc5fb4cfaa94bd56cadfe6d3c6c397", "exact_match": 1.0}
{"doc_id": 691, "doc": {"question": "where did they film woody the woodpecker movie", "answer": ["Canada"]}, "target": "['Canada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they film woody the woodpecker movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Burbank"]], "filtered_resps": ["Burbank"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ba48223ef5e811bdbea1348dcab01ad66ee8d840ce2b1a1180372e7e36a32d39", "prompt_hash": "24f81cd402d3665589a296c52bb429ab9f58c3c5d91c3972dfe4ae0a361a4391", "target_hash": "b6331bd59b4f4c96bb93005fbd31f398481003ab682305e364aab6a46846811a", "exact_match": 0.0}
{"doc_id": 692, "doc": {"question": "what is the second largest country in asia", "answer": ["China"]}, "target": "['China']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the second largest country in asia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" India"]], "filtered_resps": ["India"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2e0d16ee17463fe0ae52feda41970553610f881d941d09ca895f194e87e8ab20", "prompt_hash": "f0372e42a6776527595f1a6c4c6b17d140b6346e69240bea9ed2bc7f2bd49f5d", "target_hash": "11548c21ae9d3fb44407ddaa2e48320ccef615891642b38b1a21470d38e07356", "exact_match": 0.0}
{"doc_id": 693, "doc": {"question": "star trek the next generation season 3 episode 24", "answer": ["\"Ménage à Troi\""]}, "target": "['\"Ménage à Troi\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: star trek the next generation season 3 episode 24?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Best of Both Worlds"]], "filtered_resps": ["The Best of Both Worlds"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "69cb4b609e533654a49f1f24e52fda050130820b7fc59bdd13b63403da865d86", "prompt_hash": "2a4fc00c4c06dc320c2f2dcfc8cc76545d0c116a9e02ce107506969e817383e6", "target_hash": "4c9e51383ede8bb3883c685fcc3f3539afc97ac3db2f70ef1409dfecf9768aeb", "exact_match": 0.0}
{"doc_id": 694, "doc": {"question": "what was the alliance between great britian russia and france known as", "answer": ["The Triple Entente"]}, "target": "['The Triple Entente']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the alliance between great britian russia and france known as?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Triple Entente"]], "filtered_resps": ["Triple Entente"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cd47207e0093da3ede5ceb569f66975d59ee266730e6469f101b06cca4c8a15b", "prompt_hash": "0b9ca34c1fbeaee1696186ff7089afd56841fb05193f825c592a1c9bda48bb4a", "target_hash": "b227e578bd72f7e27c9fec286791d7d196d8f93594c7242ac8fbda6d74f34cb6", "exact_match": 1.0}
{"doc_id": 695, "doc": {"question": "who won the oscar for best actor in 1975", "answer": ["Art Carney"]}, "target": "['Art Carney']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the oscar for best actor in 1975?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dustin Hoffman"]], "filtered_resps": ["Dustin Hoffman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "976381005a7d6af17ba5e80bab8b9ebb60e3b49bc8dab702f3814438870eac30", "prompt_hash": "69c2874487b39726f92ec52000cfa822c099253972287ddec55065172925b491", "target_hash": "d5794034a0a3e2745f45bdb1f1c1186ee06a5472f93a30991d90e728072ebf70", "exact_match": 0.0}
{"doc_id": 696, "doc": {"question": "how many times have the winter olympics been in the usa since 1924", "answer": ["1960", "2002", "four", "four times", "1932", "1980"]}, "target": "['1960', '2002', 'four', 'four times', '1932', '1980']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many times have the winter olympics been in the usa since 1924?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3"]], "filtered_resps": ["3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ffefd79c4ffe0ce327eda679c5eefc5347e741a237b40ca1305554ec65c9d1f", "prompt_hash": "ef75fa096b44c169c0c75d4a0aeb959a50afc94c0e43c4b086ddd61a930aa5bd", "target_hash": "fc147016fe0ca940da63696c8d5169bb1fdd60d52f42277a6751e736314c2d68", "exact_match": 0.0}
{"doc_id": 697, "doc": {"question": "legislation regarding data protection and security in uk", "answer": ["The Data Protection Act 1998", "Data Protection Act 1998"]}, "target": "['The Data Protection Act 1998', 'Data Protection Act 1998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: legislation regarding data protection and security in uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Data Protection Act"]], "filtered_resps": ["Data Protection Act"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b011532b2940616c1b1e3737a9a765575b0dca51deac109b84fa919aaf16b03", "prompt_hash": "72d68d51e71ea50685902a5b123a2d7f6ca11c00b1c8cef89819aeca608f3be2", "target_hash": "fd4d404f510215a92f3eca7486934958cea4ff4468c5a4c959453a92754eff91", "exact_match": 0.0}
{"doc_id": 698, "doc": {"question": "when did the smoking ban in public places start", "answer": ["August 2, 1990", "1995"]}, "target": "['August 2, 1990', '1995']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the smoking ban in public places start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2007"]], "filtered_resps": ["2007"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c0032d14ff35ce915c41893be5175f08136482ade54c3cf814e91ab032af4378", "prompt_hash": "5e62562307b285623e6a8516b8f5e23ceaab659a260e9156e24b69d03e2815fa", "target_hash": "93b76d8d5321b4903ab9dccc0dc803faf4cb6f715d6cdfd892df893b810c286c", "exact_match": 0.0}
{"doc_id": 699, "doc": {"question": "who plays rooster's girlfriend on the ranch", "answer": ["Maggie Lawson", "Megyn Price"]}, "target": "['Maggie Lawson', 'Megyn Price']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays rooster's girlfriend on the ranch?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jessica Stroup"]], "filtered_resps": ["Jessica Stroup"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c2fe68c3f2ea210a5ae09607a97816c1a4dd684d32fc5f08e9d2b0ab7c9adda9", "prompt_hash": "866d62ffb81a32715fa1f49e9168f82d792a12059d70a9e5bd853a9e96bb1692", "target_hash": "f66e2b3842361e745ddf53427f60393602f2bdbcc88a94bea9117527ee2fa727", "exact_match": 0.0}
{"doc_id": 700, "doc": {"question": "who sings you are a magnet and i am steel", "answer": ["Walter Egan"]}, "target": "['Walter Egan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings you are a magnet and i am steel?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Cars"]], "filtered_resps": ["The Cars"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f0ed8d4d57bfe0fb8d93cc02de32ba4dcb9d20c0ce567abaa181efea6abc1eeb", "prompt_hash": "87bc92896ffb8b633b4937688f33054464a7bf7336c9e838db445dc8fdd256f6", "target_hash": "3b57bae1df59c9c064afd150c34312ccfa77673355ee6df4928bdb94071affad", "exact_match": 0.0}
{"doc_id": 701, "doc": {"question": "where is the tv show the curse of oak island filmed", "answer": ["Oak Island"]}, "target": "['Oak Island']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the tv show the curse of oak island filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oak Island"]], "filtered_resps": ["Oak Island"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7874b5c9c3134f67690196af15d66f561cd55f66e733c439c319d4614e5fad09", "prompt_hash": "33eef0f5537394582ce8b3e021fc54560b002754dc20ef57cecb70f7c2743dcf", "target_hash": "7ab1eb167b708b1e13a718a78dfdfa0d44c018a99454c58231b80ec55e9f5b3e", "exact_match": 1.0}
{"doc_id": 702, "doc": {"question": "who picks the chief justice of the illinois supreme court", "answer": ["the court"]}, "target": "['the court']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who picks the chief justice of the illinois supreme court?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Illinois governor"]], "filtered_resps": ["Illinois governor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b300557d8cf688fbd82c9446345bb7f103a9c5b72e1b182dcf7989a43b5fa54", "prompt_hash": "f0b6482ca598f9069c3ebb2ef77720c12037d4bea13bb0a62b8646b19a5fc80d", "target_hash": "0829be793c47e517d84c69e4f34f1cc4722e03c61d74c6a5df088d072becd601", "exact_match": 0.0}
{"doc_id": 703, "doc": {"question": "when did the first ninja turtles come out", "answer": ["1984", "May 1984"]}, "target": "['1984', 'May 1984']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first ninja turtles come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1984"]], "filtered_resps": ["1984"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "917f591cbfbcd038fe3a803fd38a09bc5e56411ad9aa659d8024f3b6bd243741", "prompt_hash": "f130a705792c30fa0b68683ea6ae5247df6763f9e53b543c8b2125559244753c", "target_hash": "57f13c2c9e2e5702066812ecb242a96f1a17f1e7d233ed9518c0c5473c634aa6", "exact_match": 1.0}
{"doc_id": 704, "doc": {"question": "where does the show the path take place", "answer": ["Upstate New York"]}, "target": "['Upstate New York']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the show the path take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d377b69a18508fd8698f203591d4839e908e6fc24c146e77543cb2f4fe9f753a", "prompt_hash": "9a724f3b7b203e95529c5ef08ebfe8fa3aa15bee5714f9501dcf5a75f2bc3b5b", "target_hash": "3c89d7aaa4818eab640d370fa3135c8053e8d255119b121934dce6129e0f19ab", "exact_match": 0.0}
{"doc_id": 705, "doc": {"question": "the world famous corn belt is in which country", "answer": ["United States", "the United States"]}, "target": "['United States', 'the United States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the world famous corn belt is in which country?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United States"]], "filtered_resps": ["United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "52941fbb5fbc660bb1bdab767bf4bdcc0e98219401c5269544b7d1f75b838d68", "prompt_hash": "cdeb557f23db20362445d7f8c20f3abd21635af819fd1bd048375da5c922acf6", "target_hash": "99b904965ee19869befba685940d27c545b204575ec9b34a6b963c6cde1a2e83", "exact_match": 1.0}
{"doc_id": 706, "doc": {"question": "who has the highest corporate taxes in the world", "answer": ["Argentina", "Pakistan", "Zambia", "Benin", "Gabon", "Bangladesh", "Burundi"]}, "target": "['Argentina', 'Pakistan', 'Zambia', 'Benin', 'Gabon', 'Bangladesh', 'Burundi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the highest corporate taxes in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United States"]], "filtered_resps": ["United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "abcd4bbbe6c344999fd54670a427c92cce8d1505a22ff1ef6a4468db5c8eea70", "prompt_hash": "34463841bc9e1d161a0c3bca504a7a5d53a5102509bbb54d6cd6749b394eefd2", "target_hash": "14e24f789336d1d7163f6bad3d72b87dc71ed94e8d3c59e2d655b671022493d2", "exact_match": 0.0}
{"doc_id": 707, "doc": {"question": "who was the first territorial governor of oklahoma", "answer": ["George Washington Steele"]}, "target": "['George Washington Steele']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first territorial governor of oklahoma?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William H"]], "filtered_resps": ["William H"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5615d25d674918d3e7247902acc09a3d209800ec5e084c62d9ebcb64c0566b21", "prompt_hash": "49e16a8034e264e55ee70b8b8a60893b9b89cd87d2ae07e414272e63fef239fc", "target_hash": "360a3dc406e52d1074fbf518a2ffa615a42639166263dd21adab5f9fe8047487", "exact_match": 0.0}
{"doc_id": 708, "doc": {"question": "when does kc undercover season 3 episode 10 air", "answer": ["November 10, 2017"]}, "target": "['November\\xa010,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does kc undercover season 3 episode 10 air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 14"]], "filtered_resps": ["February 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "51bc3035a1f605ac54776ac462e474c7cc6e4a31a0d8f9ef204083abcc08502c", "prompt_hash": "28636b12ac4883457b73ef62d5a69f30a56d72766a419ee64bcd381f4f225372", "target_hash": "f099c70f32c1c6120f75b14dc53a8c2aafe2279b5413df76b187b65e82cc8ded", "exact_match": 0.0}
{"doc_id": 709, "doc": {"question": "what type of speed does a speedometer measure", "answer": ["instantaneous speed", "speed of a vehicle"]}, "target": "['instantaneous speed', 'speed of a vehicle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of speed does a speedometer measure?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" instantaneous"]], "filtered_resps": ["instantaneous"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a0269a93f18589c3359adf22bc7939ea9a84f6e2c7dfa26c0486ec39d974a7a", "prompt_hash": "1be67a598392e527deb1930c497b10086164bd74dc29a0c328831f0baff22af6", "target_hash": "28450f5066649e667dbd54246df8f2f9b050a53ef65288148ef32b9c67d0875c", "exact_match": 0.0}
{"doc_id": 710, "doc": {"question": "how long is a whale shark in meters", "answer": ["12.65 m", "estimated at 9.7 m", "9.7 m"]}, "target": "['12.65\\xa0m', 'estimated at 9.7\\xa0m', '9.7\\xa0m']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long is a whale shark in meters?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12 m"]], "filtered_resps": ["12 m"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cb208e658673c3d83b2b514bc5908dcd9ef6ecd32b38a1e264123d9243bee794", "prompt_hash": "939c1119429294b56524dc5d0013accda3ab3350b61c21d3a4301345d2d06681", "target_hash": "a49fabfa09ab642eeda439680a836c2105da421f8c16f1ab9f5e07410db5b0bc", "exact_match": 0.0}
{"doc_id": 711, "doc": {"question": "who plays the lion in the movie zookeeper", "answer": ["Sylvester Stallone"]}, "target": "['Sylvester Stallone']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the lion in the movie zookeeper?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ben Kingsley"]], "filtered_resps": ["Ben Kingsley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0e3866d7696b4d90e478edc79d0e1f0ceaed0e8ba2ca98e86fa094911c7d0a46", "prompt_hash": "1ca6b9cfc1f7e570957d2444258b3f909c7fdffb8a0a2691180d0ce398bfa2e3", "target_hash": "874c53361f290688f76cb4a4218db4244858efd58eae49a76126122229ed4263", "exact_match": 0.0}
{"doc_id": 712, "doc": {"question": "who is doing 2018 super bowl half time show", "answer": ["Justin Timberlake"]}, "target": "['Justin Timberlake']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is doing 2018 super bowl half time show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ec469f39eee532c79dd7a4e2bed0a12cf37214247abadf5eda522242045e286", "prompt_hash": "1018fabbd981ff4d180c1646302df2ad6080e857e606651cde0926d25f933d1d", "target_hash": "5d3c152075c166190023989a2621c5a25323ea4325675ced16b03032cec5a254", "exact_match": 1.0}
{"doc_id": 713, "doc": {"question": "which level of weight bearing often comes with a set number of pounds", "answer": ["Partial weight-bearing"]}, "target": "['Partial weight-bearing']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which level of weight bearing often comes with a set number of pounds?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" state"]], "filtered_resps": ["state"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a4ca6296e45965df4808d7720634a9f53d5b9f63ef592799af7f5ba36acf170", "prompt_hash": "f27eb70b232207ced7b555a70fe6fba21a3e5869a452b35b083e2c7ad1beb5a2", "target_hash": "6000b2f9827eeb3e9c7e35a8c3880c8140efd0428b955648495f40cdae4b0950", "exact_match": 0.0}
{"doc_id": 714, "doc": {"question": "who became the it company ceo in 2016", "answer": ["Abid Ali Neemuchwala"]}, "target": "['Abid Ali Neemuchwala']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who became the it company ceo in 2016?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Daryl Connor"]], "filtered_resps": ["Daryl Connor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c39a367ce4676a5419d1c38379947b5dc9bd088b14e0cc64363a3bc8a7512f61", "prompt_hash": "acc9cd584db2df0b6d3e720e9e7b3e8a25710519b8d057bd79fcc4360ff81070", "target_hash": "678542f432489ecf30c1d8f15c0479dc958242d978d635150f69f1989dec2f23", "exact_match": 0.0}
{"doc_id": 715, "doc": {"question": "when did they stop saying mass in latin", "answer": ["from 1967 onwards", "1967"]}, "target": "['from 1967 onwards', '1967']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they stop saying mass in latin?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1963"]], "filtered_resps": ["1963"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7f6a481ad490c8e349a87170c13f8bacbee231d6669e7afe6337f3053917a8e7", "prompt_hash": "d8bffa231fbe365e919d81f8796348a7d406a5b2d813466aea64b9cc95dcaf8c", "target_hash": "14166b83087952c54f3106f33d17d85d66e28d70943ec9913f1dc8b1b13e5c90", "exact_match": 0.0}
{"doc_id": 716, "doc": {"question": "pace maker is associated with which body organ", "answer": ["heart", "the heart"]}, "target": "['heart', 'the heart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: pace maker is associated with which body organ?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" heart"]], "filtered_resps": ["heart"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6ee690cad2909f15771ad55ede64602887474353085efb7acc7e8ce7963300b4", "prompt_hash": "28a9dc83ca5afe44038000a209fa2154bc4f2ffc3cf06dcec038d3fece17a996", "target_hash": "091ec5b667bd4e9f40bdb1c95792fb5605f780eb44662b97bf29b7fd24d0078a", "exact_match": 1.0}
{"doc_id": 717, "doc": {"question": "what is the zebras name in peppa pig", "answer": ["Zuzu", "Daddy Zebra a.k.a. Mr. Zebra", "Zaza", "Zuzu & Zaza Zebra", "Mummy Zebra a.k.a. Mrs Zebra", "Zoe Zebra"]}, "target": "['Zuzu', 'Daddy Zebra a.k.a. Mr. Zebra', 'Zaza', 'Zuzu & Zaza Zebra', 'Mummy Zebra a.k.a. Mrs Zebra', 'Zoe Zebra']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the zebras name in peppa pig?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mr"]], "filtered_resps": ["Mr"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eff44301b259e6be1d1ea0948404cac1d6ee53b51c4ed688ee886859efad8125", "prompt_hash": "f6b4cd2d82b7cfd8ffa0102012c5c71f80a4de698b6effc8a5068636f40eb6e5", "target_hash": "af23ad969d75bef92bfd76b58a25edf75294d35a27477fbd2306b95782ad1643", "exact_match": 0.0}
{"doc_id": 718, "doc": {"question": "when was half life 2 episode 2 released", "answer": ["in 2007", "October 10, 2007"]}, "target": "['in 2007', 'October 10, 2007']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was half life 2 episode 2 released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" November 16"]], "filtered_resps": ["November 16"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4857f43f2d697a3d16efefb9ff60ad1edde8b9ccd9370b5bf3601c4ede56eca4", "prompt_hash": "4cea23510f81ced01ee3e19c6f594f453e452091f8e95c1d5890578cbec708dc", "target_hash": "a6b434d5d0551d4e6a815716ac9f9deccca71a7efa0d6994e66d4e0feae11069", "exact_match": 0.0}
{"doc_id": 719, "doc": {"question": "who plays genie in ferris bueller's day off", "answer": ["Jennifer Grey"]}, "target": "['Jennifer Grey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays genie in ferris bueller's day off?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jerry Stiller"]], "filtered_resps": ["Jerry Stiller"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7505ceea6d14de8838dfeff76048b70a196dfa97b30726a014e94482be9eacf2", "prompt_hash": "30ea766a81d12fc284928eb05412e02de741c7a458c267fe525bf04a0db08ffd", "target_hash": "e6cfec1e1e54e481bd416f19fc261838a32e036d71b12ae9ff9fdc22bbaaa5d2", "exact_match": 0.0}
{"doc_id": 720, "doc": {"question": "who sang national anthem at the super bowl", "answer": ["Pink"]}, "target": "['Pink']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang national anthem at the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "915f76e45fb9d5c0a3d05777dd1806397636f6a954ac0121e9f0520483e0c1e0", "prompt_hash": "dd50d3f2eca251ff12ad2594793f2ab161c0012ba85fe4aa77bd48006885093d", "target_hash": "491e226c43d39720230f8307c51a67ba28b0b68501d88129e96093803b7f227e", "exact_match": 0.0}
{"doc_id": 721, "doc": {"question": "what is the name of the lymphatic vessels located in the small intestine", "answer": ["lacteals", "A lacteal", "lacteal"]}, "target": "['lacteals', 'A lacteal', 'lacteal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the lymphatic vessels located in the small intestine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lacteals"]], "filtered_resps": ["lacteals"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3701465a857f778ff69e1158886dc2db3dad3341485c9b78525c81d5abacca9c", "prompt_hash": "5cc9cdf54ef1f2c6fdd988d566a5de208218b0894cbc0093007334073c9e529c", "target_hash": "206fbbfbd90a3b9d595d0ec45e73a246e511f7262e8673e6b267927c09e81bbd", "exact_match": 1.0}
{"doc_id": 722, "doc": {"question": "who did the voiceover in michael jackson's thriller", "answer": ["Vincent Price", "actor Vincent Price"]}, "target": "['Vincent Price', 'actor Vincent Price']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the voiceover in michael jackson's thriller?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vincent Price"]], "filtered_resps": ["Vincent Price"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "edb0302f1cf26a6a8bea5de9aa47483eb6b4e582e50da6fd0ec7f2f2be361acc", "prompt_hash": "3fa16e9ba92df4e124efabfbbbb885f778c5216a3f0f03e5f6baaa3abd77b12b", "target_hash": "9aa9e34fa703d59611f22782be35820024b3a0d3cf78540cd13698ebf7e4190a", "exact_match": 1.0}
{"doc_id": 723, "doc": {"question": "what is the term of an official in the house of representatives", "answer": ["two-year", "Speaker of the House", "Two years", "Representatives"]}, "target": "['two-year', 'Speaker of the House', 'Two years', 'Representatives']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the term of an official in the house of representatives?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" two years"]], "filtered_resps": ["two years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6669917a15961f5c96bb9f766b5d2a0bca9e3f1593233518326dcbbeafa55c95", "prompt_hash": "9a9ce0e6bc75a18c8b249822f2c6b95e284bba6185c1691910b15c5abc7a35a8", "target_hash": "c2b20d0a4ede5bd87d4bf6c86414d4170fa82a038e0921e33f7ef545f4b02f75", "exact_match": 1.0}
{"doc_id": 724, "doc": {"question": "when did reba mcentire record back to god", "answer": ["February 3, 2017", "2017"]}, "target": "['February 3, 2017', '2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did reba mcentire record back to god?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1987"]], "filtered_resps": ["1987"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6568647cbbfc8e0c1922c32e8b7d2c9141f0943b1536ccca1c733b2d5617a010", "prompt_hash": "4a62e2dfb038c0a05d0944da03275f0e7bf1ba0e2d6d7caa96e72b46dbd26b62", "target_hash": "a746d7d6ab19a8c3dce5c9eb1328de1b77ecdb8c181d9b6b192bd320bd64c646", "exact_match": 0.0}
{"doc_id": 725, "doc": {"question": "number of films in lord of the rings", "answer": ["three"]}, "target": "['three']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: number of films in lord of the rings?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" three"]], "filtered_resps": ["three"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0106d76a03e72054d68c1947acbf0574136ec028e61e8e4618dc002e32274aaa", "prompt_hash": "022f08cc56aec6a3cb01beaf498720daa9fbb312c6c9afcd6e92192a7cbc417d", "target_hash": "141a32643fe9577b35f39034369dfd8915892451c7d2aadc2c7af5642262e23f", "exact_match": 1.0}
{"doc_id": 726, "doc": {"question": "where do the phaeacians live in the odyssey", "answer": ["Scheria"]}, "target": "['Scheria']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the phaeacians live in the odyssey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Aeaea"]], "filtered_resps": ["Aeaea"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e5fb5da92881702c5a6b2aa87c4d344a8b2577130c1638eeb7caceba95f5bbc2", "prompt_hash": "ac2264244f0779d2428915bd1b6cd878cde91d1de9d079719f3e8d63caa61d5e", "target_hash": "97a9a00befdcd333743cf291fcc4b397fe198ccadd8bb586677979b7abbf2428", "exact_match": 0.0}
{"doc_id": 727, "doc": {"question": "who scored the most points in a game nba history", "answer": ["Wilt Chamberlain"]}, "target": "['Wilt Chamberlain']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who scored the most points in a game nba history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wilt Chamberlain"]], "filtered_resps": ["Wilt Chamberlain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8cc49daa96c65057599672634766fd3a27fad96e6808d74d9ee5d8d4c0dd1898", "prompt_hash": "6176d1a441368c641e7702d21bb81844440a0fd00d1abd57bb5d9cfa966f1f88", "target_hash": "84924a8be2c9aacc9097f91bbf6a08311409f600a16f07822001cb664a550a11", "exact_match": 1.0}
{"doc_id": 728, "doc": {"question": "where did the french immigrants settle in texas", "answer": ["1685", "Arenosa Creek", "present-day southeastern Texas", "Matagorda Bay"]}, "target": "['1685', 'Arenosa Creek', 'present-day southeastern Texas', 'Matagorda Bay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the french immigrants settle in texas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" French Texas"]], "filtered_resps": ["French Texas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "47ba4f8c95541b3a9ac5415f684ec58890d6dc916b891c53416dd71495d21723", "prompt_hash": "5116302028f29fe387f41b70c9c3853c8aeabd6932c17d3f403713f56eb0f959", "target_hash": "b00216996603e30eec05cf964b5b47c8ea78eef78cfacff0098ed26086744e8b", "exact_match": 0.0}
{"doc_id": 729, "doc": {"question": "who started the state of the union address", "answer": ["Woodrow Wilson"]}, "target": "['Woodrow Wilson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who started the state of the union address?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Washington"]], "filtered_resps": ["George Washington"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4efe3dd9003a15667d3237549fec9c9bf062f86f517e1cb977d878485bf1ce55", "prompt_hash": "04c88521f0824d5b92bbf08d0460d1b888153d1d8a0e64052fd57cfe3f463b2f", "target_hash": "b06fcc38c078bf2badea96ac9b85b33f1aa9d7125dc8df01d23cc3f86f871f6e", "exact_match": 0.0}
{"doc_id": 730, "doc": {"question": "when was the last time clemson won the national championship", "answer": ["2016"]}, "target": "['2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time clemson won the national championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2016"]], "filtered_resps": ["2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7bd11f9e84f99973321d10e088a9ed6542180cc36fc976b5d65f10de073ffe4b", "prompt_hash": "2ff3bfdb081d2771096e93c3d53473872d91864dbc0f05453c7c65914644612c", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "exact_match": 1.0}
{"doc_id": 731, "doc": {"question": "what are the functions of the human skeleton", "answer": ["endocrine regulation", "support", "production of blood cells", "protection", "storage of minerals", "movement"]}, "target": "['endocrine regulation', 'support', 'production of blood cells', 'protection', 'storage of minerals', 'movement']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the functions of the human skeleton?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" support"]], "filtered_resps": ["support"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a708104c278c27ddfbf1672e5a0b15fbb772ada2673a66965d1f5d480913219f", "prompt_hash": "d187a2e3e4aac0b6ddc71a587e2c8846d7623ead78be741a3cc97d68914214c8", "target_hash": "a418d5f1c5b3ff113953a9fa401b17f0d24ee8069b9483cb53108e8e9c55444d", "exact_match": 1.0}
{"doc_id": 732, "doc": {"question": "where did they film season 8 of the walking dead", "answer": ["Georgia"]}, "target": "['Georgia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they film season 8 of the walking dead?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Georgia"]], "filtered_resps": ["Georgia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9816340dee27f32a92baeaa485fdcc3dc8c5eb2e1f97d883f5cbd1fec2be3bd8", "prompt_hash": "7089da568dc76ca3119d490d76b61e8aeafd6de594f34a1311dc2f319d8048f9", "target_hash": "f441fcf617be4e4488ab57c1227272b9064e4dde54b4f8c84b75dc4ef2a21385", "exact_match": 1.0}
{"doc_id": 733, "doc": {"question": "who sings the theme song for living single", "answer": ["Queen Latifah"]}, "target": "['Queen Latifah']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the theme song for living single?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Janet Jackson"]], "filtered_resps": ["Janet Jackson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f6adf2700b1a8983f9b2a3c444fad82e24becf2d57ee7992ca662036fc500356", "prompt_hash": "5ec998452f499af71a126d50e0d419edd9dc14f9f4a9e2e616b9f8e8130c7e90", "target_hash": "406e8d10b94a81c7e6ebbb4580b80c8b25c5954cb7e527dd07e21a93bdcdba8a", "exact_match": 0.0}
{"doc_id": 734, "doc": {"question": "who plays ivo on once upon a time", "answer": ["Dan Payne"]}, "target": "['Dan Payne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays ivo on once upon a time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Colin O'Donoghue"]], "filtered_resps": ["Colin O'Donoghue"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "34203ea7c21b0d0fb2eabd9589a7eae90b95cfdb1486c3d11f682edc4fcd4ace", "prompt_hash": "506001cd87ac2265a4090a49cf03299616cb92cadefa68646691abd94e7a9f8f", "target_hash": "36ff1ce950a650d30539a35dfca518194b05a2f8a04afbb5f4c8cb6c88c9a298", "exact_match": 0.0}
{"doc_id": 735, "doc": {"question": "when was the last time minnesota vikings was in the super bowl", "answer": ["1969", "XI"]}, "target": "['1969', 'XI']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time minnesota vikings was in the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1970"]], "filtered_resps": ["1970"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58309d184672beb0c14785747c182dba5d5da22efa7a9bafdd342f1c03d13164", "prompt_hash": "9d9351579eb4523c0319e24fe4470d36aa8358bf277d617b5c76d6b3c8d3883a", "target_hash": "9c109391bdae003a285462d0891a8070f1023a55df2b8c0dececcced689c2f64", "exact_match": 0.0}
{"doc_id": 736, "doc": {"question": "who plays brad pitt's daughter in moneyball", "answer": ["Kerris Lilla Dorsey"]}, "target": "['Kerris Lilla Dorsey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays brad pitt's daughter in moneyball?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Elizabeth Olsen"]], "filtered_resps": ["Elizabeth Olsen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "230340fc0b417750d74fd33e59958aede5bb461ca3b3c10dcb3522ccc22b1674", "prompt_hash": "c32681675e58b039fc80f3a78bab98c9643f6f3f109ad9278cc88a4f599e6970", "target_hash": "0d05d8297da30bd1a71e2d819a410dbb351f638f46ae719f0cd028c30a2f0b9a", "exact_match": 0.0}
{"doc_id": 737, "doc": {"question": "who sang the songs in the movie beyond the sea", "answer": ["Kevin Spacey"]}, "target": "['Kevin Spacey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the songs in the movie beyond the sea?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Bublé"]], "filtered_resps": ["Michael Bublé"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d905c1303434ba99973af694396abde1dc2667ce1d93a34c646d888a0fa7d093", "prompt_hash": "4b66df03ef2ac94502864a66bb8ba6bec612962bab64b2c754f937e77c9181fe", "target_hash": "7447972d9e6123ef8755e57288f5afc0a0fcb2fd5684d9b5708218c36afc6875", "exact_match": 0.0}
{"doc_id": 738, "doc": {"question": "the cast of a good day to die hard", "answer": ["Mary Elizabeth Winstead", "Sebastian Koch", "Radivoje Bukvić", "Jai Courtney", "Bruce Willis", "Yuliya Snigir", "Sergei Kolesnikov"]}, "target": "['Mary Elizabeth Winstead', 'Sebastian Koch', 'Radivoje Bukvić', 'Jai Courtney', 'Bruce Willis', 'Yuliya Snigir', 'Sergei Kolesnikov']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the cast of a good day to die hard?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Magaro"]], "filtered_resps": ["John Magaro"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "66e3aafcb4a17629287f86c8d9048fa7d0560fa2a05a9d71a5dc7f24938f785a", "prompt_hash": "fbcd438af9d90fcc07709db1bb0e69d3ebf765c94e56643b3afb161af6d1b61f", "target_hash": "23621bdb3260e366fd9e4bf5222a4ae04809af92490520d0be95bdb357b47b75", "exact_match": 0.0}
{"doc_id": 739, "doc": {"question": "last world series that didn't go 7 games", "answer": ["2015"]}, "target": "['2015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: last world series that didn't go 7 games?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2001"]], "filtered_resps": ["2001"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96a71b7f97b36f27ec78d6934c25a58a66b9167295b748f71616397cd0427a52", "prompt_hash": "a9c4e0c5a6def850f5f11752932a806957404bbc71576ac877a9e9c01375deaa", "target_hash": "d2dae4566ccf9bd979a614c8c426fe4b400fccb0b129e4067af4228858ba1e39", "exact_match": 0.0}
{"doc_id": 740, "doc": {"question": "who sang our god is an awesome god", "answer": ["Rich Mullins"]}, "target": "['Rich Mullins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang our god is an awesome god?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" TobyMac"]], "filtered_resps": ["TobyMac"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "407c8ab9d9693f51fa41718af8f6e0c9bc53d79aef257c6cec6515d08dbac62b", "prompt_hash": "abbea4a87ac896b97fb9063ba5aa725098f5215b63f322d0ebd719ac16e5315b", "target_hash": "4438e760536dc6ad0b550322de826d1567760391cc788d536a29fdba0b5309bf", "exact_match": 0.0}
{"doc_id": 741, "doc": {"question": "when was the first orca put in captivity", "answer": ["in the 1960s", "November 1961"]}, "target": "['in the 1960s', 'November 1961']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first orca put in captivity?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1965"]], "filtered_resps": ["1965"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "afb23fe5b3a77e57bcd4470f4129808b590d6336ae2b6c6d3b4b6ea636b054a1", "prompt_hash": "98dfc939b36e9339ff19703f9bb0efe3838a8edf74933096de80e858df52e2f8", "target_hash": "0bd7be7e138469afb0d08201ce7e2ad4e1ca7b80a13a7f2c42354ffa0ff408c2", "exact_match": 0.0}
{"doc_id": 742, "doc": {"question": "where's the pro bowl going to be this year", "answer": ["Camping World Stadium in Orlando"]}, "target": "['Camping World Stadium in Orlando']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where's the pro bowl going to be this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Orlando"]], "filtered_resps": ["Orlando"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "63867b59e67773feb6059380e868d8020aee4c7ed6a64a022b9c2737b96d447a", "prompt_hash": "99b62633017f611b6abb1d7689d74212c144590d41c684ed2923bdf8e0aad4ac", "target_hash": "3031d07badf02fcd7ee123405e75f977a34ef7da4e5604b04fc422cac554ec19", "exact_match": 0.0}
{"doc_id": 743, "doc": {"question": "who sang the song a white sports coat and a pink carnation", "answer": ["Marty Robbins"]}, "target": "['Marty Robbins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song a white sports coat and a pink carnation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Beatles"]], "filtered_resps": ["The Beatles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ed4fd23516a71b72e0ceea3db7c2edd41f9ee4bc4cb3ec3a499491eea0035ad", "prompt_hash": "7fd30b96fef9106411cc16deab218665f46ce134f6022143ebf587063e5cf3a9", "target_hash": "a01cc43ac051b3e8edcc9fbd814185edce1a308f969403591f5b0189fe8ae007", "exact_match": 0.0}
{"doc_id": 744, "doc": {"question": "last team to win fa cup outside top flight", "answer": ["Arsenal", "West Ham United"]}, "target": "['Arsenal', 'West Ham United']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: last team to win fa cup outside top flight?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arsenal"]], "filtered_resps": ["Arsenal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ebd4bbc4131e2b8de6b8b5c36aef018718587bb9559b116708b17b51f7ba3da5", "prompt_hash": "cdb0e518d7568a5c823c01580dfffe2551bb2168f64fb13a50a144ae5d0e511a", "target_hash": "1ff4fdf22e65f7b11bde522191842dacdafd9fb21ff46455513f0f348a053504", "exact_match": 1.0}
{"doc_id": 745, "doc": {"question": "who sings the pokemon theme song season 1", "answer": ["Jason Paige"]}, "target": "['Jason Paige']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the pokemon theme song season 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mary Elizabeth McGlynn"]], "filtered_resps": ["Mary Elizabeth McGlynn"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c4143881665d2bd1199e40376c69470da4aebb50ff50a3a8ace790e957bf2484", "prompt_hash": "664bfac6208681b19f03239125bc6ff2ab48228dc3042a0a307122a69bd1505a", "target_hash": "666365c4aabcd0878f6bbc9810415cb9b11cd1b150bcc0a79e088c0ccaf703bc", "exact_match": 0.0}
{"doc_id": 746, "doc": {"question": "what is it called when you believe in greek gods", "answer": ["Hellenism", "Hellenic polytheist", "Hellenic Polytheism"]}, "target": "['Hellenism', 'Hellenic polytheist', 'Hellenic Polytheism']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is it called when you believe in greek gods?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" polytheism"]], "filtered_resps": ["polytheism"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0dceb666829f2acb4fabab9a2bf73eb87235963ed8c44be9bc67e8780a1556c1", "prompt_hash": "4c150bba8fbc980bca65a2312fa2cff03adf66a3f36f30b989f8f2044c5dd380", "target_hash": "7bda04e62096e78e38cc93cfaaf47df2b16085a3ad897f90c73f328cd4e03628", "exact_match": 0.0}
{"doc_id": 747, "doc": {"question": "who proclaimed 5th october as world’s teachers day", "answer": ["UNESCO/ILO"]}, "target": "['UNESCO/ILO']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who proclaimed 5th october as world’s teachers day?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" UNESCO"]], "filtered_resps": ["UNESCO"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "74aec48aacaf0691154246548b836505a0837e4623973884095397b55d4ff1b4", "prompt_hash": "871a9850798d5ab429cd3cc73f3c6ec250ebd6901d79c48c18a54cb3e9468ac8", "target_hash": "b936d10ea0d69beaf0506d773f4d945300f055cd80e32c530b9085f807af5347", "exact_match": 0.0}
{"doc_id": 748, "doc": {"question": "who plays reggie the robot in justin's house", "answer": ["Ben Faulks"]}, "target": "['Ben Faulks']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays reggie the robot in justin's house?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David McCallum"]], "filtered_resps": ["David McCallum"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f725ad135ba139818add9ec1d49997ebb64b83819861d45de0cd7cb5bd7e5f13", "prompt_hash": "7cfbcc1acb7c5719b99902646c4e04eb119e94c0291c7331b2e677093801204b", "target_hash": "041a9f41e6316bbbf44babc035ca0c6ebf7a879be22b0024f8738a0fe67db9c2", "exact_match": 0.0}
{"doc_id": 749, "doc": {"question": "where was the hindenburg scheduled to fly and how many passengers were on it", "answer": ["36 passengers"]}, "target": "['36 passengers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the hindenburg scheduled to fly and how many passengers were on it?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lakehurst"]], "filtered_resps": ["Lakehurst"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0578cd2cbc431e9a3540b76e20e9edaf027fa8131bba234c08f359daeb821926", "prompt_hash": "c326db85ff56547bdc5e8955f35db6f03b158473a1c122e6b801cef6aa339560", "target_hash": "4168c791c1dcbd7fb7662a36d321795916cd0e05a2cde2e9f378c47e401fe346", "exact_match": 0.0}
{"doc_id": 750, "doc": {"question": "total strength of judges in supreme court of india", "answer": ["24", "24 judges"]}, "target": "['24', '24 judges']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: total strength of judges in supreme court of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 26"]], "filtered_resps": ["26"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5fe35ddf8006071f17ec1420b50edc6fb05572ce52fbef4b80c1c903b65b6e33", "prompt_hash": "0e715f7fef9c726eea331c80fa7aefc841f9893ce3539d88a737b9202b464880", "target_hash": "91398c6e3a0461e8b350dacd383077e392fc1d7c03c39305ca16c2518f0d0fa7", "exact_match": 0.0}
{"doc_id": 751, "doc": {"question": "what is the point of a menstrual cup", "answer": ["to collect menstrual flow", "collect menstrual flow"]}, "target": "['to collect menstrual flow', 'collect menstrual flow']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the point of a menstrual cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Menstrual cups are reusable devices that are inserted into the vagina to collect menstrual fluid"]], "filtered_resps": ["Menstrual cups are reusable devices that are inserted into the vagina to collect menstrual fluid"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0df4a3126498fe116ae1b6b4eb7db3388d8d04843598abb120075f981854e50c", "prompt_hash": "debb9a16a404c4517fd29308be3bc9bc665dd60a97a48e43c642337a28fa2387", "target_hash": "99eb21e26e35be296d152b1c0f16b056ffd19d428ca12d6f4e92931f98c68bec", "exact_match": 0.0}
{"doc_id": 752, "doc": {"question": "who created the pieta and also painted the ceiling of the sistine chapel", "answer": ["Michelangelo"]}, "target": "['Michelangelo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who created the pieta and also painted the ceiling of the sistine chapel?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michelangelo"]], "filtered_resps": ["Michelangelo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2ff6fe9d4fb52ea4445ee924884d5567f5df23d8563259594588aa88462bf3c7", "prompt_hash": "64489a8d5f20189aa7d26c874635dc6e22468fb2950a75ddac0da2f857b6e63b", "target_hash": "e225dd89eb347c6a198d4398040ee0691c9a96c696ffcee20bf71322b3485467", "exact_match": 1.0}
{"doc_id": 753, "doc": {"question": "who sings far from the home i love in fiddler on the roof", "answer": ["Hodel"]}, "target": "['Hodel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings far from the home i love in fiddler on the roof?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Zero Mostel"]], "filtered_resps": ["Zero Mostel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96c3b7d643f17532dea9b1353188da722f14e88d4360700c50c6792694e04102", "prompt_hash": "1cc2ff24267d38c7bc40c97d09cac3635054a55c8148820870d20e796e3f28a2", "target_hash": "0703a974a113c45b8a8d545941423baecffc6acc531b9d8ad5dec4a93374c759", "exact_match": 0.0}
{"doc_id": 754, "doc": {"question": "where was the tv show high chaparral filmed", "answer": ["California", "Saguaro National Park", "Old Tucson Studios in Arizona"]}, "target": "['California', 'Saguaro National Park', 'Old Tucson Studios in Arizona']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the tv show high chaparral filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" California"]], "filtered_resps": ["California"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a2927475e07fe23794954122b4b43e7ebb536dfbbc37b52a82fa06a0edf4db1", "prompt_hash": "65e0254d3f7b9f33ab4be29acaab7e674d5abd14130850e5954643e022a14da9", "target_hash": "b8778d6063b6d7529b53f68c48481da34016cbb0819db98001d481b0a1e27b67", "exact_match": 1.0}
{"doc_id": 755, "doc": {"question": "what is the largest city in the united states per square mile", "answer": ["Sitka"]}, "target": "['Sitka']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the largest city in the united states per square mile?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York City"]], "filtered_resps": ["New York City"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f6aa4bf6a70911eb1a77508598130bb4edcb451136b9ca56eaf6e88ada784ae", "prompt_hash": "bda2425e0c23de458ba959aee13c137c47d64b13630759afc7a56212a761debc", "target_hash": "2d2f90a5728cf968928281ce8c11f20eb4baaf2f7ca75c81d603b6750caeca35", "exact_match": 0.0}
{"doc_id": 756, "doc": {"question": "who was the mexican commander who surrendered to the texans at the capturing of san antonio", "answer": ["Sanchez Navarro"]}, "target": "['Sanchez Navarro']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the mexican commander who surrendered to the texans at the capturing of san antonio?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Santa Anna"]], "filtered_resps": ["Santa Anna"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d45695688c4c274c166291c08af26ed4fadac8c453975fbc79ade81ff5e43c82", "prompt_hash": "b4bdb92bab2f62842c59a0326ead1c11cc00b5f7c6a9276ecec9a179d3358cdb", "target_hash": "1c5e3fbfccef86ebda778238f57f2c27fc60af672d08346c29fee2d373a39b8a", "exact_match": 0.0}
{"doc_id": 757, "doc": {"question": "who began the age of exploration in portugal", "answer": ["King Dinis of Portugal", "Afonso IV of Portugal", "Prince Henry"]}, "target": "['King Dinis of Portugal', 'Afonso IV of Portugal', 'Prince Henry']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who began the age of exploration in portugal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Prince Henry"]], "filtered_resps": ["Prince Henry"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fc2340f380b6f2d91478a895e1d639552cc196a1c265cf98162000a6e3ce974d", "prompt_hash": "8ecb47f544081f767e628031be2aa31fa1fb8748c89799cce10cae1bf6196a13", "target_hash": "5ac9c49a847adf6ee6b2c97257f90016ebfd99f87649302935e24c46497ba7b1", "exact_match": 1.0}
{"doc_id": 758, "doc": {"question": "when is the running of the bulls held in spain", "answer": ["6–14 July"]}, "target": "['6–14 July']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the running of the bulls held in spain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" July"]], "filtered_resps": ["July"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "26b22fb1bf19baf08915c94722414f6227bee9473f4e8917229b53804c6d02e4", "prompt_hash": "8098d3220f872d4cb94ab0ed0954a6bc8bb6553ce808090f658c9115136f468c", "target_hash": "f2057e76071e4f79a0699a440370e6f8a0527eca3c31023163827089ee8c8f7c", "exact_match": 0.0}
{"doc_id": 759, "doc": {"question": "who sang how long has this been going on song", "answer": ["Ace"]}, "target": "['Ace']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang how long has this been going on song?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c70f5e1e1392bb8fe8f0b0fad8035551fea000f4d8e173e8e73ebc6d91f91d79", "prompt_hash": "f21a418dea3f7322542916c79539609809f5c89d67f3df6589f98338fb98283c", "target_hash": "3bbe5250278164dc72e8acef787d41d04013f3b662e6f84872b6b3b3ebc1685f", "exact_match": 0.0}
{"doc_id": 760, "doc": {"question": "what was the lowest barometric pressure for hurricane irma", "answer": ["914 mbar (hPa)"]}, "target": "['914 mbar (hPa)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the lowest barometric pressure for hurricane irma?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 921 mbar"]], "filtered_resps": ["921 mbar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "832fce24b1cb5c48f31d3a1fc6b98fff786f816b841238f34371bd7aa97b53ae", "prompt_hash": "a93064afad278948d1395510b64d8e1eb8ec972cfc99f267bda07e20ef9b3716", "target_hash": "3d2ec9439e6f055d9227c56dde6be9749ece1412359dda2a9f3c361dacfd93ed", "exact_match": 0.0}
{"doc_id": 761, "doc": {"question": "when did david akers kick the 63 yard field goal", "answer": ["September 9, 2012"]}, "target": "['September 9, 2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did david akers kick the 63 yard field goal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2002"]], "filtered_resps": ["2002"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "83f97417e21bcbe8aeb674af7bef30669dc664379a314bec2b9c2c01c1e50909", "prompt_hash": "75bd3efaef3e615a749b90d5338afa3ea451567f813e5b02d74029f1022b031f", "target_hash": "32da52faa036316e88ed4e3db0af04198b2299e69838babab07d27c462efefa1", "exact_match": 0.0}
{"doc_id": 762, "doc": {"question": "who plays the scary nun in the conjuring 2", "answer": ["Bonnie Aarons"]}, "target": "['Bonnie Aarons']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the scary nun in the conjuring 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Taissa Farmiga"]], "filtered_resps": ["Taissa Farmiga"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bc62e6dcaf04f3b4c89cb2cd4cf705548f976ce1326878ebf002f62eeb224a4b", "prompt_hash": "b63c0703a1c828a018467c28946512b2fd82f592d885786bdb098d15b6dddf54", "target_hash": "6a82aa1c5915239ba13039879c611b155641a9ca9edf8141f3e0de2f6157bdd5", "exact_match": 0.0}
{"doc_id": 763, "doc": {"question": "season 2 attack on titan how many episodes", "answer": ["12"]}, "target": "['12']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: season 2 attack on titan how many episodes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 13"]], "filtered_resps": ["13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d65550c98d96a228ea4d58c79345bfacf22c13f873967574e49433db56dc1eb2", "prompt_hash": "ab08b58a3088136c67d02e67968b70da1c58d5be19bd453e9358d066073d1fda", "target_hash": "0485d4130dc78d35a36ed5f5c5bfea741e1e1e0ea37b856d0197e254d85d7d8e", "exact_match": 0.0}
{"doc_id": 764, "doc": {"question": "where are the cones in the eye located", "answer": ["the retina", "in the fovea centralis", "in the retina"]}, "target": "['the retina', 'in the fovea centralis', 'in the retina']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the cones in the eye located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" retina"]], "filtered_resps": ["retina"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0e7b4dfd20bee0c04dc373184c7d6ac95f4323eb13083a911d3989e970ceaa91", "prompt_hash": "ba0d12655bc47f1681e135cf1718a5b6ce852fee226cc5f69ab7c77bb9c8ddc8", "target_hash": "07311d5ad4127036d508979927ad6a176c274169cdbfe21a4437d582e925e8cf", "exact_match": 1.0}
{"doc_id": 765, "doc": {"question": "which is the lion's mane jellyfish's species within its genus", "answer": ["C. capillata", "capillata", "Cyanea capillata"]}, "target": "['C.\\xa0capillata', 'capillata', 'Cyanea capillata']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the lion's mane jellyfish's species within its genus?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cyanea"]], "filtered_resps": ["Cyanea"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c11fb56962e53a38b4471676d39da1be6b8b295aa49875697290746970bd00d0", "prompt_hash": "81d321ee64ebc9656f3e00241d748726863c07b3239ee4fe91a3a167fa574ac8", "target_hash": "3c81b5f368f4dc830fde6a4b5ce0aeec97c84b6bf7ee9070b663921d31bb8547", "exact_match": 0.0}
{"doc_id": 766, "doc": {"question": "who has the most all star mvp awards", "answer": ["Bob Pettit", "Kobe Bryant"]}, "target": "['Bob Pettit', 'Kobe Bryant']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most all star mvp awards?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kareem Abdul-Jabbar"]], "filtered_resps": ["Kareem Abdul-Jabbar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92e5509d474293df37cafd3cc8b650b5d84ada8b06456f24299770f1547fa471", "prompt_hash": "86b3d3689d1a216a068f7ef05188efc4e9f8c44f0ab8b0a7b4aae17ac589ee73", "target_hash": "fcdf59cf666de6d961f1265d71bb3f7285139b950f19ae27b9c95502f20717ef", "exact_match": 0.0}
{"doc_id": 767, "doc": {"question": "when did korn's follow the leader come out", "answer": ["August 18, 1998"]}, "target": "['August 18, 1998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did korn's follow the leader come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1999"]], "filtered_resps": ["1999"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f6e803864c3218e13e38327ebb7b7e3862f6566428b2626213b3d9f5a8a1a32e", "prompt_hash": "5197f0129f1c3c027dcb1653cb8ccd0faf5a1f47bf3b45da4698e469cf1c5988", "target_hash": "9e09dc909a0b3c6ab4fc281b8d8c652bb29fb7cc8378828b8e6981970d0d0c63", "exact_match": 0.0}
{"doc_id": 768, "doc": {"question": "when did the battle of badr take place", "answer": ["13 March 624"]}, "target": "['13 March 624']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the battle of badr take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 624"]], "filtered_resps": ["624"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d90ed2002a803d6142e235ff2f4adfd28daa7dfea22f5998c654f3225c94c3c9", "prompt_hash": "b18c152b73de2166086e67e5a56f3013d754591137ce31e8d98978d480f464b9", "target_hash": "04e17ed1a5883b34290105fc0a1b72a37b830c142b29cff174ca290c04e0fc9f", "exact_match": 0.0}
{"doc_id": 769, "doc": {"question": "which abbott and costello movie has who on first", "answer": ["The Naughty Nineties", "One Night in the Tropics"]}, "target": "['The Naughty Nineties', 'One Night in the Tropics']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which abbott and costello movie has who on first?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Abbott and Costello Go to Mars"]], "filtered_resps": ["Abbott and Costello Go to Mars"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "61b0b059dc509ad3061049503d77c39aae6bf5788b3010557c3f3b643d830223", "prompt_hash": "fc44ea367a5939d6bb963d48ceaf7e91699cae3ad7f87e4b9d9ab965eb3f33e8", "target_hash": "48e513bc9def273ae532db6c520bd02295b4b58ffe9de9882a17673789270a43", "exact_match": 0.0}
{"doc_id": 770, "doc": {"question": "how many episodes in series 7 of game of thrones are there", "answer": ["7", "seven"]}, "target": "['7', 'seven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes in series 7 of game of thrones are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ba14d15794547f4d4f3e8122db8ff6e4d066868578c0b1ce73db16b34e6bea4", "prompt_hash": "f285ffee0f7f94f68275655c4b6078d9cf2cb4575015f16ee68b04cf558b06a0", "target_hash": "47f94e48a0b9d4503db57f1a49661eb2671f9fd3c109285b1dddf1e46f90fa36", "exact_match": 0.0}
{"doc_id": 771, "doc": {"question": "how many gallons of water in the gulf of mexico", "answer": ["660 quadrillion US gallons"]}, "target": "['660 quadrillion US gallons']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many gallons of water in the gulf of mexico?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 4"]], "filtered_resps": ["4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "586f01eafc7d4bd5e7d2809d886a965a62ab2c01fe007a41a653c8f36313234b", "prompt_hash": "3bffeb3f8065cffd8ab31d9c9045f6f7b13ec19a48bca98e66015d30380162c1", "target_hash": "a0339eb9bef0ab875d04030b36528178776b011da574ffd8e723535a6c074869", "exact_match": 0.0}
{"doc_id": 772, "doc": {"question": "fast and furious 7 red car abu dhabi", "answer": ["The Lykan Hypersport"]}, "target": "['The Lykan Hypersport']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: fast and furious 7 red car abu dhabi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dodge Charger AWD AAR"]], "filtered_resps": ["Dodge Charger AWD AAR"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f1cf7ba9dd87ba7c5bfdddb39786978fde30471f51d64059620d4197379b04fe", "prompt_hash": "89a0382659b4fb6db946a8e5e071b7ca1df6f43ff4e2d22d4e6f17bb000c2fb8", "target_hash": "7ce362039f1b0439acd6f1df5e5ea493ef3727d9aea8d9e6686bcfe5e49bb32d", "exact_match": 0.0}
{"doc_id": 773, "doc": {"question": "islamic term for the community of the faithful", "answer": ["ummat al-Islamiyah", "Ummah"]}, "target": "['ummat al-Islamiyah', 'Ummah']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: islamic term for the community of the faithful?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ummah"]], "filtered_resps": ["ummah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5a4c33c8745fc9ce279cb2194341268de3056308a26e803f56ade481fa6a9135", "prompt_hash": "b0894c350ccf28945b03ade180a824c23d4493129dcb7e63ee0275ec838b16c5", "target_hash": "b89620de1766eb84a903a73c26e7d8e6e46ff9f87594ca1cbdf77415cf7ea96f", "exact_match": 1.0}
{"doc_id": 774, "doc": {"question": "when did canada gain independence from great britain", "answer": ["April 17, 1982"]}, "target": "['April 17, 1982']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did canada gain independence from great britain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1931"]], "filtered_resps": ["1931"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d4ca1a1bca10a50540994f066dd694d28ad9627348fead8daba2f3af69a7b822", "prompt_hash": "60915da13edc2fdb0ac39493692d6b4bca7a272316cb4ba4fc7224415110f39d", "target_hash": "e3f0058b5bb490873ff65be7f278942e7083ae9ea7786661177b21fedfe65f4c", "exact_match": 0.0}
{"doc_id": 775, "doc": {"question": "who unveiled the new coat of arms on 27 april 2000", "answer": ["South Africa"]}, "target": "['South Africa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who unveiled the new coat of arms on 27 april 2000?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Queen Elizabeth II"]], "filtered_resps": ["Queen Elizabeth II"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3761ce7fcd7e40fd6ccda17b41b7c20baf502a178c050bf9edce5df0e372e185", "prompt_hash": "471120c9061606e1a0d493d8dc4950bb07901c9bc554f046e5d039a9ac0bfca5", "target_hash": "731bce836454a2dcb3d053ee6199d19dde8696c43040c3067a8bc77f785d551d", "exact_match": 0.0}
{"doc_id": 776, "doc": {"question": "when was 13 reasons why released on netflix", "answer": ["March 31, 2017"]}, "target": "['March 31, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was 13 reasons why released on netflix?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" May 15"]], "filtered_resps": ["May 15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b26abec700f192cf08bd884ffaabbc5fdeb4524b1cfe6106239399d2b6c9192", "prompt_hash": "99a2a20f1404b352c54cd5286b8bdf4c3aabe58ca570a6033b3cbd0159f4f181", "target_hash": "f91078d4077ca42d15a6931f4310e4529cc3203a40173983006eac9da42c974c", "exact_match": 0.0}
{"doc_id": 777, "doc": {"question": "five tools of security management to overcome computer crime", "answer": ["Pre-evaluation", "operative planning", "post-evaluation", "implementation", "strategic planning"]}, "target": "['Pre-evaluation', 'operative planning', 'post-evaluation', 'implementation', 'strategic planning']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: five tools of security management to overcome computer crime?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" five tools of security management to overcome computer crime"]], "filtered_resps": ["five tools of security management to overcome computer crime"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9988e750945ffa7377add5f2e32c92760571b17d2c75661dab692f5b920a4307", "prompt_hash": "410194193a93a103b93c46d1968f760389fc4fdc9e5966fe12d59eab2dd8528e", "target_hash": "997b4b8a48bb126bf34ffe1843e3d629ee6daf2cd6ca9f28094b96237e85ebd9", "exact_match": 0.0}
{"doc_id": 778, "doc": {"question": "who plays the principal in santa clarita diet", "answer": ["Thomas Lennon"]}, "target": "['Thomas Lennon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the principal in santa clarita diet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Timothy Olyphant"]], "filtered_resps": ["Timothy Olyphant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8061c054f13355449a71a0c6dc067b8ed6c8f47e04c6285579571c0eb6dc32ee", "prompt_hash": "84c9082af42552c5b237d2d1f5427905d2adb9ab37b856580fac7c169c56cfe0", "target_hash": "f6a7fdc2a86880d91743e493404c32c4d8d90599e9259154d5ba47dd77687b56", "exact_match": 0.0}
{"doc_id": 779, "doc": {"question": "where is the country music hall of fame located", "answer": ["Nashville, Tennessee"]}, "target": "['Nashville, Tennessee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the country music hall of fame located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nashville"]], "filtered_resps": ["Nashville"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d25f0015463bf3d10ea00d24de30255808289d6968f13b74ce2a7cfc85da053", "prompt_hash": "033d1c4760cbb71459a571890f300aa817be4c1e57c4626a5d5ef927b69341e4", "target_hash": "b59444d531fdab926cf0d0f51072742a8bc03d66c2e09d7407d53db2d84ba040", "exact_match": 0.0}
{"doc_id": 780, "doc": {"question": "who has the world's largest standing army", "answer": ["China", "South Korea"]}, "target": "['China', 'South Korea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the world's largest standing army?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" China"]], "filtered_resps": ["China"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3f74b715a202450f718a4dc19d4571d60d96c2928aa16234c1fd59854366f855", "prompt_hash": "32de68fa52638627ca7afe5b491eb027800c69aee4df2223e2dec1146cbaed18", "target_hash": "cd34977430ec5dfa622631620c484dc2707f55c0cb110fd1504590c94faadaff", "exact_match": 1.0}
{"doc_id": 781, "doc": {"question": "how many hoops are used in a game of croquet", "answer": ["six", "eight"]}, "target": "['six', 'eight']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many hoops are used in a game of croquet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6352f571bd855889f9ff45d7fb7ec0c1cb12e59399b16369fa780cb51efef020", "prompt_hash": "824deba86e7b6fa79371d8cd91c5c5730ceb9782e856099c02fe1c2516135e94", "target_hash": "e852d9748f5deb30c577e7a1dd4ddd20898d1720080329a5fae3e624cfc9bce9", "exact_match": 0.0}
{"doc_id": 782, "doc": {"question": "who was the captain of the mayflower which brought the pilgrims to plymouth", "answer": ["Master Christopher Jones", "Christopher Jones"]}, "target": "['Master Christopher Jones', 'Christopher Jones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the captain of the mayflower which brought the pilgrims to plymouth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christopher Jones"]], "filtered_resps": ["Christopher Jones"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1b7e0adcdfa216444e6bdfd8bd00a3ab7533aaa0a73a8bf46fc61575580adc36", "prompt_hash": "79cac3b274fb7d3fd8e1a477fca5f85a5c62971ee7f1d89eb263e2c70fe91cf4", "target_hash": "b5724e04079cacb5eea7cf24bf425481cf4ad0ca8fd11de5afa4b3602061d5aa", "exact_match": 1.0}
{"doc_id": 783, "doc": {"question": "how long are the episodes in stranger things season 1", "answer": ["42–62 minutes"]}, "target": "['42–62 minutes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long are the episodes in stranger things season 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 45 minutes"]], "filtered_resps": ["45 minutes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "741de159ba4e03b55316ddfb50f8779ae767e1afcf07782547f71913457428b2", "prompt_hash": "8daa4ce1485fea52d97ea61a144129b12b14c0064fe7bebcad9cab3082fc3ab3", "target_hash": "92072f62b35bf289fa872045eb275d3ab7633fe8984c8cb1592cbdfa0dd2e45b", "exact_match": 0.0}
{"doc_id": 784, "doc": {"question": "what is the number of basic units in the international system of units", "answer": ["seven"]}, "target": "['seven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the number of basic units in the international system of units?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" seven"]], "filtered_resps": ["seven"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c38af872f9f75735da2a044aa3a1e35551f1e9308e55b21db9da09ece113632c", "prompt_hash": "7a0f7e72590da95e81951e57875feeda96422e5a8819e3bb77b503395b327280", "target_hash": "0df021bc2ee5f7a96ecc2aed6d1e8a6bbd0fc254176d8606fc5cf30f5137cd86", "exact_match": 1.0}
{"doc_id": 785, "doc": {"question": "who was elected senator of illinois in 1858", "answer": ["Stephen A. Douglas"]}, "target": "['Stephen A. Douglas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was elected senator of illinois in 1858?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stephen Douglas"]], "filtered_resps": ["Stephen Douglas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff4368b6affd5f1047509b0c0088ff9cef68a99bdc4a16eb7a31a5b67eb0efc7", "prompt_hash": "6682a31def5033a7a68f74817978aded2deae8e835816543dd73f8b0a7205d95", "target_hash": "26202f9e43c7a01289400bb670da9e08fb03092774d828767d31d91ac5c11c0a", "exact_match": 0.0}
{"doc_id": 786, "doc": {"question": "what is the number of cities in texas", "answer": ["961"]}, "target": "['961']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the number of cities in texas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 297"]], "filtered_resps": ["297"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a52d70315388008f27c704c9a9bf421f21bcc4a991c4e0220c17a423fc7772c8", "prompt_hash": "41ec01b3236340a69d070a793c3365a7d609d20f5490acf8d2a5d3bf9ecd72ab", "target_hash": "fd6c9d050aab32ea05b16fed8cd45b0f787bb10603f0c158864134b79d9be455", "exact_match": 0.0}
{"doc_id": 787, "doc": {"question": "where was the original planet of the apes filmed at", "answer": ["Utah", "in northern Arizona", "California", "Arizona"]}, "target": "['Utah', 'in northern Arizona', 'California', 'Arizona']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the original planet of the apes filmed at?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Australia"]], "filtered_resps": ["Australia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cbd1b96294c21a485d3df966f6385e2d61f5cc5e4812ef34b3fccb59d763684b", "prompt_hash": "32d347df3436eebd3d52302cd93c40ed8be1db6402f700ca79f1c26cce150c84", "target_hash": "1df1240849f5932fbbbc0aa64c795acfeed13e9139f849556a565ef7c93355fe", "exact_match": 0.0}
{"doc_id": 788, "doc": {"question": "who sings find out who your friends are with tracy lawrence", "answer": ["Kenny Chesney", "Tim McGraw and Kenny Chesney", "Tim McGraw"]}, "target": "['Kenny Chesney', 'Tim McGraw and Kenny Chesney', 'Tim McGraw']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings find out who your friends are with tracy lawrence?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tracy Lawrence"]], "filtered_resps": ["Tracy Lawrence"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0c388db81e38e75f3019efb6e548cac98b1b2779ecddbb0b0cd255b86292593f", "prompt_hash": "e5c9b4aac56ee93552dfe64dd3ee9eaa1f64ffcd638e568d186ac70a600e0211", "target_hash": "5050c937271196e14b6398d8a731f3d43217d21fc29215d17017708b943a86d8", "exact_match": 0.0}
{"doc_id": 789, "doc": {"question": "what is the 180 degree line of longitude called", "answer": ["180th meridian or antimeridian", "180th meridian", "antimeridian"]}, "target": "['180th meridian or antimeridian', '180th meridian', 'antimeridian']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the 180 degree line of longitude called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" International Date Line"]], "filtered_resps": ["International Date Line"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4a3d5d2f92aa69c47ba60b3c79baf0e3e4d6c0c8a7eb8b2d7559ce867c698f06", "prompt_hash": "a3dee63802e35cda1e6f2dd9c2d62e2ade2e96650cf81208d6fa8d4641c6675f", "target_hash": "5e6c9a58cd36e6bab35ebfb7a1b724ee9b95da3084fcb166739cad61a6364c12", "exact_match": 0.0}
{"doc_id": 790, "doc": {"question": "who was the 19th c. artist who was both painter and graphic designer", "answer": ["René Georges Hermann-Paul"]}, "target": "['René Georges Hermann-Paul']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the 19th c. artist who was both painter and graphic designer?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gustave Doré"]], "filtered_resps": ["Gustave Doré"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2b4c8ab2ba47792dca25fe90cd5fc23d861ce50723c4f8929004815a0e1feea", "prompt_hash": "e6f5ea3faad6741c30138c3dfefab15cd3a88b74b10081ffb092986b5972fcc1", "target_hash": "2aebe26c093dbd23063ca2a8737528f637892257cce95005d509b60e7b392bc1", "exact_match": 0.0}
{"doc_id": 791, "doc": {"question": "when was the nobel prize for literature first awarded", "answer": ["1901"]}, "target": "['1901']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the nobel prize for literature first awarded?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1901"]], "filtered_resps": ["1901"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "56e0bfd14a225d7e41842dba89006bbe121cbfa75a06328da8b4f44809716be6", "prompt_hash": "13c2ae67246ae6cf4cf6dcae9e1633130326f50df3129c483286b7f4f4510225", "target_hash": "c6193553236bb50f0ac2512e85a08ec906fb9a91f8b824604f528f5cd1e0846b", "exact_match": 1.0}
{"doc_id": 792, "doc": {"question": "who plays the voice of the hormone monster", "answer": ["Nick Kroll"]}, "target": "['Nick Kroll']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the voice of the hormone monster?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John DiMaggio"]], "filtered_resps": ["John DiMaggio"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1fd91c908a6ab7df67e5327369cd84a2d8040ec95bc5cdf351a20eaa9def7f7d", "prompt_hash": "6738ebe30cd4d2fd8784b26cfabe77cf75981f9f5ea33c9f2296f9d7fb6ea616", "target_hash": "c7258cbd2a2a7839a47e6522d17896d45e5526871223b96181b201d2e61ac47f", "exact_match": 0.0}
{"doc_id": 793, "doc": {"question": "when did the first movie of harry potter come out", "answer": ["16 November 2001", "2001"]}, "target": "['16 November 2001', '2001']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first movie of harry potter come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5cfca46e942411b15b9e62d0b011918931f4c5a5f2e2f7c64fba662702d42869", "prompt_hash": "cb91f6f88716138278260c2ba009e7c6c4e06c5743d4a1f9490eb7e62c56203c", "target_hash": "e1f06f5a24df166f34501920dbef797a71255158fd20f0e0aafff11afa739eff", "exact_match": 0.0}
{"doc_id": 794, "doc": {"question": "where do most star formation occur in the milky way today", "answer": ["molecular clouds in interstellar space"]}, "target": "['molecular clouds in interstellar space']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do most star formation occur in the milky way today?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the disk"]], "filtered_resps": ["the disk"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ee561f7a4ac4d68e0235692f377ec3811df553f31b79cbf5e2729b097f9f34b", "prompt_hash": "b96b99ea5f3d209b89f30c74b3397d96a7e8301e8017943bd2772d258438dcb1", "target_hash": "07368597f21b5a45c925161426f63996a68d80ff972faed4417e73c71c2f5cb6", "exact_match": 0.0}
{"doc_id": 795, "doc": {"question": "who won the oscar over wolf of wall street", "answer": ["Terence Winter", "Leonardo DiCaprio", "Martin Scorsese", "Jonah Hill"]}, "target": "['Terence Winter', 'Leonardo DiCaprio', 'Martin Scorsese', 'Jonah Hill']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the oscar over wolf of wall street?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" American Hustle"]], "filtered_resps": ["American Hustle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7a77cfec57935f45fe2d3f14da765612627c9b75ebf11183afb6b9460e9012b", "prompt_hash": "8e9abe2e6840f91984e65d22336945002beb1fe37e0bb4bde20600e6eb9969e4", "target_hash": "25f83c1ad858c30a5540aa7f76fa02b7f1d3fac51b54c06d91360dbd4144ab20", "exact_match": 0.0}
{"doc_id": 796, "doc": {"question": "total number of mna in pakistan national assembly", "answer": ["332 members", "332"]}, "target": "['332 members', '332']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: total number of mna in pakistan national assembly?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 342"]], "filtered_resps": ["342"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e99fdb6d237f4ec6d2bbc3aac15c9f349fca2a5905487f354f82d174decdcba8", "prompt_hash": "8e998725cd766a57409f557b9a7e613be2c6571dbb1d929692c166a63b487b0d", "target_hash": "aa55d7d1569ec74a31ed5dced5184ebf579b278ea3a8f5c6ffdf55cca8462b59", "exact_match": 0.0}
{"doc_id": 797, "doc": {"question": "where does saying bob's your uncle come from", "answer": ["unknown origin"]}, "target": "['unknown origin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does saying bob's your uncle come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" British English"]], "filtered_resps": ["British English"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a12bce14ec789b9148876118f5e4124f1792b07eaedc17c0daa6d6c612984493", "prompt_hash": "6add22f5e250c1364159024d2f41f0cd05b2d3a2000d7179f468b6e196da60b1", "target_hash": "a2fb48999f5179aa3b452cfb740e3468b2bc34a43dcc43bf04be3451142a38e7", "exact_match": 0.0}
{"doc_id": 798, "doc": {"question": "what type of joint connects the incus with the malleus", "answer": ["synovial", "a small synovial joint", "synovial joint", "incudomallear joint"]}, "target": "['synovial', 'a small synovial joint', 'synovial joint', 'incudomallear joint']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of joint connects the incus with the malleus?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the articulation of the incus and malleus is a saddle joint"]], "filtered_resps": ["the articulation of the incus and malleus is a saddle joint"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "674fbccdcae63da1233d3c2735fa9a625deaa31970c5f9b5c1d82bd7667c3f15", "prompt_hash": "6257788c7f05020f388d84a73465324256175749a2e6b5a3ef5bde585eb03ad7", "target_hash": "9885b2ab79d3b7cf4fbcf9712503f015d6ab57e27b03628c69bce6910750e8e4", "exact_match": 0.0}
{"doc_id": 799, "doc": {"question": "where does the highway of heroes start and end", "answer": ["Glen Miller Road in Trenton"]}, "target": "['Glen Miller Road in Trenton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the highway of heroes start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Baghdad"]], "filtered_resps": ["Baghdad"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f070a5582b4aa0e9040c5a600f64abc9573ebaebbfda28d8b1fe398a2788e3c8", "prompt_hash": "65412f9c4a07c46ce9864e448fe2d8e1dd6d691d0862b26d61e1f4c83a27cc28", "target_hash": "08eb15fbf4cc6104e41ba9475944485a77fe7fc5f8f23dc3d6c33b9f4798aea7", "exact_match": 0.0}
{"doc_id": 800, "doc": {"question": "where are the 10 plagues found in the bible", "answer": ["Exodus", "Book of Exodus"]}, "target": "['Exodus', 'Book of Exodus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the 10 plagues found in the bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Exodus"]], "filtered_resps": ["Exodus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5893c51e3f1bbf2849fc967e15d64e2a11301028f38011886eb9d95d2f8af2f0", "prompt_hash": "c12b5f48b62ef4ab42ecf2ed7c08f3fdfc5257bbd0ae306538aa6c35e73f2aa9", "target_hash": "70179874bf857ca37b19bdec2a36490fdd0ca922bd5edb9f646570e26e0445c5", "exact_match": 1.0}
{"doc_id": 801, "doc": {"question": "who played tibbs on in the heat of the night", "answer": ["Howard Ellsworth Rollins Jr."]}, "target": "['Howard Ellsworth Rollins Jr.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played tibbs on in the heat of the night?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rod Steiger"]], "filtered_resps": ["Rod Steiger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "11488b6f5d7aec295817f781625293fadbd220e494c495df34812bc7a9f5b7ef", "prompt_hash": "0e29ad453b711eebedc0ac812153929e4b46c0aff7016c758567827f52167cf2", "target_hash": "a36110d86485ea33abe4d1d2927486d404ec269451d308f8bd79a503961ee013", "exact_match": 0.0}
{"doc_id": 802, "doc": {"question": "who has sold more albums kelly or carrie", "answer": ["Carrie", "Carrie Underwood"]}, "target": "['Carrie', 'Carrie Underwood']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has sold more albums kelly or carrie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carrie Underwood"]], "filtered_resps": ["Carrie Underwood"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d1d0e0f91d9961687e5581db6760a7a26c4e33ea93212904e764c436eac49fa7", "prompt_hash": "ba4f02ff173fd0cd436ba21612a2d95fe19fba0c8f8b122daf54808b6b2d256a", "target_hash": "2ad437d5cf65e7eb72a3b7c6d40e9f0d9775d2482d65548eb7b084b5c0cdcad1", "exact_match": 1.0}
{"doc_id": 803, "doc": {"question": "theme of the song roar by katy perry", "answer": ["self-empowerment", "standing up for oneself"]}, "target": "['self-empowerment', 'standing up for oneself']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: theme of the song roar by katy perry?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" empowerment"]], "filtered_resps": ["empowerment"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cde631af8a88f07154b4d5a95637d1287c9f9c67fcc25545211b82e1735f2424", "prompt_hash": "bfc1c0f1a3ed68275defd20690d27ac9a71c99109a60a096523d1b75cf9727bb", "target_hash": "ede2c8f5337c19efca56f2dac7e4ad9a0d5e1212e367284985d97c7787a3e1b0", "exact_match": 0.0}
{"doc_id": 804, "doc": {"question": "where did the name huey helicopter come from", "answer": ["originally designated HU-1"]}, "target": "['originally designated HU-1']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the name huey helicopter come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Huey was the nickname of President Lyndon B"]], "filtered_resps": ["Huey was the nickname of President Lyndon B"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eac22b83cdc6ef06dbbaba09d6cb00949a539844fdd60cc35bb6e641898eb473", "prompt_hash": "84901a504bd5aa716fd6076c50d037718ef9f894d7a0e977eeec0f865ff5124f", "target_hash": "2a2ad04bd544a10b5cccdc12511c04e8e8e50e522d53e152f7f7fadcb495ecad", "exact_match": 0.0}
{"doc_id": 805, "doc": {"question": "who plays charles on have and have nots", "answer": ["Nick Sager", "Nick Sagar"]}, "target": "['Nick Sager', 'Nick Sagar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays charles on have and have nots?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David James Elliott"]], "filtered_resps": ["David James Elliott"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ca38f4a070addbba54ce60e9b8a53cf586f0b26cc77d450b59f9876ca141351", "prompt_hash": "5a21ed3268936fc84e264bc25a45f6c27f25d5c992e3062559ce61d7fd265716", "target_hash": "004697615b6db1a70e20e902af4badbbed9450cc679cd2d46eb4fcd3f55835bb", "exact_match": 0.0}
{"doc_id": 806, "doc": {"question": "who has the most followers on the instagram", "answer": ["Instagram's own account", "Selena Gomez"]}, "target": "[\"Instagram's own account\", 'Selena Gomez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most followers on the instagram?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cristiano Ronaldo"]], "filtered_resps": ["Cristiano Ronaldo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3ad7e8b7207c4f10bc64a362e7ee870acfe5ad03dbb5cc45e11fb56d0d2607e5", "prompt_hash": "edb405a77458c61c51b0c07a50e638ef5e724c236f8974ec867bf329d97ffabd", "target_hash": "9d3a8ba1a3c3836c9dd1e2d79a0fcec0941d2b82d229cb8dc80ce289a6407395", "exact_match": 0.0}
{"doc_id": 807, "doc": {"question": "who is the current director of the us mint", "answer": ["David J. Ryder", "David Motl"]}, "target": "['David J. Ryder', 'David Motl']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the current director of the us mint?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David J"]], "filtered_resps": ["David J"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db31d17b83eb3f1fee8cec2927ba86eaee71e767ddafabad4b0f21b01f6a5a51", "prompt_hash": "f67a7954f224b04d42d8ceee8c2a95f967e2a9889d077be4255b995e14eb87f9", "target_hash": "0691916444f5a877a04a04094079593f1c45e75b87632201de555af7612af8ee", "exact_match": 0.0}
{"doc_id": 808, "doc": {"question": "when is the 5th round fa cup played", "answer": ["29 January 2018", "February 2018", "16 February 2018"]}, "target": "['29 January 2018', 'February 2018', '16 February 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the 5th round fa cup played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11 February"]], "filtered_resps": ["11 February"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "21c14871958782702beb9643bbe2ae9e7967daf8b8f6e26057259f002f2d6170", "prompt_hash": "afe7c52052cc191917f399b3a942b8746713355c4ea319b3cb1fd7d6e82b8217", "target_hash": "26232a0b4786c17f329f336a932db1a555389cc144605c5ba09dc5e06c0ec4e7", "exact_match": 0.0}
{"doc_id": 809, "doc": {"question": "when were the beatles inducted into the rock and roll hall of fame", "answer": ["1988"]}, "target": "['1988']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when were the beatles inducted into the rock and roll hall of fame?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1994"]], "filtered_resps": ["1994"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6273902ca69eeca0adbedd86a2482c3af2e687257727985d657ad5b93a6c5d37", "prompt_hash": "dd676fb4bfb7bfc1f683acd64e8d9a602d26c04c4d53bb1a0b0cd27e3dd447b7", "target_hash": "e071edb0a03ed21345f1ef6f9c0c506a8efd6860713d87f2bf676c9c0677b897", "exact_match": 0.0}
{"doc_id": 810, "doc": {"question": "what is the name of the gap between two front teeth", "answer": ["diastema (plural diastemata)", "diastema"]}, "target": "['diastema (plural diastemata)', 'diastema']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the gap between two front teeth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" diastema"]], "filtered_resps": ["diastema"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b785e5dbcd4c7f252337de2045bd19f5b715974bc329819b4825c7d41dfa6f6d", "prompt_hash": "5d0af7e21c39ccd4e3cce45d4587ae8fd1228361bb34d37a6d627de871bfcf67", "target_hash": "3e706670115a2f83e424616e7d6185f4effb22e5d9c86b6216d8f2a676764861", "exact_match": 1.0}
{"doc_id": 811, "doc": {"question": "when did the nfl adopt a salary cap", "answer": ["the 1994 season", "for the 1994 season", "1994", "1994 season"]}, "target": "['the 1994 season', 'for the 1994 season', '1994', '1994 season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the nfl adopt a salary cap?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1994"]], "filtered_resps": ["1994"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "73f76496c56c5e83f95301d3d1a37dc456cc757105382fa1f8c91fe64190705f", "prompt_hash": "6c8f5861fc69dec079a8ff581acd9110dbc4a5a1a577d0755f3e9530d7ef3cbc", "target_hash": "230e0908057d4e95a13667deaf7889445f52d52ac3c24a0d5f6a3c334d2b84cc", "exact_match": 1.0}
{"doc_id": 812, "doc": {"question": "when did the cubs won a world series", "answer": ["1908", "1907", "2016"]}, "target": "['1908', '1907', '2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the cubs won a world series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2016"]], "filtered_resps": ["2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ed0c9e9c0f5bddc8a4eb0d7ce5be2b9c18d9cc42b67a78b8facc30ffe628147", "prompt_hash": "ff04789cca8a2373092dbeed6fa759e03c27fb57b1a4526908a9a73996227201", "target_hash": "949f397a2364066b717c0aca5d9f7b9131bec3ca7244ee60b4998eda965bf03d", "exact_match": 1.0}
{"doc_id": 813, "doc": {"question": "when did the ottoman empire surrender in ww1", "answer": ["1918", "30 October 1918"]}, "target": "['1918', '30 October 1918']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the ottoman empire surrender in ww1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1918"]], "filtered_resps": ["1918"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4aeb3a033da0cad8bda1814be5c75f72b11192093d643b2c9d9566fb9097348c", "prompt_hash": "df2a09e5d38ad69c60ac0f9e79d8abbee8af38e26c8fdd3a760cd4532bb08217", "target_hash": "fe095e6c20ec115c88544d928c67d9135ce72b1b5e44b1318f8bde0acb02da38", "exact_match": 1.0}
{"doc_id": 814, "doc": {"question": "what color is the cue ball in pool", "answer": ["white"]}, "target": "['white']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what color is the cue ball in pool?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" white"]], "filtered_resps": ["white"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d40c0b7714a88445042b398e6c6c5259c4ebdbd556c0ca8e54d8380e523e5f4a", "prompt_hash": "82d3a3822ab2135b20d22b80cc8b8b10dc296cf42abae89ff2e4261b504c1060", "target_hash": "1274cbca9f7e385c44ace3fb253c154857d1f9135dedff0988f1cdba0b42a5e4", "exact_match": 1.0}
{"doc_id": 815, "doc": {"question": "when did the 5 day work week begin", "answer": ["1908", "In 1908", "1940"]}, "target": "['1908', 'In 1908', '1940']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the 5 day work week begin?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1930"]], "filtered_resps": ["1930"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5fe7eea4e6f8f2e89e7555278355252f16ca789afa5bbc9d9e31a8ee655f6f80", "prompt_hash": "fa2d340b216a4d987790fcf486d950d2617d47b4ffcc160aef750e48c3c4ea83", "target_hash": "b7d18fd3dce520a2b861eff0edad45546acf24d19c1ab0eb7af0433e27790cbb", "exact_match": 0.0}
{"doc_id": 816, "doc": {"question": "when does april fools day end at noon", "answer": ["April 1st"]}, "target": "['April 1st']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does april fools day end at noon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" noon"]], "filtered_resps": ["noon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a15213eac574ed7ce0f43abe46d7b25a09248940e34290bc5ff1fe0e8d967247", "prompt_hash": "6cf24107e5efe9815c245d95408991c1ea1ee920a046b4fba347ea4fae8b3674", "target_hash": "a68a108cb78f6151a8095d911f3426c57d0a394554d220bdef3297532fc57e25", "exact_match": 0.0}
{"doc_id": 817, "doc": {"question": "what is the rank of pakistan in population", "answer": ["5"]}, "target": "['5']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the rank of pakistan in population?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 5"]], "filtered_resps": ["5"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "243e3a32462c59416d2ddafcb965b35088deceaabf4ef19d36a086b2605981bc", "prompt_hash": "66368ca25c1297e20c7a32a634154255c8809e1f6165cf025c33467761191c6f", "target_hash": "494a843c2b5d43b794ff50c14c5647bf4ff556a365c58f564af6c235a53602e3", "exact_match": 1.0}
{"doc_id": 818, "doc": {"question": "who grows the most coffee in the world", "answer": ["Brazil"]}, "target": "['Brazil']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who grows the most coffee in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brazil"]], "filtered_resps": ["Brazil"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2b0fdc22f240e3c52acc957c5d7f375c40a9339e15ed444f1e35955f0283ad8", "prompt_hash": "7e6c588ddca1cc1871693eb549cfa91427273beb97d2d851fbad942e70ee0375", "target_hash": "8e0dd521c47fc93e5e481a447cf3601e26be822f44a79cadcf282a23464ce58b", "exact_match": 1.0}
{"doc_id": 819, "doc": {"question": "who plays peter in what we do in the shadows", "answer": ["Ben Fransham"]}, "target": "['Ben Fransham']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays peter in what we do in the shadows?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Natasia Demetriou"]], "filtered_resps": ["Natasia Demetriou"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ce3e9e371d20050b0035c3303fec113e41f151bc212cfe3c96cbf23d6cc3453", "prompt_hash": "080ba7f94ffe0954427dfa7ad46df272db520a065c7eaf96446411881f3b1497", "target_hash": "5ff7acd82c8de6d13a37492aa43356878ae6996aa34e08a5d7b22abd56f29e04", "exact_match": 0.0}
{"doc_id": 820, "doc": {"question": "who plays connor's mom in madeas christmas", "answer": ["Kathy Najimy"]}, "target": "['Kathy Najimy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays connor's mom in madeas christmas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tamela Mann"]], "filtered_resps": ["Tamela Mann"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44e2febbcf0cfd9fffece854f2a533b4b2fff3c15c5f981145adde908022ada7", "prompt_hash": "8975b89db25ed98548c17cb7d269e6a4818af096ca8d363db3350fa9d22f4c6b", "target_hash": "fa634335e1fc7a676713a4475d08ee6248e807531ac9f87f0d9ed6e615cfb3d1", "exact_match": 0.0}
{"doc_id": 821, "doc": {"question": "who died in the plane crash greys anatomy", "answer": ["Dr. Lexie Grey", "Lexie"]}, "target": "['Dr. Lexie Grey', 'Lexie']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who died in the plane crash greys anatomy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Derek Shepherd"]], "filtered_resps": ["Derek Shepherd"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "107bad2d0acbed7c8eba713fe104ea40407286b6667595ae6000c76d3c18133b", "prompt_hash": "0bc19e2ce510d59869abb08ace5fb9ae55d2aff46a463f3b340d4d8201e56875", "target_hash": "7004e9791d3256bb6eb84ef5e45fa0ef9a8bc96ddc8fe859e66eb5a5c04cc4ad", "exact_match": 0.0}
{"doc_id": 822, "doc": {"question": "tumhi ho bandhu sakha tumhi cast real name", "answer": ["Aalika Sheikh", "Vineet Raina", "Mahesh Tanwar", "Chandni Bhagwanani", "Lavina Tandon", "Rehaan Khan", "Madhu Malti Kapoor", "Tushar Dalvi", "Neha Dangal", "Neeraj Goswani", "Kabeer K", "Sreejita De", "Dimple Jhangiani", "Aishwarya Narkar", "Neil Bhatt", "Ravjeet Singh"]}, "target": "['Aalika Sheikh', 'Vineet Raina', 'Mahesh Tanwar', 'Chandni Bhagwanani', 'Lavina Tandon', 'Rehaan Khan', 'Madhu Malti Kapoor', 'Tushar Dalvi', 'Neha Dangal', 'Neeraj Goswani', 'Kabeer K', 'Sreejita De', 'Dimple Jhangiani', 'Aishwarya Narkar', 'Neil Bhatt', 'Ravjeet Singh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: tumhi ho bandhu sakha tumhi cast real name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rajendra Prasad Yadav"]], "filtered_resps": ["Rajendra Prasad Yadav"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bc5ed73e72ec3096e551a6743e978e8a8e6295a624ce1380193b487cb715013d", "prompt_hash": "7517aebf25337127da444335b78d8abadb9e5557861603a839ef0dc107f6a552", "target_hash": "6b4285b6cda3c79153d007f0969f231f3f31c482b6982210a314fe46099b824d", "exact_match": 0.0}
{"doc_id": 823, "doc": {"question": "who wrote there's a guy works down the chip shop lyrics", "answer": ["Philip Rambow", "Kirsty MacColl/Philip Rambow", "Kirsty MacColl"]}, "target": "['Philip Rambow', 'Kirsty MacColl/Philip Rambow', 'Kirsty MacColl']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote there's a guy works down the chip shop lyrics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Steve Harley"]], "filtered_resps": ["Steve Harley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91436e072310bbe92dd27e684c2d761c20e98a70338604763edad4d8cef03938", "prompt_hash": "d1604f9c074699e9b58e26644b7c66b5e3271237bd2fb8039507a1266a9d447a", "target_hash": "d75f80dc0d89220f8c38655ca84d211bc1d039e8953cd0089a9e0e6b3e57d174", "exact_match": 0.0}
{"doc_id": 824, "doc": {"question": "how long prime minister stay in office canada", "answer": ["four years"]}, "target": "['four years']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long prime minister stay in office canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" five years"]], "filtered_resps": ["five years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e5361b3c80cde544a69893ed14dcd9ac2f202d15512720b6f5e37a29fb749b98", "prompt_hash": "418c70b468be4e52ef1d19142b7ecd66d3f9aae0d2095d5fdf80617ee62ec1ea", "target_hash": "e8dc84d64402225d8c1e46feeaab6c1b249d5d9ec735bdf243c73dcfdf212585", "exact_match": 0.0}
{"doc_id": 825, "doc": {"question": "what is the movie about six degrees of separation", "answer": ["Six Degrees of Separation"]}, "target": "['Six Degrees of Separation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the movie about six degrees of separation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Six Degrees of Separation"]], "filtered_resps": ["Six Degrees of Separation"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3dbc496ce8ffd2ceea8e632093f2b347ab4f23cff51c9ae3f497f0c99bf0dd61", "prompt_hash": "b58c7153d492ccc11eac9b242d220f9ded162e30e612b7a50895e64872c1fb7f", "target_hash": "09a996741271081b12c2dd9f8f7f261559e2dff575922bff494ac30f9d3d8dd3", "exact_match": 1.0}
{"doc_id": 826, "doc": {"question": "who wrote the text for jeanie with the light brown hair", "answer": ["Stephen Foster"]}, "target": "['Stephen Foster']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the text for jeanie with the light brown hair?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Neil Diamond"]], "filtered_resps": ["Neil Diamond"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b59768cb61de047f8bbef6e22ab0f1c9b1caa733e04b142f555d41b6d0a62c9", "prompt_hash": "316495e4b2b81cc6b193d2239088f92d2dfedd5f37e44a1792afceaa5aa6e9b8", "target_hash": "7130b5428a371d3f0040f9c10c898bcae28551d64b191ef61079493fc27bd6af", "exact_match": 0.0}
{"doc_id": 827, "doc": {"question": "who played cosette in les miserables on broadway", "answer": ["Samantha Hill", "Judy Kuhn", "Ali Ewoldt"]}, "target": "['Samantha Hill', 'Judy Kuhn', 'Ali Ewoldt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played cosette in les miserables on broadway?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lea Salonga"]], "filtered_resps": ["Lea Salonga"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "813538017fc35a82da0a4952f015423a7569c3d6ff44e3ffc9f66786c140abef", "prompt_hash": "b4b7b9f13ed07531f0092d0f10ce9d996ff866b37c1d534355dbf755b692dc33", "target_hash": "9c1989835a7745e45b46d2b573c4f6a03d439b5af69fbb65b46ac2854d32658c", "exact_match": 0.0}
{"doc_id": 828, "doc": {"question": "who got the most rings in the nba", "answer": ["Bill Russell"]}, "target": "['Bill Russell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who got the most rings in the nba?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bill Russell"]], "filtered_resps": ["Bill Russell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "67d3dbaa0302b05defb666f4d95c2ff3142bae3e6d9b35c362b82baee92cc602", "prompt_hash": "41b9772bb145436c833733776e00d9b1b3500a95a92fb3190719eddf2116d431", "target_hash": "0d1cdb5aeffc7886ddadbf1d87c7d73e851707fac3faa67ab1fcf774f504b031", "exact_match": 1.0}
{"doc_id": 829, "doc": {"question": "who sings the skye boat song on outlander", "answer": ["Bear McCreary", "Raya Yarbrough"]}, "target": "['Bear McCreary', 'Raya Yarbrough']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the skye boat song on outlander?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rachel Hunter"]], "filtered_resps": ["Rachel Hunter"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "331fa4b57464b6d2fda341792cfcda278496026a875cb928f4a54d5c23d0ab49", "prompt_hash": "dd1ca4184030fc474a404ad803403287ef7e5f2bb5e6786b6e6bf2cb42834c4f", "target_hash": "746b86639a519289f04d49e3416a6767106b0b1da94624a94d8f1282a967856e", "exact_match": 0.0}
{"doc_id": 830, "doc": {"question": "when did movies start coming out in color", "answer": ["introduced around 1940", "1895"]}, "target": "['introduced around 1940', '1895']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did movies start coming out in color?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1935"]], "filtered_resps": ["1935"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0ec9cf40a26914c3f77003e8da8be9969c7221fcecf6975c6a5bc47d4c65acf0", "prompt_hash": "c61fc242ae8571554702efb2885dfa8378fbdecf5ef3f891c6bdb3ab2c88054e", "target_hash": "bc365f52799c3c4b93d83f0b893ba7d49e1ccc7089f61758e7b8169caeedbbb8", "exact_match": 0.0}
{"doc_id": 831, "doc": {"question": "who is the girl in the stone sour video say you'll haunt me", "answer": ["Joanna Moskawa"]}, "target": "['Joanna Moskawa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the girl in the stone sour video say you'll haunt me?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Megan Fox"]], "filtered_resps": ["Megan Fox"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4d646b0f541d686dd1cd6024d83edb4916d75571c8e5f98cefa20be53d2dc0a", "prompt_hash": "926565e25ca2bad2f0bc2231e856b5d09b60ac10c713f8a8e09d012789e88219", "target_hash": "bdc9062e67c463104d1ee923329b1b015a4e075b0b4eb348e033dffc352318d4", "exact_match": 0.0}
{"doc_id": 832, "doc": {"question": "where was the summer olympics held in 2012", "answer": ["London", "London, United Kingdom"]}, "target": "['London', 'London, United Kingdom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the summer olympics held in 2012?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" London"]], "filtered_resps": ["London"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dcc0853d49bd2f6433f60c8bf5b6c471decff65fbd66b5ae4780d6c41a9b9a28", "prompt_hash": "35342e1a61fb88062f8cff424d9cfe810aa3cd97cca3da4315149ad0cf7e7bd4", "target_hash": "90181a05a73aeb49212aa019c22a74de41c09c6b3797452f805b5425da8dc177", "exact_match": 1.0}
{"doc_id": 833, "doc": {"question": "who did the steelers lose to in the playoffs last year", "answer": ["New England Patriots"]}, "target": "['New England Patriots']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the steelers lose to in the playoffs last year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pittsburgh Steelers"]], "filtered_resps": ["Pittsburgh Steelers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d3dbd66bcaaae8d4e1ecb405f9eb3c97bb182b12f94a77a0c44d5262ab7588a", "prompt_hash": "f68d12477ace9ad9c29ba1a28cb26799b93c2ab982864553529fabe663458d20", "target_hash": "1b3aea9d6dea18aff5895d0c0931b3473ecb8a2da7c1127348320da079b1e8db", "exact_match": 0.0}
{"doc_id": 834, "doc": {"question": "who wrote the song rainy days and mondays", "answer": ["Paul Williams", "Roger Nichols", "Roger Nichols and Paul Williams"]}, "target": "['Paul Williams', 'Roger Nichols', 'Roger Nichols and Paul Williams']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song rainy days and mondays?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nick Heyward"]], "filtered_resps": ["Nick Heyward"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "983c101a646ec7a97b78c411230f80d43460a3116aa7eefcbe004d1a95cf6cfb", "prompt_hash": "692cabb984da9017babccd5cdbc4efbba221b08127f619887ac93922df3be19f", "target_hash": "b4f94c012610cdc9246e36ea8abc7f3ab707e5af67ea91f9b3bba3498ad15583", "exact_match": 0.0}
{"doc_id": 835, "doc": {"question": "who is known as the philippine queen of bossa nova", "answer": ["Sitti Navarro"]}, "target": "['Sitti Navarro']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is known as the philippine queen of bossa nova?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lea Salonga"]], "filtered_resps": ["Lea Salonga"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fdfafb5966367f25136f5a44030feae830c9057c9663ee6d525554bc4203c30c", "prompt_hash": "b4d5b39afca2f2d8ab0c126f15963944a41a4604bbb79b93cd82f92732a01e36", "target_hash": "a11f5d3ed081335c4a5fec02a084a811f9e53423382ff5ecb3f88b473fe20cbb", "exact_match": 0.0}
{"doc_id": 836, "doc": {"question": "where was the outlaw of josey wales filmed", "answer": ["Kanab, Utah", "Lake Powell", "Wyoming", "Utah", "Oroville, California", "Arizona"]}, "target": "['Kanab, Utah', 'Lake Powell', 'Wyoming', 'Utah', 'Oroville, California', 'Arizona']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the outlaw of josey wales filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Texas"]], "filtered_resps": ["Texas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44e3bb643931b03393dcf832bfdf06d7aacd53cfa883d2b31e60e85588ce3a0c", "prompt_hash": "abf081cb2d2eadba41a245daea70d83eb81c77bb50a5eeeaa4dce6de1e8e7a6e", "target_hash": "40d8c1002162da005c193c629515b12cf9e25e05ed7ff196ed5ef2a22d785f6a", "exact_match": 0.0}
{"doc_id": 837, "doc": {"question": "when's the last time philly won the superbowl", "answer": ["2017"]}, "target": "['2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when's the last time philly won the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1982"]], "filtered_resps": ["1982"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "86a2140d5f21389cebf6c892fe7327c82b6af8e67ba756fb21324ee86ba4806c", "prompt_hash": "6923ce88f8db1b10466e48652b6abb84d889a19e2d3be93b06247f7c890058fa", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "exact_match": 0.0}
{"doc_id": 838, "doc": {"question": "an open-source clone of unix created in the early 1990s", "answer": ["GNU/Linux", "Linux"]}, "target": "['GNU/Linux', 'Linux']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: an open-source clone of unix created in the early 1990s?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Linux"]], "filtered_resps": ["Linux"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "266124c37f59dd6cb11d150b5039bfcfb8d5ed700a82b0bea9404b7edbdb1ab8", "prompt_hash": "da33771526102ca579f9f06e8e647983495012e7d889ebeafdec0aa4b4fc470c", "target_hash": "0ccb4f8efbdcb7b93c69dc1dcdb7e4753ed673b2524d875f9fbb19b7b69279c3", "exact_match": 1.0}
{"doc_id": 839, "doc": {"question": "how many goals did mikel score for chelsea", "answer": ["6", "1"]}, "target": "['6', '1']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many goals did mikel score for chelsea?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77068ddc3341370b5a05dcaac29baeeb7c9150ddf495ea48a8b0d4ba4c7efa0f", "prompt_hash": "02576a6930cfbef34b0e190b36e2a03e672a0eed7442a6c0a37643051ae06145", "target_hash": "f92a74077ae27fdf848fdc2021bdd6cebbf464cefa4af5b282448088f19f9280", "exact_match": 1.0}
{"doc_id": 840, "doc": {"question": "who played ben stone son on law and order", "answer": ["Michael Moriarty"]}, "target": "['Michael Moriarty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played ben stone son on law and order?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Raymond-James"]], "filtered_resps": ["Michael Raymond-James"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "634a02106e739dc3aaafa500edf894c22971d2a60f61d5274aa601f76827e3b0", "prompt_hash": "b938f7d36929bc2ef6d14a0ffbbd7d54418790c1368ea92a4cd76bd704276991", "target_hash": "f676335b68633c8899b24304fd04e28a79d3312ecbeb012a55649f5055e005c9", "exact_match": 0.0}
{"doc_id": 841, "doc": {"question": "when was the last time university of michigan won a national championship", "answer": ["1997"]}, "target": "['1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time university of michigan won a national championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c612660847045ef3b17c6a30b23c74d4d9c64fecc0a79435924fe268bc910ef7", "prompt_hash": "44ded48c177218e2f90b4427985a1bf0328371d810545b629babbd572976a49b", "target_hash": "982df96f59043cb182196071792c2904ff376f72b15cb9b414721d5b74a296b0", "exact_match": 1.0}
{"doc_id": 842, "doc": {"question": "who sang nice day for a white wedding", "answer": ["Billy Idol"]}, "target": "['Billy Idol']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang nice day for a white wedding?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Billy Idol"]], "filtered_resps": ["Billy Idol"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9b593222dbdbe2ce58f197a24a4f89cf8dec13af132f7734233feddbdc296c2", "prompt_hash": "ff61fafbc0687223a736d5d447b058cb66ecbe42e4b9d7d18b3ae320f050b5ab", "target_hash": "1c88dcd91ae9d58679ca53427c6a00f1f7bc4986355c4ec55adcfafef12001ec", "exact_match": 1.0}
{"doc_id": 843, "doc": {"question": "who played ashley on the young and the restless", "answer": ["Brenda Epperson", "Shari Shattuck", "Eileen Davidson"]}, "target": "['Brenda Epperson', 'Shari Shattuck', 'Eileen Davidson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played ashley on the young and the restless?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ashley Olsen"]], "filtered_resps": ["Ashley Olsen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ef999e6feedc3f1e2e65dcf9fe484520547b8df7355e1f003c2836f2fc46960c", "prompt_hash": "b530acc0d5358b1f59322b7e4bd6f01d20f37d7252f67a7f96aa03c3b8bb2fb2", "target_hash": "b6e8e35c243c2dbc4ed26465f95f7493b5781986780111e1fd809ad854097c4f", "exact_match": 0.0}
{"doc_id": 844, "doc": {"question": "what does g stand for in ncis los angeles", "answer": ["Grisha"]}, "target": "['Grisha']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does g stand for in ncis los angeles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gibbs"]], "filtered_resps": ["Gibbs"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3a89c5c060470988e37745e3d9516181770b996baad63698e87fd35089e8531", "prompt_hash": "ececc976d2328ec6b2b2d5319f417899d3fdfb500a885161571734e20e8e9b34", "target_hash": "834d008fd42dd1da31e6478b286c38bcf39dd9f660d37c429f07466a358b21eb", "exact_match": 0.0}
{"doc_id": 845, "doc": {"question": "who wrote the first declaration of human rights", "answer": ["Cyrus"]}, "target": "['Cyrus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the first declaration of human rights?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thomas Jefferson"]], "filtered_resps": ["Thomas Jefferson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3b4987628bef27e9f935c1277ac4296670a937b25da5c00588493fa80a2568de", "prompt_hash": "f617f1a52815ce7664aa67d19c786ae19d1912c216d2f6874261e9b70c3c75a5", "target_hash": "3aaa2e83eacb529ddda9cdbe472c6519ae41b94caaf03db29071e0e12b1dc39f", "exact_match": 0.0}
{"doc_id": 846, "doc": {"question": "jharkhand which festival is associated with cattle worship", "answer": ["Sohrai"]}, "target": "['Sohrai']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: jharkhand which festival is associated with cattle worship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Harela"]], "filtered_resps": ["Harela"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "26571c45563a07940bbef363288e5c4ba7d7a43ae92270d39b9d365f8fc0c0d6", "prompt_hash": "741e87182c300aaf6f12fe3e364cba5379711f77d7b77c0bfa9b3f27942ed1e5", "target_hash": "0bdfc1d81638ee5a7a57453039172432ff32996b33f50c561325776ac1f0baa2", "exact_match": 0.0}
{"doc_id": 847, "doc": {"question": "who was charlie writing to in perks of being a wallflower movie", "answer": ["an unknown recipient"]}, "target": "['an unknown recipient']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was charlie writing to in perks of being a wallflower movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Perks of Being a Wallflower"]], "filtered_resps": ["Perks of Being a Wallflower"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "442d89ca90451d9b6614bad5d82739c867025ffa475f48b16ceb60bf808921c3", "prompt_hash": "e27b6ec30f6015d783db150f3d1dbab003fff6ba1be1cf18801adf0a22a86346", "target_hash": "f6cf9c6581de67ea2505294b25569ed077f62de9db1604511a55b0f5e3ab7c95", "exact_match": 0.0}
{"doc_id": 848, "doc": {"question": "which central american nations border the pacific ocean and the caribbean sea", "answer": ["Nicaragua", "Costa Rica"]}, "target": "['Nicaragua', 'Costa Rica']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which central american nations border the pacific ocean and the caribbean sea?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Costa Rica"]], "filtered_resps": ["Costa Rica"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "949291205ff9a8d43b7bc3359da1fcf7b9545be4d4c9d897ebe7e8afa2daf7e7", "prompt_hash": "a636efc543f7e0483410f647278e797ca91fccce1d3610ed23c2f66c25930ace", "target_hash": "d1e41bdb9952f67a88cb1ec19e47a0b44aea4320c24a3b656660bc5e0a46d7ac", "exact_match": 1.0}
{"doc_id": 849, "doc": {"question": "who sings will you still love me when i'm not young and beautiful", "answer": ["Lana Del Rey"]}, "target": "['Lana Del Rey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings will you still love me when i'm not young and beautiful?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carole King"]], "filtered_resps": ["Carole King"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a5cd0143ce09bb031504e0149eceae72b29c81030d4e1e906a1de727740c0077", "prompt_hash": "a06263a36940992aeafa8bc7abf2407dfaa51d59e49f89637c1a8b8c3403210c", "target_hash": "522b1614bc8cbacbd9b0c59f33dd085063d28738274f1cd1172f853284254e59", "exact_match": 0.0}
{"doc_id": 850, "doc": {"question": "where are more than half your bones located", "answer": ["the appendicular skeleton"]}, "target": "['the appendicular skeleton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are more than half your bones located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" skull"]], "filtered_resps": ["skull"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35e1b140d5b8c8d36650b8c7fa987f47f257d91a37483c696182652d6fa7eb29", "prompt_hash": "7faeefbe73be50ef2b09d832f77a0f395985534d3a1407b7f512c3ed775cb8e0", "target_hash": "d7959bad00cd97065943c1ebb8fc0b987943f8b21c5c8fe04de38127e9b7f243", "exact_match": 0.0}
{"doc_id": 851, "doc": {"question": "what does aa on a license plate mean", "answer": ["cars of the royal family"]}, "target": "['cars of the royal family']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does aa on a license plate mean?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alabama"]], "filtered_resps": ["Alabama"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "da801988a582dc315699a6b22aef9a6014da1ea2277a059b709697f5c86af6d5", "prompt_hash": "8db85f262c2b4d1a958ad079d684c8e445a2019836dd139109875809337e11f4", "target_hash": "702c50e39b56cf523313be23c8e5907ed19a511ef2e76f0ee236044761758cda", "exact_match": 0.0}
{"doc_id": 852, "doc": {"question": "when did the apple iphone se come out", "answer": ["March 31, 2016", "March 21, 2016"]}, "target": "['March 31, 2016', 'March 21, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the apple iphone se come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" March 31"]], "filtered_resps": ["March 31"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "30c373bab9cb45acf5aeb034a753b22f9c606f112dab0a6ae4d7896da95c0d2e", "prompt_hash": "646c83364ff9278fa4d1c7c4f63520e4c2d1d0d832e1cb4d366097b4a5e46386", "target_hash": "19598d1bd189104783a4fd11737f341746d23c371562820871783ac435cc9d1c", "exact_match": 0.0}
{"doc_id": 853, "doc": {"question": "who does the voice of little chef in ratatouille", "answer": ["Ian Holm"]}, "target": "['Ian Holm']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of little chef in ratatouille?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Ratzenberger"]], "filtered_resps": ["John Ratzenberger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "69cf61c80ce65bf5e51463b3ce6cda5b033d49b13aaf6c50cd019434a0a83035", "prompt_hash": "79ca8cda8fda21b88cd1f45e8921075cb000da4026ec998956971d25db134c98", "target_hash": "382aa15bee211cf5145c9c73d17bb164338c77e5fdc265a7485fad9bbbe3e7f0", "exact_match": 0.0}
{"doc_id": 854, "doc": {"question": "how many times have real madrid won the champions league in a row", "answer": ["five times", "five"]}, "target": "['five times', 'five']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many times have real madrid won the champions league in a row?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3"]], "filtered_resps": ["3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e51f3364983bb0b356de70777ac1bf5bea044db4dfbd7fd8f239911a03304a5e", "prompt_hash": "ff3bfabb3d9e668340d401c139b7b1ede83b0d97eaa86ca770a49f6297b299aa", "target_hash": "398c18820f2d55f70b665f66d76965640ab329d58debcafde2822f130afcaee7", "exact_match": 0.0}
{"doc_id": 855, "doc": {"question": "in the song i drive your truck who is he talking about", "answer": ["his brother"]}, "target": "['his brother']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in the song i drive your truck who is he talking about?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" his truck"]], "filtered_resps": ["his truck"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19d12f318b60a488e15119adbc69c494c16dc9bbfa806d493d3c24d5c0d3c380", "prompt_hash": "e9afc484a762a763ec0fe1e0ab9300f7ffa130d6a3ca623af3946f15239a8fe4", "target_hash": "8540450607d32703bea7b37be73edfcf8a87aebab48135e4b3df80acc525d5c9", "exact_match": 0.0}
{"doc_id": 856, "doc": {"question": "at what age are you no longer a toddler", "answer": ["36 months"]}, "target": "['36 months']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: at what age are you no longer a toddler?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3"]], "filtered_resps": ["3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d6e6ae34213b35eb83207e5b3f3c9c9faf7cb42d48392e5b2fc591dede0240ba", "prompt_hash": "fbb7268a856d75ae8e417ed1d30b951793ca940f6c11de0adb778f8406f506a3", "target_hash": "1b7c894b187518bee7f8b68fa431842e0e6d0c3498bb0e25e49bd0fc62b47af9", "exact_match": 0.0}
{"doc_id": 857, "doc": {"question": "what is on a mcchicken sandwich from mcdonalds", "answer": ["a breaded chicken patty", "shredded lettuce", "mayonnaise", "breaded chicken patty", "toasted wheat bun", "a toasted wheat bun"]}, "target": "['a breaded chicken patty', 'shredded lettuce', 'mayonnaise', 'breaded chicken patty', 'toasted wheat bun', 'a toasted wheat bun']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is on a mcchicken sandwich from mcdonalds?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lettuce"]], "filtered_resps": ["lettuce"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2470e043aada13af81949b67d761ad575a9324fc707127486f10ad044ae84dd", "prompt_hash": "bb68ba223d7a104a85a026d37718ae76e076739b6c2cf4583726c4776dbb365b", "target_hash": "a3295a8c71418f9bc621808c6707f13d364cd06de5414b8a1f9179cb917bf60f", "exact_match": 0.0}
{"doc_id": 858, "doc": {"question": "whats the movie called with justin timberlake and time", "answer": ["In Time"]}, "target": "['In Time']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: whats the movie called with justin timberlake and time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Social Network"]], "filtered_resps": ["The Social Network"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8017f297e2a569e963575833488c49c763c58ef4e3b6910526c9314e1d216e93", "prompt_hash": "d0208480dd24e18c312a031924390e7f1f6e55347e190fca7f7ca80ddab531e2", "target_hash": "7508876c6378a9d14ff668885b88d40ef0d2721055ab0549b9e45abb02e3223e", "exact_match": 0.0}
{"doc_id": 859, "doc": {"question": "a concave mirror can form a real image which is a copy of an object that forms", "answer": ["inverted"]}, "target": "['inverted']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a concave mirror can form a real image which is a copy of an object that forms?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" virtual"]], "filtered_resps": ["virtual"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ab437130823b9e6b5daa68cd3ee46b84dd7674b9a3a068f80d4730c6b6fa5995", "prompt_hash": "5a675094703a3d0786f8668e9069a8ccbb88a04857e18c4a27727294a2523677", "target_hash": "8054740b9de7d6fd7f2eeeae392a4d9124b67473651501f8d4018e1465a87bf2", "exact_match": 0.0}
{"doc_id": 860, "doc": {"question": "who had the best strike rate among batsmen who scored over 200 runs in the world cup", "answer": ["Chris Gayle", "Rohit Sharma"]}, "target": "['Chris Gayle', 'Rohit Sharma']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who had the best strike rate among batsmen who scored over 200 runs in the world cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" AB de Villiers"]], "filtered_resps": ["AB de Villiers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "607a723c16225e0370988da76a603e725506fbd89058f7da39d667145e7c3c50", "prompt_hash": "6cfd0eaa3b580e917ea3eb73dd2b7fdfdc7071e9cf17f52ae6878b04c2b5773d", "target_hash": "5937e99682e5f977f03ff1c675a79e36d8c0ad5900f33469fbc0b4430e962398", "exact_match": 0.0}
{"doc_id": 861, "doc": {"question": "when did the first battle of ypres end", "answer": ["22 November 1914"]}, "target": "['22 November 1914']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first battle of ypres end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 22 November 1914"]], "filtered_resps": ["22 November 1914"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "95d92ffb6f4431cb296eee9bc8b605e7a66579decf40e49d1aaf4437a75cffee", "prompt_hash": "818fb8e9038bca1c785b39fb5fa3e1c7a6fc74a3402d03bc1c8cf7490809ccfc", "target_hash": "cf0fe7c18358634ce7cbfcbaf464ff34bc0a9d8a00da6ddf417c20e63ac80dd7", "exact_match": 1.0}
{"doc_id": 862, "doc": {"question": "who sings why does it hurt when i pee", "answer": ["Frank Zappa", "Frank Zappa's"]}, "target": "['Frank Zappa', \"Frank Zappa's\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings why does it hurt when i pee?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pink "]], "filtered_resps": ["Pink "], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6debf06f95157342ebd3ca9ff0e7061fbea1786c9bb0e891bdcdfc7096a9923c", "prompt_hash": "942ad52c9215ca149245c5715de02024b198d9bbe04be7f9bf0098a051f6af0a", "target_hash": "db7c9fd1d7d030fa8e2a91c462dfd6934bd8d0d07c77d0102b1bfc0093ffe63b", "exact_match": 0.0}
{"doc_id": 863, "doc": {"question": "who were farmers who kept a small portion of their crops & gave the rest to the landowners", "answer": ["Sharecropping"]}, "target": "['Sharecropping']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were farmers who kept a small portion of their crops & gave the rest to the landowners?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" serfs"]], "filtered_resps": ["serfs"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e0dad5050c22cbb17d1b4f130137885191b2fd7b597c5e2a40a5ce6fc96f1342", "prompt_hash": "ee1abe0057bd46db7791933d529917075524390366a104b8b513224a025e2673", "target_hash": "f3fd2763db62d9fd55ba5746ba9a6713f71ad6b1bf074fd5263353ffb6b1ae88", "exact_match": 0.0}
{"doc_id": 864, "doc": {"question": "who has won the most olympic medals in curling", "answer": ["Torger Nergård", "Eva Lund", "Anette Norberg", "Kevin Martin", "Cathrine Lindahl", "Anna Le Moine", "Mirjam Ott"]}, "target": "['Torger Nergård', 'Eva Lund', 'Anette Norberg', 'Kevin Martin', 'Cathrine Lindahl', 'Anna Le Moine', 'Mirjam Ott']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the most olympic medals in curling?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Canada"]], "filtered_resps": ["Canada"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "499a79a7348b8ff7e44e4331ddd1de6af41658f7f50e8d3ba694947a213345b7", "prompt_hash": "e6c6edc7301dbbe3ac4a9a86c76d2b4bd22bacda33334ffd347a5f1c5e5d80d3", "target_hash": "5c36f9cdeca7a148e66c9c72b02411d17026aed4075d963ca1948e2b818aec35", "exact_match": 0.0}
{"doc_id": 865, "doc": {"question": "what is one of the main goal of the democratic party", "answer": ["social and economic equality", "economic equality"]}, "target": "['social and economic equality', 'economic equality']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is one of the main goal of the democratic party?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to promote civil rights"]], "filtered_resps": ["to promote civil rights"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "677b11108dda214535b67d0c5e8873461126fb0a81e592e6ffeba22f3c384aae", "prompt_hash": "f90c9f371329094f31e3fc19989111a4d96ffb50c207810d30a941dc41b3af61", "target_hash": "14c3b63015f4c4a66d2a7a24a09fcd890ddcacf6867432459f1a32c2be3ada3f", "exact_match": 0.0}
{"doc_id": 866, "doc": {"question": "who sings you can't get what you want", "answer": ["the Rolling Stones"]}, "target": "['the Rolling Stones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings you can't get what you want?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Beatles"]], "filtered_resps": ["The Beatles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5297774d4372c41678648d581ff9c27e7b05b3b0cc0cb91483aabaf5f61d7753", "prompt_hash": "4a3eaecddce736b2890f5ca54c619f1db6eebd1a58d3272a0c05c10b136ea521", "target_hash": "1dec7a2acc1df79feabfd0cf98a83f06e8f36b886790b21f88883e1c0eb21c76", "exact_match": 0.0}
{"doc_id": 867, "doc": {"question": "where is the world's largest thermometer located", "answer": ["Baker, California, USA", "Baker, California"]}, "target": "['Baker, California, USA', 'Baker, California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the world's largest thermometer located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" El Paso"]], "filtered_resps": ["El Paso"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77b65f1ca10e1a79ca9384fc4fa77380bab28e86247471ecfe3aeee0569b2b14", "prompt_hash": "43be30a6e4c722dd20444d0e3e092e0e16e4c1c212c0d5ae80441e14fae6502c", "target_hash": "9281dc19f8259cc7c90d448d5b61dbbf015f8cd946787bcfe3d5f41cacc8d8be", "exact_match": 0.0}
{"doc_id": 868, "doc": {"question": "who plays the beast on the new beauty and the beast", "answer": ["Dan Stevens"]}, "target": "['Dan Stevens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the beast on the new beauty and the beast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dan Stevens"]], "filtered_resps": ["Dan Stevens"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "17d65120025c22db1e1ae3b7c463123a130c9be620c6ef072a4f722bc9b4a992", "prompt_hash": "22a61a795ec0e77fd245338df37452ceef6f0a7a6f8b822f76986d30ae2454da", "target_hash": "6a04d8fe5396ef286d29fe0326bf1b89ad9f548594e9c7200756d5277e71c444", "exact_match": 1.0}
{"doc_id": 869, "doc": {"question": "north carolina delegate to the second continental congress", "answer": ["Timothy Bloodworth", "William Blount", "John B. Ashe"]}, "target": "['Timothy Bloodworth', 'William Blount', 'John B. Ashe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: north carolina delegate to the second continental congress?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Hooper"]], "filtered_resps": ["William Hooper"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cdf73b7bb86b35815095b4deb8e2324977e3c2898843723c1e5cf5919559857c", "prompt_hash": "864423105d574a7742a74121153d14e0d6a9a68cb12dbe56df1070fb73b306e0", "target_hash": "c363e800ebeee9e556287d1a436c4430790f4e404379d7f3a01ef5f7cb2e74aa", "exact_match": 0.0}
{"doc_id": 870, "doc": {"question": "who gets the most punishments on impractical jokers", "answer": ["Sal"]}, "target": "['Sal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who gets the most punishments on impractical jokers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joe Gatto"]], "filtered_resps": ["Joe Gatto"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3ae72f9bdf69e021464c58ddcff6ff29340a2de19d9f10ab4535521e65d65586", "prompt_hash": "bbcdd4994cab899fb0c4ae5772fcbd43eea26f29894cfc3e7fea2bbf2914f08e", "target_hash": "097dba968b85158f80009aa4129dac95bdd64c560f51ccea47c4e14666382a2e", "exact_match": 0.0}
{"doc_id": 871, "doc": {"question": "what is the name of governor of maharashtra", "answer": ["Chennamaneni Vidyasagar Rao"]}, "target": "['Chennamaneni Vidyasagar Rao']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of governor of maharashtra?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Devendra Fadnavis"]], "filtered_resps": ["Devendra Fadnavis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "52bf59aaff6020c1837ed8b8226ec24b8fa1bf5411bbf4c92ecfaa464b31ebd9", "prompt_hash": "54f555fae219929dd409929bf5c6d537cc7c9b4a733c1c74cdb401c815272247", "target_hash": "8c4ba2fdb2331899518aa57f8661990424bcec907c04cbde5b986a61376d095b", "exact_match": 0.0}
{"doc_id": 872, "doc": {"question": "what is the lead singers name of staind", "answer": ["Aaron Lewis"]}, "target": "['Aaron Lewis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the lead singers name of staind?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Aaron Lewis"]], "filtered_resps": ["Aaron Lewis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "135a0c7b70c1efc6fd275c2368c6021f5753fbdf8bd13300979da3bb030d3103", "prompt_hash": "19255f206a85150b62af5559da48d85e4d8b7a83ccdff132557cda863a3317e5", "target_hash": "f89e808e6ec352ba2fe25f0c04574c69113cc81e23088cc017e57e29676d6b05", "exact_match": 1.0}
{"doc_id": 873, "doc": {"question": "when did the vikings first sail and reach britain", "answer": ["6 January 793", "786–802"]}, "target": "['6 January 793', '786–802']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the vikings first sail and reach britain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 793"]], "filtered_resps": ["793"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cbd1c5a53e76ce355380b1a924a4c7380e2b66f2de67540b057a33c3b2ca3e32", "prompt_hash": "388b047b19192ae9c99da462c99cf52fb2f63bfb3a2a2d7060968d50bc14aaa5", "target_hash": "0b2677a71c3efcb3ddfcb8956ad56a824bd4be6c2e01ed8be9f6601526e34429", "exact_match": 0.0}
{"doc_id": 874, "doc": {"question": "who plays nathan's mother on general hospital", "answer": ["Donna Mills"]}, "target": "['Donna Mills']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays nathan's mother on general hospital?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Susan Flannery"]], "filtered_resps": ["Susan Flannery"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5941ba46daf1cbf923796d14026e791dd46642e76236826c4dd57dac3e7ff3c0", "prompt_hash": "c2bf76c2a2f952fff7561af1f316f119b86c12b83389de4d4f7b6fefddf37ab4", "target_hash": "13874e9133446c5d8fabca408f984f9f955d1c5c64d0f3a2a389f9b0aa802ee7", "exact_match": 0.0}
{"doc_id": 875, "doc": {"question": "who supported states rights during the civil war", "answer": ["the Confederacy"]}, "target": "['the Confederacy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who supported states rights during the civil war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Confederates"]], "filtered_resps": ["Confederates"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a98002caa541d6ab16e0e28be963fc34cc0edc0cee8e8ff91aaf8e3900bd499", "prompt_hash": "e78838c5d7ab0127d3fb75d54578b5f6d5c69eaed770d1982659cebd73cd3ea8", "target_hash": "ff07c395f01680603d2ca1a8f0787c345b4048358ca23d5017b0e527e91b83f2", "exact_match": 0.0}
{"doc_id": 876, "doc": {"question": "who is the actor that plays sneaky pete", "answer": ["Giovanni Ribisi"]}, "target": "['Giovanni Ribisi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the actor that plays sneaky pete?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jesse Plemons"]], "filtered_resps": ["Jesse Plemons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f006d2ab50d50bfce7c00cdd9b39bc11d69971881ca2688824442f9edaaf90e", "prompt_hash": "2a0daf325fc950dc738e9c0f04ac1b3ab231cc93a84257dc51694fa404dfbe62", "target_hash": "558d83eed15eedaad91abb00d874a63fd1acaea389e40343ea473ec5d5cd21b4", "exact_match": 0.0}
{"doc_id": 877, "doc": {"question": "who sang i want to be bobbys girl", "answer": ["Marcie Blane"]}, "target": "['Marcie Blane']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang i want to be bobbys girl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Monkees"]], "filtered_resps": ["The Monkees"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "949355732c2b51e090c7893fb4e7d09d75c648bbf8b5ad979af6c2346c7aa4dd", "prompt_hash": "a659c962e2c73470d973ca7f0e63b85f2c15e14efeca89ceaf5add4f1b382926", "target_hash": "6be0f5962c78aa60ef24bf9c97cc1fe9e3eb59b1acf76fad29223f34d75cb521", "exact_match": 0.0}
{"doc_id": 878, "doc": {"question": "where was 2017 beauty and the beast filmed", "answer": ["Surrey, United Kingdom"]}, "target": "['Surrey, United Kingdom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was 2017 beauty and the beast filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paris"]], "filtered_resps": ["Paris"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "43f36fabbe9828ae266835d2fec7a133bd8bdfd793ffd971fd58e19601ffcb52", "prompt_hash": "4c1c8cbfef330475c072fb232f8d07b8e379280a78da18e3e1b10c61aedeb461", "target_hash": "af92dc8d4776856b67129cbdbff8f84553812397e6141c4827f498c00a1fad82", "exact_match": 0.0}
{"doc_id": 879, "doc": {"question": "how many episodes of the white princess will there be", "answer": ["eight", "8"]}, "target": "['eight', '8']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes of the white princess will there be?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 6"]], "filtered_resps": ["6"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8fe6aea2a76d871f5fef5529787b029aebcf40df5c65b0075d41f9779d349152", "prompt_hash": "b8b88de909ae55f1a7fffe41dbce077f90745f7645b94beab599cca2d6721743", "target_hash": "2e4c7857030d0fc02e171bc29d61abdf9c155e0d789be6eca1f160a12e59ace3", "exact_match": 0.0}
{"doc_id": 880, "doc": {"question": "what part of the brain is in the middle cranial fossa", "answer": ["pituitary gland", "the temporal lobes", "the pituitary gland", "temporal lobes"]}, "target": "['pituitary gland', 'the temporal lobes', 'the pituitary gland', 'temporal lobes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what part of the brain is in the middle cranial fossa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the petrous part of the temporal bone"]], "filtered_resps": ["the petrous part of the temporal bone"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "54f458aa60fe739c5ecc8d11a99871be29ad544d6583fc563ebded5d7ea73c34", "prompt_hash": "5819188bb87a0e82f0d7105ffe2d9c7cf66f405cb656cd851417d2a7d5071741", "target_hash": "e54d66a1b52e4482508596bbcb47f064d2874b6fe68c03853f6cf412c5fb502a", "exact_match": 0.0}
{"doc_id": 881, "doc": {"question": "when does flash come back on season 4 episode 17", "answer": ["May 1, 2018", "April 10, 2018"]}, "target": "['May\\xa01,\\xa02018', 'April\\xa010,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does flash come back on season 4 episode 17?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 11"]], "filtered_resps": ["October 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f671bb2cf5662c0bc55b03d701520331d2f8366128713aa7b7d591401cecfd06", "prompt_hash": "5be24890ac2d2ef8bba6f72ebbff40e566d45b8c174ba4c3bda80149529e4edd", "target_hash": "59a97ac3fe67f2e5375ea64405e67782773a3d5b5a8c609334914a02e507ba39", "exact_match": 0.0}
{"doc_id": 882, "doc": {"question": "where is dakar located on the world map", "answer": ["Senegal"]}, "target": "['Senegal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is dakar located on the world map?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Senegal"]], "filtered_resps": ["Senegal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "378bb4c1100c75e2b4b0c53d96b4604f56025ddf8a58dee7e8143b5b320677b2", "prompt_hash": "e37d58ae8a7ad21b433c03f84a25a0f4e2b7746a970e4125da2df0901f267446", "target_hash": "2193a35851aaf66eac2d5f7996453275635e0c818b928d4da2c12bf0b88f6ca4", "exact_match": 1.0}
{"doc_id": 883, "doc": {"question": "how long were the pyramids the tallest structure", "answer": ["over 3,800", "over 3,800 years"]}, "target": "['over 3,800', 'over 3,800 years']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long were the pyramids the tallest structure?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3800 years"]], "filtered_resps": ["3800 years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "56ced37cce79d88ae6021fce460c569594beb723f9d7eada57c71c1827513297", "prompt_hash": "31ecd91c33f15e4f8c956e38eb9421e157cba11a38e0efd4fd19596cb7db5bf9", "target_hash": "c4ff84a0a4c94fbdb9934a342402fd07708d3b6b6548d305eb1ac6b9be7978db", "exact_match": 0.0}
{"doc_id": 884, "doc": {"question": "which is the site of the light dependent reactions of photosynthesis", "answer": ["thylakoid membranes", "the thylakoid membranes"]}, "target": "['thylakoid membranes', 'the thylakoid membranes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the site of the light dependent reactions of photosynthesis?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" chloroplast"]], "filtered_resps": ["chloroplast"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0c237cea7e99059e93d729d84ca9c3ad95cdfdfc7d1e8965b3f0e5c90ef9ae9c", "prompt_hash": "cbd053c79f83e82ffbf5f04fec53b78a9bb2a9f02ee9c2c56480ff216ecee544", "target_hash": "56facb783841ce79355092c0baf68f2b4f1afade9afffbac4cd0a99dcc2b9d27", "exact_match": 0.0}
{"doc_id": 885, "doc": {"question": "where was the movie silver linings playbook filmed", "answer": ["Ridley Park", "Lansdowne", "just outside Philadelphia, Pennsylvania", "Upper Darby"]}, "target": "['Ridley Park', 'Lansdowne', 'just outside Philadelphia, Pennsylvania', 'Upper Darby']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the movie silver linings playbook filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Buffalo"]], "filtered_resps": ["Buffalo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c7827deeca05b71ae88de62eab7357b95307f77cbe7337c7356a79aa59fba644", "prompt_hash": "ac31104cb3b2e97badca6b33fc5f716957d68ceb1c2ff654803ec27081467204", "target_hash": "a2789d630e2b2f336bf5f5178faaff2250769c2ae1d9ce9405c13ca15e58a410", "exact_match": 0.0}
{"doc_id": 886, "doc": {"question": "the duluth model is an intervention program that emphasizes", "answer": ["re-education"]}, "target": "['re-education']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the duluth model is an intervention program that emphasizes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Duluth Model"]], "filtered_resps": ["the Duluth Model"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a710a3680c90b970e5ec73ed842f2e6f56bb0a570817a43c3a45c08368253fd", "prompt_hash": "9ee529981d12f0fc6b81f3d07ac7aa858049630f8f295a39eff1c2b2fe39f12c", "target_hash": "7f5d5cc6c7ee17f1eea95661401f7c1892eb144a8d2e3fa1b364d3220ff51a1f", "exact_match": 0.0}
{"doc_id": 887, "doc": {"question": "what is the full form of ib board", "answer": ["International Baccalaureate", "The International Baccalaureate"]}, "target": "['International Baccalaureate', 'The International Baccalaureate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the full form of ib board?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Indian Board of School Examination"]], "filtered_resps": ["Indian Board of School Examination"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a57c24892d558e1b3d0ab71eab8bc82b6de2df1df950b5dcc5f9554757a7ca90", "prompt_hash": "1c58acac3f48515476b20d9d8ded0976d989e3cc2ccc0466a545ae5f5ecc31e2", "target_hash": "7d2c668bd7615d21d840403bfb2e0b77c3b3508614b7c876399b4aaa0bc99a3d", "exact_match": 0.0}
{"doc_id": 888, "doc": {"question": "when did slave to the rhythm come out", "answer": ["May 8, 2014", "2010"]}, "target": "['May\\xa08,\\xa02014', '2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did slave to the rhythm come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2008"]], "filtered_resps": ["2008"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2bc584aa4ef4a2360765b425ab543d84d1f0f64753e1b8a47182efe0e8cb834e", "prompt_hash": "1a6e17aee1f54167badaafea562168ccd9ddb65e3d4e985cc6cb61663c93b6aa", "target_hash": "5f63eab7abdad5b9df99cc2cb234fa81e4b17d92947384c9d6497005e1fd6006", "exact_match": 0.0}
{"doc_id": 889, "doc": {"question": "state with highest population growth rate in india 2011", "answer": ["Dadra and Nagar Haveli", "Meghalaya"]}, "target": "['Dadra and Nagar Haveli', 'Meghalaya']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: state with highest population growth rate in india 2011?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Uttar Pradesh"]], "filtered_resps": ["Uttar Pradesh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7056ac6dd4a3cd3f9cd6b3fd9d7ea97be3f1ac2dc01b2b505415c50cc601f339", "prompt_hash": "b86ec789264cdeaccfa4138305430d9c06e74f2c57c2cfb8608becea09107e74", "target_hash": "cd29137d6a02b5893383f7e6c3388b3de46a69224fcf53937c8408a115cbe02a", "exact_match": 0.0}
{"doc_id": 890, "doc": {"question": "where are the first nations located in canada", "answer": ["south of the Arctic"]}, "target": "['south of the Arctic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the first nations located in canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" British Columbia"]], "filtered_resps": ["British Columbia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ced8191d029f06735eb75849e9bd971d70f64d3669f7f8709f59edb445226d00", "prompt_hash": "32a091e130855568ce6b2330a6a5cbf68b15f5974a7a7ae04684fb6de8c08767", "target_hash": "dd4e73ba19793ae3581483e28465439466672933755a23175752bc388fd66663", "exact_match": 0.0}
{"doc_id": 891, "doc": {"question": "where does puerto rico's power come from", "answer": ["Puerto Rico Electric Power Authority"]}, "target": "['Puerto Rico Electric Power Authority']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does puerto rico's power come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" oil"]], "filtered_resps": ["oil"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2060d438fffda68c09847a862de500bf6cd2e9b6a5ad9caf7d4b84286e42cb22", "prompt_hash": "3bb1f4835aee288243073d03f3e53e73775db34361ef799b357e88da96afcbea", "target_hash": "ee8b11f41ec75218d82c4c60f844222b7611ebce25fab4625a4315845f46d136", "exact_match": 0.0}
{"doc_id": 892, "doc": {"question": "who was elected the president of constituent assembly", "answer": ["Rajendra Prasad", "Dr. Rajendra Prasad, INC", "Dr. Rajendra Prasad"]}, "target": "['Rajendra Prasad', 'Dr. Rajendra Prasad, INC', 'Dr. Rajendra Prasad']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was elected the president of constituent assembly?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rajendra Prasad"]], "filtered_resps": ["Rajendra Prasad"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "45c59a4cb7de598352c0b4a9677944a51936518ab63a18c5a037e5bc40930fa7", "prompt_hash": "0d823d6d718a3767264a682dc6ab33c2063b85983c0892dbf483aee79512a81a", "target_hash": "14bc91cf7c7710c664f2fe1315901b1b0b8eacf3c32b9edc16b91d7fb8dd190e", "exact_match": 1.0}
{"doc_id": 893, "doc": {"question": "a synonym for the content component of communication is", "answer": ["information"]}, "target": "['information']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a synonym for the content component of communication is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" message"]], "filtered_resps": ["message"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91fb92974ca274be934060d2a723241bcec5817b14f23b399c1410deb01ff636", "prompt_hash": "0c2e4a991106f65857a2b8d81c678f00d5207dbac531434d6d7b30a4587a6353", "target_hash": "9af6be53e5c965a220ac4ce9f5c903f3b17e1d6c4189fda35b919afd393b02d1", "exact_match": 0.0}
{"doc_id": 894, "doc": {"question": "different ways to spell corey for a boy", "answer": ["Coire", "Kori", "Correy", "Cory", "Khouri", "Corrie", "Corie", "Kory", "Corey"]}, "target": "['Coire', 'Kori', 'Correy', 'Cory', 'Khouri', 'Corrie', 'Corie', 'Kory', 'Corey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: different ways to spell corey for a boy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cory"]], "filtered_resps": ["Cory"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a07405f7c0c448191e15072dcf8fa0a315e28450bb8fcc2cf3a06ec2eade498", "prompt_hash": "90b824847c2c41a0b20310098880339c5b6946915fd1c7a9292b57e7bc940ee8", "target_hash": "ffe26d081d0d8b656840c89e55d167dcf30ee3134ed7f40b70fe14c3df7d2953", "exact_match": 1.0}
{"doc_id": 895, "doc": {"question": "active absorption of water from the soil by the root is mainly affected by", "answer": ["SURFACE AREA OF ROOTS", "ATP"]}, "target": "['SURFACE AREA OF ROOTS', 'ATP']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: active absorption of water from the soil by the root is mainly affected by?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" root pressure"]], "filtered_resps": ["root pressure"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf76240842b990343615f3be6dd6cc3aa1db556e57f1cc5fb16e8ea3db21d6f7", "prompt_hash": "8fecd7cbd989ed3c088ce2d6650756ba520f2947371fbcb3949766194243e548", "target_hash": "4cd616ddbc8202c5dcce7a480673c36d4f0d7a260b92bc7771914250aa37d108", "exact_match": 0.0}
{"doc_id": 896, "doc": {"question": "when was the first ford f 150 made", "answer": ["1975"]}, "target": "['1975']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first ford f 150 made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1975"]], "filtered_resps": ["1975"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "34321b1f23a4666967097d1b11fd4f4a5fb0e6a7762b1a01097f4973545604dc", "prompt_hash": "d3acaf9ab2bcfc727b18ef01285afd49d909cc2ec26b91826ef16720baac00a5", "target_hash": "30547784fdb6aac7ba2fd06b246ee5c7e1d5fb3c600cdce64155caba0d079427", "exact_match": 1.0}
{"doc_id": 897, "doc": {"question": "who votes in the speaker of the house", "answer": ["The House of Representatives"]}, "target": "['The House of Representatives']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who votes in the speaker of the house?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Members of the House"]], "filtered_resps": ["Members of the House"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f79ec7f2d9555f93de64ecf902251c2c2497b97d82ae13d7036d6c8e730ba92", "prompt_hash": "c33bbe768dd176bc48dc180f6e7c3c1a448e2cf6b1daeb3dc4839321024ce66a", "target_hash": "7493a4e6108200471473cd23d54c5df84944c7007593d6e91105634e23a5b6e2", "exact_match": 0.0}
{"doc_id": 898, "doc": {"question": "who plays young agent o in mib 3", "answer": ["Alice Eve"]}, "target": "['Alice Eve']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays young agent o in mib 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chris Hemsworth"]], "filtered_resps": ["Chris Hemsworth"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "690da4c5e56958c19ab2667ff07e7a8df0c7b977cd67d30eb98f77bb076676d0", "prompt_hash": "a6a48ecc181296f8840aaddf0568a33e95980dcba2d658e878a4830ba6f06fc1", "target_hash": "f8105eefa8c7ae22082f7514cab8322dc0398db078c1274dc017faf57282818e", "exact_match": 0.0}
{"doc_id": 899, "doc": {"question": "who plays hulk in the thor and avengers series of movies", "answer": ["Rick D. Wasserman", "Fred Tatasciore"]}, "target": "['Rick D. Wasserman', 'Fred Tatasciore']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays hulk in the thor and avengers series of movies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mark Ruffalo"]], "filtered_resps": ["Mark Ruffalo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55ab9db3fe141f479ebf7714e9034d75163f4c724c337242a49b84ae253a5b19", "prompt_hash": "bf70bde4f81d37e8f9efa211582177e103de61ed1d18ddafd4271eef3b760546", "target_hash": "b63af0e06f605e91fd5da0fe2f25b5f407dbb412a3c185b097fede42b411f384", "exact_match": 0.0}
{"doc_id": 900, "doc": {"question": "when does boomer find out she a cylon", "answer": ["Kobol's Last Gleaming"]}, "target": "[\"Kobol's Last Gleaming\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does boomer find out she a cylon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" in the finale"]], "filtered_resps": ["in the finale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "267173568490a7a21070ff77d0ee5b3c5d0d95d33b26943e490d58c349c3ae3f", "prompt_hash": "8b3bf00ed0a173fb10cee9c62c5236f026288b04b783e6339433f57956c3a91e", "target_hash": "01068cdc58c0d8997ea24296fcfeb6b705f933b1b48818247291780659b3eeaa", "exact_match": 0.0}
{"doc_id": 901, "doc": {"question": "what's the largest city park in the united states", "answer": ["Franklin Mountains State Park"]}, "target": "['Franklin Mountains State Park']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the largest city park in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Central Park"]], "filtered_resps": ["Central Park"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ab9fc733245a99d528680fead4d899810c66ad5c1e60dc44ad759087294c40f", "prompt_hash": "1de138500e15c598def60ac55166dd838ce3c927b4e474f5e919e1620f2377fa", "target_hash": "00739c8549adc09e2c95eae00a3f63c51996f852a3b8146ab4190d927448a6cb", "exact_match": 0.0}
{"doc_id": 902, "doc": {"question": "who wrote the poem for whom the bell tolls", "answer": ["Ernest Hemingway", "John Donne"]}, "target": "['Ernest Hemingway', 'John Donne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the poem for whom the bell tolls?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ernest Hemingway"]], "filtered_resps": ["Ernest Hemingway"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e311540fd6e43ffd775b5eb9f6719a841d5105ff1da8c8b3f034d1dac97688ea", "prompt_hash": "1c1038c90e66913959c4f9fb60816c7bcd5e5acde7be0d8ccb27f98bb8614d3d", "target_hash": "592790256c8a6ec3f321e187ec000602e469b7649d2f206f5f06cd5caa936af3", "exact_match": 1.0}
{"doc_id": 903, "doc": {"question": "three largest cities in the world by population", "answer": ["Beijing", "Shanghai", "Delhi"]}, "target": "['Beijing', 'Shanghai', 'Delhi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: three largest cities in the world by population?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tokyo"]], "filtered_resps": ["Tokyo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "99f3bd337657c9538c767ee012a5941ce8f7533f3d0113d1ebe2ea36252b69a3", "prompt_hash": "1a1a5c5eb314d98e0874bd86ee50cd39746054df83ac8053f3168ef12a229abb", "target_hash": "9885776e948a209d99242ff995559585974d57f5a9e6aa8755cf35510f04c176", "exact_match": 0.0}
{"doc_id": 904, "doc": {"question": "where can the mona lisa be found today", "answer": ["Musée du Louvre, Paris", "Louvre Museum in Paris"]}, "target": "['Musée du Louvre, Paris', 'Louvre Museum in Paris']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where can the mona lisa be found today?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Louvre"]], "filtered_resps": ["Louvre"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "da5bcb7a0c21be90e3867b6305038d81d1cac76247179c7a2ebd1b0ad44f09c3", "prompt_hash": "264536c4b6ea51c4635409411af8bbbb57cd30bb46d98557f393979fce64c092", "target_hash": "1e6d9b79e439d3067b80c8bbbf5f142fc2aa44b1cb909124c37fff8fb5e17564", "exact_match": 0.0}
{"doc_id": 905, "doc": {"question": "how many levels of protein structure are there", "answer": ["four"]}, "target": "['four']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many levels of protein structure are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" four"]], "filtered_resps": ["four"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "016b1de880de1fbcd552c483abef441e460e9825359e93d1104aa9d4aa3e2f32", "prompt_hash": "19d8c832ec76ea827deda70711c4fe18ec8a84b13ce84e9f78f9c3946c09822e", "target_hash": "c0b12eaa48bad2eab0833b330f1f9a7a1caf1ef07c30f4fc2a1bd97e38e98618", "exact_match": 1.0}
{"doc_id": 906, "doc": {"question": "when did university stop being free in australia", "answer": ["1989"]}, "target": "['1989']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did university stop being free in australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1989"]], "filtered_resps": ["1989"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c8c81c260803dd63cc9b1062db50062167cb9a89b74bc9007d4cd91589b9d4a", "prompt_hash": "98350343c0b69458d7f783b8ddbe7dacbd04dfbf18c8f40f1c4cbfa941b7574f", "target_hash": "f29cbf85fafcf80ace6273501b873d82d249a9b85142c12822207ef416c8728f", "exact_match": 1.0}
{"doc_id": 907, "doc": {"question": "panic at the disco song about a wedding", "answer": ["I Write Sins Not Tragedies"]}, "target": "['I Write Sins Not Tragedies']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: panic at the disco song about a wedding?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" A Rock Star's Guide to Love"]], "filtered_resps": ["A Rock Star's Guide to Love"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e50adaa6d79c64b695b4deec44ed632cbf0d2d1303239fe4ea99d3b5dece553e", "prompt_hash": "2754a7c5e3ece988277f21a29b7f30c8d69909eaba5fd81d35d480bb4a83ae12", "target_hash": "02bf5fc11099e932fbaeafefbd307a252154e51ecbd73d17b956500f211da196", "exact_match": 0.0}
{"doc_id": 908, "doc": {"question": "how many pages are in the book inside out and back again", "answer": ["263"]}, "target": "['263']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many pages are in the book inside out and back again?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 224"]], "filtered_resps": ["224"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8c7fc828a81caf2f2a89a2fd1f763b4b87bb2ac7d96e840a6c8d92ebe8d86428", "prompt_hash": "f7f165e2e458a2da4bd125c60540c647bb33bf3704673078f4d6c7996c76e7a6", "target_hash": "91da5a689c54a005f149713c54d5cf72fa912da1095e01a002eb9cb0ec02fd9c", "exact_match": 0.0}
{"doc_id": 909, "doc": {"question": "what is the most famous building in rennes", "answer": ["Parlement de Bretagne", "The Parlement de Bretagne"]}, "target": "['Parlement de Bretagne', 'The Parlement de Bretagne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the most famous building in rennes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Church of Saint Peter and Paul"]], "filtered_resps": ["The Church of Saint Peter and Paul"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c8f112e0d5afaff2bfc363b8d583842dcf1745369d4cd188a2ccbdcbb8dc30ed", "prompt_hash": "954bf19e2302e5b5831147e0e7494b3f0a5948896f94e4a3d56caa2bd02d8c8e", "target_hash": "3ab07c14990d3a8b1056f4cd2af151a92b5517030fa1a9792c8490bd88e74074", "exact_match": 0.0}
{"doc_id": 910, "doc": {"question": "when did the song things that make you go hmmm come out", "answer": ["June 1991", "June 23, 1991"]}, "target": "['June 1991', 'June 23, 1991']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the song things that make you go hmmm come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1981"]], "filtered_resps": ["1981"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "72b809e4791590e90ba7d534e46c9021adb9f604d075c317aeb84703862bbe83", "prompt_hash": "1eef1ea75d3ce17b8264b4eba192686848367480701ec1a078f5c2d936166222", "target_hash": "cf62f21c3ae23f88c6cb605ebf8fc8b9ebe28e72a8e91bff446b432e14397cbe", "exact_match": 0.0}
{"doc_id": 911, "doc": {"question": "who plays the dad in girl meets world", "answer": ["Ben Savage", "Cory Matthews"]}, "target": "['Ben Savage', 'Cory Matthews']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the dad in girl meets world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jason Earles"]], "filtered_resps": ["Jason Earles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "106e895056b94bd8b98030e3db2435a4a4762503ed2f220c0af7449ac82d2d2c", "prompt_hash": "cdde9f1bf2a4f3da34cc31f9766b77756378b45de24b292b54232398763174e4", "target_hash": "26e8b9a35d8ec45540424297408c4ea53c7fdf323a9ccd266616c90281723eb3", "exact_match": 0.0}
{"doc_id": 912, "doc": {"question": "who played the dad in a christmas story", "answer": ["Darren McGavin", "Ian Petrella"]}, "target": "['Darren McGavin', 'Ian Petrella']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the dad in a christmas story?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Henry Travers"]], "filtered_resps": ["Henry Travers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0b9c23a94415c4a487c0ce89849e133146b6eab9fd33d8f1433e184b4756c189", "prompt_hash": "c509eed852b587c449d9c759ea9d98fefd8fd0ae4a8478e25b1c2c7f74a2cbcd", "target_hash": "549a80da69dbd10be49a837ea7d1f4570bb4442d019bf5ba8634923353a7998c", "exact_match": 0.0}
{"doc_id": 913, "doc": {"question": "who plays the dwarf king in the hobbit", "answer": ["Richard Crispin Armitage"]}, "target": "['Richard Crispin Armitage']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the dwarf king in the hobbit?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Richard Armitage"]], "filtered_resps": ["Richard Armitage"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "15a190e9e2d66b7456d1dd52df01ef6921f30c5ac59a56e170289c21188b6929", "prompt_hash": "a07ed84f9dbe4a6245c2ad0910175fed7d8ad96382cbe685ac883a4fc71435fa", "target_hash": "62b88c30bda1b3c31fe38f78869586f0617e7af57998988093d9f2a518452bac", "exact_match": 0.0}
{"doc_id": 914, "doc": {"question": "when do you declare honors in contract bridge", "answer": ["any time after the auction"]}, "target": "['any time after the auction']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do you declare honors in contract bridge?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" after the auction"]], "filtered_resps": ["after the auction"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b98d64eeac4bc217d077806c2cedae8545d21a50b9b81f2c08a5fab47223be9", "prompt_hash": "91d5031b416add1c4034ea3abf33efd84feb3a10a631d19d7f54b033798fe1b6", "target_hash": "afcb62b19ae11df83ff124c70777c928c3074a40b0d6481383dfbba4a960ba51", "exact_match": 0.0}
{"doc_id": 915, "doc": {"question": "what is the name of manchester united stadium", "answer": ["Old Trafford"]}, "target": "['Old Trafford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of manchester united stadium?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Old Trafford"]], "filtered_resps": ["Old Trafford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18a078e93ca76d9bbee05056511a805ef0a9fb45cd90973c6681a712d35f2318", "prompt_hash": "333fdaf452d72db0a5962a4f2782166d060a93bf0b63be492fe658f6df9d44f2", "target_hash": "b4d854e3b0fa66a5b3dcf1b4b30757e21fd10af8b32358385873481e165ab961", "exact_match": 1.0}
{"doc_id": 916, "doc": {"question": "when is if loving you is wrong coming back season 4", "answer": ["September 19, 2017", "March 7, 2018"]}, "target": "['September\\xa019,\\xa02017', 'March\\xa07,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is if loving you is wrong coming back season 4?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b71ce337ea970f7f01b406d01c56a1c5ecd251662f3f1e776329aca5f6ca9f24", "prompt_hash": "97e19c341b71439143684d536652538a7464e5cf062533c1edbe86f073ef6a41", "target_hash": "2938c6e8e25bbe6260a6cc62381b0aa15cb02e50fb1d6cd5d693b99d63ee5b66", "exact_match": 0.0}
{"doc_id": 917, "doc": {"question": "who won season 16 on dancing with the stars", "answer": ["Kellie Pickler", "Derek Hough"]}, "target": "['Kellie Pickler', 'Derek Hough']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won season 16 on dancing with the stars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Miley Cyrus"]], "filtered_resps": ["Miley Cyrus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "859335f90e76522d23d52e43427d4f59c8c4b87b6ad93e52b50a74c1a37e0913", "prompt_hash": "f4e216d06549272833dca61378c3a70e4b8bfcbe39765d8bcd3e0d27b4ca8c27", "target_hash": "935522deb187c125cd11ffc4c8ee942c1df79190d6588ad7be7168b784f533e4", "exact_match": 0.0}
{"doc_id": 918, "doc": {"question": "what is the account number of airtel payment bank", "answer": ["Your Airtel mobile number"]}, "target": "['Your Airtel mobile number']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the account number of airtel payment bank?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 0000000000"]], "filtered_resps": ["0000000000"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f9b953a795866a4a139b94b1f899c9a74e79251287aa0d0e7c2f503d60dc3e54", "prompt_hash": "e0096aa246b76b3cc55bb88ef318bdb1b1867e707c3333a5359f89f25c0dc5aa", "target_hash": "aa27767e02ab798bbc66f14c432b09dc12dd1e3f6abde9c623c0c08767d2f46b", "exact_match": 0.0}
{"doc_id": 919, "doc": {"question": "when did the crucifix become the symbol of christianity", "answer": ["in the 2nd century", "4th century", "the 4th century"]}, "target": "['in the 2nd century', '4th century', 'the 4th century']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the crucifix become the symbol of christianity?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 13th century"]], "filtered_resps": ["13th century"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c7bc88c350c48b8109be76378c519fa29823eb7e15f83022b38ea5b49ab22be1", "prompt_hash": "a8935275782ed1d437319a0f633fb80330fc08b4aaa4a001f037b90b12012c5c", "target_hash": "a28140348f53413484a6b8b649634f7966788288af9aadd4b68cfa20e090a5b7", "exact_match": 0.0}
{"doc_id": 920, "doc": {"question": "when was the first commercial cell phone released", "answer": ["1983", "1973"]}, "target": "['1983', '1973']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first commercial cell phone released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1983"]], "filtered_resps": ["1983"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "009444933657095a6a96f7c046c6973acb1dfb70e421c49e5c7c3ad320031f38", "prompt_hash": "ea76e580fbb39033776ce9cc2ab973b5173490afff636ed787fc35b395796314", "target_hash": "159152e6be482c7a0c5e433bce90cfab5bfbdb856961aed68d7d0af7b297beac", "exact_match": 1.0}
{"doc_id": 921, "doc": {"question": "what type of boundary was the mexico earthquake", "answer": ["a subduction zone"]}, "target": "['a subduction zone']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of boundary was the mexico earthquake?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Transform"]], "filtered_resps": ["Transform"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "62cc41087b822faeec72ce08fa1814c10f230bf69b4d5ace2ec9c4838f5bafee", "prompt_hash": "bb730bd2dcff30de4bd32ab29cf3a114e062540911ce9f36dd9fdbcd819cfe13", "target_hash": "2e49f4fc3650d9baa7eed8d00ba537b58f2031be06668c3c0e6ac8822c4852de", "exact_match": 0.0}
{"doc_id": 922, "doc": {"question": "how long is one full rotation of the earth", "answer": ["about 24 hours"]}, "target": "['about 24 hours']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long is one full rotation of the earth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 24 hours"]], "filtered_resps": ["24 hours"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f382d1f462593bf89f1540eb6078a96b69301e5a24cea88c612ff3bfb2296c8b", "prompt_hash": "ca359307e30e2ea55f771b69bdf0b66e0f3df08a7f485736eb24d77bff71f772", "target_hash": "3c2c7bc93ff931bf18866251ac36853e89fdfac81df84845e648da07b4a2cd02", "exact_match": 0.0}
{"doc_id": 923, "doc": {"question": "on which river did the exploration of the louisiana purchase begin", "answer": ["Missouri River", "the Missouri River"]}, "target": "['Missouri River', 'the Missouri River']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: on which river did the exploration of the louisiana purchase begin?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Missouri"]], "filtered_resps": ["Missouri"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "41e5d8c36ce69b0a1e9e81c639c22c3d87ac7d720fedec5599764d170ef0279e", "prompt_hash": "44b26d54411d8933365fb7f8631a081201108517a0116b88c80d1aa84c361d1d", "target_hash": "2aad2561a96c619690097453dd066a12c987d4c4079b113179e13d176aabbcdc", "exact_match": 0.0}
{"doc_id": 924, "doc": {"question": "where did the world's largest recorded wave occur", "answer": ["Lituya Bay in Alaska"]}, "target": "['Lituya Bay in Alaska']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the world's largest recorded wave occur?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mavericks"]], "filtered_resps": ["Mavericks"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "97f82de297294ec18eb5155e814e209a849581d715602ab2c1d6761dc10ef13c", "prompt_hash": "51da15aa2ce73e6e2d13daf2a50cd0e6cb747c6ead6c8778c136712714acb2cb", "target_hash": "80e8a94a00b5f8c6cf21b74613bb3b5ef494456bdf997db18b1e1e11d6f12f07", "exact_match": 0.0}
{"doc_id": 925, "doc": {"question": "where does half life 2 episode 2 take place", "answer": ["outside City 17"]}, "target": "['outside City 17']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does half life 2 episode 2 take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" City 17"]], "filtered_resps": ["City 17"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "672d9d79a7757023e8e6c960745dc6a713383fc83db207e0b638691e4b4dd7c4", "prompt_hash": "4b56409e5f01080f27fb4f405e63780fb12a990157dea73ad25ebc0456109059", "target_hash": "0f07504a764cdddcc0a129bc6be97e02b99867fa90388627bc8047bfe9a8822f", "exact_match": 0.0}
{"doc_id": 926, "doc": {"question": "sequel to a christmas story it runs in the family", "answer": ["My Summer Story"]}, "target": "['My Summer Story']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: sequel to a christmas story it runs in the family?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" A Christmas Story: Marley's Revenge"]], "filtered_resps": ["A Christmas Story: Marley's Revenge"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fcf88516dcfc60dcb559f3e36234dde1e28b94a5f53e1753499634a95f7002e1", "prompt_hash": "02db4c819d977fbcb12c1a0b8adc4276b260585786576d9577ab252743529c15", "target_hash": "6b1d18d3bb9531a879507c28480c95022c9be4e029c0924c8811f13c156e1acd", "exact_match": 0.0}
{"doc_id": 927, "doc": {"question": "who sings the song rock you like a hurricane", "answer": ["German rock band Scorpions", "Scorpions"]}, "target": "['German rock band Scorpions', 'Scorpions']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the song rock you like a hurricane?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mötley Crüe"]], "filtered_resps": ["Mötley Crüe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "37b2e7a045c1ab24c6d34a2ef74048c692792553aa2e4ff7a4f821682fdecfe2", "prompt_hash": "ced0d71e94da76b3a3c29240929f06ee5af37626e236980bd4e6872161dfd626", "target_hash": "7b22535961600b6cb1782e5e1a50e9d565961dda463372277e78a36148536b42", "exact_match": 0.0}
{"doc_id": 928, "doc": {"question": "who played bailey in the sisterhood of the traveling pants", "answer": ["Jenna Boyd"]}, "target": "['Jenna Boyd']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played bailey in the sisterhood of the traveling pants?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Amber Tamblyn"]], "filtered_resps": ["Amber Tamblyn"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "73270fb6938f46f7a9930f8ec6af89be9f917a43470a61c4e0b70cf0c04dccb6", "prompt_hash": "df96244a8fcdcdc137f72822a02acfd4760817e34d0d643d16449181565b983d", "target_hash": "086fd4d21ea3eca27e5048c6141fa9a395d0eff34da2437efd0e9de36b88d0ea", "exact_match": 0.0}
{"doc_id": 929, "doc": {"question": "when does season 18 of law and order svu start", "answer": ["September 21, 2016"]}, "target": "['September 21, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 18 of law and order svu start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" September 22"]], "filtered_resps": ["September 22"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b3651cf908d34c9ddaaab25d04587ac72706de949d6617a307c1865eb60c330", "prompt_hash": "e7c33b00f94b98b7f2d629174e010e4af4f8050c245e777018aa7b7367d7fcce", "target_hash": "ba5aa1c985eeb41dd0d65d5e8d99f4d4b815230b3d33c3d3a7507569297537f2", "exact_match": 0.0}
{"doc_id": 930, "doc": {"question": "where does the term jack mormon come from", "answer": ["Jackson County, Missouri"]}, "target": "['Jackson County, Missouri']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the term jack mormon come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mormonism"]], "filtered_resps": ["Mormonism"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3db6a06d8b4deb39e9c2e8fd40af4435705701176c359ccb0828c6e99f71a821", "prompt_hash": "da1e1e2703cce23d9b5b990a8a154892fe1b97cd0005f6a4c654204a32f904c2", "target_hash": "ddf33a71e93bdcfa3a39f82aa4a1c786b848e922a7c97f717f64f8cf0aeb2983", "exact_match": 0.0}
{"doc_id": 931, "doc": {"question": "what kind of animals live in the great victoria desert", "answer": ["malleefowl", "water-holding frog", "great desert skink", "sandhill dunnart", "dingo", "sand goanna", "crest-tailed mulgara", "southern marsupial mole", "chestnut-breasted whiteface", "perentie"]}, "target": "['malleefowl', 'water-holding frog', 'great desert skink', 'sandhill dunnart', 'dingo', 'sand goanna', 'crest-tailed mulgara', 'southern marsupial mole', 'chestnut-breasted whiteface', 'perentie']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of animals live in the great victoria desert?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mammals"]], "filtered_resps": ["mammals"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b920ccd5ab0eecd7f4e36f166b3b3016b61e6263be8da90cc64d5c714844fe1", "prompt_hash": "f2d16cb534f311068106ed1ce62fa3b234e5ea47c1ea3848dc920d957662437b", "target_hash": "384e1b0a271f7aaed7e8c6a64a285d8d6c3ee5dd033a34a7ce84b63cff4b802a", "exact_match": 0.0}
{"doc_id": 932, "doc": {"question": "what is the coldest it has ever been in antarctica", "answer": ["−128.6 °F"]}, "target": "['−128.6\\xa0°F']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the coldest it has ever been in antarctica?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" −89"]], "filtered_resps": ["−89"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3256d9cdebf8d534b718cbb7e2282cb06632c7cf2fb34d7174e5a186f33436cf", "prompt_hash": "05706a7f397bd6a131008aadea221fd6926b764317bd9c2a191c52fcf8309ec5", "target_hash": "861de22c5b547e247f769a7bfd71a682fc52aa69aaf842aa7692cb6f6ac540e7", "exact_match": 0.0}
{"doc_id": 933, "doc": {"question": "authorities involved in formulating accounting standard in india", "answer": ["government department", "FICCI", "other professional bodies", "ASSOCHAM", "CII", "Accounting Standards Board", "ICAI", "academicians"]}, "target": "['government department', 'FICCI', 'other professional bodies', 'ASSOCHAM', 'CII', 'Accounting Standards Board', 'ICAI', 'academicians']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: authorities involved in formulating accounting standard in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Institute of Chartered Accountants of India"]], "filtered_resps": ["Institute of Chartered Accountants of India"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55d29a49f63df5c82b595847752746b972191d762a3fb170f20b69906270fb28", "prompt_hash": "3bd197ab4b0077287b2e31d6157eb4a3ea0b18141ab5045356136053621a0ef3", "target_hash": "c7c876fcd6923bebd25a137076aea219aa2577d247c7ee9dd40f7bad369d2470", "exact_match": 0.0}
{"doc_id": 934, "doc": {"question": "what kind of metric system does the us use", "answer": ["international units", "United States customary units"]}, "target": "['international units', 'United States customary units']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of metric system does the us use?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" US customary units"]], "filtered_resps": ["US customary units"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "25f86bcae845dfe71f257342e3bf84e086c2a7cf0a6f3e2aadd29ffd6fefd9db", "prompt_hash": "e16b792bacece9efe7667212047615f56691970e7688c185ef7416bc383a38a5", "target_hash": "bd1d71e99335d711b57015ca97ebd49941af939802cbe13f02692dffc4620b83", "exact_match": 0.0}
{"doc_id": 935, "doc": {"question": "who is the president of the republic of zambia", "answer": ["Edgar Lungu"]}, "target": "['Edgar Lungu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the president of the republic of zambia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Edgar Chagwa Lungu"]], "filtered_resps": ["Edgar Chagwa Lungu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e51f7cdf6075064b1e3aeacbda8f8aaa29eefe62b613507076e7a05878ee78d", "prompt_hash": "7c46a97340ce02589f30914e14061f1549ac48631fadf7eb02fd444390369815", "target_hash": "ae44a5c7fcf5ef7d278909c418e88d6ed719a860fa1b4d4bb71d2060c7611947", "exact_match": 0.0}
{"doc_id": 936, "doc": {"question": "in another world with my smartphone japanese title", "answer": ["異世界はスマートフォンとともに"]}, "target": "['異世界はスマートフォンとともに']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in another world with my smartphone japanese title?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Another World with Smartphone (スマートフォンの世界へ"]], "filtered_resps": ["Another World with Smartphone (スマートフォンの世界へ"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b5814171344b717a2ae5d2d43282d08eafadb3d39c6843ff4403962f391481f8", "prompt_hash": "2e4810bd7756c131f15ededcfa8b4d0d3ef5f7ac4e316ddf40cd8b396c508ab0", "target_hash": "aab2284f4685a745a2e63c57f3ffbf1513291dfaeaa4ee7d9532fa09bc8d0095", "exact_match": 0.0}
{"doc_id": 937, "doc": {"question": "who are the permanent member of un security council", "answer": ["China", "United Kingdom", "United States", "Russia", "France"]}, "target": "['China', 'United Kingdom', 'United States', 'Russia', 'France']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the permanent member of un security council?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" China"]], "filtered_resps": ["China"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9dbf81cf5957d9e5c95cd186c02db1f2ac31f6b05dfd23f3d712a549318f0a44", "prompt_hash": "167b2b921e1afbd3001fa6244e9ffd6e28f6928926b3b84cf77521a1c09089ce", "target_hash": "6f4a75cafc5d05df2565e617a2552cc8f73dda2c97125d34ad56f7cc28885404", "exact_match": 1.0}
{"doc_id": 938, "doc": {"question": "who designed the earth day flag in 1969", "answer": ["John McConnell"]}, "target": "['John McConnell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who designed the earth day flag in 1969?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul Jensen"]], "filtered_resps": ["Paul Jensen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a35e7ad50545cdf232ff8d12d68a64aa8ba1876b1d4c3b841fdb2e20bfabe58b", "prompt_hash": "727db040b820149d8687d4ebcd8edbae4ab010d4381843b78ed9592ecbf061e1", "target_hash": "e1181776486a5181f8c015d9fef009ee30abfef1e7d9dcf882fe970f74d29600", "exact_match": 0.0}
{"doc_id": 939, "doc": {"question": "when was the last time villanova won the ncaa basketball championship", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time villanova won the ncaa basketball championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2016"]], "filtered_resps": ["2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "afffc57bcc41dd8db7746fb533a085e323ff4603e34a3399fc3b1b9d412841a0", "prompt_hash": "ecc26672c3a513e435adfee43726f2d45373c0be95cdf1fe9236279582616a4f", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 940, "doc": {"question": "what is the biggest determinant of climate in venezuela", "answer": ["altitude"]}, "target": "['altitude']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the biggest determinant of climate in venezuela?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mountains"]], "filtered_resps": ["mountains"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "23fac47f94a9f41055a269bb8c5e681ea9f52d90464d63d9a8089d7d8baeb91c", "prompt_hash": "3c225b6c6dd6e344a8711dd18635beff5d3d7efc1fdbb28a0b1bcd6d6dddd0b2", "target_hash": "e725aa2a6b230542dc85c393d9f62b3434b59d269a4565528ed919f452f87462", "exact_match": 0.0}
{"doc_id": 941, "doc": {"question": "the king of kalinga in the mauryan period was", "answer": ["Ashoka"]}, "target": "['Ashoka']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the king of kalinga in the mauryan period was?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ashoka"]], "filtered_resps": ["Ashoka"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ab7366217bee9ee811eff7e36a54fb5e29408f4168a37c01c3a4f170331b0e05", "prompt_hash": "12b1a1170c2e2ea329da70b767c0616ee89de3b58331cb7089489bd6adb83f8a", "target_hash": "6172951230bd4fb14e15ce8f5ba6439e9c8c7ae87fd0644795f2bcd98c75132a", "exact_match": 1.0}
{"doc_id": 942, "doc": {"question": "who is the no. 1 ranked tennis player in the world", "answer": ["Rafael Nadal"]}, "target": "['Rafael Nadal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the no. 1 ranked tennis player in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Novak Djokovic"]], "filtered_resps": ["Novak Djokovic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7217c539faa8228082e749164ce8f9b52d60d24b306b3a903994344eab3f994", "prompt_hash": "4403ecaa34a3acb71f7d60182f17b73015a1fb86982a0f70c5d28dbe0493c834", "target_hash": "b7d195251a4b3f626fc43d60f7a8f31653df1dc358e3c647ec40669c451bcf3d", "exact_match": 0.0}
{"doc_id": 943, "doc": {"question": "where was the capital of the habsburg empire located", "answer": ["Prague", "Vienna"]}, "target": "['Prague', 'Vienna']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the capital of the habsburg empire located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vienna"]], "filtered_resps": ["Vienna"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ef2e7b9a58a079c85723dc79a35bafee821b88b345a2a7c1b5fda42909aaf33", "prompt_hash": "7f520bb08eb73790b4b3ab481a004f7e18ae46a07e0a41840ad751253fff5a07", "target_hash": "d308edf87e7362e4e4e072f4f4997157cf196a5157f1c8ea3051144411186941", "exact_match": 1.0}
{"doc_id": 944, "doc": {"question": "which is the fastest train in india and what is its speed", "answer": ["Gatiman", "160km/hour", "Mumbai Rajdhani Express"]}, "target": "['Gatiman', '160km/hour', 'Mumbai Rajdhani Express']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the fastest train in india and what is its speed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shatabdi Express"]], "filtered_resps": ["Shatabdi Express"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cf9d00333c46678b60415dc73a81ee0c295b8b964586fae29e733e13ed79962d", "prompt_hash": "9ca2b81cf13164929140dae409ad931950ec0e4fd2a3f4db8a84e26f7fcfdeed", "target_hash": "229cba974864c2950da6bdee25007cfbd185cdda9d1f053796d2ea92c7922172", "exact_match": 0.0}
{"doc_id": 945, "doc": {"question": "who votes in the baseball hall of fame", "answer": ["or the Veterans Committee", "Veterans Committee"]}, "target": "['or the Veterans Committee', 'Veterans Committee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who votes in the baseball hall of fame?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Baseball Writers' Association of America"]], "filtered_resps": ["Baseball Writers' Association of America"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e21d78ff366b77e9decca56a6aab7bc2a9db2d2c0fa29d856028364b14eb84e9", "prompt_hash": "76b5d9a8e8f28b63199d08d1be8f6afb4ec6a00c6d81bfa144c79a6923dccc44", "target_hash": "51503ac3d46c59573453cd51de0608b8889e3b08e7be28d4c9617efb640daedb", "exact_match": 0.0}
{"doc_id": 946, "doc": {"question": "who does the voice of cruz on cars 3", "answer": ["Cristela Alonzo"]}, "target": "['Cristela Alonzo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of cruz on cars 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" O"]], "filtered_resps": ["O"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "00f9b7ab1c74a18a5668089523d394ca1c7a4c64d48d8efd56cf43823d3538fc", "prompt_hash": "ddf01c75ee48b9545a812283b294d77ae786afe8f5b2e6ee8c8d90febc1ef17c", "target_hash": "4ac5ae04838d28e02c54b341219c183d3c19a381d3fbc32c4903721b3df393fa", "exact_match": 0.0}
{"doc_id": 947, "doc": {"question": "when did hootie and the blowfish come out", "answer": ["2003", "1986", "March 4, 2003"]}, "target": "['2003', '1986', 'March 4, 2003']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did hootie and the blowfish come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1994"]], "filtered_resps": ["1994"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "57a0857bc01db6d96dc57be37b2d24408fdaa08d03da997bc6fa41f5bd7ce8a9", "prompt_hash": "b8756a289ba9cd8129dcbd3efbceb1eb7793f3ace578e8132cb2d6a2eb9d0648", "target_hash": "3d8b56e4f303379627c852846faa1547bb9edd1eb4bf32ec6581371042d8e78f", "exact_match": 0.0}
{"doc_id": 948, "doc": {"question": "who were the the continent of the americas named after logically", "answer": ["Amerigo Vespucci"]}, "target": "['Amerigo Vespucci']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the the continent of the americas named after logically?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Amerigo Vespucci"]], "filtered_resps": ["Amerigo Vespucci"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1851e0fe72028cfd8b5d08181fc9eea566e1e582001356fd9b489c3193b4044e", "prompt_hash": "e093c3a3635d354debd860055f89447529ca142a288e97eb9615a929dacc970d", "target_hash": "7cb498bccdfe86550220f9aba1e22b462cbeacdc2dfbd50d53a1f565e693ed72", "exact_match": 1.0}
{"doc_id": 949, "doc": {"question": "who is the guy that jumped from space", "answer": ["Felix Baumgartner"]}, "target": "['Felix Baumgartner']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the guy that jumped from space?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Felix Baumgartner"]], "filtered_resps": ["Felix Baumgartner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "082acbac8ce5172a53b6698832c946fb21bdcdd724d81a687eee1f1377d1b8bd", "prompt_hash": "d6ea78e7b52dc9a636b05fa66453d77f5fe0fb19d3adcb5392c8f4470bbcda56", "target_hash": "7800e0398272ed96043bf283b92f036bae7b255197556ce4098054208011662a", "exact_match": 1.0}
{"doc_id": 950, "doc": {"question": "who starred in the remake of true grit", "answer": ["Hailee Steinfeld", "Jeff Bridges", "Barry Pepper", "Matt Damon", "Josh Brolin"]}, "target": "['Hailee Steinfeld', 'Jeff Bridges', 'Barry Pepper', 'Matt Damon', 'Josh Brolin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the remake of true grit?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Wayne"]], "filtered_resps": ["John Wayne"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e8b9171f5767f8850a0347d93991aa3171945353b5e8f43d491d68499cb066c3", "prompt_hash": "3d1b4318dcb25f1861410b646b0d60f9074da697f2eb21263f294bb27c022a4d", "target_hash": "28cb7df1ca9e893c5a89e5371512f9896c0be7669d4b2aba0ad4c030c81cbff5", "exact_match": 0.0}
{"doc_id": 951, "doc": {"question": "when is the next step season 3 coming out", "answer": ["March 16, 2015"]}, "target": "['March\\xa016,\\xa02015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the next step season 3 coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "36e176f934ee5e004e8a5567eb0fa971a2245495d4feeb34e9088331803cbe41", "prompt_hash": "0293136973cf953908329fdcb6e3d6c9b31a4c3c041f467904c17ef3ed6eda53", "target_hash": "cc3c1df8d106a43f066347f7427c7afa1c0ca69d1b920b9a67780a4b33e251ee", "exact_match": 0.0}
{"doc_id": 952, "doc": {"question": "when was i don't like mondays released", "answer": ["1979", "21 July 1979"]}, "target": "['1979', '21 July 1979']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was i don't like mondays released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1982"]], "filtered_resps": ["1982"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "de80faa30e7adc3c783ff2177b52f8fd324dc13839228a34a6379089801bfbe8", "prompt_hash": "127aff06a5ca6221c9a72a47535c1aa3269b1578d1292bde0b054235bcb00061", "target_hash": "919254c7cac4dae4ce5d469a236f767f85ab2ead629d73ab3f95988227b1b742", "exact_match": 0.0}
{"doc_id": 953, "doc": {"question": "who decides where the super bowl is played", "answer": ["The NFL owners", "NFL owners", "the NFL"]}, "target": "['The NFL owners', 'NFL owners', 'the NFL']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who decides where the super bowl is played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" NFL"]], "filtered_resps": ["NFL"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "727563a44393b6fc19a02cab81262bc8bf59b45cfb88fca4f663ed598936935f", "prompt_hash": "648d720f7573fff01222cbe91c51fc82c2bdb31217883249f90d2057a5f5d36f", "target_hash": "b1a75c81659a7f16af422673f28c84066454ee0d51ef6f2a5fdec72ba63c5fa1", "exact_match": 1.0}
{"doc_id": 954, "doc": {"question": "when did the wall go up in east germany", "answer": ["), starting on"]}, "target": "['), starting on']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the wall go up in east germany?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1961"]], "filtered_resps": ["1961"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6438390e1b51d743c65690d5589cb5d01120e60eca4f846b48a74fc1a4a12de6", "prompt_hash": "e1d89d5708d5f3c48ca6d163699658715a26c693c5c55bf6092c63c926401f3b", "target_hash": "99ab9cc1221eae30b85e2d51e18438e64bcb13f3f61b2a7645045460a29637d6", "exact_match": 0.0}
{"doc_id": 955, "doc": {"question": "who is the longest serving manager in manchester united history", "answer": ["Sir Alex Ferguson"]}, "target": "['Sir Alex Ferguson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the longest serving manager in manchester united history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matt Busby"]], "filtered_resps": ["Matt Busby"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92e164c1b2e7e5a7213c71ae870290ef31569c9d29529ff5adc56627689425d6", "prompt_hash": "34bdefde7ae5bc679f676f4f20c77a9ee00efbb0abae7d2ab3c8767ba393f18c", "target_hash": "e9dc204c4e1cfff0610169a6efd43f8cd8b9b7ae3f51cd1963ee82d456655bf6", "exact_match": 0.0}
{"doc_id": 956, "doc": {"question": "what river is associated with the city of rome", "answer": ["The Tiber", "Tiber"]}, "target": "['The Tiber', 'Tiber']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what river is associated with the city of rome?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tiber"]], "filtered_resps": ["Tiber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7f561c0a6f2cfabe00678d1802d0be27dfddbf9b7380fef520954a012b9d616", "prompt_hash": "b20ba0e100cdd444ffe60e3a059845c78043bf0442858737163fcedfc86a876a", "target_hash": "463d37f8add16ba60f40a639d510dfa9b936ed8af0c6b3e13525ede53f31cf24", "exact_match": 1.0}
{"doc_id": 957, "doc": {"question": "what is bermuda competing in the winter olympics", "answer": ["Cross-country skiing"]}, "target": "['Cross-country skiing']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is bermuda competing in the winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" bobsleigh"]], "filtered_resps": ["bobsleigh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "adec657f0d535fe5fc6935b6e21d8d88029843f4e04adfd4f3d04afea3c18c80", "prompt_hash": "d41779e264671c52c822b77235e1e427f5de24f4309bacdae452573cd2b7a2dc", "target_hash": "c4eb1d1274b37ec64c9bbd3dd4a581fd99eeb5dbb2f15febb9d2bd6773d3c941", "exact_match": 0.0}
{"doc_id": 958, "doc": {"question": "fictional character whose favorite song was blueberry hill", "answer": ["Richie Cunningham"]}, "target": "['Richie Cunningham']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: fictional character whose favorite song was blueberry hill?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Elvis Presley"]], "filtered_resps": ["Elvis Presley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "26c42ee0a0e382c9ea1c13ee7d22a6967d851aefc53ece471a309c94bca3cef4", "prompt_hash": "2ac4a853b0b523df973c61b481c50f74cf0e96706882b5b92ae79ec86a446a49", "target_hash": "7d1c59fbac5bed7a75aa8c3155b059918f5f31ae15b11c04c7f0597e6d06452e", "exact_match": 0.0}
{"doc_id": 959, "doc": {"question": "where is cinque terre italy on a map", "answer": ["Liguria, Italy"]}, "target": "['Liguria, Italy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is cinque terre italy on a map?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Liguria"]], "filtered_resps": ["Liguria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3856c56ee5c9ab71cd1e109fe982d6f91b4616fea91253fcc2a4dc7b7755dd74", "prompt_hash": "253e6a05604bc9aae1156845bdbbaf20430288215eff2ff93d66a3b4bfef1bd0", "target_hash": "4d49f821ba929103b88486bbd92a5cc2faef23a878ef5162989f9bd95f126e16", "exact_match": 0.0}
{"doc_id": 960, "doc": {"question": "who is the highest paid baseball player in the major leagues", "answer": ["Clayton Kershaw", "Alex Rodriguez"]}, "target": "['Clayton Kershaw', 'Alex Rodriguez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the highest paid baseball player in the major leagues?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Clayton Kershaw"]], "filtered_resps": ["Clayton Kershaw"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a437bda505842b3bd3d1f856085fd2b4daf7e8f77184f148ea3e983a049a41e0", "prompt_hash": "7aff5934dedb1e048eb33a6519bfa2c341659fe69b159b0602fa24168eddafad", "target_hash": "2706323d83b30bae0c8b18fc16d8e07da814ce8fb3c9d8e3adce8a8e7918f871", "exact_match": 1.0}
{"doc_id": 961, "doc": {"question": "what is the average height of a chinese man", "answer": ["5 ft 6 in"]}, "target": "['5\\xa0ft 6\\xa0in']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the average height of a chinese man?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 172 cm"]], "filtered_resps": ["172 cm"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96308ac13ce391118c1045c149fcb410c91a3a511c819d04c2f8908eda1ff149", "prompt_hash": "6c3f38c8a8ef0a3ca232592cfe74c937dda681f2f51c4fa26b9bd191c0460cec", "target_hash": "8d383a23153247fdeadcfaeb64159d7e3d766182689666e435c3a216f8b47645", "exact_match": 0.0}
{"doc_id": 962, "doc": {"question": "where does us highway 1 start and end", "answer": ["Fort Kent, Maine", "to Key West, Florida", "Key West, Florida"]}, "target": "['Fort Kent, Maine', 'to Key West, Florida', 'Key West, Florida']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does us highway 1 start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Atlantic City"]], "filtered_resps": ["Atlantic City"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb9e0103789abbb7529ef04fe8423b1c3aab87ce31dcadac85b7e2db96da6b3a", "prompt_hash": "0daba073f3f14766f693a0048b33417277446d39da05d72d298a50f29b17b10e", "target_hash": "da6ead520db40b46fa367f51ddd470e778740b916ef0fd4acad6bcb0bdb29380", "exact_match": 0.0}
{"doc_id": 963, "doc": {"question": "who laid foundation stone of islamia college peshawar", "answer": ["Haji Sahib", "Haji Sahib of Turangzai"]}, "target": "['Haji Sahib', 'Haji Sahib of Turangzai']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who laid foundation stone of islamia college peshawar?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nawab Sir Syed Ahmad Khan"]], "filtered_resps": ["Nawab Sir Syed Ahmad Khan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1f1693e7f9d34bc677e162fcb2edc561218893c7804d70836d3dfcb5f8396277", "prompt_hash": "ed05d911c0e7ad42f32cc2a6c2129b32d1721584323c93be5dbdee1d9a311f0d", "target_hash": "f6969defb6a7d85b31ef400075485eac9fb648fa649b8c59f174cee2f907b4ef", "exact_match": 0.0}
{"doc_id": 964, "doc": {"question": "what is the big gold dome in jerusalem", "answer": ["Dome of the Rock"]}, "target": "['Dome of the Rock']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the big gold dome in jerusalem?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dome of the Rock"]], "filtered_resps": ["Dome of the Rock"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d393acb0ed7204d1faaa90113a8e502c78f99f8414433c67d51ae788d05702e9", "prompt_hash": "7e2d1660f0920897b729b7c8c7248809a8c9ad210fed2294d58160d599f5575c", "target_hash": "02a9907b426babf5e9c962095859637ab26006ef9332ce09c968ce0cae333900", "exact_match": 1.0}
{"doc_id": 965, "doc": {"question": "when did holland become involved in world war 2", "answer": ["15 May 1940", "10 May 1940"]}, "target": "['15 May 1940', '10 May 1940']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did holland become involved in world war 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" May 10"]], "filtered_resps": ["May 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b5b925faa4ee62a02af75362dbb4a7a2840f1d1fdade6764063267be9206734b", "prompt_hash": "fb0bad398dcbe13d52d166bcad9022552e0019d5b6017636bfb7f51b69e5194d", "target_hash": "e60932ec4e289f82ff7ec1dd6bd7c625bd9dd083ac82d75dae0b90b04bbe9bd9", "exact_match": 0.0}
{"doc_id": 966, "doc": {"question": "which philosopher advocated the idea of return to nature", "answer": ["Bacon"]}, "target": "['Bacon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which philosopher advocated the idea of return to nature?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jean-Jacques Rousseau"]], "filtered_resps": ["Jean-Jacques Rousseau"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "21f39f344d56512e87cfb3b9c5232f0196359912d057264c26338568935df672", "prompt_hash": "91fffb0cf3c7dee7381d99f327649545e255e7e98321e5560488bda2cd797cc3", "target_hash": "53a1d623a51ed80cd2382beb66386a98fbb55bc9bbdebbe58664be1e3587d0cc", "exact_match": 0.0}
{"doc_id": 967, "doc": {"question": "who led the conquest of the incas in south america", "answer": ["Francisco Pizarro", "conquistador Francisco Pizarro"]}, "target": "['Francisco Pizarro', 'conquistador Francisco Pizarro']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who led the conquest of the incas in south america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Francisco Pizarro"]], "filtered_resps": ["Francisco Pizarro"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "769746c976065011d6653d049b0016cae9584903fb6e00525026c76af6880f33", "prompt_hash": "6aadc1bfdfb96ec70576e03d8c9ee2286a730a0e293deb6606fabdc4a278c373", "target_hash": "0abe5706a7439859120289b0efacdc17689dfc2557ffe77e036661a342873900", "exact_match": 1.0}
{"doc_id": 968, "doc": {"question": "where does the pulmonary trunk receive blood from", "answer": ["the right ventricle", "from the heart", "the heart"]}, "target": "['the right ventricle', 'from the heart', 'the heart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the pulmonary trunk receive blood from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" right ventricle"]], "filtered_resps": ["right ventricle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "65d4a5157339cdc8f3e3bbc8e1e015a2671e77205a695fc5943d254e2b79aa48", "prompt_hash": "03631ae50e5d96660814f925c805da85df8544d2560588d16b598a0561cdb9ca", "target_hash": "48079c6019a8b67af72320edd1bf63b0279cc91c5e0b216b71b89f9c428b5eb5", "exact_match": 1.0}
{"doc_id": 969, "doc": {"question": "what is the symbol for christmas in brazil", "answer": ["Nativity scene", "Christmas Tree"]}, "target": "['Nativity scene', 'Christmas Tree']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the symbol for christmas in brazil?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Noel"]], "filtered_resps": ["Noel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a8b69898fbdc97fc543bd31f6e2c258a8be6d0bec92a0384b27f6c25372c61de", "prompt_hash": "7eb6a9f3f04e32dc99442c060963732ade3c083ff727d630e4c93b22f85ca725", "target_hash": "0fef3a876d988b8c4a772241304229dc042732b6831b01a961763f3e79f04f7c", "exact_match": 0.0}
{"doc_id": 970, "doc": {"question": "what is the genus of a bald eagle", "answer": ["Haliaeetus"]}, "target": "['Haliaeetus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the genus of a bald eagle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Haliaeetus"]], "filtered_resps": ["Haliaeetus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fc77e44aa9ec5ce7acee9bbf170172b2f646dd202a3ca7640213d22e3c5d8e0a", "prompt_hash": "eb9e407cd4683c5e1394724c3f0f3cc827af5095dfab35c64df70eeaeac1046c", "target_hash": "b44dd28b4e67cddb242434552f847cc0e7a00f78cfb223b417b259d6ee60336b", "exact_match": 1.0}
{"doc_id": 971, "doc": {"question": "when was the last time the los angeles lakers won a championship", "answer": ["2010", "in 2010"]}, "target": "['2010', 'in 2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the los angeles lakers won a championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2009"]], "filtered_resps": ["2009"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0b05e6c9a5330cf8a189d908dd9477a8f24b64e91372ad74f01223cfce4d9c5e", "prompt_hash": "60e639fe1e71a1f83d43c91901c8d5bae5778f36fb0b983022ee9ce409c303c5", "target_hash": "96e8e104d923fe58102f2ff28487ea646621131b12429dee43397c0ab240158e", "exact_match": 0.0}
{"doc_id": 972, "doc": {"question": "what is the collection of the districts to the east of the jordan river", "answer": ["Jordan", "Golan Heights"]}, "target": "['Jordan', 'Golan Heights']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the collection of the districts to the east of the jordan river?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Transjordan"]], "filtered_resps": ["Transjordan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2ba0aba136a24f661b443e9a61bff6ba19ac07ac47cfb18e1ec7dd07e305eff6", "prompt_hash": "59e594d5500d1fcffa4f508dacbd1a7090a1e8ddc726c34632812ff4fe8ca072", "target_hash": "73b4205f8907e5bdf0fba962f7ba9601a6f9c2baa885b35d5962c2353f8d6cdc", "exact_match": 0.0}
{"doc_id": 973, "doc": {"question": "who plays the characters in yo gabba gabba", "answer": ["DJ Lance Rock", "Gooble", "Brobee", "Foofa", "Muno", "Plex", "Toodee"]}, "target": "['DJ Lance Rock', 'Gooble', 'Brobee', 'Foofa', 'Muno', 'Plex', 'Toodee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the characters in yo gabba gabba?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Roiland"]], "filtered_resps": ["Justin Roiland"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9c2ff200e31f0dc7168e1595712e994aeae9e5ba2dd6a4283b00fa09c743955", "prompt_hash": "53cb96199b2dcce74e76809b2fbac14b7e316b51b02a7ba611fc2d376a67d7c8", "target_hash": "9b4262f3cd4df38cb714fe80b5f5ee056205307782ed1da5ff9a2a9202c03917", "exact_match": 0.0}
{"doc_id": 974, "doc": {"question": "who won oscar for best director this month", "answer": ["Guillermo del Toro"]}, "target": "['Guillermo del Toro']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won oscar for best director this month?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Damien Chazelle"]], "filtered_resps": ["Damien Chazelle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d31a9eb87df0ead0fef42de19622537d0624561c86d527fe7913937e8c487437", "prompt_hash": "8403a1a797338176c6e9b91c9d7fbfe3ec1046f5622ae02f5d46849c52459a19", "target_hash": "14d633216164e3a21da75c3fa307e9d45b20fc4eb7ec7a573e829a258301bccd", "exact_match": 0.0}
{"doc_id": 975, "doc": {"question": "what type of reproduction do whiptail lizards use", "answer": ["parthenogenesis", "Parthenogenesis", "parthenogenic"]}, "target": "['parthenogenesis', 'Parthenogenesis', 'parthenogenic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of reproduction do whiptail lizards use?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sexual"]], "filtered_resps": ["sexual"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ebccb645ab2e966fcb7121e558774574d2ff366da174d8173e07c145b0e5a8c", "prompt_hash": "80879efc861ed038061f4291ec4c3c198d47ea0771379ddef6ced60fb486036b", "target_hash": "6dcea78acff9fe9fdd9f9009e83fe3e251947e83e673cccb0647a7881c51cae6", "exact_match": 0.0}
{"doc_id": 976, "doc": {"question": "who sings love will keep us alive by the eagles", "answer": ["Timothy B. Schmit"]}, "target": "['Timothy B. Schmit']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings love will keep us alive by the eagles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Linda Ronstadt"]], "filtered_resps": ["Linda Ronstadt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4a6a43a590e919f1a767c37e165d56533937717a2279fe8ad40dcb9704a017fd", "prompt_hash": "0c47416d04b1a514bef108525ce812c3374dadabe8a3a77f8c5d7cd8ce76a3b0", "target_hash": "99f3e7e844bff6b7576474a96f6c357d8682a16cb4051b74aed6233cc02c3645", "exact_match": 0.0}
{"doc_id": 977, "doc": {"question": "symbolic interactionism is the basis for which theoretical model", "answer": ["role theory", "versions of Identity Theory", "sociological"]}, "target": "['role theory', 'versions of Identity Theory', 'sociological']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: symbolic interactionism is the basis for which theoretical model?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" conflict theory"]], "filtered_resps": ["conflict theory"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bd8178fbbf95b1d3064ce25dedd15a3ff1b921ea1f51ae401fe2c7e565ef9252", "prompt_hash": "7dd61b63a072d439002a5fb0ce7299ff58aab5e604a7abd452a4f483679619b6", "target_hash": "38edf6d8ade951fed815a7f05bafa0a75c51ecbbfc84c39b5e02dc85683cdcbf", "exact_match": 0.0}
{"doc_id": 978, "doc": {"question": "who hung the lanterns in the old north church", "answer": ["Robert Newman", "Captain John Pulling"]}, "target": "['Robert Newman', 'Captain John Pulling']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who hung the lanterns in the old north church?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul Revere"]], "filtered_resps": ["Paul Revere"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9118cc98ff7842c632146346d3715d0345fae3d7522c018f2065a2fe5ec3c594", "prompt_hash": "60a256352d9defe9aaed095a34605cbb4b1ec623c9c534c7ad22a0b7c3105165", "target_hash": "c30d8f64c42b7307ed2e90a88ae507053c05d92c7934f6420bc9d634b4c7dbf1", "exact_match": 0.0}
{"doc_id": 979, "doc": {"question": "who designed the garden city of new earswick", "answer": ["planner Raymond Unwin", "architect Barry Parker", "Raymond Unwin"]}, "target": "['planner Raymond Unwin', 'architect Barry Parker', 'Raymond Unwin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who designed the garden city of new earswick?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ebenezer Howard"]], "filtered_resps": ["Ebenezer Howard"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cf99560884ccc20ef6c2129a24082d6c6d3be6b149da00fddb64cf8a160a133d", "prompt_hash": "9e2ead20c4a9e447a588784039a37536000d11f0a1d5559ea63d1052e2cc9039", "target_hash": "d3963bec37b36e6a6474dbb9a039f69b49d6b3c444e6ebf27881b1bd194d39d1", "exact_match": 0.0}
{"doc_id": 980, "doc": {"question": "where is the university of wisconsin madison located", "answer": ["Madison, Wisconsin"]}, "target": "['Madison, Wisconsin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the university of wisconsin madison located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wisconsin Madison"]], "filtered_resps": ["Wisconsin Madison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "514a8082c1d2314384d87e70a3f63d31d6da9741a4b26144039d5328276497e9", "prompt_hash": "5c40e48997a714592460df60fed79f8c3057f9cdd0f0f791dbc43954f8a537cd", "target_hash": "fdb6d6bdc181feef756ded76effda4e30399be0584ef972bb2fdd4d970609c08", "exact_match": 0.0}
{"doc_id": 981, "doc": {"question": "where is dia de los muertos celebrated in mexico", "answer": ["the Central and South regions", "October 31"]}, "target": "['the Central and South regions', 'October 31']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is dia de los muertos celebrated in mexico?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dia de los Muertos"]], "filtered_resps": ["Dia de los Muertos"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bede6217dddf7d19db4bba31ed26436ddaff51febc2ec9afa326a08d14c3430b", "prompt_hash": "9ace3bf3d6019655579e3261285f149ac01bb628c3d05771cea79d66b924b620", "target_hash": "95a02b838728b26a94c54b662d79ac4418459779a84c292ae80daa16358badd0", "exact_match": 0.0}
{"doc_id": 982, "doc": {"question": "a town in west yorkshire on the river aire home to a rugby league team", "answer": ["Castleford"]}, "target": "['Castleford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a town in west yorkshire on the river aire home to a rugby league team?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Leeds"]], "filtered_resps": ["Leeds"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b460f120fe5de32938665fa06806cf8aeeab31203d4893faba3924bca3992379", "prompt_hash": "a207f2415295ed3d2e8d7a722723fdde9146a0e370b6efafa2964a583a36c24f", "target_hash": "31767b36bdfe71c8f6f03f67ad2726a28f0b40db7b3da43c5c27b2f698907eca", "exact_match": 0.0}
{"doc_id": 983, "doc": {"question": "how many seasons of the glades were made", "answer": ["four"]}, "target": "['four']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of the glades were made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" three"]], "filtered_resps": ["three"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f0054dad66db1601ba870e7bb590eac4fc904788eadcbefd041612e4847147db", "prompt_hash": "5545206eb3c7b52674ab19c61c71529c2b8ea3a525c33e410626e24861b27059", "target_hash": "c0b12eaa48bad2eab0833b330f1f9a7a1caf1ef07c30f4fc2a1bd97e38e98618", "exact_match": 0.0}
{"doc_id": 984, "doc": {"question": "who played the mom in the partridge family", "answer": ["Shirley Mae Jones"]}, "target": "['Shirley Mae Jones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the mom in the partridge family?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rosemary Murphy"]], "filtered_resps": ["Rosemary Murphy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "911e6fd3c1ec23d5692b5c196a77d53d1eddc48995c089028cc22c5c32a99ebb", "prompt_hash": "db83983e9fa13fb9781795fab9a1538c36e4f731cecd147bbd6d9a8fd0e7109c", "target_hash": "068579a347a9ce52c392d2d79a38bfa1c918f990a522402e7d88a8fe3e0bee41", "exact_match": 0.0}
{"doc_id": 985, "doc": {"question": "who won the academy award for best original musical score", "answer": ["Alexandre Desplat", "The Shape of Water"]}, "target": "['Alexandre Desplat', 'The Shape of Water']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the academy award for best original musical score?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alexandre Desplat"]], "filtered_resps": ["Alexandre Desplat"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6a2045694e5a5e43f56c03cea93b3cf7f2cf2599e1f2266f2b76c1a2908032cf", "prompt_hash": "c20c021c5335a0bbdbf9e422c14a35b22977edb6ae2962e6e129377b22fefa07", "target_hash": "1b5d0cbc43514d77d2c372fea081920354bc012a2d558d38545db330e4b0115d", "exact_match": 1.0}
{"doc_id": 986, "doc": {"question": "where are the highest average incomes found in north america", "answer": ["Virginia"]}, "target": "['Virginia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the highest average incomes found in north america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Canada"]], "filtered_resps": ["Canada"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b566c8b86f6a0bfb343bb3625f33f708cf1d31ddd2341265103b6f936b3f570", "prompt_hash": "15e1242cf468a6c9d3db3dff37fbccb292ca0c1a477ba378946b597d9999a82b", "target_hash": "8b605053708c94d2f8829601fdf3d6121e791a22e056104f877b2266253da5bc", "exact_match": 0.0}
{"doc_id": 987, "doc": {"question": "what hangs from the trees in the bayou", "answer": ["Spanish moss"]}, "target": "['Spanish moss']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what hangs from the trees in the bayou?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spanish moss"]], "filtered_resps": ["Spanish moss"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "861abb73a423230e6c32c7daa89167e68870764c00ee111911ab03cdc31a779b", "prompt_hash": "d2e58de10a7a4eabb1dab68708ed249f0d2ea2fd03ab85301fe6b55dfbeb7010", "target_hash": "447f1eb30db603aca1a2573dc7cfe68c5692d2d01a1e6d65fd766674b86cea5a", "exact_match": 1.0}
{"doc_id": 988, "doc": {"question": "where is the food stored in a yam plant", "answer": ["an edible tuber"]}, "target": "['an edible tuber']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the food stored in a yam plant?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" roots"]], "filtered_resps": ["roots"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "73eb64dc5b9a172217c862d95f01be262722e6815613a697b6dd32f998e17c90", "prompt_hash": "f5abfc541353cc96f494d918138738c340c4830d476bb84238d45a83fdd80344", "target_hash": "396ed4a0c494f87d8eb3dfa4116f662dcd6c230279429855687ef5ce79de5298", "exact_match": 0.0}
{"doc_id": 989, "doc": {"question": "whens the last time the patriots played the eagles", "answer": ["Super Bowl LII"]}, "target": "['Super Bowl LII']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: whens the last time the patriots played the eagles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a63596212c8765f4018c4fc57d7d1e75cc2ad8be9e83942b7c66d5034933ea24", "prompt_hash": "f7b757e50f29142e3c76e2009c895c86b39c34a2b4f140251c0c7d033eb2664e", "target_hash": "4ce78f59d65d03e2f7288cbabcade727d1f49c50f9d5c06dece175258b25402a", "exact_match": 0.0}
{"doc_id": 990, "doc": {"question": "another name for the torso is the blank region of the body", "answer": ["trunk"]}, "target": "['trunk']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: another name for the torso is the blank region of the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" trunk"]], "filtered_resps": ["trunk"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "65790b002de12f6a3b154a442d1948cab29079ad82ce1393271845f6e3c8d3b1", "prompt_hash": "499ee457ca98bbb797be8780fcad094c4ba386b333c22a31c030af0cecc4e9e8", "target_hash": "68ff003941fcd91f954aab00305b3db59cdcca0c2347b3c272928a5a253c0335", "exact_match": 1.0}
{"doc_id": 991, "doc": {"question": "who played the oldest brother in 7th heaven", "answer": ["Barry Watson"]}, "target": "['Barry Watson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the oldest brother in 7th heaven?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David James Elliott"]], "filtered_resps": ["David James Elliott"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c1154edbaa694e4f754ae92d1d0fe998b2c7f4e8e4411c0a677db6dc6a94b2d0", "prompt_hash": "be124cf04b258d87a36afd25f577694ba2fe787c3caf6e04d5768fba97d34bfc", "target_hash": "923461c5fa575995258b0ee2a772a29d24f641283661fb90f5c321b9c326a323", "exact_match": 0.0}
{"doc_id": 992, "doc": {"question": "how many seasons of from dusk till dawn are there", "answer": ["third season"]}, "target": "['third season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of from dusk till dawn are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3"]], "filtered_resps": ["3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d3451d709715db6f2cabdb4bc4abbd1fd13b02723ee9ca1f123ca1e073ee2ed", "prompt_hash": "d5b70c784a665154c13554d16e3157d16ec17f18fb8bb50deb2ae119653a8474", "target_hash": "1272a70cf65f8d16bb0b46a3f37707a0a32adab97b2c39ff7db1ea9108c32598", "exact_match": 0.0}
{"doc_id": 993, "doc": {"question": "what is the female lion called in lion king", "answer": ["Nala"]}, "target": "['Nala']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the female lion called in lion king?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Simba"]], "filtered_resps": ["Simba"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad2e131affa49aaefbfd154797eee5e0825530a0b2053ac82e130b58997a129d", "prompt_hash": "a64e102f88c6a1352b8a31f63b1723bc54b4648b8548dabbdd66dfcdddbab949", "target_hash": "c5e382528ae45a7e2b1adf4fa9b57c6c41beb24176e07f1751119e89f050c629", "exact_match": 0.0}
{"doc_id": 994, "doc": {"question": "when did martial law in the philippines end", "answer": ["1981", "81"]}, "target": "['1981', '81']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did martial law in the philippines end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1981"]], "filtered_resps": ["1981"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0723a4de3faa0836692aaa2f3342daa52a964f36d9f065dfe0e3225136fff57d", "prompt_hash": "cc1e131459f5fd379fc78570c2858db3e146c0556fdb66a58b0a24e7fbaeab39", "target_hash": "b3a99334b3782777bd7576ecbbdfc1a89e4b98792ae568a6c3b84cfbc70440a5", "exact_match": 1.0}
{"doc_id": 995, "doc": {"question": "number of degree of freedom for plane mechanism", "answer": ["six", "two"]}, "target": "['six', 'two']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: number of degree of freedom for plane mechanism?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "838dc7ea07ab954641346c3c786143f2bc613895f803e599f0953fe01f995a91", "prompt_hash": "48a2291ca41027f1077fc1ad3d65029c10699aadf6ac800a18af6bdb219c57a5", "target_hash": "70fda02f29ac3a3c54dbbebe381a0850c4c714cf60396cc66a14c87326d6c960", "exact_match": 0.0}
{"doc_id": 996, "doc": {"question": "name of volcano that erupted in iceland in 2010", "answer": ["Eyjafjallajökull"]}, "target": "['Eyjafjallajökull']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name of volcano that erupted in iceland in 2010?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Eyjafjallajökull"]], "filtered_resps": ["Eyjafjallajökull"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c2b0385dc7ae4f6eb68e1be791e22a889a34111d5dcb3ff3c81453fb2f17cfae", "prompt_hash": "62eb9d42d2332add073a5f3398169ecc563992a25b143adc1d9939dad74f6cf6", "target_hash": "5d7d61574c7ca1da7c8d6058c5c132a3c54f92e53143b08760cfa537040920c9", "exact_match": 1.0}
{"doc_id": 997, "doc": {"question": "where does summer of the monkeys take place", "answer": ["Oklahoma", "Missouri"]}, "target": "['Oklahoma', 'Missouri']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does summer of the monkeys take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Florida"]], "filtered_resps": ["Florida"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c2398d4063bf8f358b8195239d33b35207b3101bcd07329f7589f3c77674b330", "prompt_hash": "3c62f8f023bdf21c387010efc7e35ea65dceb5f06aa4f67cc3ae196e8202b8af", "target_hash": "b5fab8e6e1ccb26f5351a156e771c3148ffc844cbe03752877cfd092c8616295", "exact_match": 0.0}
{"doc_id": 998, "doc": {"question": "who played young monica in love and basketball", "answer": ["Kyla Pratt"]}, "target": "['Kyla Pratt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played young monica in love and basketball?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tatyana Ali"]], "filtered_resps": ["Tatyana Ali"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a3f39bd55bbe31b920984e1a1a2f3354b223a4a5514d87fdba6b080fa6812402", "prompt_hash": "cb4e33f21d2bbab4893c00d6650840b367508c40a1a7e03756e5caeadb38fc47", "target_hash": "1e969cc0fab9b0d98610c88ca5308779474980cdfe276660598f0615f3d51cb0", "exact_match": 0.0}
{"doc_id": 999, "doc": {"question": "in the texas legislature the house has members and the senate has", "answer": ["member", "31-member Senate"]}, "target": "['member', '31-member Senate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in the texas legislature the house has members and the senate has?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 31"]], "filtered_resps": ["31"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5998008958980c1787bc8190a1f2acccf3aafb2a638ce427efd02f311fecbc2f", "prompt_hash": "7ee4068bee2b427b097ae7203d574bc4ca5b910a189e572d5c083cf0cd2561b0", "target_hash": "1104cecc2eadc8b2884989cc25d6a43ef3d74aeb6b2b927a762ae2df718cb1ab", "exact_match": 0.0}
{"doc_id": 1000, "doc": {"question": "what does g stand for in baseball stats", "answer": ["Games", "Games played"]}, "target": "['Games', 'Games played']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does g stand for in baseball stats?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" games"]], "filtered_resps": ["games"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "650ce29d12f561d3c2e73950c12128f40e37be2ae7aa4fd53abc950f1fad0dc8", "prompt_hash": "27ff1c917b91eeb4b2742a658aabb39e9bb8ef41fb8a20ac595893c50a73c0ef", "target_hash": "71650be805f759002b1e81b9af8b0cfc4f7142817ccf69e2635d7e860b4e7e0e", "exact_match": 1.0}
{"doc_id": 1001, "doc": {"question": "who was the last nba player to get drafted out of high school", "answer": ["Thon Maker"]}, "target": "['Thon Maker']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the last nba player to get drafted out of high school?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dwight Howard"]], "filtered_resps": ["Dwight Howard"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9e3990da485432b733f39d89f4c18071bc71ae318a491ad165309406b67616c", "prompt_hash": "0b8d7c96d97e16a7c8eaebd60e386d7dcacf2632e715ff390a2f6b1df10555cb", "target_hash": "5cf5b5022702723b6c372ad8f94e67f1061282d8c3513ba2296be3fadb2732a7", "exact_match": 0.0}
{"doc_id": 1002, "doc": {"question": "who was the person who escaped from alcatraz", "answer": ["Clarence Anglin", "John Anglin", "Frank Morris"]}, "target": "['Clarence Anglin', 'John Anglin', 'Frank Morris']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the person who escaped from alcatraz?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Frank Morris"]], "filtered_resps": ["Frank Morris"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c0a33684e44cd0fe24fcc3dbfb1a829ddbd28d0274a7cd42ef1b6cb40d7de81f", "prompt_hash": "aa6e62f99483ce5b32fee10ab3bb619aaddb6006a690f64b2b27465c5695a077", "target_hash": "3658c72bb3a58b874cc8445a59bb39b1415a54a7e0f86601d404f2dda7792b58", "exact_match": 1.0}
{"doc_id": 1003, "doc": {"question": "when was the last amendment to the constitution passed", "answer": ["May 5, 1992"]}, "target": "['May 5, 1992']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last amendment to the constitution passed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1992"]], "filtered_resps": ["1992"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eed3c400c10750a54a544b65b6e9538716e0493df45b50b480588a82311c2594", "prompt_hash": "ffb46cb5910fbb26c253ee86ac8081a6b0b5a428cac479e546636371cc15e5af", "target_hash": "8b7002c5fc93d7ba95ee3621173084ddb1c266210b296334f2344b11f0add6a6", "exact_match": 0.0}
{"doc_id": 1004, "doc": {"question": "who has scored the most tries in rugby union", "answer": ["Daisuke Ohata"]}, "target": "['Daisuke Ohata']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has scored the most tries in rugby union?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brian O'Driscoll"]], "filtered_resps": ["Brian O'Driscoll"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ef4f8d6ab38447c1ec190e26a2b9a664d0b6f48a58ff5f426c37a29790fb4e3", "prompt_hash": "05d46240314420231f884ff098c5fac578865ac3eb75bee993dfa27e37cefbb3", "target_hash": "a714385461905fc2ed84fa6ffa4edf2753531c8fd814381316026ba96af75eeb", "exact_match": 0.0}
{"doc_id": 1005, "doc": {"question": "who is the new york state senate majority leader", "answer": ["John J. Flanagan"]}, "target": "['John J. Flanagan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the new york state senate majority leader?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Bonacic"]], "filtered_resps": ["John Bonacic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "20f3a5bda09c094201ee7448f87aef7f3924b17a79ebc2541e954f9820d5ec6d", "prompt_hash": "52b0488dd01313e8d84f5e51232b79edc813fdd90d49b8e4410e8341e5ab6e56", "target_hash": "1d4439fa83cf76d97849667e04d3f42b75111f52b49b4a2ad0ad7a5504482cf8", "exact_match": 0.0}
{"doc_id": 1006, "doc": {"question": "when did the first marvel vs capcom come out", "answer": ["1998", "in 1996", "1996"]}, "target": "['1998', 'in 1996', '1996']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first marvel vs capcom come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "76afd509b99cc2a83df8fb56faee83f55336d4d9e7e99de683179292f5ddcd53", "prompt_hash": "063fda2ac8e4b5675432b843850db707585f9018200716d0f3b75e9110c381c7", "target_hash": "6d489f0064123d02f56b8af5fb19a4500af61e97782261de99772c9fbd1313a9", "exact_match": 1.0}
{"doc_id": 1007, "doc": {"question": "who carried the usa flag in opening ceremony", "answer": ["Erin Hamlin"]}, "target": "['Erin Hamlin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who carried the usa flag in opening ceremony?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Caine"]], "filtered_resps": ["Michael Caine"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "23661acbf5cde69118f5d1123a6da823a29dc208bacd81b89a2234040756431e", "prompt_hash": "215aa16161fadb7f4028221e391cd40f5568e28b0ef412c88bb6d025989acb04", "target_hash": "485432c74d200b7cd83a4fa10b69f5ff2c560a95c4601cc0a82a6b330a2653be", "exact_match": 0.0}
{"doc_id": 1008, "doc": {"question": "who played dwight's brother on the office", "answer": ["Thomas Middleditch"]}, "target": "['Thomas Middleditch']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played dwight's brother on the office?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael J"]], "filtered_resps": ["Michael J"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a948de614522d6b979f65b1d07a4a6081944ed111fd4d384d38f3f72c20134a2", "prompt_hash": "3ca30110341d38f3359130c69c06774c49c16ad3ca1dd041d136c3636440f7ee", "target_hash": "e88aad63f4b7e8d4924766facdde19f6259e937f621ff9c85efc4cf91d6ba18d", "exact_match": 0.0}
{"doc_id": 1009, "doc": {"question": "who was toot toot in the green mile", "answer": ["A trustee"]}, "target": "['A trustee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was toot toot in the green mile?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Coffey"]], "filtered_resps": ["John Coffey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fd4f48c59aa067b8dd17bdb4705c50cc1d7205b40f3d463135711705abb86a7d", "prompt_hash": "97fd9433ded6cb5c6ad3545dfeaf7e075d50224addc6bda5aa5d794cbf62c85a", "target_hash": "a2896bc3bbc45f461159100194cf0fe6605608e0d6a9fcda45f5ef3b81fe529a", "exact_match": 0.0}
{"doc_id": 1010, "doc": {"question": "who is the actor who plays king joffrey", "answer": ["Jack Gleeson"]}, "target": "['Jack Gleeson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the actor who plays king joffrey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Aiden Gillen"]], "filtered_resps": ["Aiden Gillen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4f1ee612dc23e8c76d80596a8b0a28bd6f1c7b74ec8b7b197a6d77d47131cbdb", "prompt_hash": "bf81c7336d266a04b7a1b12e4212269261c202b50081b291774a96b836cd4c5b", "target_hash": "eb73f3d890f89a9d81374f9335b7ced264100d490ed620cbe276342af739d60a", "exact_match": 0.0}
{"doc_id": 1011, "doc": {"question": "who starred in the movie natural born killers", "answer": ["Robert Downey Jr.", "Robert Downey, Jr.", "Woody Harrelson", "Juliette Lewis", "Tom Sizemore", "Tommy Lee Jones"]}, "target": "['Robert Downey Jr.', 'Robert Downey, Jr.', 'Woody Harrelson', 'Juliette Lewis', 'Tom Sizemore', 'Tommy Lee Jones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the movie natural born killers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Woody Harrelson"]], "filtered_resps": ["Woody Harrelson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c369f53752cf0244322661eb867d35cb96d86a87f0fb0caf96f1170b20eae6ba", "prompt_hash": "568a57d9306c7ff39b34616c46416f9f8c68a0232bdeb5de361e55dafac9aedd", "target_hash": "108db092872faa7ec8f5f049dc284b5a74f291a94fb80896aa033f643611366a", "exact_match": 1.0}
{"doc_id": 1012, "doc": {"question": "what was going on in the world in 1900", "answer": ["Galveston hurricane", "Boxer Rebellion", "U.S. population exceeds 75 million"]}, "target": "['Galveston hurricane', 'Boxer Rebellion', 'U.S. population exceeds 75 million']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was going on in the world in 1900?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the 1900 Summer Olympics"]], "filtered_resps": ["the 1900 Summer Olympics"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "73cb587ce9c6c8de1afea98fcfc9fb16760e90033b6ad8808aefdc19688a67df", "prompt_hash": "3b47d83acc8eb8b86c31e18587eef942df4c8280282177aa76b6ba8833ca6921", "target_hash": "eca97414950620f67475d747dc0ba4cb7566ddc1f8fd6b55af94a8b735e56156", "exact_match": 0.0}
{"doc_id": 1013, "doc": {"question": "when did the nest 3rd generation come out", "answer": ["September 1, 2015"]}, "target": "['September 1, 2015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the nest 3rd generation come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "25776758e4c111baa8ef41fdd98be9b008515e2d813bdcecd9b1562a8b987e2d", "prompt_hash": "7337aef79c13aa8799361c98ad07afda5d469642229834f582b9d24fc2d736fd", "target_hash": "85011d6c2184bc90156097b9ff287fbf9b8f620b32d5ce693ecf9fdd68e0015b", "exact_match": 0.0}
{"doc_id": 1014, "doc": {"question": "who has won the most united states championships", "answer": ["Lex Luger", "Ric Flair"]}, "target": "['Lex Luger', 'Ric Flair']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the most united states championships?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wayne Gretzky"]], "filtered_resps": ["Wayne Gretzky"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5c53a1caad7afff80222b92a092830309b7cdcf228c9c12c3f1f434e8755e68", "prompt_hash": "83691d225194e26351bd1db7490650f5dd3c5292be8caf40ef71f57b02dd81be", "target_hash": "5d095cd72239970c366d5e2d4390b7855863edbf61093e89277a2a1ff78fc8a8", "exact_match": 0.0}
{"doc_id": 1015, "doc": {"question": "how much is a 72 oz steak at the big texan", "answer": ["$72"]}, "target": "['$72']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much is a 72 oz steak at the big texan?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $72"]], "filtered_resps": ["$72"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d80650d43a3d90f9b1c7325a7be407230cf45cd900494c7e9431fee279024cf1", "prompt_hash": "e4435b41b8170a773799909769b7265ed198e3c27bc1797a36b7a8eb0da4ceaf", "target_hash": "9050021face97df53ec3f127ca63800e6079cff24b108ee8c29a41299e893ae5", "exact_match": 1.0}
{"doc_id": 1016, "doc": {"question": "when was the first wonder woman comic released", "answer": ["December 1941", "January 1942", "October 1941"]}, "target": "['December 1941', 'January 1942', 'October 1941']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first wonder woman comic released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1941"]], "filtered_resps": ["1941"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a0e7b52fa4ba3c5b2b81eb2119579d2fa4b0a1d02354499c580044101e7ea599", "prompt_hash": "b61d641117522f9f9af9a3fe9cc5f3300912996c8ba2ee363b68fb4c9da8c242", "target_hash": "2a8c70cde5abcc26d8099a7ac23d87a2011d3618cb1c80a3ac50bc4d4ab24f6d", "exact_match": 0.0}
{"doc_id": 1017, "doc": {"question": "what are the four requirements to establish criminal liability for gross negligence manslaughter", "answer": ["the defendant breached this duty"]}, "target": "['the defendant breached this duty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the four requirements to establish criminal liability for gross negligence manslaughter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" unlawful act"]], "filtered_resps": ["unlawful act"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06fbe8faa9664885b2c74c5328f982f6aebae450d29eac7e714b9f54fa418797", "prompt_hash": "4ed1b8a587c3d355f4516cffba381943aea97f9eb6c9dd01bbadb6c399b178fb", "target_hash": "ce33a8d0c7916efefc15ec376fe89f9bb7f626aaeb681c772ac4c1967d672749", "exact_match": 0.0}
{"doc_id": 1018, "doc": {"question": "who does the voice of alistair in dragon age", "answer": ["Steve Valentine"]}, "target": "['Steve Valentine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of alistair in dragon age?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mark Meer"]], "filtered_resps": ["Mark Meer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6a6996f3e757bf13b3924509be136ca316bb795a54267eb6a898be8b34c8bdb4", "prompt_hash": "db35853682c8fa27ba6afe0dad6ca80b3f5001248a1be1419311f1b821abcd48", "target_hash": "bb89c175d386e5a2f90f7daf2fda9fff8fda920a9fa8b7dc856f3f604b038d59", "exact_match": 0.0}
{"doc_id": 1019, "doc": {"question": "who were the twins that played for kentucky", "answer": ["Aaron Harrison", "Andrew Michael Harrison"]}, "target": "['Aaron Harrison', 'Andrew Michael Harrison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the twins that played for kentucky?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Anthony and Aaron Harrison"]], "filtered_resps": ["Anthony and Aaron Harrison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0c20977dd943b34e2fa82c7f5bab20b0974e230c5e0c5251d245bc1205a4d67a", "prompt_hash": "6727a8c98a18bedcac0bf3961c620c16a53a99db1fa4abb94ec73dbaa2d98f79", "target_hash": "2fb7695e0621d080cc083eb85731797ee2c03884da31c7a646033d302552ae12", "exact_match": 0.0}
{"doc_id": 1020, "doc": {"question": "who coined the phrase let get ready to rumble", "answer": ["Michael Buffer"]}, "target": "['Michael Buffer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who coined the phrase let get ready to rumble?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rocky Balboa"]], "filtered_resps": ["Rocky Balboa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0a53c3583ad8091991e46660d4d02f1e904e8d720ae80696f885761e91a0f7d8", "prompt_hash": "b53870e931859f9c2b4e63f8331016f147aa02678fff8b078f2b182dcb266cd5", "target_hash": "7c33bac2df564926e39ee1386970fa34621c7ad1a993506bcb7606f2dca2d29e", "exact_match": 0.0}
{"doc_id": 1021, "doc": {"question": "who is command sergeant major of the army", "answer": ["Daniel A. Dailey"]}, "target": "['Daniel A. Dailey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is command sergeant major of the army?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sergeant Major of the Army Michael A"]], "filtered_resps": ["Sergeant Major of the Army Michael A"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fd43389bef15e8b494e26e5ed8c9ed1a8b7a782aae8e0ef0604f7ed4ad19eb99", "prompt_hash": "85382a88c356ceb74c7294e407286084a4b3b90b2abe3d36a23002dd2a3402c1", "target_hash": "bb7b78ff5c584277b3cc46c594bb9589b4bb0ea8c2fcc701ca1a053753ec3c90", "exact_match": 0.0}
{"doc_id": 1022, "doc": {"question": "who is in charge of new issues not mentioned in the constitution", "answer": ["Congress"]}, "target": "['Congress']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is in charge of new issues not mentioned in the constitution?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Congress"]], "filtered_resps": ["Congress"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d2455eee4572b308e8369897899fb780a1f539596ceabec86409937e4b5f504", "prompt_hash": "53bfbd46293f5a60afb5ddf2e34cf25c1901d8b964627873b31581352a367337", "target_hash": "e912602cea4fbf15b184b56ec696e2e2d6538310956826358940376a97bdc0e9", "exact_match": 1.0}
{"doc_id": 1023, "doc": {"question": "where does trick or treat for unicef money go", "answer": ["UNICEF's global programing"]}, "target": "[\"UNICEF's global programing\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does trick or treat for unicef money go?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United States"]], "filtered_resps": ["United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e691963b3805bc2805a30b55e8c7b7e4b7a1f469664054bb5d9aaf15dcb5d394", "prompt_hash": "0f3bb2ff792408873cd49afcc5d673ea49924a3920822a74a6535eb9a056be7b", "target_hash": "181801dd036ffd3ba014de1ccac734d25e55f87d6d8172d43460d4240728f9a6", "exact_match": 0.0}
{"doc_id": 1024, "doc": {"question": "when was the first episode of scooby doo", "answer": ["September 13, 1969"]}, "target": "['September\\xa013,\\xa01969']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first episode of scooby doo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1969"]], "filtered_resps": ["1969"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f0417bbf30d08aed98dbb87e3cd4bc675bd3960413a79d6fe980b8c472fea189", "prompt_hash": "ec72c83ae6f66d389d8805ca7ad71ee21e1c16d3b2377410c95bfce442acb3f3", "target_hash": "0562a25e289d89b6a54abae653abffc734b64c3d2d891d93cf134323d676f43b", "exact_match": 0.0}
{"doc_id": 1025, "doc": {"question": "when did the us dollar leave the gold standard", "answer": ["October 1976", "August 15, 1971", "1934"]}, "target": "['October 1976', 'August 15, 1971', '1934']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the us dollar leave the gold standard?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1971"]], "filtered_resps": ["1971"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "26e50d2a3c152e4e5712a4f4f4e776bc0719598b817b1530e51193553825ec63", "prompt_hash": "2a7f8f1bfab63dc0d7481c3fc268d1e3e5b2b3635218cc7796255c980a90f30e", "target_hash": "86f23fdfecdbb3d81462f9b3205a42fe90cae86483cddbd28f96ada6a6b99400", "exact_match": 0.0}
{"doc_id": 1026, "doc": {"question": "who founded mission nuestra senora de la soledad", "answer": ["the Franciscan order", "Fermín Francisco de Lasuén"]}, "target": "['the Franciscan order', 'Fermín Francisco de Lasuén']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who founded mission nuestra senora de la soledad?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Father Fermín Lasuén"]], "filtered_resps": ["Father Fermín Lasuén"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6433e36c59b48537b8c1ba65e039be800090793c19f91fc9340f7f5a10551fbb", "prompt_hash": "97468e141c35a65e7a0f64625622b04a6b6b6c89b0a8c97f83e6d0696384a189", "target_hash": "5c4a6d14f643e10dab0a2d2579704d0a550d1718367d70efb907e73b9a562cb9", "exact_match": 0.0}
{"doc_id": 1027, "doc": {"question": "the type of display that google glass presents to its users is called what", "answer": ["head-up display", "optical head-mounted display", "Prism projector"]}, "target": "['head-up display', 'optical head-mounted display', 'Prism projector']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the type of display that google glass presents to its users is called what?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" heads-up display"]], "filtered_resps": ["heads-up display"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "705c45177ca5d913b41dbfd122c8b0dfb1ecb5a63dd336c6238e8f964b059b7f", "prompt_hash": "4adf03a70571fc2e18a19ee174eacbeb5f889ee3d3aa5130d40d2bb173a6e7c6", "target_hash": "ffe4a78990947ad973b1eed2c797c67d209362dcf39917e351a18d876c6e2523", "exact_match": 0.0}
{"doc_id": 1028, "doc": {"question": "where are the winter olympics and when do they start", "answer": ["Pyeongchang County, South Korea", "Pyeongchang, South Korea", "9 to 25 February 2018", "9 February"]}, "target": "['Pyeongchang County, South Korea', 'Pyeongchang, South Korea', '9 to 25 February 2018', '9 February']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the winter olympics and when do they start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pyeongchang"]], "filtered_resps": ["Pyeongchang"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8162a755fb7e9dd65b829040e9dfe395bb56f5f37995715f871fbc5f5cd79091", "prompt_hash": "d9cb84fd1576fd21a7ecd2f7785a58389d12d37e081097cfd485b36c5428754d", "target_hash": "6f786c37ea00dea22cfeaf59f8bed42d26f0626c9edbf3c081d21f56e2435f9a", "exact_match": 0.0}
{"doc_id": 1029, "doc": {"question": "to whom do the powers not given to the federal government by the constitution go", "answer": ["the states or the people"]}, "target": "['the states or the people']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: to whom do the powers not given to the federal government by the constitution go?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the states"]], "filtered_resps": ["the states"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5a6587eb0565920d6c39a67cd76686c4689b3ec8902a008e9a83f4c3e2ec3d61", "prompt_hash": "e326f357aaf9ef995a4178cd63ea7c2cf81c822de22b2a6b1a83cacb975cbd20", "target_hash": "c5afd7452368c3080532d1c7108f7dee332eef6e569807fc1c561d2788694185", "exact_match": 0.0}
{"doc_id": 1030, "doc": {"question": "what is the name given to the common currency to the european union", "answer": ["euro", "The euro"]}, "target": "['euro', 'The euro']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name given to the common currency to the european union?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" euro"]], "filtered_resps": ["euro"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f6221f365f9bea5de2d6d1bd3128613df8d527034e3677175c0bcd8e0c79a114", "prompt_hash": "2441f82572932f3eba9156161ec0489b4c902023fc3e1ae5b66289821b05f1aa", "target_hash": "ac2dff53cfd6a96af48b973e3a304e9b121ab50d483c3ee921feee9387e53f1d", "exact_match": 1.0}
{"doc_id": 1031, "doc": {"question": "which term describes the replacement of damaged cells to mend a tissue", "answer": ["Stromal cells"]}, "target": "['Stromal cells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which term describes the replacement of damaged cells to mend a tissue?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" regeneration"]], "filtered_resps": ["regeneration"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e5c039a11a921b9530e05d7c4100054b9db116d24c681d20b1eafc76a9386ed4", "prompt_hash": "8556425beb03755b6e2a7495cebb5ecaa2470933a4a792083ff3ae2570adb766", "target_hash": "418a7ab0677c18310e02a3c113e2020c1c5be48895309b59d9ce42f6c18501cc", "exact_match": 0.0}
{"doc_id": 1032, "doc": {"question": "what process causes the continents to drift apart how", "answer": ["Continental drift"]}, "target": "['Continental drift']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what process causes the continents to drift apart how?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" plate tectonics"]], "filtered_resps": ["plate tectonics"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "751e8420fb601c41c2e0611112542731c6a2fb2740423e57512862f2e75a0f55", "prompt_hash": "8294d2ce6a7e94a8cbd1adcf3254bf4f2970a2368feba24c91336884af3eef46", "target_hash": "ac04d7ed099f09ca7d0e3c678f643b1580f01f6ac48c800722ea60f105c03d3e", "exact_match": 0.0}
{"doc_id": 1033, "doc": {"question": "who was the first person to successfully introduce a device exclusively designed for data processing", "answer": ["Herman Hollerith"]}, "target": "['Herman Hollerith']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first person to successfully introduce a device exclusively designed for data processing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Charles Babbage"]], "filtered_resps": ["Charles Babbage"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "065f04f285ce8192430c300bde0270eac876caf30c80623d7acdaee0858bb62d", "prompt_hash": "afa440ec8a7827372feff5bb56f1c4772098a8a0ed4e6761f17bb52afc259d31", "target_hash": "e6ed6c838613df9f5506fc822b570100506f7e1620a8ea50304fd1e915c61410", "exact_match": 0.0}
{"doc_id": 1034, "doc": {"question": "prior to 1948 north and south vietnam were a part of", "answer": ["France"]}, "target": "['France']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: prior to 1948 north and south vietnam were a part of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" French Indochina"]], "filtered_resps": ["French Indochina"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "75c5a86f5fec95930143c20426b8d5ee1216e07f76c9fa504eccdc7abd6f1cd1", "prompt_hash": "eb4ef123fa6c183352aa57c258dce4af43c47ea725f2a1306c259f2c4c0a5e37", "target_hash": "cb35d42049d03c1558695af87e0fcccad41b04f8b0a63de291599f9b84eb4afa", "exact_match": 0.0}
{"doc_id": 1035, "doc": {"question": "who sang if this world were mine with luther vandross", "answer": ["Tammi Terrell", "Cheryl Lynn"]}, "target": "['Tammi Terrell', 'Cheryl Lynn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang if this world were mine with luther vandross?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Whitney Houston"]], "filtered_resps": ["Whitney Houston"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c0b9472ad088062743e68b7506129ec416ec1877f9aa0b0555f2691ef87a108", "prompt_hash": "6a0672756ab1d9f6671eb69c41a105a53647a27cb9943219e2c768ad444f0a18", "target_hash": "3f23de481179eeaa1b9576ad2e5863293a13348b0fcfff40c6bf6f9593a0c575", "exact_match": 0.0}
{"doc_id": 1036, "doc": {"question": "how many times has south korea held the winter olympics", "answer": ["two"]}, "target": "['two']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many times has south korea held the winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 0"]], "filtered_resps": ["0"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "36fe7dc225a931bfb36ab1dc1b15447a69dd6514db07fa7de3a14403a9141fc3", "prompt_hash": "2d19cd224464457b4c78b4eb5ad71184dab3fda54233a8eb4e4a4adda917cb61", "target_hash": "e5308d30c3c259f8b22b83f586470d404a7a19aa1351fe2216e81cb611e4af57", "exact_match": 0.0}
{"doc_id": 1037, "doc": {"question": "which gene segments code for the variable region of the heavy chain", "answer": ["V"]}, "target": "['V']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which gene segments code for the variable region of the heavy chain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" V"]], "filtered_resps": ["V"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c974f96bccd6fe4e0dc3bc8c507ef9d474d238e5079a46e9731c6117e691f92", "prompt_hash": "645e276402d2d03c0299587d984090474ed4ad72543e9b47b4e5bcd07e4b17c3", "target_hash": "9e348eb62f9e538a9d7d66f5d0e035cacf5786e7527384db3c4d02070ea04078", "exact_match": 1.0}
{"doc_id": 1038, "doc": {"question": "who sang what are we doing in love", "answer": ["Dottie West", "Kenny Rogers"]}, "target": "['Dottie West', 'Kenny Rogers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang what are we doing in love?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pointer Sisters"]], "filtered_resps": ["The Pointer Sisters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7b0c0a560c471582fdd96205aa952f4229eb57830e2e4c497d118b055ec152e", "prompt_hash": "025333b246e1f6080b656671b0a9d6251691f7799c7c5cf4b2b73aa41738f801", "target_hash": "19f773c61719a5b8f54e1c29a920b593e6ea8ba7556f5164a8da950d73af02f9", "exact_match": 0.0}
{"doc_id": 1039, "doc": {"question": "who won the national championship in volleyball 2017", "answer": ["Ohio State"]}, "target": "['Ohio State']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the national championship in volleyball 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Penn State"]], "filtered_resps": ["Penn State"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3b9178b88c247d5124fcbb686619e7da0f70ac0ef0f563682cb76fdc879fa7ee", "prompt_hash": "6a30e68685ad09e9891648f5249bfe0e847c2083e4db0460d2964113252a5e8e", "target_hash": "586c33b85898d3edf9ffa8673d227e5ceb36cd9be05f7bf3042278a03f6bba6c", "exact_match": 0.0}
{"doc_id": 1040, "doc": {"question": "when was zero first used in mathematical operations", "answer": ["By 1770 BC", "7th century"]}, "target": "['By 1770\\xa0BC', '7th\\xa0century']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was zero first used in mathematical operations?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 7th century"]], "filtered_resps": ["7th century"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3741b82e07767abf7fa29981ac99abb4fd14e6ca723e57725074d9e8b349b953", "prompt_hash": "36ab095dc5fe17adb1cb43c28f7a04c1d0c098b524afb2aba74ba147593b8e77", "target_hash": "3f9ea5ac67ca8adb48aeb91b59f0fa4c1b14b12faddc0596e5cee1a0dc9f87ff", "exact_match": 0.0}
{"doc_id": 1041, "doc": {"question": "the lowest temperature at which a lubricant will flow is called the", "answer": ["pour point"]}, "target": "['pour point']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the lowest temperature at which a lubricant will flow is called the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pour point"]], "filtered_resps": ["Pour point"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4175235052fb19cfe6adca7982854e963e6d7eab9d888c7b62d943df8a9f6f55", "prompt_hash": "a55b5bda91dc5f0e50a41e4a731d40847e82162ae022f51f6434bd052659fc7d", "target_hash": "66906b32fc051797489203d03d5dd68c95eb4c3dcda44328a41cdd98790c6314", "exact_match": 1.0}
{"doc_id": 1042, "doc": {"question": "who wrote the song mary had a little lamb", "answer": ["John Roulstone", "Sarah Josepha Hale"]}, "target": "['John Roulstone', 'Sarah Josepha Hale']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song mary had a little lamb?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sarah Josepha Hale"]], "filtered_resps": ["Sarah Josepha Hale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cb3791e5c8ebbfcf9eba68f26952c1774be881ac317dd1f8aab650eaf64e001f", "prompt_hash": "bc9f75e771540f3194ddf105ae5d09e52e3719159de1475a0fc6c47e79c441a5", "target_hash": "6c815df3d184f9ebc0e9d3a07aebaef37f0543e4ebb338e2684dbbd806fc7bc4", "exact_match": 1.0}
{"doc_id": 1043, "doc": {"question": "who was an early advocate for using scientific methods based on inductive reasoning", "answer": ["Aristotle", "René Descartes"]}, "target": "['Aristotle', 'René Descartes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was an early advocate for using scientific methods based on inductive reasoning?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Francis Bacon"]], "filtered_resps": ["Francis Bacon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "916b437526306e65ee8a820ae4e97511e34341db443bb5d6525587613f164d1d", "prompt_hash": "24009aecda21657c385ad1b16338c5cd06acf3cec358e66bdd593ccf330f5466", "target_hash": "3db27842df25630761e57cecb17701b2d853f7362c5ea49bd81591387d95c610", "exact_match": 0.0}
{"doc_id": 1044, "doc": {"question": "when does the call of the wild take place", "answer": ["1890s", "the 1890s"]}, "target": "['1890s', 'the 1890s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the call of the wild take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1903"]], "filtered_resps": ["1903"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6e15c1f69cb7761166e881d696208a307e1c35fe5681e9281da7bfdd4ac2bc12", "prompt_hash": "063d605f7c58ecf8e49be2d2491d6a4bb2166ef5a4ab7fc1abe3b24011abb636", "target_hash": "51cf674330cbcfafcd3f6c125a9de392692aecc2d93fcba77e175ff21417aa58", "exact_match": 0.0}
{"doc_id": 1045, "doc": {"question": "when was the first driver's license required", "answer": ["1888", "1 January 1904"]}, "target": "['1888', '1 January 1904']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first driver's license required?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1903"]], "filtered_resps": ["1903"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "becc98fd7503dd0792d5dab9a062f512537eab6c5eb6e0df11faee6e589bb86a", "prompt_hash": "1b859a9ff629dea8f845f7efc108c1932b0707bed672551e904ddeb52541b4d7", "target_hash": "661ab65f0569767e61f4e96004167dbf12001fcb13324ebd84d6792233ba6fb5", "exact_match": 0.0}
{"doc_id": 1046, "doc": {"question": "when was the last time iran had an earthquake", "answer": ["2017-12-20"]}, "target": "['2017-12-20']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time iran had an earthquake?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012"]], "filtered_resps": ["2012"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2ad8d2a58a041214c04f8178767c1cf3aef82fddf5ddb98b1e89f61748e9327b", "prompt_hash": "a109142a2b5d051849c7cfb99c2437863de6e860f98aa8df51d4805c1dbb50d5", "target_hash": "b5648e5c04ce8123b56360840efeb4694e3a1fd61838d9166c0256b05e93754f", "exact_match": 0.0}
{"doc_id": 1047, "doc": {"question": "where is the deepest lake in the us located", "answer": ["Oregon"]}, "target": "['Oregon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the deepest lake in the us located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Minnesota"]], "filtered_resps": ["Minnesota"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0e1de1443d7cc74ab0a5e9326ea5eac59ad3cd3287eb10e3cf892f826f434256", "prompt_hash": "7bb1abe099086a121ad60ad48cc2a3043d820dbed2731aee47d7268c3e8d664a", "target_hash": "84fc8dd1a274beb86fdf72c695b4d2f5eff78c657ebfba9fc05fa82ed9ed5e61", "exact_match": 0.0}
{"doc_id": 1048, "doc": {"question": "who is the largest supermarket chain in the uk", "answer": ["Aldi", "Tesco"]}, "target": "['Aldi', 'Tesco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the largest supermarket chain in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tesco"]], "filtered_resps": ["Tesco"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cd0f17f698bd4e6b051876f760c1d5d56403c61b1bc26e82eeb77ff41d1e7326", "prompt_hash": "3327df0676f326bc8757059da43045b19e8fa7c029e206f2d101bf2ab7a1b49a", "target_hash": "a16708bc09f21f6392baea0a9bfd2d38376eddfa1fabe61fb3b2387448fbb131", "exact_match": 1.0}
{"doc_id": 1049, "doc": {"question": "what is the origin of the name marvin", "answer": ["Welsh", "the Welsh name Mervyn"]}, "target": "['Welsh', 'the Welsh name Mervyn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the origin of the name marvin?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hebrew"]], "filtered_resps": ["Hebrew"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4931addcac8e076c8504e8ea7aa762b7fee070ad45c80edfac069095c2bcf221", "prompt_hash": "c369b4e7b76962f8c24d6d71a030bf9a8f99e3260d63adbb6899f6ef7a7226d0", "target_hash": "7ce1d84919ef37eed307b2433fc9e247c65aaada7c2dc6dcb712babbb98ccd3c", "exact_match": 0.0}
{"doc_id": 1050, "doc": {"question": "when did italy enter the second world war", "answer": ["10 June 1940"]}, "target": "['10 June 1940']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did italy enter the second world war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" June 10"]], "filtered_resps": ["June 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b2be629606b515891cdca280d6761cca13eebb60f83ae1eec77234f18f7d5f9", "prompt_hash": "b317a665f9c1b8366c458a73604c365e3268678e1ca61cfddbdec28c89463800", "target_hash": "d29c4b9a896222e045d03929bf0aeee258b5f828baf6ca1715733b99b727c8d7", "exact_match": 0.0}
{"doc_id": 1051, "doc": {"question": "what river flows through charleston west virginia’s capital", "answer": ["Kanawha River", "the Elk and Kanawha Rivers", "Kanawha", "Elk"]}, "target": "['Kanawha River', 'the Elk and Kanawha Rivers', 'Kanawha', 'Elk']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what river flows through charleston west virginia’s capital?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kanawha"]], "filtered_resps": ["Kanawha"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32cab18ec6524a82a89d04658de589de5aba7f50bdcb4501e833424d339da645", "prompt_hash": "e3d2aa06999a5f1e5db08d395da82b65f5cdcc159e2fcc9e1e15b6d1edc28982", "target_hash": "22b791aac1cae2db7e9cef5dc2003a666aa424a0ba80317c991935b5aad8a2a9", "exact_match": 1.0}
{"doc_id": 1052, "doc": {"question": "what is the main objective of criminal law", "answer": ["Incapacitation", "Deterrence", "Retribution", "Restoration", "Rehabilitation"]}, "target": "['Incapacitation', 'Deterrence', 'Retribution', 'Restoration', 'Rehabilitation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the main objective of criminal law?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" punishment"]], "filtered_resps": ["punishment"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cee60420da4e9771b8a1e4a5dbbce349da80eccf57044b190a082fa44bb26c00", "prompt_hash": "569bf83fcbb9be736671cd45c5aa41326c53a86389a2d9f35145b0447cd21c56", "target_hash": "106820c52b54c586a328f364d304792f42d7305887e0aae029dc6982818ff20f", "exact_match": 0.0}
{"doc_id": 1053, "doc": {"question": "what theater number was the aurora shooting in", "answer": ["9", "Theater 9"]}, "target": "['9', 'Theater 9']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what theater number was the aurora shooting in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "76d6effbe4c976a857c6287241d5a1e7fae36530060e595785775385dcb4aa72", "prompt_hash": "27d58605d5b627545a06e192e6406237553752694330cde53c40475c912a9893", "target_hash": "5d857d7054dd1020280734617917e16aead98235e824341229f04e1815403d14", "exact_match": 0.0}
{"doc_id": 1054, "doc": {"question": "where does blood go when it leaves the pulmonary artery", "answer": ["lungs", "The pulmonary vein"]}, "target": "['lungs', 'The pulmonary vein']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does blood go when it leaves the pulmonary artery?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lungs"]], "filtered_resps": ["lungs"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2242f8d674590efa308512d1eed5dfda34c3fcf4a45ebc6e5c4c8e13a4f73ba8", "prompt_hash": "a32b2a461c89d41e04ad274bcd036049485a05f86353f258f102c92c07f00dbe", "target_hash": "21fd2dd7b23d4884596c6e13b27cd8da072e3a9f489ac0ddf3215bbc73c5bfea", "exact_match": 1.0}
{"doc_id": 1055, "doc": {"question": "what states do not allow daylight savings time", "answer": ["Navajo", "Hawaii", "Arizona"]}, "target": "['Navajo', 'Hawaii', 'Arizona']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what states do not allow daylight savings time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arizona"]], "filtered_resps": ["Arizona"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d546e8503f6c549d5d081109d16ddce45ed5dd85538f088c2090750ce02830c0", "prompt_hash": "9a41905817d861ef1c68cbca1dd8a83017eaa09dc14bb25b99a8807a2c1f5ee7", "target_hash": "3be56b15df50edee6b01fed8aa725c70817dc536583265b39d0cac5b241d012e", "exact_match": 1.0}
{"doc_id": 1056, "doc": {"question": "who is the highest selling r&b artist of all time", "answer": ["Rihanna", "Michael Jackson"]}, "target": "['Rihanna', 'Michael Jackson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the highest selling r&b artist of all time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Jackson"]], "filtered_resps": ["Michael Jackson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35bfb6679e72ea8061561965a5823d09a1f772d1b518c6f69e68a9ff14773b04", "prompt_hash": "41975cc489672fa15ab668b5e9dfbc7814d26d091ffac384a2dda8315aa2987a", "target_hash": "6de544957cb3068e52b53ac684cc33cd7c52bd3787bc8242176b7f0aec1444a6", "exact_match": 1.0}
{"doc_id": 1057, "doc": {"question": "how many books are in the one piece series", "answer": ["83", "more than 80"]}, "target": "['83', 'more than 80']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many books are in the one piece series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 98"]], "filtered_resps": ["98"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e7270e1a71d36c9303714a10dca22d6f5d2432dcd7104c0e54b8579cb1c1be59", "prompt_hash": "519fff78c3aa5550981d90ed3fc31c64a24d872345afa0861e9e7958eec58a08", "target_hash": "cb0303c533e9a8e0526a75d3f785fdc3169ddd0757cf89aaccbbe447e960a4ef", "exact_match": 0.0}
{"doc_id": 1058, "doc": {"question": "what stations in seattle are owned by sinclair", "answer": ["KOMO-TV", "KUNS-TV"]}, "target": "['KOMO-TV', 'KUNS-TV']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what stations in seattle are owned by sinclair?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" KIRO-TV"]], "filtered_resps": ["KIRO-TV"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f223a83359c490f700c40e2c4663e2b481f1babf468cfaa02b380d990a3a8e83", "prompt_hash": "def292bc4babffaf17e9cb0368cf061c58528a709c1434b5885c0a0feaec588f", "target_hash": "7de689d55dd5a58658a23a707f74e3d5ae263807b1f1129c1f8dfb77aabf5090", "exact_match": 0.0}
{"doc_id": 1059, "doc": {"question": "which state is the richest state in nigeria", "answer": ["Lagos State"]}, "target": "['Lagos State']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which state is the richest state in nigeria?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lagos"]], "filtered_resps": ["Lagos"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8bcc40489be9861abc329b209fddb12a7ec603242b7ee5147dd60306caa786c8", "prompt_hash": "924ae9a6dc612d98d2973edb8e1de2c25960a103c91b3ae53940f34d0904b6fc", "target_hash": "a00b640187426aeaf0964574cda976a32abb23412a98a73869191044a497fc09", "exact_match": 0.0}
{"doc_id": 1060, "doc": {"question": "how many seasons are in star wars the clone wars", "answer": ["6"]}, "target": "['6']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons are in star wars the clone wars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 6"]], "filtered_resps": ["6"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "04e5a56a29f6550070d689773be5b8a0aa086df4794f9a0b3c451ef11dbe13c8", "prompt_hash": "c8029b5938ab308448575bec5697d6fd33ceca74639df90dafe8045fdacb5340", "target_hash": "b36d04a43c188fa9042ccfe2c13d40316eef06f92e6790f9e9b2395eb813a6eb", "exact_match": 1.0}
{"doc_id": 1061, "doc": {"question": "when did world war 2 end in the pacific", "answer": ["14 and 15 August 1945", "September 2, 1945"]}, "target": "['14 and 15 August 1945', 'September 2, 1945']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did world war 2 end in the pacific?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" September 2"]], "filtered_resps": ["September 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8449099486345fbe59eb1dcc3a218f311f28f483e32f4cc3968baac6f91cc755", "prompt_hash": "d155d8c5805bb9821f8e713260d512f6216190a6e6f6486daf11def157f4edd2", "target_hash": "0b801542c02534385b194527bf774cab6f4bed070d61a8c529cf37f6ad0ebe99", "exact_match": 0.0}
{"doc_id": 1062, "doc": {"question": "how many books are in the diary of a whimpy kid series", "answer": ["12"]}, "target": "['12']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many books are in the diary of a whimpy kid series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 15"]], "filtered_resps": ["15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a8481bf8ef8b907efcc73c4021704dac7bf747cc501a9af93236e51cb9694acf", "prompt_hash": "4615a045cc20df429f4d663466cbbb0c0246ffcfd4defa344fcde7080a4e9e70", "target_hash": "0485d4130dc78d35a36ed5f5c5bfea741e1e1e0ea37b856d0197e254d85d7d8e", "exact_match": 0.0}
{"doc_id": 1063, "doc": {"question": "who did cubs beat in 2016 world series", "answer": ["the Indians", "Cleveland Indians"]}, "target": "['the Indians', 'Cleveland Indians']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did cubs beat in 2016 world series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Colorado Rockies"]], "filtered_resps": ["Colorado Rockies"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7f075f39a9c4ced991c863a5fd647b47a2b7d18f8f19b4e641ca6e0ec5ad3b58", "prompt_hash": "ebc8137d1547ba94f20ca3d5f8e97a450cfe97ebfbdfd9fa66138d04c781fe1c", "target_hash": "885b33137db879d33854b515288e79e1ff12a97e89f97a09877bcd003c5be4d3", "exact_match": 0.0}
{"doc_id": 1064, "doc": {"question": "who played the girl in my two dads", "answer": ["Staci Keanan"]}, "target": "['Staci Keanan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the girl in my two dads?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mary Steenburgen"]], "filtered_resps": ["Mary Steenburgen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4661fa32ac93e0a70b79e22c43c1dfb868c557cf57a70e7f609220a6a0080a5e", "prompt_hash": "27efe910516e002898a3c69e322022e10153a1478d1e4c8aff86758cd99ef264", "target_hash": "4c2a1f1849e50d7c6bfde133abfac3f15530ff506ab40c9c1db0e8dc12a2eef8", "exact_match": 0.0}
{"doc_id": 1065, "doc": {"question": "when do love and hip hop new york come back on", "answer": ["November 13, 2017", "October 30, 2017"]}, "target": "['November\\xa013,\\xa02017', 'October 30, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do love and hip hop new york come back on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 7"]], "filtered_resps": ["season 7"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b7d3f9521441267ce3dc68bd6d872a3235391d1f7d461917e3318d6be6b26e4", "prompt_hash": "e8f55966f876af6f12ca732690c9a285f76783da7ee2fcdcc819cfca940244c2", "target_hash": "9c319d004a3657894c721959300eb4a4554271df4b2b0f48a34f49ea32cd4840", "exact_match": 0.0}
{"doc_id": 1066, "doc": {"question": "when did ibuprofen become available over the counter", "answer": ["1983", "1984"]}, "target": "['1983', '1984']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did ibuprofen become available over the counter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1984"]], "filtered_resps": ["1984"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3849cc21cdee345f13fb1f5be2132e253da90d7878cbaba29841eaed8f588857", "prompt_hash": "01e27aa2cb7c1ceaa9126d6dd83b945db9562d8e597bbb0b987786976d36b680", "target_hash": "bf13c7164016e59921bef5b97a184bb194aad560b61bb45006b16e0db1cbd3b8", "exact_match": 1.0}
{"doc_id": 1067, "doc": {"question": "who played the judge in good will hunting", "answer": ["James P. Flynn"]}, "target": "['James P. Flynn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the judge in good will hunting?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stellan Skarsgård"]], "filtered_resps": ["Stellan Skarsgård"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91e2bc602f83a063fa44f20b894f3f8240580b58c10af730c977fa96aea244f3", "prompt_hash": "fe0913f36b288d2ddbc013daa0537654b107999decfd24e8e33c4c3218188f3a", "target_hash": "9bdae1bc34247f623dbfcca371642f7d95a3146936a61bd7bfa6390505fbf5ad", "exact_match": 0.0}
{"doc_id": 1068, "doc": {"question": "when does the next episode of izombie air", "answer": ["May 7, 2018", "May 14, 2018"]}, "target": "['May\\xa07,\\xa02018', 'May\\xa014,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the next episode of izombie air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "315db3f555dc132f68126e03794519fd02a308a4c4d0ffb7dd88cc221b903890", "prompt_hash": "814a2ddce7964e28c0d1039bd54b2a5657face02162f40427d5dde2735489a8e", "target_hash": "1f7e5acf888e2e9fb5515769d01b5d4033e72e2565ffe0c0676e8e93975d5604", "exact_match": 0.0}
{"doc_id": 1069, "doc": {"question": "when is the fa cup semi-finals being played", "answer": ["Wembley Stadium"]}, "target": "['Wembley Stadium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the fa cup semi-finals being played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11 February"]], "filtered_resps": ["11 February"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f9f379744965765cc4c35e05a635d13be4a466544a38e76e092c43381b1595e1", "prompt_hash": "765e38de147c0102ae175a5046fd68c4a20860e6083e593d16ef0cdeab15e9da", "target_hash": "475630a1705590ac67d907f046f9c08ac787de7d684a055d4c925586cc380e7d", "exact_match": 0.0}
{"doc_id": 1070, "doc": {"question": "who won the most medals in the 1924 winter olympics", "answer": ["Norway"]}, "target": "['Norway']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the most medals in the 1924 winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chocolafrog"]], "filtered_resps": ["Chocolafrog"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "78afb9cf0231f188c3585267eaa8e9969584a04e7de55976fd35eefb375d915c", "prompt_hash": "74bde3cd5bd6b5dc7d60624300ed8fd8ffb2b811c444ecca14fc187e6c1415a1", "target_hash": "591af220b37bc7f77728189b7bed35d88db34e3243e62a46d9aa15883100846c", "exact_match": 0.0}
{"doc_id": 1071, "doc": {"question": "when did muhammad ali win an olympic gold medal", "answer": ["1960"]}, "target": "['1960']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did muhammad ali win an olympic gold medal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1960"]], "filtered_resps": ["1960"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "26f7db21f3792dd3c8c5714c0b012dda260f2a05d6311be8749300a34095dbf4", "prompt_hash": "487b22bde158fb0b8c4938f647328465f886ecaffda67864157df4d9dd4b46f0", "target_hash": "e6eb59589fc11b4a06dcdbd45c76c091d0b77679983b180d59f554396ab8d2a8", "exact_match": 1.0}
{"doc_id": 1072, "doc": {"question": "what is the largest ethnic group in mexico today", "answer": ["K'iche'"]}, "target": "[\"K'iche'\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the largest ethnic group in mexico today?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mestizo"]], "filtered_resps": ["mestizo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "97786334618c34b469eac01ce679fb7bb5c278fa1a16f2967eda7f4fe451a545", "prompt_hash": "bcac442c5771e3995dbddecdc9e72a6535bca6d92976a475ffb13a57df9abd66", "target_hash": "f4409ebf1f9925a72e6046f2f5951d645b05705f7904260a2a3a0009e0aace01", "exact_match": 0.0}
{"doc_id": 1073, "doc": {"question": "where are the mitochondria located in the sperm", "answer": ["midpiece"]}, "target": "['midpiece']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the mitochondria located in the sperm?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" head"]], "filtered_resps": ["head"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "09fe5ce547380f9baa1213333ba0aa128ff87f255d7e6e56ae557b0a210f3638", "prompt_hash": "76b56ed82540138b8fcd7cfd732b7d4f81c8a1417584f8f3f2f75634c5209fd4", "target_hash": "38cb59c297f8f3913273c0644c3be0c14f5c9c9fccfd20ae295093fceb92efcb", "exact_match": 0.0}
{"doc_id": 1074, "doc": {"question": "what's the highest point in the us", "answer": ["Mount Whitney"]}, "target": "['Mount Whitney']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the highest point in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Denali"]], "filtered_resps": ["Denali"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d1216386c60e4bcf9ca0105088cc99860e5c25eebc825e8721047adfee21546f", "prompt_hash": "6ad66d1359686ec512b2b46baef8fd59d83f1972008a9717cd8758cb8962d0f5", "target_hash": "83ac3e58187b500ebe82cbcec8102e510178bcc2b3022e4a19dbc4fcbd25d60d", "exact_match": 0.0}
{"doc_id": 1075, "doc": {"question": "where does kfc get their chicken in south africa", "answer": ["reared in South Africa", "South Africa"]}, "target": "['reared in South Africa', 'South Africa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does kfc get their chicken in south africa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United States"]], "filtered_resps": ["United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d0d0e02f2bbfa7ae8bbc03fba771ec214f4bb7574043244768ef64d75566485", "prompt_hash": "b7c65d90c1a6dc142a2b76ffa3f76ae869d50ed3fb0777f7f69cdf844b2e5aa3", "target_hash": "07bb80d16252ecf2e7a3b3dc58318e632be6645d6f400e04d3ac0ab08a876c82", "exact_match": 0.0}
{"doc_id": 1076, "doc": {"question": "who has trained the most melbourne cup winners", "answer": ["Bart Cummings"]}, "target": "['Bart Cummings']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has trained the most melbourne cup winners?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bart Cummings"]], "filtered_resps": ["Bart Cummings"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed9d26c01a4e73ee3f82227c4290b5f6cdaa3eb763fbc0cc5e7c0c7e1eaf93b2", "prompt_hash": "ef31bc4289c38f61cc8221151772b0815ab6be81db1c91c1d3d1ce0e672a73f0", "target_hash": "4ef49f79e062d2cdb1a5486c9e71726fadc0b40bca2415e82f5ae06d60310dc4", "exact_match": 1.0}
{"doc_id": 1077, "doc": {"question": "what kind of trees have heart shaped leaves", "answer": ["Linden", "Eastern Redbud", "Catalpa"]}, "target": "['Linden', 'Eastern Redbud', 'Catalpa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of trees have heart shaped leaves?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" maple"]], "filtered_resps": ["maple"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8c1e6805dcf79dd4d1829718260062697107c0dd9501de0c463a647f65808838", "prompt_hash": "530efcf0a9214ff264346aec168a5e7ff07da80399c85dbf61669ae87e9dd8b0", "target_hash": "26bd72639b514df587d0f5e06e050225d9720e5f81c7c400fb8a8f23882adc3a", "exact_match": 0.0}
{"doc_id": 1078, "doc": {"question": "book series about the end of the world", "answer": ["Left Behind"]}, "target": "['Left Behind']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: book series about the end of the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Stand"]], "filtered_resps": ["The Stand"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d2113e27028ecd7c7e41af2f7addbacdf3884241228365906049145dfb633ce", "prompt_hash": "4ed063ca108a5d5fc4d8b960f6e90d7ab25d3c4808e061443cd49f03b0bb5b2b", "target_hash": "5291abb02085848bd2af2fc4670cd7f7640568d4f204e8cfb49543367cf01ad5", "exact_match": 0.0}
{"doc_id": 1079, "doc": {"question": "who sings he thinks he'll keep her", "answer": ["Mary Chapin Carpenter"]}, "target": "['Mary Chapin Carpenter']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings he thinks he'll keep her?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55e1d7cf637ccaef9437dec0174b57b4a14cade91f6f4b3cde07725e0695a901", "prompt_hash": "0b613798c7be16befd46a46526d6812261a65131f58941190b9d6fa060fc071d", "target_hash": "4d04fc8b2aab2724daaf3d68d82b431595934177ec10c9fde12d94d3dade320e", "exact_match": 0.0}
{"doc_id": 1080, "doc": {"question": "who sings gone gone gone she been gone so long", "answer": ["Chilliwack"]}, "target": "['Chilliwack']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings gone gone gone she been gone so long?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alison Krauss"]], "filtered_resps": ["Alison Krauss"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ab8db7353da74d348f067fa12f2eb13118d02a2232cdac70572d7690d580fa66", "prompt_hash": "0adcced64015015d6cb549eddc74a863d8066ed1fe9eaf2c1a0f575dc334ad2b", "target_hash": "575ab6031e2ff4efedabdb9a9b664a21263e318b096f04d8d5df446b987fefdd", "exact_match": 0.0}
{"doc_id": 1081, "doc": {"question": "when did the movie princess bride come out", "answer": ["September 25, 1987", "1987", "September 25, 1987"]}, "target": "['September 25, 1987', '1987', 'September\\xa025,\\xa01987']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the movie princess bride come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1987"]], "filtered_resps": ["1987"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32195716ebd70b4d38bcca07769ca380f3da76bf51860834cd869bbeb8398c16", "prompt_hash": "0d102c08b7fc2952a46d36da55df58d1e126571637ec77011fea907c6d2b6dad", "target_hash": "44838f67740dad67e37bad298edf55a51e960262dd0bb6c8f7cd317b19cd9654", "exact_match": 1.0}
{"doc_id": 1082, "doc": {"question": "india participated olympic hockey for the first time", "answer": ["1927", "1928"]}, "target": "['1927', '1928']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: india participated olympic hockey for the first time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1928"]], "filtered_resps": ["1928"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0553c726ce7977bc4f1ee6d21ae9ed88486c1b43e8965f2f2456d72d60a90a9f", "prompt_hash": "929a993fb05066f27a40cfa8ceee696b518fdf5cf121eba3b6f6d1add3189ea1", "target_hash": "66572a00d2ab5519138f9bcd2dd75e8f15865570a50b9b8f213b0255da3b59d6", "exact_match": 1.0}
{"doc_id": 1083, "doc": {"question": "what does it mean when your baby comes out feet first", "answer": ["A footling breech"]}, "target": "['A footling breech']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does it mean when your baby comes out feet first?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" breech"]], "filtered_resps": ["breech"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3cbf249f84f83755ca8a16b042b9478edf749d0084bb1746651819202a0789c", "prompt_hash": "7d84c659498aa294e639c3aa5033b6f3eae783d72bcd98e7653d89af78739611", "target_hash": "af875cd7659199a0e19fd7e0607cdf307f9bb01a1dd23084a93b06f14686e1c3", "exact_match": 0.0}
{"doc_id": 1084, "doc": {"question": "when did they start vaccinating for whooping cough", "answer": ["the 1920s", "the 1940s"]}, "target": "['the 1920s', 'the 1940s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they start vaccinating for whooping cough?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1940"]], "filtered_resps": ["1940"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b35e8727c94f75e136baebe054e8e494973924ff954ecb02ddda4b259ecba9f", "prompt_hash": "4d247d4732e91a8c87db48a4008802fe21b4927d64c369b5ea26671ee99f0f05", "target_hash": "45075b2620b2d43c49a10b3ee592c3de627750242787b7d9caf087227e5a0920", "exact_match": 0.0}
{"doc_id": 1085, "doc": {"question": "where was the world economic forum held this year", "answer": ["Davos"]}, "target": "['Davos']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the world economic forum held this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Davos"]], "filtered_resps": ["Davos"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e97a1dc153b7bc70adb63c5d54d6fe3dc4531244802daf471c534eab97a56c45", "prompt_hash": "4e7ac280fb7b1d37dd4bf59d867544e6fb5fe40bbdcc15fe89911c0fa1668466", "target_hash": "6f77f8eba90f88f0cc7baa6dc8ee198e6e2069e27f4dcd782b03576fd15c55b3", "exact_match": 1.0}
{"doc_id": 1086, "doc": {"question": "where was the war of the planet of the apes filmed", "answer": ["the Lower Mainland in Vancouver", "Lower Mainland in Vancouver", "the Kananaskis"]}, "target": "['the Lower Mainland in Vancouver', 'Lower Mainland in Vancouver', 'the Kananaskis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the war of the planet of the apes filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Australia"]], "filtered_resps": ["Australia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e43942897048f84e5b1af4f4362ddc240cfc95db8d9cc1d067cda38af3c56dd2", "prompt_hash": "e76544f507824ecc157204aec3bb8a7b70a5d6fb9aa87597c8274da1f7625b7d", "target_hash": "cee1b232fb1a89479f9103f2cdf6c70890a7770eb91beb21e3a06886e73d8916", "exact_match": 0.0}
{"doc_id": 1087, "doc": {"question": "what kind of beer is st pauli girl", "answer": ["Special Dark", "Lager", "Non-Alcoholic Malt Beverage"]}, "target": "['Special Dark', 'Lager', 'Non-Alcoholic Malt Beverage']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of beer is st pauli girl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pilsner"]], "filtered_resps": ["pilsner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e28326b5b2eacc2bc464c7cd7e88766e75a72cc9e88ea338d67c1b7760f9e746", "prompt_hash": "9088b5f6cdbf98faa4313637fbe96000143a6c340f5f2efd3238e32b2d26d434", "target_hash": "e61d5f4ae91d9ce04f52f817ee54c6571b25f1e67b4c179f9dc9688f1af77aeb", "exact_match": 0.0}
{"doc_id": 1088, "doc": {"question": "who is emma dating in once upon a time", "answer": ["Hook"]}, "target": "['Hook']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is emma dating in once upon a time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Nolan"]], "filtered_resps": ["David Nolan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a8018e7f4a2dfc9cebde58274b99eb0136b80ca02e304dfd07d293364655330c", "prompt_hash": "f631750bb07af763270d7b0c627ec59fc511e8f872c8d412dc07d1a5e2470972", "target_hash": "df83e8cdf6a17636aba1b3ddc1d51c3b0b6c8db13f2815aca9feacce8a1ab02f", "exact_match": 0.0}
{"doc_id": 1089, "doc": {"question": "when does the world cup of hockey start", "answer": ["2020"]}, "target": "['2020']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the world cup of hockey start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2018"]], "filtered_resps": ["2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cebfaddba2cf8a41799caa8e390ff226dd7c72976b373278db9c670bbccf1139", "prompt_hash": "e38dec489e8ae3c2a94d9ffe6e4972169e3b684ad1421d25bc9b2d80189ea994", "target_hash": "e1c889ef9bce53ec45ad030136f2024254bcb0ad3d4ff2bf83f59b9b881297d6", "exact_match": 0.0}
{"doc_id": 1090, "doc": {"question": "what is the source of information for account receivables", "answer": ["invoices"]}, "target": "['invoices']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the source of information for account receivables?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sales invoices"]], "filtered_resps": ["sales invoices"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49e0d33665f82076d53b422f9c7372f7391963e2c92df95f7ec703d2042a2377", "prompt_hash": "f7610e9f031418ec88ea0496c9b027e2710f106f91f0018c17eaefaae41a6a57", "target_hash": "8c210d52fef386cb2739f16c077fc70359165ea67cef14e4179b83444f1a383e", "exact_match": 0.0}
{"doc_id": 1091, "doc": {"question": "what was hawaii's primary export to the united states", "answer": ["macadamia nuts", "coffee", "honey", "sugarcane", "pineapple", "livestock"]}, "target": "['macadamia nuts', 'coffee', 'honey', 'sugarcane', 'pineapple', 'livestock']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was hawaii's primary export to the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sugar"]], "filtered_resps": ["sugar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1cf122a6a52afeffbb6b8507da007ef9d1c3af3b1d362c7b8bde646083e52b39", "prompt_hash": "cb84ad6c9d228a92077e8eff233326a0ef6af454f570ad48e9f08b447dc9259e", "target_hash": "7d0bc3f0af391b25921940b1200a8c25804375144086aa0f37a2fe912a62331e", "exact_match": 0.0}
{"doc_id": 1092, "doc": {"question": "where is the villa in call me by your name", "answer": ["Moscazzano"]}, "target": "['Moscazzano']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the villa in call me by your name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Italy"]], "filtered_resps": ["Italy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "84d668964a4f21ed3dc4d5c1d9bcf1ed6c2081c5e4899088f50f4b1263c8e61b", "prompt_hash": "c4b89391c94926272dbcafca0e5a794bc8384dfce756d54a37a812660bd990ef", "target_hash": "2d34331069ac0e0b0e7d683ab2fa5939aa8ca68420fe71220b944d35588983df", "exact_match": 0.0}
{"doc_id": 1093, "doc": {"question": "when does a wrinkle in time come out in canada", "answer": ["March 9, 2018"]}, "target": "['March 9, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does a wrinkle in time come out in canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1962"]], "filtered_resps": ["1962"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfac8b812544ca0e1dc666ebd8e02115b81e332a46f5d85a2f912acdf7b14bfe", "prompt_hash": "5ee6a644b2ed8a3d4a57b695110a25d5c92785793d41bc0120d27a64a4f56b75", "target_hash": "735cc8f957bbeb080265915f6c6b586c0edcc9a2631e3ceb0be1bb1168599706", "exact_match": 0.0}
{"doc_id": 1094, "doc": {"question": "new york heart association functional classification of angina", "answer": ["II"]}, "target": "['II']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: new york heart association functional classification of angina?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" II"]], "filtered_resps": ["II"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ac000061923588d5cafab937a468383d70f7f82f41a13c4390fb0ef8e72bcad", "prompt_hash": "d75a063fc0a953397550382f20daf65ba6f060367e2d0b2a4619d7586652ba81", "target_hash": "f970bf56427ad71bad1490e11e18ffe0066474ead5c808838bf2c588da95584d", "exact_match": 1.0}
{"doc_id": 1095, "doc": {"question": "when was the movie cool hand luke made", "answer": ["1967"]}, "target": "['1967']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the movie cool hand luke made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1967"]], "filtered_resps": ["1967"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d61cd7a2d68f82dd0f71acdc8359ed136552cbbbd7e5e531d32099ae16552b23", "prompt_hash": "80c052baeee867e0eb1d3c30d753abb134bef733b746a163303a6840a6f6b85e", "target_hash": "e7491d796092f11923cd5c6df2bcf505e7272ef0f6adb6cd9331c84b42cf70c2", "exact_match": 1.0}
{"doc_id": 1096, "doc": {"question": "who sings gimme shelter with mick jagger", "answer": ["Merry Clayton"]}, "target": "['Merry Clayton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings gimme shelter with mick jagger?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Merry Clayton"]], "filtered_resps": ["Merry Clayton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e3e9a3314f55300941f341835166688217f8eaca69a1fe647dcfd18e6630c40f", "prompt_hash": "06a941e3df86db76fde282e15b48ef7738ed5d479f062adae9f74e7e3a78292d", "target_hash": "3296ff8b0f4c853f90a4d72e18448f6851f615d124d7aa5349ef89db6081a9df", "exact_match": 1.0}
{"doc_id": 1097, "doc": {"question": "what is a buckeye the mascot for ohio state university", "answer": ["Anthropomorphic buckeye nut"]}, "target": "['Anthropomorphic buckeye nut']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is a buckeye the mascot for ohio state university?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" football"]], "filtered_resps": ["football"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "755daf24c89832890a5e967f13932f4da52d95533c7f7d6a354ffc533867cfa6", "prompt_hash": "7f90ff8fc4368605ad1f3295ad6cf4ac2181fca222a2ed7d23ce22841f760118", "target_hash": "09400fa82cf05b3c2cb4bf170b10517230e75960222520d1f560d3b50da529a3", "exact_match": 0.0}
{"doc_id": 1098, "doc": {"question": "the fellowship of the ring director's cut length", "answer": ["228 minutes"]}, "target": "['228 minutes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the fellowship of the ring director's cut length?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3 hours"]], "filtered_resps": ["3 hours"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "679bb254dac119d2f4569bb035ddf4dfff35de4ceed55560f087c0b2e445bc3f", "prompt_hash": "0a4d3bf9e74408176de485666bc541bf35a2f02a403f860c39f948a93a113164", "target_hash": "16e60262f5a7ac6d9d7ddfdc03eba0d7632d75723922886c2c5b84e60dc68738", "exact_match": 0.0}
{"doc_id": 1099, "doc": {"question": "who plays addison shepherd on grey's anatomy", "answer": ["Kathleen Erin Walsh"]}, "target": "['Kathleen Erin Walsh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays addison shepherd on grey's anatomy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ellen Pompeo"]], "filtered_resps": ["Ellen Pompeo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3eca5131603800b78b0049b78be1f16458dfdb1d788d87095eb9e56f057d72fa", "prompt_hash": "7dba48e91e7acc7e4795567baeb6db6e39c22993c0b3f773e2b8f3a891c5ded2", "target_hash": "0747b640b27cabb55ca0fc7175154021f88bbbcdb1733a22d93ff22164ac6ad3", "exact_match": 0.0}
{"doc_id": 1100, "doc": {"question": "who won the womens 2017 ncaa basketball tournament", "answer": ["South Carolina"]}, "target": "['South Carolina']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the womens 2017 ncaa basketball tournament?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" South Carolina"]], "filtered_resps": ["South Carolina"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "850b2f56bbba3bce70e8dff5269ffcfb81e8ec4aae866773ce70d23baaf6c0be", "prompt_hash": "858e1219383bbe1b83cdefe4ecb7e150cec286cf05a01f82b028d1c57c9b8ead", "target_hash": "5616b2e72a65cb242b419dc62f16fa7ad9af2093be0fa3fcb99a53efb73b9d14", "exact_match": 1.0}
{"doc_id": 1101, "doc": {"question": "when did canada sign the un declaration of indigenous rights", "answer": ["May 2016", "12 November 2010"]}, "target": "['May 2016', '12 November 2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did canada sign the un declaration of indigenous rights?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1990"]], "filtered_resps": ["1990"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "694cd46c20a5171540b96a2c43cf7a3d1d089bb7b750b46e6ccac2dd49db0aaa", "prompt_hash": "c9e211f945452fa6f2fee1633fdef73be8a535eaabd454f0632b8dcf1cd56a55", "target_hash": "e4ba5ec89f0bf3b0cd13430693e8ab40be2b79c85ad72cc9336d03b3c34da982", "exact_match": 0.0}
{"doc_id": 1102, "doc": {"question": "distance from one side of a bridge to the other crossword", "answer": ["span"]}, "target": "['span']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: distance from one side of a bridge to the other crossword?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" span"]], "filtered_resps": ["span"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "028ae7b938c17d1015384bdbd9adc732f4ba5222a6dfe81f5c5e8f8fb4433f6a", "prompt_hash": "bfdd1539d9eced64338af727fc3896ac804f3b1b70e5ad941d62810400ec43db", "target_hash": "fbdbbed159f8e956dbb97cb82df1cd900451cf798bc44bf336a9a1433fa38925", "exact_match": 1.0}
{"doc_id": 1103, "doc": {"question": "the cuban revolt against spain was led by", "answer": ["Máximo Gomez", "Antonio Maceo", "José Martí"]}, "target": "['Máximo Gomez', 'Antonio Maceo', 'José Martí']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the cuban revolt against spain was led by?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" José Martí"]], "filtered_resps": ["José Martí"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f9e2a11ca7cac92a7e73065d766769a8e8ebb2b3fe2535cb24801660e7f74248", "prompt_hash": "437d3ca87f391f498f7e4abd2901341aa481e26831f8350a96cc02290278c6d8", "target_hash": "d0b7f9e39fc045bbfa87f81176baf1fbba513a7f3a1f26c95afb50bd17dbf482", "exact_match": 1.0}
{"doc_id": 1104, "doc": {"question": "when did sweet caroline start at red sox games", "answer": ["1997", "at least 1997"]}, "target": "['1997', 'at least 1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did sweet caroline start at red sox games?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1974"]], "filtered_resps": ["1974"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ddf6c43c4d9409a46fcf8292f5da86764901eae2ea216628d3e2d71b8a55279", "prompt_hash": "5ce62af93c65c5b2fd25883245b3a26d93cb3b90a20dfefd511a48c02c111d93", "target_hash": "08794ce26b7ddca9952b446e816cca368c59189c8a74c2b1799b3f1f6fb3976d", "exact_match": 0.0}
{"doc_id": 1105, "doc": {"question": "what does the adrenal gland produce that is necessary for the sympathetic nervous system to function", "answer": ["epinephrine"]}, "target": "['epinephrine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does the adrenal gland produce that is necessary for the sympathetic nervous system to function?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" adrenaline"]], "filtered_resps": ["adrenaline"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e9c2089deec75e3ed27f1b66030c06479ef6f4feef977dd0246f9c434e2c5c37", "prompt_hash": "a20e67f2c3d60e260ade2907969b58dfd7f8628608737231171b49e9080a5327", "target_hash": "88e179f7f51c013fb2b545499457146039f0dbe42e2eab98e3f87e200cae5c2b", "exact_match": 0.0}
{"doc_id": 1106, "doc": {"question": "colby chees is named after a town in what state", "answer": ["Wisconsin"]}, "target": "['Wisconsin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: colby chees is named after a town in what state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wisconsin"]], "filtered_resps": ["Wisconsin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0f060eda16322e58cf04cc7507efb942732e2f27e2ab07698b42f4622a38a721", "prompt_hash": "e9134dfd697928412a4aab2fcc40a1ca61e9ab6f49d25a38c177b49932117add", "target_hash": "256b5a5262e9bf6e58e4d65e78a8d734beb656b59e121a1b774566753bf4b770", "exact_match": 1.0}
{"doc_id": 1107, "doc": {"question": "when does season 8 of hawaii five o premiere", "answer": ["September 29, 2017"]}, "target": "['September 29, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 8 of hawaii five o premiere?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" September 24"]], "filtered_resps": ["September 24"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7c0497d2636e4db2257f88766e1639010b6cd6ed9ad246f5105394a355f3733d", "prompt_hash": "6b3d8839388603df8ceb161254ed2c09ee1ad538b2d35001a4f35d9e8e658b7d", "target_hash": "dfa2d9567f46b0aaa1a45e1ab8f613de86eb4368ca3e1941dcbb25ced7ed0eb5", "exact_match": 0.0}
{"doc_id": 1108, "doc": {"question": "what happens when iron reacts with oxygen and water", "answer": ["Rust"]}, "target": "['Rust']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what happens when iron reacts with oxygen and water?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" rust"]], "filtered_resps": ["rust"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ec0225f0e2737f3cb699f73cce508e98c160b8d09698dfa52ea30045b2e0305", "prompt_hash": "05f88ea716c6112a107737c94d887199784ea182e349671b3ad3b288434f39aa", "target_hash": "f1b4c2b121fd8d6f98d4f2b06c6ae27c8626d78c8ca13134b784d91f70d178c6", "exact_match": 1.0}
{"doc_id": 1109, "doc": {"question": "when did the first train run in england", "answer": ["1560s", "As early as 1671", "1830", "1804"]}, "target": "['1560s', 'As early as 1671', '1830', '1804']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first train run in england?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1825"]], "filtered_resps": ["1825"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77bb8a2e37a0ba750cd77b7c4d917f7f2fc2ec5503a60bf15927800cdfe73bc8", "prompt_hash": "280ab11a35c74e7ad58e30d86414db536624c1ebcb59b0f2fb083dcf4877cc13", "target_hash": "96a6907291142db5ee8bb76d65a880424b1ff6c9f740e6cc0bb5c93132170071", "exact_match": 0.0}
{"doc_id": 1110, "doc": {"question": "how many ammendments to the constitution have there been", "answer": ["Twenty-seven", "Thirty-three"]}, "target": "['Twenty-seven', 'Thirty-three']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many ammendments to the constitution have there been?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 27"]], "filtered_resps": ["27"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a563124720d6134d67d3a28ae87dcdb8e8f68504cefce194b326ca24032a7acc", "prompt_hash": "b7573c19eaa2f2af1a9f9fd5b1c781cdd23d6ca937662496e80e8f9d29250438", "target_hash": "2a4a108089f410590433e2bc730a7b36eb5dcd01ecddb3379a549fbdf187bb6c", "exact_match": 0.0}
{"doc_id": 1111, "doc": {"question": "who sang it must have been love but its over now", "answer": ["Roxette"]}, "target": "['Roxette']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang it must have been love but its over now?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pointer Sisters"]], "filtered_resps": ["The Pointer Sisters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cf1a4770365a0b72132106efcd2e1f34928f9a8c2b939ba94458cb4b82f5e9dc", "prompt_hash": "a5992bd7d6cd4e2de3b1056d0a416021376063eec9fa9bdbfad5e8a95a2d927e", "target_hash": "f29bd4cf960ff8edd93603ec21503d31f3445725531822004b688e8ceefc5e07", "exact_match": 0.0}
{"doc_id": 1112, "doc": {"question": "who invented the first computer game in 1962 and what was the name of the game", "answer": ["Spacewar!", "Steve Russell", "Wayne Wiitanen", "Martin Graetz"]}, "target": "['Spacewar!', 'Steve Russell', 'Wayne Wiitanen', 'Martin Graetz']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who invented the first computer game in 1962 and what was the name of the game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Steve Russell"]], "filtered_resps": ["Steve Russell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "683fbab9074fa48699b5f844e5ae70f61985394f472b9836c80e6f09f435539a", "prompt_hash": "950e228bcd097adf75cb83f159613f82895e5e0042482b3b87ea77eb86008435", "target_hash": "2d6f618b6fd7645bace4345c33771f589a0970259907319e0807bcd6fc4ebff6", "exact_match": 1.0}
{"doc_id": 1113, "doc": {"question": "who did the original spirit in the sky", "answer": ["Norman Greenbaum"]}, "target": "['Norman Greenbaum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the original spirit in the sky?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Eagles"]], "filtered_resps": ["The Eagles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "95e7aee9ed370ac6d1313c18349da9b0452eaa4680503d2ca372685da728a952", "prompt_hash": "137e96e2a2dfb02e21751a34f587481c83164ac46f1c5bc1c3a686ab76197111", "target_hash": "29313ecd987e0219396c3a776b77234dabf687e04198b209753b4041d98ccf39", "exact_match": 0.0}
{"doc_id": 1114, "doc": {"question": "internet based test of english as a foreign language test", "answer": ["TOEFL"]}, "target": "['TOEFL']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: internet based test of english as a foreign language test?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" TOEFL"]], "filtered_resps": ["TOEFL"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f1aa484eceb77fed728946bb7e29f9770b813fbb7e411a8374e5212c3fd31c0b", "prompt_hash": "f8d96616288c8224d4bb70ac7a5659b6cfbcd7d86fbf7fa39ae551de33426183", "target_hash": "7e6abdfabb5b3cf05b6b7476e6007c96d4af8efad5ee3790808667c8eba0ce27", "exact_match": 1.0}
{"doc_id": 1115, "doc": {"question": "who plays rachel on jessie punch dumped love", "answer": ["Isabella Palmieri"]}, "target": "['Isabella Palmieri']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays rachel on jessie punch dumped love?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rachel Bloom"]], "filtered_resps": ["Rachel Bloom"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8b16cfcecca49ccbf42f976201ba136c2872027d36455f99538bbed9450d31ad", "prompt_hash": "0932119a80458e7c6ffe5db34a5ba979c45c126b7894fb437429dba8c04a000a", "target_hash": "b2cca783a585717f12368c5fb448cbe7743bfe06be38251cc06c8f4304fe0584", "exact_match": 0.0}
{"doc_id": 1116, "doc": {"question": "when is season 8 for game of thrones", "answer": ["2019", "in 2019"]}, "target": "['2019', 'in 2019']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is season 8 for game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "71eb490e383766e94308c57a18079ba9933ec5eb67cd28c77f92d3e7cd316760", "prompt_hash": "950d990bd16230cf7a03cfa1f6b3782f6b1d70209eeec42cce6e5a889ecbea8f", "target_hash": "9cc0f16e125e848f11c19e4226cf06a218e32ddb9116f2848a3fb64336259bdb", "exact_match": 0.0}
{"doc_id": 1117, "doc": {"question": "who are considered to be the founding fathers", "answer": ["George Washington", "Benjamin Franklin", "John Adams", "Alexander Hamilton", "John Jay", "James Madison", "Thomas Jefferson"]}, "target": "['George Washington', 'Benjamin Franklin', 'John Adams', 'Alexander Hamilton', 'John Jay', 'James Madison', 'Thomas Jefferson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are considered to be the founding fathers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Washington"]], "filtered_resps": ["George Washington"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42d204abe521e956040292bd9ff885882a10a177fb67374602e459c94c8f0652", "prompt_hash": "0c868d6cbc985e58782d5ae7dbc05ea78605dc629cb3dba791a19adbca88f910", "target_hash": "2b3c78f3d23dd14a33c8780f3f3552e6ef8003de4f873b3646ad80b4e410bbde", "exact_match": 1.0}
{"doc_id": 1118, "doc": {"question": "how many players on the line of scrimmage in american football", "answer": ["at least seven players", "at least seven", "seven"]}, "target": "['at least seven players', 'at least seven', 'seven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many players on the line of scrimmage in american football?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" eleven"]], "filtered_resps": ["eleven"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a9e570e96d00f8c22701b661fb36e447432ad780eac4fa770b000b45e80b81b", "prompt_hash": "b5455f5653694abf2a82c2be932eba9173ed4093876efff210e856fb97c74f19", "target_hash": "b9e0a4c52aef01969b266c9de8d9c3caff137ee86e24a0c82448de6e162c2337", "exact_match": 0.0}
{"doc_id": 1119, "doc": {"question": "who holds the record for most everest summits", "answer": ["Kami Rita", "Phurba Tashi Sherpa", "Apa Sherpa", "Phurba Tashi", "Kami Rita Sherpa"]}, "target": "['Kami Rita', 'Phurba Tashi Sherpa', 'Apa Sherpa', 'Phurba Tashi', 'Kami Rita Sherpa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who holds the record for most everest summits?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Reinhold Messner"]], "filtered_resps": ["Reinhold Messner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "46d4e4be172a48dda716c665894a4615c956097f15a44e00fbf91d88029216f4", "prompt_hash": "d149576a7f5c2a1528907f48a9c510e09abec1bb27b156b34c4a842416398e91", "target_hash": "dfc6c737394e88b0f5ffc69514edb65511682851588da99fce8d0eb3c6109ae8", "exact_match": 0.0}
{"doc_id": 1120, "doc": {"question": "who is the director of nightmare before christmas", "answer": ["Henry Selick"]}, "target": "['Henry Selick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the director of nightmare before christmas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Henry Selick"]], "filtered_resps": ["Henry Selick"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "280ddddf3faf10f5f13fbccee6dd4fd798e5e2f5dbbe9a33ba5c2873fa897e70", "prompt_hash": "2bac7fc5bd73fa800e560f995fd5e63789d19f097ae7e804ee1c55aa939fe4b7", "target_hash": "7cc0903e1db8cd8a8184b53a170e02f4720692f5a7d72f20775498217202b5ad", "exact_match": 1.0}
{"doc_id": 1121, "doc": {"question": "what are the four main types of precipitation", "answer": ["sleet", "hail", "graupel", "snow", "drizzle", "rain"]}, "target": "['sleet', 'hail', 'graupel', 'snow', 'drizzle', 'rain']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the four main types of precipitation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" rain"]], "filtered_resps": ["rain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "142d348098d065028525dfba8159f7d5903e7f293538600cc0d61a00470f0c73", "prompt_hash": "b84d5aeba41b06b38e7c0007ac564df9010de75fe92f6e01f7c89162b7fd6402", "target_hash": "9bf96e2b92352db00cd5b33f961ea7ef6b6be036602aaf034d4ede7826777d44", "exact_match": 1.0}
{"doc_id": 1122, "doc": {"question": "who was the 11th prime minister of canada", "answer": ["R. B. Bennett", "Arthur Meighen"]}, "target": "['R. B. Bennett', 'Arthur Meighen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the 11th prime minister of canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mackenzie King"]], "filtered_resps": ["Mackenzie King"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "702268b59502e31302c212424a9c783bf30939e73603fc6f6a23e12131378291", "prompt_hash": "74a0b56ea9ecdf6a0c2da0ee1e671703adc831616382309d5804799280bb7594", "target_hash": "7bd0bb064fb3ffbef7ac7288ddd693e7e2a26129f33aa9eb473da30385994ff2", "exact_match": 0.0}
{"doc_id": 1123, "doc": {"question": "where is hallmark channel home and family filmed", "answer": ["Los Angeles, California"]}, "target": "['Los Angeles, California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is hallmark channel home and family filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nashville"]], "filtered_resps": ["Nashville"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "65ea93ab90eb45a683700142ef47c7f9f0915358b02d58cc2706a8336310489b", "prompt_hash": "d22457d9c46ff47d144308d918123b7d6f3e3d67bdc4bd59b74dcd0263bec2da", "target_hash": "a2c10f4279386797abf2e91bbe55c04dcd583ec0f43eb8b0dff753b6ea911942", "exact_match": 0.0}
{"doc_id": 1124, "doc": {"question": "which term means the study of the nature and causes of disease", "answer": ["pathology"]}, "target": "['pathology']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which term means the study of the nature and causes of disease?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pathology"]], "filtered_resps": ["pathology"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfb8fb452d8f134bfd55bca783a1a862f69ba73caf5d5ac70cd1d9560452ccb0", "prompt_hash": "cf2886b30eece7f08c82ef59d16b924ce68c5bae8350a8a3014a56061d866c5a", "target_hash": "ef8ab5ddcd4e02aaecb4ebbe33dbbd13dd06c84ead3b9a612e76095eabf3106e", "exact_match": 1.0}
{"doc_id": 1125, "doc": {"question": "where are the next two winter olympics going to be held", "answer": ["Beijing", "TBD"]}, "target": "['Beijing', 'TBD']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the next two winter olympics going to be held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pyeongchang"]], "filtered_resps": ["Pyeongchang"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b50a8e5c950ebd3f9207ba5d0a54fc8edbc16bb74882c7459510b30d648c8b5", "prompt_hash": "d51aa9e04a6f27462d51c3f3c47f9fe348447fed0a3d8dda98a0da723ec5d82f", "target_hash": "1cf739fe486fd46fc05f0537469c8342e113333905affe2c6c2f61e69815ffe4", "exact_match": 0.0}
{"doc_id": 1126, "doc": {"question": "what is the poorest county in the state of florida", "answer": ["Union"]}, "target": "['Union']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the poorest county in the state of florida?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Liberty"]], "filtered_resps": ["Liberty"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5c7bc0af97b1ab5edda3d183b9568462242c7761f2160bab8664eb357e8bc8e4", "prompt_hash": "f065cae74a46d260f3e289aeed648576234df67516637f079871c015e9ff2b63", "target_hash": "0542e454c1572a9cc5d2be94174e358023680dfdc9e9b241cbabf7a038a682a2", "exact_match": 0.0}
{"doc_id": 1127, "doc": {"question": "who sings i can't take my eyes off of you", "answer": ["Frankie Valli"]}, "target": "['Frankie Valli']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i can't take my eyes off of you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Frankie Valli"]], "filtered_resps": ["Frankie Valli"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "08bfccd0c0adbeab35f6128f826bf2ed81031d0b90f0bd5a1b4a24f42440c7ac", "prompt_hash": "6d16b4097084a2ee90ca69aa7edb7915cbfbdb5f8a31c3513eb8f7c60af6816c", "target_hash": "9f1ea4712f9b6f40ad73e753227ca43e742f152f9bf5f9db8946ee23fdadb8bb", "exact_match": 1.0}
{"doc_id": 1128, "doc": {"question": "who plays colin on young and the restless", "answer": ["Tristan Rogers"]}, "target": "['Tristan Rogers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays colin on young and the restless?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William J"]], "filtered_resps": ["William J"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "004e32f44b08e019184223bb9e9102efd1d0a552968ceb75c26ceb2d40504253", "prompt_hash": "e212fdcd89bae54c9cce3057775feebc485ab2bce484de23484f789213828d20", "target_hash": "29d6caaaf147fe50fdbeb32a4573e9ce43316986324bc422817cf82ffcaed3d4", "exact_match": 0.0}
{"doc_id": 1129, "doc": {"question": "how many indian astronauts have gone to space", "answer": ["2"]}, "target": "['2']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many indian astronauts have gone to space?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 0"]], "filtered_resps": ["0"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5bd85862a5c20101332e762a416a7bb9ced0bea42669a6e2c77353205e37fefd", "prompt_hash": "a725f3ba319fa57f6240693ec0e240c59a5ce06b7b90349483b962238ac144f0", "target_hash": "64328e4bf8426c9e380b32f3a0cad98d4a85b413e586d37feb2fe8d2ea50e31f", "exact_match": 0.0}
{"doc_id": 1130, "doc": {"question": "what is the width of a cricket bat", "answer": ["no more than 4.25 inches"]}, "target": "['no more than 4.25 inches']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the width of a cricket bat?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 38 centimetres"]], "filtered_resps": ["38 centimetres"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae9987fba102305c562add109e5b019dfa08876bfc4aa4bfca1c5da79bd84d43", "prompt_hash": "b35902358944dd24d0656e8f08d119ac32ac1e4cf01182f365bc98654739e575", "target_hash": "0f16b5820c8ccbf0f413295582d43028d022f03930d761d153671f7173107c4b", "exact_match": 0.0}
{"doc_id": 1131, "doc": {"question": "why did ric flair go back to wcw", "answer": ["February 1993"]}, "target": "['February 1993']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why did ric flair go back to wcw?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" In 1996"]], "filtered_resps": ["In 1996"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f9256013278475a00956246c12783cb45090d85f8484237e6c436456f7e961a4", "prompt_hash": "bc17f55d74811a162e0633ea77ffafded1c42e6fdd9aed5cc0d6e3b458447994", "target_hash": "c8d0d4724b989b36d081f8e70093f7d35279de9637fa1cfea7fc03e3a10abf2c", "exact_match": 0.0}
{"doc_id": 1132, "doc": {"question": "who plays manny in diary of a wimpy kid", "answer": ["Connor and Owen Fielding", "Wyatt and Dylan Walters"]}, "target": "['Connor and Owen Fielding', 'Wyatt and Dylan Walters']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays manny in diary of a wimpy kid?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Zachary Gordon "]], "filtered_resps": ["Zachary Gordon "], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a0482e1f16ee57dbae413d3c963422905ff61b57825542612a65fd897d94df5", "prompt_hash": "2ecb3f2facd86574848c10c64caef39ce76a80aace8aa8d0b7148cd0f020d08e", "target_hash": "644a694829613d1e13f7624f49782f6df0bafdf27f9447e5a1e2579826257ece", "exact_match": 0.0}
{"doc_id": 1133, "doc": {"question": "where does the light reaction of photosynthesis occur", "answer": ["on the thylakoid membranes"]}, "target": "['on the thylakoid membranes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the light reaction of photosynthesis occur?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" chloroplasts"]], "filtered_resps": ["chloroplasts"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dcaa6f7b94d195017a76cd57e2e1a17cbff897236011f616b5c3738f57858ff6", "prompt_hash": "77b7fbe62e6057d00653b591114724741113373879a9b42a7d7fa19ae42673d6", "target_hash": "580c57b8285d94c06b4f56fd346a5f31847e53ba11b3220d57f78144e2c7d3e9", "exact_match": 0.0}
{"doc_id": 1134, "doc": {"question": "who has the most super bowls in nfl history", "answer": ["Pittsburgh Steelers", "The Pittsburgh Steelers"]}, "target": "['Pittsburgh Steelers', 'The Pittsburgh Steelers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most super bowls in nfl history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pittsburgh Steelers"]], "filtered_resps": ["Pittsburgh Steelers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9ae7cdc80cfdbcb9823625bc2ba7fb333c1572f7c91d918b4fdfc88fc8e008e", "prompt_hash": "f562129120f60ea5fbb6c0a122affcbb176861a78945fc5352f8de74a03c3308", "target_hash": "5d9cea0f8bade5d8003784609f82573be8623a7c1268eedab1a53f091d9754d2", "exact_match": 1.0}
{"doc_id": 1135, "doc": {"question": "who has appeared in the most royal rumbles", "answer": ["Shawn Michaels"]}, "target": "['Shawn Michaels']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has appeared in the most royal rumbles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" CM Punk"]], "filtered_resps": ["CM Punk"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ffa858c4410f5537a43e8d170dbdd9f627e8b773307703a7dbce2eb8d2307761", "prompt_hash": "e05cf23b6b0c5473328382484d6a2a7fb8291221f55386c60a128341ec1fc9a7", "target_hash": "d589d5501a9e3ac6f195e81cf42e594cbce95f5362de6b3054b58fab8f2b0455", "exact_match": 0.0}
{"doc_id": 1136, "doc": {"question": "where did they film the game of thrones", "answer": ["Croatia", "Iceland", "Canada", "Spain", "Morocco", "Malta", "Scotland", "United States", "Northern Ireland"]}, "target": "['Croatia', 'Iceland', 'Canada', 'Spain', 'Morocco', 'Malta', 'Scotland', 'United States', 'Northern Ireland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they film the game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Croatia"]], "filtered_resps": ["Croatia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9015fb6d395a884e37f04dc0c44b2a67366dd94545f0639158954db949607c6", "prompt_hash": "a5fa247ba373a40da23f46c0e0ae150dd58819695c437a1da59c9e780491802d", "target_hash": "d718106366651d561d0419525d8c540ead0b0c96a2978d29cfc2a2ce944eec6a", "exact_match": 1.0}
{"doc_id": 1137, "doc": {"question": "where did the last name taylor originate from", "answer": ["Old French"]}, "target": "['Old French']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the last name taylor originate from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" English"]], "filtered_resps": ["English"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "363c3458365ab476ddd9dc1dc094e7ad4f8a5dc96b05674d027f51376a083aa9", "prompt_hash": "1cce97b96bbe9fc59b341b361d4988bdb3e6e84b8428c464b0c9f3b21b2fd70a", "target_hash": "cc8d4e6719f22e52327782fe1631b7702defbe4523d50750245a830a22463b5e", "exact_match": 0.0}
{"doc_id": 1138, "doc": {"question": "american horror story freak show girl kidnapped by clown", "answer": ["Bonnie Lipton"]}, "target": "['Bonnie Lipton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: american horror story freak show girl kidnapped by clown?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Wayne Gacy"]], "filtered_resps": ["John Wayne Gacy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5fc0f3fc217832b68b151e890ecb23d02e67c73009893649843c3cde967c8cd5", "prompt_hash": "518770d4ff753b140c88e1515af0798699cb2d86ffb91042e5bc29301305bf77", "target_hash": "776c1fe791b9042a9f6e9a3df248a298a513d6efef1abe2e18afe45c5b7738ae", "exact_match": 0.0}
{"doc_id": 1139, "doc": {"question": "who won the most mvp awards in the nba", "answer": ["Kareem Abdul-Jabbar"]}, "target": "['Kareem Abdul-Jabbar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the most mvp awards in the nba?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kareem Abdul-Jabbar"]], "filtered_resps": ["Kareem Abdul-Jabbar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49cc365df978e4ae521cafe9dd12cd6449f67229c7a9e26e8ec02e597a13ba7c", "prompt_hash": "aecbf7b103901ed12c758f565bbe524e05127285ee4a9fdd592a9fa795fa39ea", "target_hash": "d953184597acb7a70d0d0cd3f9418219906cbc9eb2f304e619c6487486496bb2", "exact_match": 1.0}
{"doc_id": 1140, "doc": {"question": "who plays sven in the santa clarita diet", "answer": ["Leo Howard"]}, "target": "['Leo Howard']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays sven in the santa clarita diet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Timothy Simons"]], "filtered_resps": ["Timothy Simons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b012d1290a901ac8ea768b729ba3b22467613e1370fd6df18e199c3ce8d57f7d", "prompt_hash": "c1e6690f256a0d1ac310d4f693bb8d2334f036e968849879237e0b3c76cba0cd", "target_hash": "23612cebbf862ef39bfce3e527cccc4f865ef36a40a53ce31d482a2ea1466548", "exact_match": 0.0}
{"doc_id": 1141, "doc": {"question": "who is the current cruiserweight champion in wwe", "answer": ["Cedric Alexander", "Cedric Alexander,"]}, "target": "['Cedric Alexander', 'Cedric Alexander,']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the current cruiserweight champion in wwe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Drew McIntyre"]], "filtered_resps": ["Drew McIntyre"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c5bf952b76fa9b609464cfa863f7a366cd5c6e26fcd3ea506a412dfbd89d78e2", "prompt_hash": "723ec4e83ea663e14d73afc0ab41a4a71e17666f01009e7dc4af285549b1c0ed", "target_hash": "912593b1d0bc32c504cfeeaf307c55dcaa1c577bb84f89e098e1b90e8bc58b1c", "exact_match": 0.0}
{"doc_id": 1142, "doc": {"question": "who did corbin dance with on dancing with the stars", "answer": ["Karina Smirnoff"]}, "target": "['Karina Smirnoff']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did corbin dance with on dancing with the stars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Grey"]], "filtered_resps": ["Jennifer Grey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2ea655595ea5fa1f3d11e9e013bf77c30d2a41d1c94fd2c26eff516370f2b364", "prompt_hash": "33bed9481e5af2b7d6ca676ae1f6ff8587ec26117fe08cc3c29a141cdacccc0a", "target_hash": "cec25e245acdbaebd8847072ad2634ac6d424c2e03bf864a5bb88d39fa15867d", "exact_match": 0.0}
{"doc_id": 1143, "doc": {"question": "who abolished the monarchy and declared france a republic", "answer": ["Convention", "France's Legislative Assembly"]}, "target": "['Convention', \"France's Legislative Assembly\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who abolished the monarchy and declared france a republic?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Napoleon Bonaparte"]], "filtered_resps": ["Napoleon Bonaparte"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "511101653c590434fd84da3aeb863015634817e9c8009f3dc30e130ec073bf46", "prompt_hash": "f3872fa3aaf1f8a2dbd3b9d2936903779792330f7a9f7ea20c8e0e81095e58b5", "target_hash": "de637c8cd55cd314dc3f4844dae66e8d0f03517868d4056628348854d5485c5c", "exact_match": 0.0}
{"doc_id": 1144, "doc": {"question": "who plays harley quinn in the lego batman movie", "answer": ["Jenny Slate"]}, "target": "['Jenny Slate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays harley quinn in the lego batman movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Margot Robbie"]], "filtered_resps": ["Margot Robbie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "39dca08ff327c76587baf144464e5f6f66c2f761afe34b0d31a381e9456a8f61", "prompt_hash": "53a0a256e96cea3721f99d6a479dfa6213179d47b8dd2de2afbc92147a6baf71", "target_hash": "7f35cf5b9e1fe424e92ff37b3af0a89ee2e8affe23cac9af6ec5f524b79afff7", "exact_match": 0.0}
{"doc_id": 1145, "doc": {"question": "how many different kinds of pokemon cards are there", "answer": ["eleven"]}, "target": "['eleven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many different kinds of pokemon cards are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11"]], "filtered_resps": ["11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a0e2fcb1fa4f02456add485c1164a00d08a83a310191e7e3d43013ff8efcb8f9", "prompt_hash": "11b00607e2371b87f8636e6c8d20f0ce497d2cdc84eeb8ab5bee882204c9ed98", "target_hash": "f694c3ddf6cc74f18ce5dc428cfa884074a5dd6f14c1be18aa7c3ac1718654a6", "exact_match": 0.0}
{"doc_id": 1146, "doc": {"question": "what is australia's location in the world and region", "answer": ["Oceania"]}, "target": "['Oceania']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is australia's location in the world and region?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oceania"]], "filtered_resps": ["Oceania"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c3de960671c2984c3f629f6a37847cf1b5b9c3aeebca705ea7f436fb3d311b1f", "prompt_hash": "47f4db5bfa1e5e24f301f90cafb775f41942ff5047a1177b93263e3d60a0d850", "target_hash": "3dac37d9976f2c27ee7ab445b37867e7101d5046af5ef98bed5c7bb793befb45", "exact_match": 1.0}
{"doc_id": 1147, "doc": {"question": "batman and robin episode only fools and horses", "answer": ["\"Heroes and Villains\""]}, "target": "['\"Heroes and Villains\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: batman and robin episode only fools and horses?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Batman: The Brave and the Bold"]], "filtered_resps": ["Batman: The Brave and the Bold"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5e99ce8668a89714055fdd10b57a9f1782560ad700f77816233e12ccee65ead", "prompt_hash": "903a804ed1595a5ba17e213ca4f6753c84f9d08c60c1170bd4f46a369e522cec", "target_hash": "02ada774c5e253a03a1a4d4f4b62b7f460b1d29bcf2733294930f7c0f2d6fbff", "exact_match": 0.0}
{"doc_id": 1148, "doc": {"question": "who won the 30 man royal rumble match", "answer": ["Shinsuke Nakamura", "Randy Orton"]}, "target": "['Shinsuke Nakamura', 'Randy Orton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the 30 man royal rumble match?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" AJ Styles"]], "filtered_resps": ["AJ Styles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "de49976c7e0830d36b9ed807ddfd093b6e2d452a5a08d38c1889ea8e89b65e37", "prompt_hash": "164a2569fb33c7289776f5b88a864462bb63cdb71da09ddd4aeaa29435e54929", "target_hash": "5253e24ba2127846a0e6fe14979b9d033ae8214763d501037734b9cb3542e0fa", "exact_match": 0.0}
{"doc_id": 1149, "doc": {"question": "who is the supreme justice of the supreme court", "answer": ["John Roberts"]}, "target": "['John Roberts']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the supreme justice of the supreme court?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chief Justice of the United States"]], "filtered_resps": ["Chief Justice of the United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "930799b7d7305a0ed344e63c3390d992cd99382a244afa4e57298d79e042b630", "prompt_hash": "eb8f1e41a726cb4fe7954141de2df08c014622bcdc5f0fe72627223470bcbff3", "target_hash": "ab23b866b20c52cf430d26ddb7bc465b017d061396fca508b5464007b4f6808b", "exact_match": 0.0}
{"doc_id": 1150, "doc": {"question": "what is the most common blood type in sweden", "answer": ["A+", "AB+"]}, "target": "['A+', 'AB+']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the most common blood type in sweden?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" AB"]], "filtered_resps": ["AB"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "058ecd476862080aba8cbcd15f13b0b7b7e5fb8afb1e234ea87523c5d5c28b78", "prompt_hash": "f77b9f34838fb46d2680101728bf796e7361921882aee1dfd5f7a5a8a3fac77f", "target_hash": "28fe6f223c9032ad04077ea97f3ed1b108b02c1c531cf88f84e226e6e88f8a7f", "exact_match": 1.0}
{"doc_id": 1151, "doc": {"question": "what does a heart rate of 131 mean", "answer": ["Tachycardia", "tachycardia in adults"]}, "target": "['Tachycardia', 'tachycardia in adults']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does a heart rate of 131 mean?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" resting"]], "filtered_resps": ["resting"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f0f7f444bd26be75dba9e8c0057791fecb8ed3645b83f8a4ff302226f4102906", "prompt_hash": "b65c45b4ef96c3ccac09b2dd279191bee36386fd64fe7ccba76683b3e98ef2c6", "target_hash": "41c176b65f938b2b1b882450d5dac0842bde7b97d0ee46d867e922b7dca694ae", "exact_match": 0.0}
{"doc_id": 1152, "doc": {"question": "when did the government change the retirement age", "answer": ["2015"]}, "target": "['2015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the government change the retirement age?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1983"]], "filtered_resps": ["1983"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8be42e0532d6e9cf2f47bd302fc90a80114ba2ac023937afde212c18f07a542a", "prompt_hash": "25cd68f57f814522a8580ea36bbc03501864b7b718327a658ce825da86fb475a", "target_hash": "d2dae4566ccf9bd979a614c8c426fe4b400fccb0b129e4067af4228858ba1e39", "exact_match": 0.0}
{"doc_id": 1153, "doc": {"question": "how oxygenated blood returns to the heart from the lungs", "answer": ["pulmonary circulation"]}, "target": "['pulmonary circulation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how oxygenated blood returns to the heart from the lungs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pulmonary veins"]], "filtered_resps": ["pulmonary veins"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d1bc31b3c545f951232999ce8bce6bff0972b133e0b54940fe30cf9488e54620", "prompt_hash": "59e3a7bff36a674e65ba2266f326d3a20111a75bc88823a5d04afc4f53c68035", "target_hash": "0187c5bee0611b2711b015d1c84921e11413c36dcfaff23a92a692da2ae5e005", "exact_match": 0.0}
{"doc_id": 1154, "doc": {"question": "who has won the most f1 grand prix", "answer": ["Michael Schumacher"]}, "target": "['Michael Schumacher']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the most f1 grand prix?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lewis Hamilton"]], "filtered_resps": ["Lewis Hamilton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e3a0a6165e22018f599beec58543ef31724b203715c8776766ef3624c2dce2f", "prompt_hash": "55eebaaef54bcd54be8be46901c3d4e339c6f0a5ae72aa17935780ab52aa8de5", "target_hash": "3fd1e16eccf5a75746e8a6d94b2104275bc132c8daf123beefd2c6f53806fcea", "exact_match": 0.0}
{"doc_id": 1155, "doc": {"question": "how many games does a team have to win for the world series", "answer": ["best-of-seven"]}, "target": "['best-of-seven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many games does a team have to win for the world series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" six"]], "filtered_resps": ["six"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77140f789cd14265edefacc9dd7fb7b15f03fe48f85e9f63afa30627c5b8087f", "prompt_hash": "2689999fc7d455c1896d2f91889ff8c30dc25687f42c16346429b766cca25b06", "target_hash": "2e1a33fd9d3a719ff71dbbd4380d734a971951e3acc6472982cf3241bceb32ba", "exact_match": 0.0}
{"doc_id": 1156, "doc": {"question": "when did the log flume closed at alton towers", "answer": ["10 October 2015", "2015"]}, "target": "['10\\xa0October\\xa02015', '2015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the log flume closed at alton towers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012"]], "filtered_resps": ["2012"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e1d5e9eff4f877cd1819f683d92350aa1855979b028816890d8b7017f9ff98a", "prompt_hash": "9a5106484cbc5da356869ea554aa03cd9079099dc5d27b679361b98013c49c2b", "target_hash": "f88025a4f86a4e4a41ff2ed623313aa8f4b0c1b3929853f13d9a5b5e8984d20b", "exact_match": 0.0}
{"doc_id": 1157, "doc": {"question": "what happens when an air mass is pushed up and over a mountain range", "answer": ["Orographic lift"]}, "target": "['Orographic lift']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what happens when an air mass is pushed up and over a mountain range?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" orographic lift"]], "filtered_resps": ["orographic lift"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "def48ebdd1c86efd4ff152f5bf9a30a971f89f1f330266780bfd7ad4cc62be19", "prompt_hash": "1bf677772d93fbb336911735566aae3fa1f1dbdd387c16e16ecdd7091a4e921f", "target_hash": "3fa6014b5fea35e241a444464c74d68564d19d3f9b8bba50300c8f047e561e9c", "exact_match": 1.0}
{"doc_id": 1158, "doc": {"question": "where does the last name hansen come from", "answer": ["Norway", "the Faroe Islands", "Scandinavian", "Denmark"]}, "target": "['Norway', 'the Faroe Islands', 'Scandinavian', 'Denmark']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name hansen come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Scandinavian"]], "filtered_resps": ["Scandinavian"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "495bdfa9bb9c2c6d32ad68d49ce88a5a3f9dfd79f99e30354bc1205bb38b84dc", "prompt_hash": "f2055dbe608d220c2e8c2b50ea9a2250b2fe0340e47d661684f42a8f15715458", "target_hash": "f1183091d0f0cc768753325618ea709921d2a211d77529f93d5645b9051f9e6a", "exact_match": 1.0}
{"doc_id": 1159, "doc": {"question": "who said life is like a bowl of cherries", "answer": ["Lew Brown"]}, "target": "['Lew Brown']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said life is like a bowl of cherries?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jimmy Durante"]], "filtered_resps": ["Jimmy Durante"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c02e3a436acd834bb5d1cdc6988fe41831a756ceb7556f7aeb4645e79937a11", "prompt_hash": "0c815955b9cc5a767da2e54bf729b537036ad5fb37659ae1242e1e4c8c7571bc", "target_hash": "d4db65f44d42c25cb3f26f073ac73fd2dbfa5dc49a7bbeed0c446dfbaad0d4e5", "exact_match": 0.0}
{"doc_id": 1160, "doc": {"question": "how long is the famous suspension bridge in san francisco and what is it called", "answer": ["one-mile-wide", "Golden Gate Bridge"]}, "target": "['one-mile-wide', 'Golden Gate Bridge']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long is the famous suspension bridge in san francisco and what is it called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Golden Gate Bridge"]], "filtered_resps": ["Golden Gate Bridge"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "50bd73a9ecbab0080585e14764a65dd18772e85e2357f200590a71e240bb372d", "prompt_hash": "9a27b7dedf2c5694ba71555610c0b2b5e7c1f52c491659ee480ed4b94c256663", "target_hash": "717da103127a90d1f6ed3a4e38f16af7ae8502d8393f9fc643362ef31a4d77b3", "exact_match": 1.0}
{"doc_id": 1161, "doc": {"question": "name a food you might eat on thanksgiving", "answer": ["a large roasted turkey", "turkey"]}, "target": "['a large roasted turkey', 'turkey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name a food you might eat on thanksgiving?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" turkey"]], "filtered_resps": ["turkey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c61171694833e6a31aaeb1e45a0d89c373ecb2e2b05662f4a5e223442b72537", "prompt_hash": "33f6c7dcfa8c5ad0f17baa0757652231c659e35f7fa055d7bd3e2f44fbd6bcfe", "target_hash": "5559a2cceb7fd385ca6b755a529330e72ed4f41b8157a1741c6c5a4276983355", "exact_match": 1.0}
{"doc_id": 1162, "doc": {"question": "when did sierra nevada brewery open in asheville", "answer": ["early 2014", "January 2012", "2014"]}, "target": "['early 2014', 'January 2012', '2014']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did sierra nevada brewery open in asheville?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1988"]], "filtered_resps": ["1988"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bd7f1958bcc8a99f825029ecb2284712587b44c856b2c39fa6da2ba019af11c9", "prompt_hash": "fcd6af6cb62e0b872c686ec5b8ce8f1e2c02776ab2a1901fe655a0383f9b5951", "target_hash": "dfe2a18b56a25e1ea67410efa07ae5a6aaef8819b2161017ee6288dfd49f0000", "exact_match": 0.0}
{"doc_id": 1163, "doc": {"question": "who said have you no sense of decency", "answer": ["Joseph Nye Welch"]}, "target": "['Joseph Nye Welch']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said have you no sense of decency?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joseph R"]], "filtered_resps": ["Joseph R"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2741858f8c3313fc41e62c0e23874ae027b1d096e13fa7c83fc3035619d455de", "prompt_hash": "538002e0f35bfbb4dfe57c7e579d2f9f6e2428e0b512e0a1f1ccad43c5a23b0e", "target_hash": "4115841c9fbafb1fd4c97b178ff73ebeb6ff7659bd36525bf23dd9a273d836c5", "exact_match": 0.0}
{"doc_id": 1164, "doc": {"question": "who was the chicago bears quarterback last year", "answer": ["Matt Barkley", "Jay Cutler (5)", "Matt Barkley (6)", "Brian Hoyer", "Jay Cutler", "Brian Hoyer (5)"]}, "target": "['Matt Barkley', 'Jay Cutler (5)', 'Matt Barkley (6)', 'Brian Hoyer', 'Jay Cutler', 'Brian Hoyer (5)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the chicago bears quarterback last year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mitchell Trubisky"]], "filtered_resps": ["Mitchell Trubisky"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19e21ef2894cfb47054dfd68845d00f546b9b1074f19983c4e5f9350e21c7750", "prompt_hash": "20deb60e57cb1f06f9d4b24b75ebc0cef9f73f01d7c8841fb0648139baf6df9c", "target_hash": "bc7103b85d3ac97a5181422535d30130ad5417ac4e016c5ccb1a782df42fc3bb", "exact_match": 0.0}
{"doc_id": 1165, "doc": {"question": "when did the dallas cowboys win their last playoff game", "answer": ["2014", "2016"]}, "target": "['2014', '2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the dallas cowboys win their last playoff game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2007"]], "filtered_resps": ["2007"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d8b36a95b92a2cd844fdf7275a7976aada5a0e506ea8416c061dc31965702c4a", "prompt_hash": "ce7ffa3076fd4f4e57bc834c043b3b4248834381afa6a45057743dca08b0671a", "target_hash": "8875d8c684998424a3118c0eead896387d83a17ee1ccc61d5d48001fb47a5863", "exact_match": 0.0}
{"doc_id": 1166, "doc": {"question": "which animal is the carrier of the h1n1 virus", "answer": ["Pig", "pigs"]}, "target": "['Pig', 'pigs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which animal is the carrier of the h1n1 virus?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pigs"]], "filtered_resps": ["pigs"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "11e56f12c69183099aec2c5cae2e6f49aadc190ddd5e5e72a92cbffdab0e436a", "prompt_hash": "238142e67e9f0e761ee5bedeb20fc2314c4d48d852383ebf1395eeb72ebf2132", "target_hash": "5479550d23e45792d9f874d22926986203c311e3e4f8ff3c357abeb670d7a08e", "exact_match": 1.0}
{"doc_id": 1167, "doc": {"question": "where did the ancestors of the domestic goat originate", "answer": ["Iran"]}, "target": "['Iran']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the ancestors of the domestic goat originate?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" India"]], "filtered_resps": ["India"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff4c48cb85bb38ef00e364e1e43f54f1f3420b15e8f9ca7fd29433d82264aa81", "prompt_hash": "e236ed526c1aeac571f460ec0867e16e16c5873df3e9e6e99171b01efd178d22", "target_hash": "1f55ed6e040b8dd434cf9d300e48e8052a1bc3bef6d438ffe8a4f0e6c0d14e09", "exact_match": 0.0}
{"doc_id": 1168, "doc": {"question": "how many episodes in season 4 of last ship", "answer": ["10"]}, "target": "['10']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes in season 4 of last ship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 13"]], "filtered_resps": ["13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8336651048973596b9c7d8f84b717ba4d464ca6f6fde096ac0b7f327ae3fa09e", "prompt_hash": "f0d5a4e5c1db0934d0d20951921dd01454f0b883d78df7ffda6fcdf893851464", "target_hash": "9503346d501aeaec3f521be685eb235cda81c7929ba2f1acdae1be91400b787a", "exact_match": 0.0}
{"doc_id": 1169, "doc": {"question": "when does part 5 of jojo take place", "answer": ["The manga begins in 2001", "2001"]}, "target": "['The manga begins in 2001', '2001']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does part 5 of jojo take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1910"]], "filtered_resps": ["1910"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1fdcbb0b38fbf16d706f094655b0ac6fcb3db4d909bbeebba52dd2f2696377e2", "prompt_hash": "93d1b0ee28d199ed73a04e09f4f105f5fceeebd8be907c382856d449dff14d99", "target_hash": "f12ee33b0250cd9bcc0d525d7ce7e0bca95944ba5b8101b1e29b76aceed203ba", "exact_match": 0.0}
{"doc_id": 1170, "doc": {"question": "where are the washington redskins based out of", "answer": ["the Washington metropolitan area", "FedExField in Landover, Maryland"]}, "target": "['the Washington metropolitan area', 'FedExField in Landover, Maryland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the washington redskins based out of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Washington"]], "filtered_resps": ["Washington"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d81d4a23e45bf2933ef8081359478b50e8c1df3a1a2b4587fb851f707b6b844", "prompt_hash": "75efcc6b9abedb3347311d1e2100dce819f02143ce4db2c536b08617fc1d64fd", "target_hash": "d080321b7cd8eded9900562031d0b1078369f03dda596258030ee38ae40d6a01", "exact_match": 0.0}
{"doc_id": 1171, "doc": {"question": "what is the highest base on the ph scale", "answer": ["greater than 14"]}, "target": "['greater than 14']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the highest base on the ph scale?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 14"]], "filtered_resps": ["14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c454721795dd269727d1f572e2f04281eac8cb15defa183536a9cbc8a8aa0dc2", "prompt_hash": "7363ca11fe91260b01008107632172dc32e02892abd03717c2dd54a2d5119d0f", "target_hash": "910d2729c46a900a25ee6cafe03e7492e45a50d02b52a73fd0a48009400f456a", "exact_match": 0.0}
{"doc_id": 1172, "doc": {"question": "where is the love meaning of the song", "answer": ["lament on various worldwide problems"]}, "target": "['lament on various worldwide problems']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the love meaning of the song?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kanye West"]], "filtered_resps": ["Kanye West"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a8bd48c44624fc11ce5633e0279adf26a503c871291599f8a47fbcfe2da951d", "prompt_hash": "23f6bd48c038c2762a19996fe276b833c200624771067d8ce61cc30255f9c299", "target_hash": "41ec922b5f311f18d20c9efcdc4d32155255372b354cda930f87bc5723e76ab8", "exact_match": 0.0}
{"doc_id": 1173, "doc": {"question": "who played marius in the movie les miserables", "answer": ["Eddie Redmayne"]}, "target": "['Eddie Redmayne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played marius in the movie les miserables?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hugh Jackman"]], "filtered_resps": ["Hugh Jackman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b96857ba8afe999f686df580c53e53552128954a717db4ff3b80e2ee1b0a4317", "prompt_hash": "5eb7bdbacdb6e242e386ec033ec3e8536fedb01e5a3f33c79c476bd8dd9704c8", "target_hash": "a2103dfe5d55b30134fd3da3a48c505e22d792572d586894221d0e95b441c608", "exact_match": 0.0}
{"doc_id": 1174, "doc": {"question": "when is the strictly come dancing results show recorded", "answer": ["Saturday", "Sunday night"]}, "target": "['Saturday', 'Sunday night']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the strictly come dancing results show recorded?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Saturdays"]], "filtered_resps": ["Saturdays"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fe4089c86b72b902679af18c66117698b441113376b60b98a591ddd76e19a4e4", "prompt_hash": "54b96f61fcb77c7cab568c6fcadf2875e08c5b9dfe94e90d8a3b0a34c0a631eb", "target_hash": "f70a51ea5ad457f999aa459a8a8c4cad78c94cbde6f4951ad7d3f50f72c394c5", "exact_match": 0.0}
{"doc_id": 1175, "doc": {"question": "who played chaka on land of the lost tv show", "answer": ["Phillip Paley"]}, "target": "['Phillip Paley']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played chaka on land of the lost tv show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Anders"]], "filtered_resps": ["David Anders"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "34c507223baabca5317148f7e0a3552d0eac7c14a1621a51ea7de4c4f695db06", "prompt_hash": "a378a3d6889fbc7f22aaff7e6fd857be1216b6c2593479eb0ce220a66a8625cb", "target_hash": "7f8e5494d0a819efc8126078247f42b0bfcd186f20056e45f69eb2d417bb7d54", "exact_match": 0.0}
{"doc_id": 1176, "doc": {"question": "who invented the first home video security system", "answer": ["her husband Albert Brown", "Marie Van Brittan Brown"]}, "target": "['her husband Albert Brown', 'Marie Van Brittan Brown']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who invented the first home video security system?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John R"]], "filtered_resps": ["John R"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9ac2de06eedf6e47facf2209cdfd68ec47af550787ea07e919e153ef63822e58", "prompt_hash": "ac95e907a21cd0c0df61e80c1ac35a2dd7d0a27fdcbd7fde4805e534859dc40e", "target_hash": "f0d855c06f5fb34ba6949b09707a9d6d0243d7e2326f22981a1e2f606eb7d489", "exact_match": 0.0}
{"doc_id": 1177, "doc": {"question": "when did sussex split into east and west", "answer": ["1832", "1889"]}, "target": "['1832', '1889']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did sussex split into east and west?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1974"]], "filtered_resps": ["1974"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7557e18268292dfeceb200d6bdc5d6a0ecd26254c1ed16fd5848d79eae06e5f0", "prompt_hash": "8af32e95e781e27ee4534970974f4358cce96af27dad628ca98d609e1df6294b", "target_hash": "167d5bef8c684c63a2f474870b923a5f39a0bfb4118388892d0f9c7fe76866aa", "exact_match": 0.0}
{"doc_id": 1178, "doc": {"question": "who sold the most records elvis or the beatles", "answer": ["The Beatles"]}, "target": "['The Beatles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sold the most records elvis or the beatles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Elvis"]], "filtered_resps": ["Elvis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9997d689eecb06e657b2b6d808a683e2d39622830353c5237e6fc8a36af6390", "prompt_hash": "1855d764d56de8f6b1d01dfc708c36525526ac3bcee59628d3a101478c2d4ae8", "target_hash": "1656edfec581107e0ad591f64a6c0a50abf71367761f6330cd410765c366acaf", "exact_match": 0.0}
{"doc_id": 1179, "doc": {"question": "what does a bull shark tooth look like", "answer": ["needle-like teeth", "needle-like"]}, "target": "['needle-like teeth', 'needle-like']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does a bull shark tooth look like?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" serrated"]], "filtered_resps": ["serrated"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fc53563bb78caf766d2690c00194c95aa74acadeb0716f2c5485008368c663a8", "prompt_hash": "9d25132c127dc98f1d652d4e6214d2c7a33c1d100d945b2fcb5daefa50d18a1e", "target_hash": "264236b3af9eb05d629342a71f3c8f6066cb0452204c42d5500c0cb80f5c2fe8", "exact_match": 0.0}
{"doc_id": 1180, "doc": {"question": "bosnia and herzegovina croatia macedonia and slovenia all used to be parts of", "answer": ["Yugoslavia"]}, "target": "['Yugoslavia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: bosnia and herzegovina croatia macedonia and slovenia all used to be parts of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yugoslavia"]], "filtered_resps": ["Yugoslavia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b6ae52c68b32419eb3aeac9c02887c5cdff436eaac4bd8807e0d9bf974e2cf0", "prompt_hash": "ef92988881c968b0dffb04d5526dec4d4d4ec0bd16ba48ac148ad199c32c14c2", "target_hash": "6eac20b2f066178d36fc749443ec70f020ea7a74d307ea7e6d11a00b2a5b2df9", "exact_match": 1.0}
{"doc_id": 1181, "doc": {"question": "how many quarterbacks have a receiving touchdown in the superbowl", "answer": ["1"]}, "target": "['1']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many quarterbacks have a receiving touchdown in the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "24b0c0dc3c346e0ff1519b3cbc0375a67bf1e9a75e1f94661b3d69fc83e9095b", "prompt_hash": "a95c6aedb5f688280b5755ec2d3999318c9948a558f564ede9b4fdc995bb879b", "target_hash": "31ddcada1d7c1fc1613304acb1fa93dd118bc955ad0081db064236cddab6fb64", "exact_match": 0.0}
{"doc_id": 1182, "doc": {"question": "what act did parliament pass after the boston tea party", "answer": ["The Intolerable Acts", "Intolerable Acts", "the Coercive Acts"]}, "target": "['The Intolerable Acts', 'Intolerable Acts', 'the Coercive Acts']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what act did parliament pass after the boston tea party?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tea Act"]], "filtered_resps": ["Tea Act"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "443652b49dc66b83bc6ca166a3bf8c08aacf0b86b9f78df62403696b2e6b7cea", "prompt_hash": "98d98e7b6ef7c28eec2462dfe91a52a82e16ece1179d7fe4a29597ddc1face85", "target_hash": "90c918b60caff8f1c0eb887a421f4124a1ec6105ccbb6c658ff5ccc4fe2a7e4c", "exact_match": 0.0}
{"doc_id": 1183, "doc": {"question": "how many seasons of johnny bravo are there", "answer": ["four"]}, "target": "['four']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of johnny bravo are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e7cf49aa944fc98c0af87dacbc7bc72e00340e966bd643b6be371702de7f64da", "prompt_hash": "ee185f476f6139166e531f115f32f48f0f94437ecf28eb46fd7798bbadbf2e3f", "target_hash": "c0b12eaa48bad2eab0833b330f1f9a7a1caf1ef07c30f4fc2a1bd97e38e98618", "exact_match": 0.0}
{"doc_id": 1184, "doc": {"question": "where does the sweetness of fruit come from", "answer": ["Fructose, or fruit sugar", "Fructose"]}, "target": "['Fructose, or fruit sugar', 'Fructose']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the sweetness of fruit come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sugar"]], "filtered_resps": ["sugar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "43047c79ccfd03ebfd0290669db6337299893070af9486759929b9ec0ac640d1", "prompt_hash": "76fa685f965c43c37121ea54326e5c69e9b7451c423681a2a14699cf0634fda3", "target_hash": "db5f68bb4746991dd2a4bfc2616937ca78de3e6800eded3eb472b79aff7c7c0d", "exact_match": 0.0}
{"doc_id": 1185, "doc": {"question": "who plays drew's boyfriend on the night shift", "answer": ["Rick"]}, "target": "['Rick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays drew's boyfriend on the night shift?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jason George"]], "filtered_resps": ["Jason George"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dbc7eb945ad5f0a9faaad51bd1c9bbcf29ccf81335a877b11017102d67c7c42b", "prompt_hash": "62188eadeca6732d2bdc8f44ff920aad674472bb04b1b43edc9c437cb018d45f", "target_hash": "9f0fc16f72e5d2d8eb414ca72f13efc70d5a34be20d15908dcae2385558014f6", "exact_match": 0.0}
{"doc_id": 1186, "doc": {"question": "when does kubo and the two strings release", "answer": ["August 19, 2016", "August 19, 2016"]}, "target": "['August 19, 2016', 'August\\xa019,\\xa02016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does kubo and the two strings release?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" August 19"]], "filtered_resps": ["August 19"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "216f59e97e4ca2d6acb68eae7edb45e01f117b09078a716a1e3b6d0cc4b01af4", "prompt_hash": "6ca8cf6cd6a92dfc40c4a6740a271c47ab3a51fa71f395f66738c74f43dead41", "target_hash": "e16a535ee33a55413bfcd6eb6192b273c28b8474751f7ef1d283e1aa085291da", "exact_match": 0.0}
{"doc_id": 1187, "doc": {"question": "who plays nikko in the wizard of oz", "answer": ["Patrick Walshe"]}, "target": "['Patrick Walshe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays nikko in the wizard of oz?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nick Offerman"]], "filtered_resps": ["Nick Offerman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "09a801bb9b3d61fab056e05eae1f95768d9baeb2e5cc0b8371c8202cb36523aa", "prompt_hash": "b88a33748965ed45310ec187c582dd324b6fa4ece49bf9aeb5ce126f7453dfad", "target_hash": "2f8e1401114eea0b03b0efb6a70564628d6f1940f06d5f0f3fadd4104762a517", "exact_match": 0.0}
{"doc_id": 1188, "doc": {"question": "what is a e8 in the marine corps", "answer": ["Master sergeant", "First sergeant"]}, "target": "['Master sergeant', 'First sergeant']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is a e8 in the marine corps?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sergeant"]], "filtered_resps": ["Sergeant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dbe960e63975b53182f1cc9879aefe46732240fc8a51e148ad317b5a04fbd2dc", "prompt_hash": "dc4c0538e0be4c21e2802cadab13bc5fefb7286807239ef33712bdfe6a11f71d", "target_hash": "fce1e5a96925b42dfee477acef49e0aac4ce149f4b3619be70b77e7844bbab36", "exact_match": 0.0}
{"doc_id": 1189, "doc": {"question": "who stole the mona lisa from the louvre in 1911", "answer": ["Vincenzo Peruggia"]}, "target": "['Vincenzo Peruggia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who stole the mona lisa from the louvre in 1911?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vincenzo Peruggia"]], "filtered_resps": ["Vincenzo Peruggia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ec3afdf58e01dc1294780f368812864511d67a94c18cf614c5d0f1766d6b11b", "prompt_hash": "5cb4413ba45e680a8899cb19f7f9c421c84c30951eb35a7b81a54528645df46e", "target_hash": "4d1c9d1c281b6675e9be1f78015efd584045be891587a84debc4c0844f0e0a5d", "exact_match": 1.0}
{"doc_id": 1190, "doc": {"question": "who sang the them song for as told by ginger", "answer": ["Melissa Disney", "R&B artist Macy Gray", "Macy Gray", "Cree Summer"]}, "target": "['Melissa Disney', 'R&B artist Macy Gray', 'Macy Gray', 'Cree Summer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the them song for as told by ginger?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Black Eyed Peas"]], "filtered_resps": ["The Black Eyed Peas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fa6a32c325f355cfd864594843c6a10cffab30c36aa2432c82534adb0ad3c4a3", "prompt_hash": "155910d07f0f733c44b9416ed4dbfe3d3896a2e04d0239d242e586d868fbead7", "target_hash": "009bc9697930bd64e7db957efc0052a8e8788b823e22b749f48986f0edc1a6de", "exact_match": 0.0}
{"doc_id": 1191, "doc": {"question": "when does brooklyn nine nine season 5 episode 12", "answer": ["March 18, 2018"]}, "target": "['March\\xa018,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does brooklyn nine nine season 5 episode 12?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" May 17"]], "filtered_resps": ["May 17"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ecd0df5b14c8570cb6a1eefb879aa6276befaa75a35df61c56b39b2dec5ebf0", "prompt_hash": "6264b1822a6ef75b040d9e73367953366d05c4e2162cacf7826ab53495e8dc04", "target_hash": "a76bdd2cfc00231b964f1ee3a77f61e5b9c7d473cb15ccc0bffd731daa16c893", "exact_match": 0.0}
{"doc_id": 1192, "doc": {"question": "who is going to host the 2018 winter olympics", "answer": ["South Korea", "Pyeongchang County, South Korea"]}, "target": "['South Korea', 'Pyeongchang County, South Korea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is going to host the 2018 winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pyeongchang"]], "filtered_resps": ["Pyeongchang"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "340bd04dc4d403154137c332122c1e7b3c2fe83bef0939bc737d363b46fdd3aa", "prompt_hash": "6c74d42e228181c5662200cb135c2e5a9cb6367bc7207eaa9d1851a8f3f82b51", "target_hash": "3407bfa09847378ffbb0a5609322de14e995716780ecc7a6cea132c66b8e5777", "exact_match": 0.0}
{"doc_id": 1193, "doc": {"question": "which came first the walking dead comic or show", "answer": ["comic book", "comic"]}, "target": "['comic book', 'comic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which came first the walking dead comic or show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walking Dead comic"]], "filtered_resps": ["Walking Dead comic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8463a4e049618da765d9a20ccdd2ccf0e5a466ce81a0e02a863f923edb5cf190", "prompt_hash": "417a0584f6b6a01a4040171519cff6eac499922e7f53bd92cae57b8e2994ecb6", "target_hash": "eaff1f951d236eb39673aa593d59a7762d4ca31ae776b8cccc53bbfaf4677078", "exact_match": 0.0}
{"doc_id": 1194, "doc": {"question": "who played john clark sr on nypd blue", "answer": ["Joe Spano"]}, "target": "['Joe Spano']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played john clark sr on nypd blue?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Caruso"]], "filtered_resps": ["David Caruso"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c1502f0d47549421e433eee35d70aef5c8654517ea60b83bc6ee534ece13fc00", "prompt_hash": "69ce718fc852f8d25d9debf551fe0ca9bbd08101e4d120a6f81a53fefcbab63e", "target_hash": "e36ad5257526c32540ccba27877237289fe4bf9140afc4f73d2966be1b071125", "exact_match": 0.0}
{"doc_id": 1195, "doc": {"question": "who played peter benton's sister on er", "answer": ["Jackie Robbins"]}, "target": "['Jackie Robbins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played peter benton's sister on er?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Love Hewitt"]], "filtered_resps": ["Jennifer Love Hewitt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df6c728b0197083e10696cd463194298d672b2977cc793667e7a1c320bbbcabe", "prompt_hash": "432eff0a1a9916a3c0bd0cad5fe97467c7be0900e2a1eae6fd08106a77f63cda", "target_hash": "3082756d67605e770ad8952b03812cbd81dbfca96f93bbfd140b00ba40153669", "exact_match": 0.0}
{"doc_id": 1196, "doc": {"question": "where is the protien made in the cell", "answer": ["in the cell nucleus", "cell nucleus"]}, "target": "['in the cell nucleus', 'cell nucleus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the protien made in the cell?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ribosome"]], "filtered_resps": ["ribosome"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9696756fdcaca1e71b6533c361b72af01ad5d9629a36b0b27fc15d71b5c55934", "prompt_hash": "0e1f127a785f5d39713791ee8f92279746526fe9b2ac261dd475e62a88f12442", "target_hash": "1d94becc2f0134925b03630ce6c8679d03c89e2a035db3961cd9a3c6ca10c634", "exact_match": 0.0}
{"doc_id": 1197, "doc": {"question": "who is given credit for inventing the aqua lung", "answer": ["Jacques Cousteau", "engineer Émile Gagnan", "Émile Gagnan"]}, "target": "['Jacques Cousteau', 'engineer Émile Gagnan', 'Émile Gagnan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is given credit for inventing the aqua lung?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jacques Cousteau"]], "filtered_resps": ["Jacques Cousteau"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea3c59a09b279119cd15d96df3b6c67b236d39fc60a700f3333b57978e8f3326", "prompt_hash": "8b898abd014b4e67b8cb311eca4d4cde45a495fd24628bdd4e701299d59460d2", "target_hash": "a894c3466efb748682ddfa64f9b6f18696a548065e5991461e0fae9d7b07ad09", "exact_match": 1.0}
{"doc_id": 1198, "doc": {"question": "who won the 1st battle of bull run", "answer": ["Confederate victory", "Confederate forces", "Confederate"]}, "target": "['Confederate victory', 'Confederate forces', 'Confederate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the 1st battle of bull run?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Confederates"]], "filtered_resps": ["Confederates"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7894d7f1824313b79664c3fbb9fff789711099ba88c49b463061027218150b9e", "prompt_hash": "7194cb7d261fbbd2e6047e37e6452f006ee0e35cd11ed5dd43bc799ece709140", "target_hash": "31d847934266342c40c1741f38d09aaea4e2796f073b34d9e61a20fd8874256f", "exact_match": 0.0}
{"doc_id": 1199, "doc": {"question": "who plays the voice of john smith in pocahontas", "answer": ["Mel Gibson"]}, "target": "['Mel Gibson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the voice of john smith in pocahontas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Willard Scott"]], "filtered_resps": ["Willard Scott"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "82f7b1ae110dc5cc957f9b363b72ec5fb4856b22f6b9adcfca579b72ec93d96f", "prompt_hash": "1c3f6b576c37fc573a2c04616807dc83f8052066c26fb3f456d7dc9b71012eff", "target_hash": "2706e8d30b7d7f3dd7cbe3b95922c3b6f5c371512d1ebd8eac2324c2e0438416", "exact_match": 0.0}
{"doc_id": 1200, "doc": {"question": "computer based models of real life situation are called", "answer": ["Computer simulation", "Computer simulations"]}, "target": "['Computer simulation', 'Computer simulations']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: computer based models of real life situation are called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" simulation"]], "filtered_resps": ["simulation"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d0c57405b75d9818ad8416b23664b3565e719a8b4f903d1c865c9b633421da45", "prompt_hash": "80068944814fbd2986046a518d1fe84700f306117dc38b2a3513dd74d11e02c4", "target_hash": "afd67a7a4b73dc5175883be21a6c4f6fd7c91339c8b43e61578a3dd9230e4f20", "exact_match": 0.0}
{"doc_id": 1201, "doc": {"question": "how many inches is the iphone 5s screen", "answer": ["4 in", "4-inch screen size"]}, "target": "['4\\xa0in', '4-inch screen size']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many inches is the iphone 5s screen?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 4 inches"]], "filtered_resps": ["4 inches"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c0480fd02b5c8b033564cf00dad026f34b428d7ce6acd8ff831d8423559aeabe", "prompt_hash": "0746f487a3159d1eb1dd1821bdd397b762b419e5af4824656761180cf2f5e33c", "target_hash": "02081905db115edcce534c29ace2a17f86d933d0fbb32c723bfa09aa287354b1", "exact_match": 0.0}
{"doc_id": 1202, "doc": {"question": "5 cities with the highest population in europe", "answer": ["London", "Saint Petersburg", "Berlin", "Moscow", "Istanbul"]}, "target": "['London', 'Saint Petersburg', 'Berlin', 'Moscow', 'Istanbul']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: 5 cities with the highest population in europe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Moscow"]], "filtered_resps": ["Moscow"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e434ae22217981ff38a73247bac6723b7998980548fb1e9b5d5697b747f1b218", "prompt_hash": "2713fe5c00b99878be76b736180cb34a4c9f321205c95941e29ac12564688a4f", "target_hash": "22160559e29152b87b4783c9779780fe7f8dffcef3694b4626f3654d0dc8c6ed", "exact_match": 1.0}
{"doc_id": 1203, "doc": {"question": "who is the valley of the dolls based on", "answer": ["Dean Martin", "Judy Garland", "Ethel Merman", "Carole Landis"]}, "target": "['Dean Martin', 'Judy Garland', 'Ethel Merman', 'Carole Landis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the valley of the dolls based on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mickey Mouse Club"]], "filtered_resps": ["Mickey Mouse Club"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b6d97e5c9b663f7d067e55f8fa9e3f928d16b74a039c0e9aaf962efe5dfb611", "prompt_hash": "0a927bb32dff7a8b9699a15b1d4b4856f4ef3061ff15d00747c9d9f79e8204f2", "target_hash": "42331552593bf574e321e73e92489465b91a7e4e18a3919b3b639ca71f5da6d3", "exact_match": 0.0}
{"doc_id": 1204, "doc": {"question": "when did the movie the post begin filming", "answer": ["May 2017", "May 30, 2017"]}, "target": "['May 2017', 'May 30, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the movie the post begin filming?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "201c8a1812747d6ab81f6be1b286d713c24a5b32b89a0a2ae4c2b20ef86aa43c", "prompt_hash": "1a271441d73a759fdf4b19359ad65c7e7b6e84e5c2563a07becba7812b9ac4c6", "target_hash": "34357d8556de96dd5502e884b9103a4b31ee3eeb657471fe42d5a4c6cd13f229", "exact_match": 0.0}
{"doc_id": 1205, "doc": {"question": "who is the girl who played in the grinch", "answer": ["Taylor Michel Momsen"]}, "target": "['Taylor Michel Momsen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the girl who played in the grinch?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Taylor Swift"]], "filtered_resps": ["Taylor Swift"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db8abed5fb80b9e5e7ff4209ec32af8085be86b8208733da4b9404737a4575b5", "prompt_hash": "be14ecad736e59fe5803bc6dd9b79964cbcb578f63e8db4afe756f245dc2863d", "target_hash": "cea6f7043b22eefeffbb5d2ec25b6e88709a94aafe819dfb3fc252b3078340d3", "exact_match": 0.0}
{"doc_id": 1206, "doc": {"question": "who took over the dutch colony in north america", "answer": ["England", "English"]}, "target": "['England', 'English']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who took over the dutch colony in north america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New Netherland"]], "filtered_resps": ["New Netherland"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "600e163cb1886c8e575bf595b57e28d8bb2ca9f1117382db400973dd800bba8a", "prompt_hash": "9263211b83cd027b28a870e5dabeded4eab44aa7749662ff488b87f292971c51", "target_hash": "77d9417e617f11d0935115c3f6de4e5aa9cc5d8b8f4564c5a4cc310443b1a9d7", "exact_match": 0.0}
{"doc_id": 1207, "doc": {"question": "who wrote the theme song for mission impossible", "answer": ["Lalo Schifrin", "Argentine composer Lalo Schifrin"]}, "target": "['Lalo Schifrin', 'Argentine composer Lalo Schifrin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the theme song for mission impossible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Bowie"]], "filtered_resps": ["David Bowie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9d27fb7cb53dadc28cc7f131c4190eed412b2bee4997adff35aaa10709429e6d", "prompt_hash": "78b5754ccac745cf08a2547af82c36fd6524b22b5da6b61e26193d865b8540db", "target_hash": "625a17f2b3a82ca98d5fd305bb4841f9ba9d8b8cc468c3ec9e46ef7a842acaac", "exact_match": 0.0}
{"doc_id": 1208, "doc": {"question": "who wrote the theme to last of the mohicans", "answer": ["Dougie MacLean"]}, "target": "['Dougie MacLean']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the theme to last of the mohicans?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Horner"]], "filtered_resps": ["James Horner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7520c2ab81f94c9325bfcd746a46c71dc5409e3340570795b9a4a4b9972051e9", "prompt_hash": "62c4409a0de16ee53ba6a460c3b78de6547b48d666f272cd945e152dde1a2e5c", "target_hash": "17d233946b157dc2fdf53eab9ac443028f1b558f01c77cb5349aefd696b2d999", "exact_match": 0.0}
{"doc_id": 1209, "doc": {"question": "when did day light savings start in the us", "answer": ["March 31, 1918", "1918", "March 19, 1918", "March 11"]}, "target": "['March 31, 1918', '1918', 'March 19, 1918', 'March\\xa011']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did day light savings start in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1918"]], "filtered_resps": ["1918"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5363871ef97da7f4c9b871ff8904d854e7351d90029fa153c916dcf50afee832", "prompt_hash": "2995e436f7b1aba621ff6557b37ba98b1aa9f99fb6eaf918fe4942d30ff9cf40", "target_hash": "a147be5ed3258912d619339bc421b1bc70a96a5b54d046274e2a6da50b8853c9", "exact_match": 1.0}
{"doc_id": 1210, "doc": {"question": "what does the m number mean on a pint glass", "answer": ["imperial fluid ounces", "the manufacturing company or site"]}, "target": "['imperial fluid ounces', 'the manufacturing company or site']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does the m number mean on a pint glass?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" measure"]], "filtered_resps": ["measure"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "495c34a7ed96f84fac5b7689e1606ccdbf944b202b7c5deed2ca0a95f83b2f42", "prompt_hash": "b9abf497ead1d34c85061dd7fc691c190802ff21d6f71ccbbb0c46e90262ba89", "target_hash": "90f6cfe75ac76633133264a463c88cabdcf865b49d1774e2815796ed2c2c3bc8", "exact_match": 0.0}
{"doc_id": 1211, "doc": {"question": "where is beijing located on the world map", "answer": ["northern China", "in northern China"]}, "target": "['northern China', 'in northern China']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is beijing located on the world map?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" China"]], "filtered_resps": ["China"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "82170b94144cf2a4d52aab3a117d63de3f3e71a9887bb98092ff192f8671e211", "prompt_hash": "649ebbfaabdb08aa5946c16a46b3d544505b649fcb9b6a4b1dc2c9aa9a1a2747", "target_hash": "8784f53b68f3111c2317cc21e7946a9bb492dd732e8e4cb9de268040def51194", "exact_match": 0.0}
{"doc_id": 1212, "doc": {"question": "when does mexico play in the winter olympics", "answer": ["9 to 25 February 2018"]}, "target": "['9 to 25 February 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does mexico play in the winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1972"]], "filtered_resps": ["1972"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "643316ce81e18f986bbdac3f2bfd0896db8717c8bbbf72feea85b1a2b1b2c7c0", "prompt_hash": "8c807dcb091ab20285c6d09d7a0747e016656e0d8a4f5c86de8aa04193b51239", "target_hash": "ed50a1444bacc46a300d1b6309ff61020f35d61a8da905335b78bbc4b0bda1e3", "exact_match": 0.0}
{"doc_id": 1213, "doc": {"question": "who dies in the lost city of z", "answer": ["Manley"]}, "target": "['Manley']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who dies in the lost city of z?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Captain MacKay"]], "filtered_resps": ["Captain MacKay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c66e581eebb4d2c920bef4b55e6f1cff32bcdd9ec8052d313d93f75e68e37991", "prompt_hash": "ca9057e1ff4cc142c7d9cbafbf26fded2003beae063b67661e3fc69d932ce31a", "target_hash": "8df5aea99a13f9c2ff767ec4b252eda0b5f40bfbc04159ee93f9d6fc3c207745", "exact_match": 0.0}
{"doc_id": 1214, "doc": {"question": "when does the black panther movie soundtrack come out", "answer": ["February 9, 2018", "February 16, 2018"]}, "target": "['February 9, 2018', 'February 16, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the black panther movie soundtrack come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 16"]], "filtered_resps": ["February 16"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6c4db77981679fc7241f10d19dad98bbad13011cb916cbf1cc498281bc32f66d", "prompt_hash": "62574bd7036b18b6f3416e83c946e9339333bbb965ac477d79f14265fe3f8dde", "target_hash": "4a8bf31276e4ccc79468864e919a032599de855aa114885b10466d3d0078702d", "exact_match": 0.0}
{"doc_id": 1215, "doc": {"question": "when was the letter j introduced to the alphabet", "answer": ["in Middle High German", "1524"]}, "target": "['in Middle High German', '1524']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the letter j introduced to the alphabet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 16th century"]], "filtered_resps": ["16th century"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bcb6c45f61e9847a969ba1957bc5e31cd454415d3aa38c772ae1d33910efba03", "prompt_hash": "36d029e415ffac217d02f036855b90f1e7e8bb8014ce1accb6742721db1a1374", "target_hash": "842e3dc77c7ae7a805434c829b681378454ae8d0120da683cbc1468de41c7553", "exact_match": 0.0}
{"doc_id": 1216, "doc": {"question": "what flag is red and has a gold star", "answer": ["flag of Vietnam", "Vietnam", "The flag of Vietnam"]}, "target": "['flag of Vietnam', 'Vietnam', 'The flag of Vietnam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what flag is red and has a gold star?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the flag of Libya"]], "filtered_resps": ["the flag of Libya"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b35adc3c027b9f5a7b10567e35108f4b0b72c652cfabddec403ae1a20098d201", "prompt_hash": "b4bd3885feef86eb81456c08043912e2d0193c43ab3999a74176a72987c1a62a", "target_hash": "394fce9e9f2892462d50bcba9d11d0cca25a6d269f6ba68f4633b60e1f81e3d1", "exact_match": 0.0}
{"doc_id": 1217, "doc": {"question": "who expanded the palace of versailles to its present size", "answer": ["Louis XIV", "Louis XV"]}, "target": "['Louis XIV', 'Louis XV']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who expanded the palace of versailles to its present size?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Louis XIV"]], "filtered_resps": ["Louis XIV"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf225f08e0ae96d67d456fd5d87d2c5803f5b87e625c1dad5ab7fc5ebd20fb8c", "prompt_hash": "9944882bcac1b0df1ce37ed7b9dbc040bf654b4ec2c4860ff731b88016f6ef10", "target_hash": "5d8155d00a42b2067983faf7f81b42d897689ea450714f2ca794d4b5a560a54d", "exact_match": 1.0}
{"doc_id": 1218, "doc": {"question": "when was the last time the vikings made the superbowl", "answer": ["in the 1970s"]}, "target": "['in the 1970s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the vikings made the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2018"]], "filtered_resps": ["2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ce0351c65ebd0daaa1dfc76e3141a1573f5eea8a10c4de01f5bff6138a9d3082", "prompt_hash": "cc997c3d105e169ee08c83bbb09e18287ac55bc5c86318b3ba2c4526180d8e6c", "target_hash": "1c2decc0a2c073c8bd10d20118c20714717039a01d3d11221bdf903a10a13958", "exact_match": 0.0}
{"doc_id": 1219, "doc": {"question": "when do we find out luke and leia are siblings", "answer": ["Return of the Jedi", "In Return of the Jedi"]}, "target": "['Return of the Jedi', 'In Return of the Jedi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do we find out luke and leia are siblings?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Episode V: The Empire Strikes Back"]], "filtered_resps": ["Episode V: The Empire Strikes Back"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "629b0fa4225c309476c9de6222943fce699ea90e7bbdf490eed2ca334d699a88", "prompt_hash": "ab1ee29803e87da0df3733dc28de8d4deb20e71b2963de3f0d08d15ff67b0f0b", "target_hash": "bc555d140a0ce84111e4bc2265a410ce1a8c434ca83c725c98e2d078dceff7b0", "exact_match": 0.0}
{"doc_id": 1220, "doc": {"question": "when did the uk and us become allies", "answer": ["Since 1940", "1940"]}, "target": "['Since 1940', '1940']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the uk and us become allies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" World War II"]], "filtered_resps": ["World War II"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6e632ad767b462c9649e22a1e07a4a918d549ebe419d549029274d779d54cd07", "prompt_hash": "71f3f5f0606885f77734f861ffcbd640f13232a2ea6f65a12b1ca5d37ca556ac", "target_hash": "0e1591aaeb7a714b066e9b65fc3c9789175cead3e4620f4fcac3d2629cd8872a", "exact_match": 0.0}
{"doc_id": 1221, "doc": {"question": "who wrote the song going to kansas city", "answer": ["Jerry Leiber and Mike Stoller", "Jerry Leiber", "Mike Stoller"]}, "target": "['Jerry Leiber and Mike Stoller', 'Jerry Leiber', 'Mike Stoller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song going to kansas city?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ray Price"]], "filtered_resps": ["Ray Price"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f98431913f55da33f9002be3130a5c473c73f58d37ed9536969a81612d9eedf3", "prompt_hash": "c7e9b51d8242c23212d45f4ab13ccf10b06357586e2fb1c34068553269a0b9a1", "target_hash": "1037ed00faa6a762cae0dd8901ef56d78377927ca51949513ed79a64a373e350", "exact_match": 0.0}
{"doc_id": 1222, "doc": {"question": "where did the battle of issus take place", "answer": ["southern Anatolia", "in southern Anatolia"]}, "target": "['southern Anatolia', 'in southern Anatolia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the battle of issus take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Syria"]], "filtered_resps": ["Syria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df9407b98ab4d23e87b9f511c2acac865a10018036a4c37b9309f78f8e78538c", "prompt_hash": "cd2c3c530736105c985d2e695cbcca67ac10037f17e2316facc1ff7aead80337", "target_hash": "0336cbf9c50bf16e59e27fbb5112879e5cce978872ba2dfe17ab7c199fc3eee7", "exact_match": 0.0}
{"doc_id": 1223, "doc": {"question": "what is the name of the muslim dress", "answer": ["hijab"]}, "target": "['hijab']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the muslim dress?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" burqa"]], "filtered_resps": ["burqa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2eb66d11829b7604767e63f4bf3a9fd17c15fde744732d9696c79ebf6005cb37", "prompt_hash": "d490b64ecdab86fcbdd2cb65ff9149a5a22d6a2e3478a3ec962bfa61028e4722", "target_hash": "919f4ff91add3d7365615eb3bb8a88fdc3450766844d671635df052e1d1dc7d4", "exact_match": 0.0}
{"doc_id": 1224, "doc": {"question": "when does the new saw 8 come out", "answer": ["October 27, 2017"]}, "target": "['October 27, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new saw 8 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 13"]], "filtered_resps": ["January 13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42305d55a9aa693a5750a8a6f410796b775625f4f3613e1f59988913ee37a065", "prompt_hash": "7b6daf13baa5cb8a956b6181c6bafb5417758e83cde01dc671f97537b94682ea", "target_hash": "a4bdb1618154916767edaa1c3ff5d8d546cb31f00d5846152f65a2cd863060e4", "exact_match": 0.0}
{"doc_id": 1225, "doc": {"question": "when were cigarette ads banned from tv uk", "answer": ["1 August 1965"]}, "target": "['1 August 1965']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when were cigarette ads banned from tv uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1971"]], "filtered_resps": ["1971"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0c529ec28eb2b7eb01c86497bd19f97c231973278904121da1ebc725cdcdec3e", "prompt_hash": "81fa21c181cdfe37eb50a033033a8a1b9d50ba1f4709d10978d3fcf9f38702b7", "target_hash": "f1fa58def02c19a935709eb4f98de209ec958ec0f26c130a47756987d4f34887", "exact_match": 0.0}
{"doc_id": 1226, "doc": {"question": "who invented the transtheoretical model of behavior change", "answer": ["colleagues", "Carlo Di Clemente"]}, "target": "['colleagues', 'Carlo Di Clemente']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who invented the transtheoretical model of behavior change?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Prochaska"]], "filtered_resps": ["James Prochaska"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ddc8167eee476beda70ea9f11a163df9f3f8c9e0e65d7bc40478fa74a43889d9", "prompt_hash": "438e74439d8458429a5dacb42e4bb8a709732320889c1b256c7fdc3ea403e5e8", "target_hash": "d9281cf07a65bf24a66bbcf75d569a43f17f4890807c58b77a8a80da08c6ae2e", "exact_match": 0.0}
{"doc_id": 1227, "doc": {"question": "who recorded i can't help falling in love with you", "answer": ["Elvis Presley"]}, "target": "['Elvis Presley']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who recorded i can't help falling in love with you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Elvis Presley"]], "filtered_resps": ["Elvis Presley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b117c896e239abc80a7d369064b3bc3118774b722bd5264eb93a026433674b09", "prompt_hash": "b18caa97467aa6225267ef2e93b1d030ffca5f028ec32cb3afefeee257ab8ba6", "target_hash": "ebaf9e4f0b5d961c14bffcffa8efab73f9a10688397e19dc60d207d7e4e10bc7", "exact_match": 1.0}
{"doc_id": 1228, "doc": {"question": "when did india win their first cricket match", "answer": ["1952"]}, "target": "['1952']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did india win their first cricket match?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1932"]], "filtered_resps": ["1932"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e30c4155236a0ece04d19cd05a8bc3741fa5edc382bf8e1aa3bf3c69271fd5b7", "prompt_hash": "32a805fa2a359b92261e8fb65f7a8565b2e5b2602a4a32cf5150b9fb9247d5e6", "target_hash": "f95d7f7a2dc14a84704ee6f46648604414b05b7d27db79ef434de9f657b3b1b1", "exact_match": 0.0}
{"doc_id": 1229, "doc": {"question": "which term is used to identify having official power to act", "answer": ["authority", "Authority"]}, "target": "['authority', 'Authority']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which term is used to identify having official power to act?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" authority"]], "filtered_resps": ["authority"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8415ec71719fad1a050396925d871d67cc97f3125bb0250c12e9a16defd5f8e1", "prompt_hash": "c5fc8254f39ce6e42e7e3ebd10b0ac340a6c7aeafecfb85be5295a09c967be48", "target_hash": "290cb7a3e36d7db6eaa9304dea3dd2ff0f784f4eeb340df58f93acf084df71fa", "exact_match": 1.0}
{"doc_id": 1230, "doc": {"question": "when does grey's anatomy come back on in march", "answer": ["March 1, 2018"]}, "target": "['March\\xa01,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does grey's anatomy come back on in march?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2018"]], "filtered_resps": ["2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4f88e098386587c438bb2a748e31b1cabfe7eb7b5d624276100dcd89937299ad", "prompt_hash": "2ed2a602407ba3f18ff335790d7b1a17ac1aa1e4a71370c34f3e1283ad8d3cbf", "target_hash": "87e093b36fdc0e29a992364c47e011b55288b0f85d8b63f6b9c04adad77b2cee", "exact_match": 0.0}
{"doc_id": 1231, "doc": {"question": "who was president when the panama canal was built", "answer": ["Theodore Roosevelt", "President Theodore Roosevelt"]}, "target": "['Theodore Roosevelt', 'President Theodore Roosevelt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was president when the panama canal was built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Theodore Roosevelt"]], "filtered_resps": ["Theodore Roosevelt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a3b356ab1557abbfb3239f997c4da1fa36be84df6348a3581674079bf3808194", "prompt_hash": "2691786c818052a0a66620b7d979dd611797513539e575df4af3203caadf9172", "target_hash": "f32c54320f9366366ae4a1f36b3ca7043342c1b109ee410c1b338f14fb49d6af", "exact_match": 1.0}
{"doc_id": 1232, "doc": {"question": "where was the remake of dirty dancing filmed", "answer": ["High Hampton Inn in Cashiers", "Hendersonville, North Carolina", "western North Carolina"]}, "target": "['High Hampton Inn in Cashiers', 'Hendersonville, North Carolina', 'western North Carolina']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the remake of dirty dancing filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lake Placid"]], "filtered_resps": ["Lake Placid"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a23b343f4e60634c426e2f6e9307cb1ce860cf285623eca62f4defd86a30921", "prompt_hash": "722338f52c9a9b93a3016944799d302c717011e166751bbc8f7543790d1e95f8", "target_hash": "1b1de7e1c6efe634c5647984ff57480036513c7d9ea1295e85b323383774ed91", "exact_match": 0.0}
{"doc_id": 1233, "doc": {"question": "who played the little boy in polar express", "answer": ["Daryl Sabara", "Dante Pastula"]}, "target": "['Daryl Sabara', 'Dante Pastula']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the little boy in polar express?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jacob Tremblay"]], "filtered_resps": ["Jacob Tremblay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfe474a2c8a6f312df27f99a651d388d8cf49aa068fc8ab171e7265119a8d8ef", "prompt_hash": "2a546946ececd223c1df212fefefd0e370bccd449e4d80e6ab5edde6a92ecc9a", "target_hash": "148b3f00282e8adab9779dd127fc0a814df6497777f0a092fdf0adf5840200b6", "exact_match": 0.0}
{"doc_id": 1234, "doc": {"question": "what channel does thursday night football come on regular tv", "answer": ["NBC"]}, "target": "['NBC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what channel does thursday night football come on regular tv?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" NBC"]], "filtered_resps": ["NBC"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dc0b5d46cf64af54c7af8c77fd7d8092f7c0413a3a982874cd0f9a079cd6c810", "prompt_hash": "e75e7ade4dec2ac53debcea3127afc2dcf7080406127dbc1a709fcae3e99e5b8", "target_hash": "3ea285a0c32ca93a6c3d23533c7ffe81cc565abeb1e8d3e1609c9d464b523dd2", "exact_match": 1.0}
{"doc_id": 1235, "doc": {"question": "when did how you remind me come out", "answer": ["August 21, 2001", "August 21, 2001"]}, "target": "['August\\xa021,\\xa02001', 'August 21, 2001']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did how you remind me come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fbf57cfe09418d97e8261d257ff3f0b423331253bfb73a83eca14c5076cd2d7a", "prompt_hash": "bbd2887d151a4f6aaf9e01f7f149e948cd81a8a028ff35186b74954e670d7bab", "target_hash": "5c36b20bf6cf56b369b228427e40541fae5e2d9b82f034a25be7b576e0109b08", "exact_match": 0.0}
{"doc_id": 1236, "doc": {"question": "who is new prime minister of italy 2018", "answer": ["TBD"]}, "target": "['TBD']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is new prime minister of italy 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paolo Gentiloni"]], "filtered_resps": ["Paolo Gentiloni"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18c2e0d98c9f85ad0a6eb0193fc46c18f34bd9f81b9e41d4e9814335d18827e7", "prompt_hash": "07f93500eef69b45d66a14b293faedd80bfdf7bb4c56d16302ad96e4eb25a32d", "target_hash": "4bb0826e38dde93e4d1d11e9b1e4463ff47ce4462b6a0caad7dccc87d0bdab4e", "exact_match": 0.0}
{"doc_id": 1237, "doc": {"question": "when did the subway open in new york", "answer": ["October 27, 1904", "1904"]}, "target": "['October 27, 1904', '1904']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the subway open in new york?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1904"]], "filtered_resps": ["1904"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "132a4dd48b6feb770fc1a6279992ac850e6f1c77f3886b4639f474100159496e", "prompt_hash": "0d9fb946dcf65272068d118fc2bf1955c352919b5d1e7b6331f7a92eeef41432", "target_hash": "b4a74e5461f313ea6d5057f9ff4dbb52cfe0813c24626c1e05e3346a1a98132f", "exact_match": 1.0}
{"doc_id": 1238, "doc": {"question": "what is the speed of the nascar pace car", "answer": ["pit road speed"]}, "target": "['pit road speed']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the speed of the nascar pace car?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 30 mph"]], "filtered_resps": ["30 mph"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "46bc6a36b17b93735dfc0c819c7bbe79712fca0f9ff09a27ecb2d99e45d84ef0", "prompt_hash": "cc9463190f0efb2b175c92328b30873e66d2528f43f698c1fd58e38ae0dec2ae", "target_hash": "cdc700ccc9df73fa3383848bef498aa4d75b8779d1bdc8c93fae9944915c063d", "exact_match": 0.0}
{"doc_id": 1239, "doc": {"question": "who won the silver medal in womens figure skating", "answer": ["Evgenia Medvedeva"]}, "target": "['Evgenia Medvedeva']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the silver medal in womens figure skating?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yuzuru Hanyu"]], "filtered_resps": ["Yuzuru Hanyu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4a8105b812a48f1793fc57a88706efa75d2c45b8fccd9ff3e2329a03f5242b20", "prompt_hash": "77159b09549efc43b02f1429faaa74ca089507231d06d70058d716036207436d", "target_hash": "ea953072b857eb65f9f69dfb8dd39c160ffa63d2aa68368e70d1d5a527f798dc", "exact_match": 0.0}
{"doc_id": 1240, "doc": {"question": "what year is the deer hunter set in", "answer": ["late 1967", "1967"]}, "target": "['late 1967', '1967']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what year is the deer hunter set in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1984"]], "filtered_resps": ["1984"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8c64aefd6d1e85c22831e5ea066a35eb055acd24d2774a0abf85663a71713d50", "prompt_hash": "6757d808e441afc20f1d32914e62dca38aa1974c092fb4c2a2c7bc7943ad8e1c", "target_hash": "25686166cbd589e5f507c904d04d1e79e656ca2f9c3e653d3a1ae2de88462822", "exact_match": 0.0}
{"doc_id": 1241, "doc": {"question": "what type of writing did ancient egypt use", "answer": ["hieroglyphs", "Egyptian hieroglyphs"]}, "target": "['hieroglyphs', 'Egyptian hieroglyphs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of writing did ancient egypt use?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" hieroglyphics"]], "filtered_resps": ["hieroglyphics"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "50b4b480824f429d5dab93e59cbf7739c55d246f1fec5e6e9aa63098eb76e9b8", "prompt_hash": "60b2d5ebbde2859b4217a525b74ad16e8f5dc6d8d86b5e2986bc4adbad6ac33f", "target_hash": "9a88a9fe49343e600c0fbf3a945dcd5b1eb3c861d8aa64ba4953a36d2ff66060", "exact_match": 0.0}
{"doc_id": 1242, "doc": {"question": "who did the united states win its independence from", "answer": ["the British Empire", "Great Britain"]}, "target": "['the British Empire', 'Great Britain']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the united states win its independence from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Great Britain"]], "filtered_resps": ["Great Britain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f1cc87e4c5e670d44cc2184837dc60fe63e81ddfd65be230dacaec278e46ee5", "prompt_hash": "290c2326a59e84524ddbf3fa680743d0640350945459a7657a7d09c405b52845", "target_hash": "5df89dde24766e10b4365582236fd4c1c0788dd2796db1638324977395bf094b", "exact_match": 1.0}
{"doc_id": 1243, "doc": {"question": "the first element on the periodic table is", "answer": ["Hydro­gen", "hydrogen"]}, "target": "['Hydro\\xadgen', 'hydrogen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the first element on the periodic table is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hydrogen"]], "filtered_resps": ["Hydrogen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4bfe56db1de72ea7954c134e4149af15ee7fbd06c355910bdb7cd4725d2c3467", "prompt_hash": "c582f62cb6cfc6adf0ceef38167c007635d4553011bc9ed45cd1266e30037788", "target_hash": "e74b8b848a278ec126b04a19d8b61247a1e3db0b6b338df30b909f9c175d1cdf", "exact_match": 1.0}
{"doc_id": 1244, "doc": {"question": "who wrote the song oh lord won't you buy me a mercedes benz", "answer": ["Janis Joplin", "Bob Neuwirth", "Michael McClure"]}, "target": "['Janis Joplin', 'Bob Neuwirth', 'Michael McClure']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song oh lord won't you buy me a mercedes benz?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gloria Gaynor"]], "filtered_resps": ["Gloria Gaynor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "797aac53f3660e264743afde805633c25041fd30f52a9c0a4ce050fd1b1f118f", "prompt_hash": "3700a75bc00c09344f584209aa6cfcc91a0b6b32161a78078d8d18ca5e1a5fd4", "target_hash": "0bef7ad7ac8eb80d119b31470c9c7c406d650874e4d9b962b394d89f69b44eda", "exact_match": 0.0}
{"doc_id": 1245, "doc": {"question": "who was the first imperial power that was fighting in vietnam", "answer": ["France"]}, "target": "['France']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first imperial power that was fighting in vietnam?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" France"]], "filtered_resps": ["France"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "308e8a01cdafeb3c3c75a84a37301e3d1da5c14dd466e5ab5d061cdc7748d10c", "prompt_hash": "b4ed5a09907ca4096122d9d4e9aed2641b162d4b21de16a350e1647b1c81e85a", "target_hash": "cb35d42049d03c1558695af87e0fcccad41b04f8b0a63de291599f9b84eb4afa", "exact_match": 1.0}
{"doc_id": 1246, "doc": {"question": "who is the captain of richmond football club", "answer": ["Trent Cotchin"]}, "target": "['Trent Cotchin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the captain of richmond football club?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Trent Cotchin"]], "filtered_resps": ["Trent Cotchin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "12bbeffe5c6d8c8986272828ef565c3ae30f9a51bec31e19252d086b931ac030", "prompt_hash": "04381475a58f421a0d5f74d50d56607905cbc929f40fea6bf767d8420d01bc08", "target_hash": "d03e5d8a9062be4ae16b5b6e7a469f0afb0ebd15ce8f1b315a44c1423eb21350", "exact_match": 1.0}
{"doc_id": 1247, "doc": {"question": "who is the first indian climber of mount everest", "answer": ["Bachendri Pal"]}, "target": "['Bachendri Pal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the first indian climber of mount everest?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Suresh Kumar Achari"]], "filtered_resps": ["Suresh Kumar Achari"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "165696cda6610650f70f92f046cc5755863711273092d57c482b0cfbe9c4989e", "prompt_hash": "2d120893039ff4587b4c4fc246040a34e0085313b36b127e9a64def7a0437f20", "target_hash": "b3e25565477dc6c98d5f5ad4df8c627dc92f757f736d30de93a834b226a80031", "exact_match": 0.0}
{"doc_id": 1248, "doc": {"question": "who is the writer of 50 shades of grey", "answer": ["Erika Mitchell Leonard", "E. L. James"]}, "target": "['Erika Mitchell Leonard', 'E. L. James']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the writer of 50 shades of grey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" E"]], "filtered_resps": ["E"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dd35eea8bee1be2b39e6f801943102bbf3c4535a3bbdd8f1dec214f56055aaf5", "prompt_hash": "3af50d74bccee211d54d1a13fa9eca2ee02f6c3ee36e100edd1c2a22c7bd0716", "target_hash": "10d16324194cc502eaa9e9227ea6a2b0f3de803d3223d1b09a6e6c5f1f95d1af", "exact_match": 0.0}
{"doc_id": 1249, "doc": {"question": "who votes to elect a rajya sabha memmber", "answer": ["state legislatures"]}, "target": "['state legislatures']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who votes to elect a rajya sabha memmber?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" state legislatures"]], "filtered_resps": ["state legislatures"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "93c64b8999adf051d652cb10f73f7d990d89df471dc945f8d93a1597d562578b", "prompt_hash": "513f1f279ba27cd0248e376df7af2c2e39c21fcf126937658e9f188bca469fa7", "target_hash": "a157ad4885d7d7f082dbe65227979ac91dc7a25d8a1366b11fd3d21f87dcb060", "exact_match": 1.0}
{"doc_id": 1250, "doc": {"question": "what land mass was north america a part of about 300 million years ago", "answer": ["Pangaea", "Pangaea or Pangea"]}, "target": "['Pangaea', 'Pangaea or Pangea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what land mass was north america a part of about 300 million years ago?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Laurasia"]], "filtered_resps": ["Laurasia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f2c069db64198c6e53fd5dd1f21e1239a013434980d27046d7d46f6481c57a92", "prompt_hash": "fe7b5e28baa3f2bea1d07af4d9770f7e90e6dd77f4c7ff7ff54cef20d77e74eb", "target_hash": "3d01cf15017ea674fb2b1d9141b11e8dcfe92814e1b099aabfa5de5f89883227", "exact_match": 0.0}
{"doc_id": 1251, "doc": {"question": "who was defeated in the french and indian war", "answer": ["the French", "Wabanaki Confederacy", "British", "France"]}, "target": "['the French', 'Wabanaki Confederacy', 'British', 'France']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was defeated in the french and indian war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" British"]], "filtered_resps": ["British"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a62211322c50f19f7234d289f830eb70a4fe07ef385b1dba8e0545c4ded5f11", "prompt_hash": "71b620a7457191b175c053b12a7c31dac18f1c64c85265510fa043c409048aff", "target_hash": "23af9f6f8e87d612f129f119f6cdbc3adef2c1c85f0a9fa56a1fa3c5e6f7250d", "exact_match": 1.0}
{"doc_id": 1252, "doc": {"question": "how many episodes curse of oak island season 5", "answer": ["18"]}, "target": "['18']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes curse of oak island season 5?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1dc16dd3f5eaa81a41afebff43c8f7516c41e98b1682d73342e76a772d8bd29b", "prompt_hash": "a99a211b6065a5524de6c27b72e21346cf811fa73fb7e47f2552f3573ccf2876", "target_hash": "fe29453a161fbb6965ccdbdca03593905c1e66cc520a49919b2d999ddf5c0c18", "exact_match": 0.0}
{"doc_id": 1253, "doc": {"question": "where was the first nhl hockey game played", "answer": ["Ottawa", "Montreal"]}, "target": "['Ottawa', 'Montreal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the first nhl hockey game played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Montreal"]], "filtered_resps": ["Montreal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "22adc81d7d4e9b93bf1348d502efa9e01408e76c67944e5eacfcb5eb4bed4970", "prompt_hash": "c246caee20763523706c984f55a2d4a209c0ff2bd4a77550e3ddd9a3d844c389", "target_hash": "0c9f0bb400a762d56a1fa600615e708aefc0a7cd8d2cdfd0ac21d1aca6c90a79", "exact_match": 1.0}
{"doc_id": 1254, "doc": {"question": "who played mr thatcher in when calls the heart", "answer": ["Garwin Sanford"]}, "target": "['Garwin Sanford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played mr thatcher in when calls the heart?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael J"]], "filtered_resps": ["Michael J"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "146aee14e5688f6b519557c9b8803387f0d40994c72eec049a4c8b5ab1064969", "prompt_hash": "c472e4b9e1c1838581e8844ab1a236e9a1a2a51f200b1f1c60b482b797f7517a", "target_hash": "a5117ab71f6896e80f891d609baf509a61acbad6faa13d4ef47544e9c70d25cf", "exact_match": 0.0}
{"doc_id": 1255, "doc": {"question": "who plays cesar in the show on my block", "answer": ["Diego Tinoco"]}, "target": "['Diego Tinoco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays cesar in the show on my block?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Pena"]], "filtered_resps": ["Michael Pena"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ed86d13767db29b155e5acb0f2fb63c448e53127a86e75ca67f1199b5ac3896", "prompt_hash": "80eb608049796b74a90e3fc3c72aab1953c29620dc96cebb68f91d9f4fd5dccf", "target_hash": "124cec4444cb0702c8a9475461e8c9610b29098ad1141192f2e05a0d974215cf", "exact_match": 0.0}
{"doc_id": 1256, "doc": {"question": "where would the group bacteroids likely be found", "answer": ["the human colon", "human colon"]}, "target": "['the human colon', 'human colon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where would the group bacteroids likely be found?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mouth"]], "filtered_resps": ["mouth"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "510cbc0e30f140c2b8d7e5b456d2b0db03f6f9502f94cb8b969fc45352350135", "prompt_hash": "df42b3c3d1226607e8dea8607cf5be8890428fbe3e8d7dab2a0f9544f4350330", "target_hash": "703fe1852618c2284c0ffb2a46848466673d224c7ffcd61996778df89e9836db", "exact_match": 0.0}
{"doc_id": 1257, "doc": {"question": "who was the pinkerton detective agency's first female detective", "answer": ["Kate Warne"]}, "target": "['Kate Warne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the pinkerton detective agency's first female detective?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kate Warne"]], "filtered_resps": ["Kate Warne"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c21b8c91bd8c56149d5f64b1fd4bc919fe4347fbd9f6614fdb36faaa7a7a4e3a", "prompt_hash": "05c0aae194c95b4d24acb29d36010604b7e51eaf1c8b6a556f04cef473c37f5c", "target_hash": "0cacb4e66b99fd60bcb9682921d2ff4e380ccc0b7bf891883dcb5253f30de54e", "exact_match": 1.0}
{"doc_id": 1258, "doc": {"question": "who played harley in harley davidson and the marlboro man", "answer": ["Mickey Rourke"]}, "target": "['Mickey Rourke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played harley in harley davidson and the marlboro man?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Diane Lane"]], "filtered_resps": ["Diane Lane"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7636621448d51d0e1d2f01cb2b54cf18173c345fef4ea0afd67941e9aeaf0175", "prompt_hash": "f0affc74a5925d80f236b171580c76836d58a080d5aaa8d75e7d67dc77980303", "target_hash": "8727429b079660bf39a066f150820200ca06fc76d5869937ff9f07fd49d3b3cc", "exact_match": 0.0}
{"doc_id": 1259, "doc": {"question": "who were the original voices of the flintstones", "answer": ["Verna Felton", "Bea Benaderet", "Don Messick", "Harvey Korman", "Alan Reed", "Mel Blanc", "Jean Vander Pyl", "John Stephenson"]}, "target": "['Verna Felton', 'Bea Benaderet', 'Don Messick', 'Harvey Korman', 'Alan Reed', 'Mel Blanc', 'Jean Vander Pyl', 'John Stephenson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the original voices of the flintstones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mel Blanc"]], "filtered_resps": ["Mel Blanc"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1281c6f27b7c3475adc11882c62e0b5eed7179cfc5505e83d44d0f88939b64e4", "prompt_hash": "d0519cc2426285d4719edcd15aec03935e448da53cc036c028b2eec3cf489d11", "target_hash": "40d8266912bafc9f0f551a21235b38c1b3761072f139dca735ec2e656c6c70ee", "exact_match": 1.0}
{"doc_id": 1260, "doc": {"question": "where did they get the vibranium to make captain americas shield", "answer": ["Wakanda"]}, "target": "['Wakanda']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they get the vibranium to make captain americas shield?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wakanda"]], "filtered_resps": ["Wakanda"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "97c5a54d860f192c41a918801c8c5ede8007c35ab35728e3aa3e37945a278fc1", "prompt_hash": "80259b15100d8e8599e5bc2ee429a3466a7452a6e219ad5c61431e0126511afd", "target_hash": "4dc8c90a59099a83cb7eb60674aaa22c78815fe9b5f27bf5f0fb29eb4f226981", "exact_match": 1.0}
{"doc_id": 1261, "doc": {"question": "who plays the short cop on will and grace", "answer": ["Ryan Pinkston"]}, "target": "['Ryan Pinkston']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the short cop on will and grace?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael McKean"]], "filtered_resps": ["Michael McKean"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d3d6381eea75e2fe1f10572b76a30aa8c69889fabdc7bb265da08838bbc002b", "prompt_hash": "be0e9f4639686e7e89fd3bcb47efa995954482a4d52443eb80a6d6995e43f1b5", "target_hash": "76c278f309707ee24c4d48349d57ba5ef2cb3e8cbcca2c2dadd10674108284b4", "exact_match": 0.0}
{"doc_id": 1262, "doc": {"question": "what is loihi and where does it originate from", "answer": ["from the Hawaii hotspot"]}, "target": "['from the Hawaii hotspot']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is loihi and where does it originate from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hawaiian"]], "filtered_resps": ["Hawaiian"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "307618cb2bc7f277c75252568cd2a4efbc0a60d2f9d518096ca46cb1ca02bffb", "prompt_hash": "48749ab3a2a95d3f8d381962297ac330723c8b237ea7df93212071eee9b747b6", "target_hash": "946c7199452cf885be05e54e3f5b1e19a0de5d1cfe426bf4af54793ca3532ec2", "exact_match": 0.0}
{"doc_id": 1263, "doc": {"question": "anastomosis between the epididymis and the vas deferens", "answer": ["Vasoepididymostomy"]}, "target": "['Vasoepididymostomy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: anastomosis between the epididymis and the vas deferens?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ductus deferens"]], "filtered_resps": ["ductus deferens"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "02c606e9313610bc74cc50ee142a4dd7e36dc6f2de9286049e2bcf24409f0c94", "prompt_hash": "e38b389ad56bd8d626c00bad7a0010c650cba9006935d403a2ad6f0fb43e8116", "target_hash": "2699aa6135dc8c938f32779995df4cfd58b732ded60512b56bedb6b894354e9b", "exact_match": 0.0}
{"doc_id": 1264, "doc": {"question": "when was the last time astros was in the world series", "answer": ["2017", "2005"]}, "target": "['2017', '2005']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time astros was in the world series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "221b2e1833a8f71996350f3a73840293ff5cb7dedeb34dc3fb80d26261a63594", "prompt_hash": "867dfd0c738c50ef60e7a4b7386ecc476cbf80288a8a96936f5a30ca6845af2c", "target_hash": "a1be31f5ef84afbbec7ce8641609a0aff56d3b478ba0b9b7adb8d08e9fbf666d", "exact_match": 1.0}
{"doc_id": 1265, "doc": {"question": "who won so you think you can dance season 13", "answer": ["Kida", "Leon \"Kida\" Burns"]}, "target": "['Kida', 'Leon \"Kida\" Burns']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won so you think you can dance season 13?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jenna Johnson"]], "filtered_resps": ["Jenna Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "57f103829c4b59135dd5725139c0bda7945f9aa2709953f16c9063861ac14d47", "prompt_hash": "c725c09f8b5ea54240295645919956efca8418a2cc54cceff4dc19d28b83ad29", "target_hash": "8e424b6d8b7dd6221e9c74c433e315a7454b264933ca1acee6e88b1d08c71192", "exact_match": 0.0}
{"doc_id": 1266, "doc": {"question": "who played lionel in as time goes by", "answer": ["Geoffrey Dyson Palmer, OBE", "Geoffrey Dyson Palmer"]}, "target": "['Geoffrey Dyson Palmer, OBE', 'Geoffrey Dyson Palmer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played lionel in as time goes by?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walter Abel"]], "filtered_resps": ["Walter Abel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "62ebcd8bffcd2f223daee32f80eb02c6c1b15011ef317d5fb4330ccb3bef5ea0", "prompt_hash": "a4a36e241dad1d78b5c2b8f4c4ebb2b0201a7bf2898c2ff7cb4fc661bc223c46", "target_hash": "c84d67bd8f1a6c401d4c6750f36f3ba7e0da3b9ad634ee655ab9c964072543f5", "exact_match": 0.0}
{"doc_id": 1267, "doc": {"question": "who helped the british in the french and indian war", "answer": ["Cherokee", "Iroquois Confederacy", "the colonies of British America", "Catawba", "Iroquois"]}, "target": "['Cherokee', 'Iroquois Confederacy', 'the colonies of British America', 'Catawba', 'Iroquois']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who helped the british in the french and indian war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" France"]], "filtered_resps": ["France"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b8f6c1b4c6334744a3a1a5769fb4fd053b64e94b8bca7f9e16639a6e3173ec5", "prompt_hash": "60f94d38467845292766b40dd9683cff63d833631a3602622385c32fd1f754de", "target_hash": "39af5f4f767b988228ac22f9bcb3b6dcd96c4395607b218c58fea1dd289a0dd9", "exact_match": 0.0}
{"doc_id": 1268, "doc": {"question": "who was the german leader who signed the treaty of versailles", "answer": ["Gustav Bauer", "President Friedrich Ebert"]}, "target": "['Gustav Bauer', 'President Friedrich Ebert']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the german leader who signed the treaty of versailles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Woodrow Wilson"]], "filtered_resps": ["Woodrow Wilson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a1144e5a460edc09e588f5350c0419cac04a927cc6e34d56ab9b0d0a0d5d1c4e", "prompt_hash": "db6a16a0a0d46cfe986beec327caa05e74af23b5c3ae4b4d049bd8a9583cff01", "target_hash": "1dd67752dafacc46338310cabfb09f051a590581727499cbb8b6f832b8ed8149", "exact_match": 0.0}
{"doc_id": 1269, "doc": {"question": "who played ricky in secret life of the american teenager", "answer": ["Daren Maxwell Kagasoff"]}, "target": "['Daren Maxwell Kagasoff']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played ricky in secret life of the american teenager?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Welborn"]], "filtered_resps": ["Justin Welborn"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb42882dc0cdff7722bd49384c3d80ac92c123077fd0cd5e1577837c1b58c659", "prompt_hash": "f7124ef0cb16bfd6c3403b863f951c63312cb78b6e2401ead30a155939576f51", "target_hash": "f6835effea9f19c6e29e740fc993f1020385f6caf26ff8a27fb2ea49dc06a817", "exact_match": 0.0}
{"doc_id": 1270, "doc": {"question": "a good that can be used in place of another good", "answer": ["A substitute good", "substitute good"]}, "target": "['A substitute good', 'substitute good']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a good that can be used in place of another good?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" substitute"]], "filtered_resps": ["substitute"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9fc12424a7fed39a2835d881132828f48eb168aee8d96d93d9542f979f7f25f", "prompt_hash": "3ceeba7711bfcfb84a3e8b4b36be04c64d34f780d3080aaea70fb152b13e4510", "target_hash": "5d1218aa2918374ab147bb49d9b7887a8fb2204441ea49fa5aef454c6f3ba329", "exact_match": 0.0}
{"doc_id": 1271, "doc": {"question": "what is the meaning of the name sinead", "answer": ["God forgave/God gratified"]}, "target": "['God forgave/God gratified']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of the name sinead?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Irish"]], "filtered_resps": ["Irish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c961298ce0bc05ad608b98cae25193d8c348a45c98cdeb4b7169386a1ca09656", "prompt_hash": "c44c8127cf054166dfdc6ff496c9885fc1f1b265ccb5f7e2bdad320324457841", "target_hash": "601339370ca15c50a9fe0e560fe15c7e9702890f25bbaa8b060073250e0e13e0", "exact_match": 0.0}
{"doc_id": 1272, "doc": {"question": "who gets first pick of fa cup games", "answer": ["BBC", "the BBC"]}, "target": "['BBC', 'the BBC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who gets first pick of fa cup games?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the team that finished highest in the league table"]], "filtered_resps": ["the team that finished highest in the league table"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "242e485bd47d4ba0bf23c1428ac802983df016887b10b17d71fa4b2cb40158d2", "prompt_hash": "2934f47ce303ecd91a2aaf0f8f502216605477cff3552ff567e8c7da638cb780", "target_hash": "16bb1e39ac707b472eb3db31907f93d34c29a033c2c005773228c0eb9a6a1b79", "exact_match": 0.0}
{"doc_id": 1273, "doc": {"question": "what is the written dialogue portion of a musical called", "answer": ["script", "libretto"]}, "target": "['script', 'libretto']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the written dialogue portion of a musical called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lyrics"]], "filtered_resps": ["lyrics"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "85a213af33f94b5d3e73c87b77ac088a9473fdae99c992439dfa8f349acf0bfd", "prompt_hash": "2699393f51f2736dc219dfedd28862f4f344a4f2333aa17fc3cdc7139288ee00", "target_hash": "259d2728412430c3ca67c729ac1a85a3d4cb0daac94a8a6eb39a2b39c78ac7d3", "exact_match": 0.0}
{"doc_id": 1274, "doc": {"question": "in florida it is illegal to sell alcohol before 1 pm on any sunday. this is an example of", "answer": ["Blue laws"]}, "target": "['Blue laws']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in florida it is illegal to sell alcohol before 1 pm on any sunday. this is an example of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" law"]], "filtered_resps": ["law"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6ad2a0611617b029e4046af602b463b8a7882150922d6edc148932a840856f92", "prompt_hash": "dfc4eea9eeb293a86eaa95e7ce5219263d4f6a84dca701a68abc6a5bb55597a2", "target_hash": "639e879989e2d16f732dfad1f10e02f78866b71590b6e74619a29450acad910b", "exact_match": 0.0}
{"doc_id": 1275, "doc": {"question": "who played the original wonder woman on tv", "answer": ["Lynda Carter"]}, "target": "['Lynda Carter']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the original wonder woman on tv?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lynda Carter"]], "filtered_resps": ["Lynda Carter"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e25e2bae3e264903a0401463f79b427201725aa38362cc7ebd0bb08d1e360713", "prompt_hash": "c925f54c827ed7c2e8407dfd9ecd1747ed340d865e27ce0e273a167eb9a6bc68", "target_hash": "cbe2ace13b56800d26ea79b60a7d498d3dbf34b33d217bb4d256f40f0de0ef0b", "exact_match": 1.0}
{"doc_id": 1276, "doc": {"question": "the man in the high castle episode 1 season 1", "answer": ["The New World"]}, "target": "['The New World']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the man in the high castle episode 1 season 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Frank Frink"]], "filtered_resps": ["Frank Frink"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "134d6b14dfd85488444c263878df079df2b0d6211944dd41890ebe579bc295fb", "prompt_hash": "36e305262642d168f5dfa0c72b19597f21db42c23f46733899ac2008cf6d5700", "target_hash": "0d5b7aa28083c63a1e8bca19ce61bf14844fc8db67930ee68f4b6a88bb686996", "exact_match": 0.0}
{"doc_id": 1277, "doc": {"question": "who is the voice of the other mother in coraline", "answer": ["Teri Hatcher"]}, "target": "['Teri Hatcher']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the voice of the other mother in coraline?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Teri Hatcher"]], "filtered_resps": ["Teri Hatcher"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a7e5db2d021671087266d33097416f3b6378c0f3ec454991ae68da746567e6d9", "prompt_hash": "8a78c93330d41cccd14f536f716078f0d2cf717ae7407d7cbd94d4ccf4adff7b", "target_hash": "976e9a5c4c9e3203f011b4ff07391009ceeaf109cc85d73b2c8258db2c3a40be", "exact_match": 1.0}
{"doc_id": 1278, "doc": {"question": "when was the last time the dodgers played yankees in the world series", "answer": ["1981"]}, "target": "['1981']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the dodgers played yankees in the world series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1981"]], "filtered_resps": ["1981"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "02a286818875eb46706082a5b5db2002b9be03a3f61079ff35f06b9403e2e5a4", "prompt_hash": "14e8beaffbb6dfef970a69022888802e0b65f9291082011008ab406f064d660b", "target_hash": "957d512e662d9fcb3974a8776c0b0a6d5ff8134c98ca9d129f2941b247a8d498", "exact_match": 1.0}
{"doc_id": 1279, "doc": {"question": "who has the highest paid contract in the nba", "answer": ["Stephen Curry"]}, "target": "['Stephen Curry']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the highest paid contract in the nba?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" LeBron James"]], "filtered_resps": ["LeBron James"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4f5a5f6aeb95b8c61685c4592475e760731536a85e3b63d04af10c702dfe60f3", "prompt_hash": "f8a9753d02b612fa07612d980ca5e1109290dde720b77e5a15a1a8137530298a", "target_hash": "ae93610587e3845abdeaa483058dedf150002cdfa86664576ccce17f56b81493", "exact_match": 0.0}
{"doc_id": 1280, "doc": {"question": "when does jo come in grey's anatomy", "answer": ["season 9", "2012", "July 2012"]}, "target": "['season 9', '2012', 'July 2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does jo come in grey's anatomy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 11"]], "filtered_resps": ["season 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "471453a617e21cb8a5b1e9994a42ebe34a9fbf1376e5893eb833853f7dd26eeb", "prompt_hash": "0ad6fdd7a3cad090c263928a8ea78eca59f0973751c4c2a027d6a0295740e46a", "target_hash": "e5c5c18224aa3bf7febf65026ddc083241165f319d354dac9492399ee54aa39f", "exact_match": 0.0}
{"doc_id": 1281, "doc": {"question": "which financial statement involves all aspects of the accounting​ equation", "answer": ["balance sheet", "The balance sheet"]}, "target": "['balance sheet', 'The balance sheet']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which financial statement involves all aspects of the accounting​ equation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" balance sheet"]], "filtered_resps": ["balance sheet"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b68065b20244d5561cec4ebbd05d1593fe677ee53e03edd9509db58afd04a7b9", "prompt_hash": "85d334322e0225d22068cef2a733f89cf4b4cf3de7c0c331523d34f9da85c2c7", "target_hash": "ff06ff9eb6ad9916fa05a95337dce649e25f0fb4e1c5cffb91dfe53c73c48f07", "exact_match": 1.0}
{"doc_id": 1282, "doc": {"question": "who got the most passing yards in the nfl", "answer": ["Peyton Manning"]}, "target": "['Peyton Manning']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who got the most passing yards in the nfl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Peyton Manning"]], "filtered_resps": ["Peyton Manning"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ecff77d8604f5880413f2324ff74a88a8d39b77b4cf4a997e1e06a5709e47da6", "prompt_hash": "b959845b479609ef777f9d0bf1448bb1eebec0e053922459040936c7834e84a2", "target_hash": "bc385caa5bbfc7a91e3a902e6a0d3a60cd7dea88f5247dc91576253c579a6200", "exact_match": 1.0}
{"doc_id": 1283, "doc": {"question": "when was it was not death for i stood up published", "answer": ["1890", "1891"]}, "target": "['1890', '1891']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was it was not death for i stood up published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1981"]], "filtered_resps": ["1981"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d143893c6ed932211b7df0c2e4f98b9bb8113fd09a9e9e2a782ab2198cfeec6", "prompt_hash": "fa97f5a41d0b2ed1f3e277904e3464986547b06bb9012a454938f797281189cd", "target_hash": "22694422d8184c542ef923f1c0e86d8e18c0f8563693c4f9bd299f716b1acd70", "exact_match": 0.0}
{"doc_id": 1284, "doc": {"question": "bacterial cell walls are made rigid by the presence of", "answer": ["Peptidoglycan"]}, "target": "['Peptidoglycan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: bacterial cell walls are made rigid by the presence of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" peptidoglycan"]], "filtered_resps": ["peptidoglycan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a29638a445908a40f79c6786c4bc41bdbe17a18af21c3d6c7e154c46c455eb77", "prompt_hash": "f5c7e68d3cae99649b45b1d463ac5b1cdb399c4e87db375c4596e003c220e643", "target_hash": "01e816676bd268ddb3de02c6c30ed96d5daf5c063617298d028281a2ea30c229", "exact_match": 1.0}
{"doc_id": 1285, "doc": {"question": "when did 10 shilling note go out of circulation", "answer": ["22 November 1970", "1970"]}, "target": "['22 November 1970', '1970']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did 10 shilling note go out of circulation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1988"]], "filtered_resps": ["1988"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "89228d9f41809c4b60dda3159132c49822504ff030452f999c8968c493531130", "prompt_hash": "b358c277e6f73281ea0c43b2965e7f90a7de1fca455b6323c9eeebcd2ad2f4c3", "target_hash": "25b27714755b5962cb43eb29ee067d44d06855dc3bad0091665b6f128ef0bfce", "exact_match": 0.0}
{"doc_id": 1286, "doc": {"question": "how many medals did austria win in the 2011 alpine skiing world championships", "answer": ["8"]}, "target": "['8']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many medals did austria win in the 2011 alpine skiing world championships?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "99e479980b3f7df4ed5350333b2ccf21cfd9a5e126cffd195ac6ba8706bc55f8", "prompt_hash": "0790021ab90fdd417f91e9044485e317e62a8de05a007efff1b7422cb024775a", "target_hash": "3eea823137f77893123deb53d5ed402841ef0531a82e7467061d5ec91d0a6bb8", "exact_match": 0.0}
{"doc_id": 1287, "doc": {"question": "when did the song rapper's delight come out", "answer": ["1979", "September 16, 1979"]}, "target": "['1979', 'September 16, 1979']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the song rapper's delight come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1979"]], "filtered_resps": ["1979"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b0210330be0282d63a54601a69553ddb25bea0bc1fbdeb68b7e3d470f7014ab5", "prompt_hash": "88fdfda27263d0a7719c593861da77685872aab40d427d1389146f313430c0bd", "target_hash": "1b7142f4caa3baf2c30603c0f0a4520c1c43ba126a940b9a0379af6aecd31c21", "exact_match": 1.0}
{"doc_id": 1288, "doc": {"question": "when did the flash first appear on arrow", "answer": ["second season"]}, "target": "['second season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the flash first appear on arrow?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 2"]], "filtered_resps": ["season 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "544c5fa1effa56234489fd7d52ebe4739938f43719054492118e59e98a98dff1", "prompt_hash": "5167d4f0d2ff30313f6674e4f2a6c3016969d79e5fa1bef9ced3219b22031743", "target_hash": "796cf9e707dab590008ba5392505488661df1e4eb441dd0dfe0aa9fd314eca78", "exact_match": 0.0}
{"doc_id": 1289, "doc": {"question": "who sang how can you mend a broken heart first", "answer": ["Bee Gees", "the Bee Gees"]}, "target": "['Bee Gees', 'the Bee Gees']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang how can you mend a broken heart first?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tom Jones"]], "filtered_resps": ["Tom Jones"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7d90657242ed11faee04a9ec2f27e201f6a2b553c5459f9959dafdb93213e7bf", "prompt_hash": "2f888fd4f1902cbb08542e429762ded4c11526844884c25d7fbebbdd3e6a7b81", "target_hash": "b0362bd193d2d30a868cc7b84971b444ac2b2029e1ed8379dafc395bd3377bb5", "exact_match": 0.0}
{"doc_id": 1290, "doc": {"question": "who played in the stanley cup finals last year", "answer": ["Nashville Predators", "Pittsburgh Penguins"]}, "target": "['Nashville Predators', 'Pittsburgh Penguins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played in the stanley cup finals last year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nashville Predators"]], "filtered_resps": ["Nashville Predators"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e50038a4c4ffd247c075e239941e7c0b1453392331fa2a04947a6da026d33eb3", "prompt_hash": "bd0489eabdc31e88d4f5c295d8057ed3c1f08b11bdd17294c61379ce3718e9d0", "target_hash": "3b652aee23715532487bdd7421b43a5303750a7dbeae86524e1436c08f7eec76", "exact_match": 1.0}
{"doc_id": 1291, "doc": {"question": "when did the continental congress vote to adopt the declaration of independence", "answer": ["July 2, 1776"]}, "target": "['July 2, 1776']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the continental congress vote to adopt the declaration of independence?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" July 4"]], "filtered_resps": ["July 4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "898fb25994a94f8540a36fde3e88382ece346cc805dafe08d17d5cefb2a1f637", "prompt_hash": "6842e16bb821d1dd2d90f4c76a8c3af868674b35317cbc598b97344e04fd7ace", "target_hash": "d0ddd12fd2c7f01b9399b2aa03f26b946f5408290e9f862dcc2c82c7d98554ee", "exact_match": 0.0}
{"doc_id": 1292, "doc": {"question": "where does the us launch space shuttles from", "answer": ["Florida"]}, "target": "['Florida']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the us launch space shuttles from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kennedy Space Center"]], "filtered_resps": ["Kennedy Space Center"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d35aa4b4e7fbfb1cee06af9d546c6ba676195b04bda8f886d2191ea0e57a62ed", "prompt_hash": "ab5b989d52e91b443c1b5fe65494b5f34e11b7eb09b493a28015a5b67eb1bc9e", "target_hash": "3a10045e60753262513a93b91b133b58cb1ba7f091839dc5fb133bfd7baa4547", "exact_match": 0.0}
{"doc_id": 1293, "doc": {"question": "a country having an island location and a country having continental location", "answer": ["Australia"]}, "target": "['Australia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a country having an island location and a country having continental location?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" island nation"]], "filtered_resps": ["island nation"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2795e017db4f24ac3ee6ced69efedbbffa334135d7f64d983f8c3976d0abcfb", "prompt_hash": "efe694a7042d7e184260a145d166e4a3326154c0860dc63ccb728131a64a3926", "target_hash": "e93046d7b9b679fac171cfe1c97d475a9a91f20f122092146289e932cf18c258", "exact_match": 0.0}
{"doc_id": 1294, "doc": {"question": "which horse and jockey won the melbourne cup in 2002", "answer": ["Media Puzzle", "Damien Oliver"]}, "target": "['Media Puzzle', 'Damien Oliver']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which horse and jockey won the melbourne cup in 2002?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Best Man and Mark Zahra"]], "filtered_resps": ["Best Man and Mark Zahra"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e98748165231fe427135f5ae39e5825a46b00242187f7c96019ac10111f4c7df", "prompt_hash": "93b8f2429efe09f56221bfe5daa3c1d9a84cc5f2eac0bbc454ea2d2ae8e06b31", "target_hash": "1683ec5478de490b28630c858e73fa7534fd35d2497c78a087e9e8da45079a15", "exact_match": 0.0}
{"doc_id": 1295, "doc": {"question": "when was last time bayern munich won champions league", "answer": ["2012–13", "2013", "2000–01"]}, "target": "['2012–13', '2013', '2000–01']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was last time bayern munich won champions league?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012"]], "filtered_resps": ["2012"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ccaf90885bd36fb171a5d0d30c982b5acab53e55fb8d4eb6e4a50c101165c5ef", "prompt_hash": "6a7f1b827bf7ee41c4bfaf8359b552dd766a7e079e8242b7bdc1f5d21e637a5f", "target_hash": "a3ea9beb5feffe6ec9691de8487201508a1c5b2a58d5cab46ed6003a98be37a9", "exact_match": 0.0}
{"doc_id": 1296, "doc": {"question": "who sang what i like about you originally", "answer": ["The Romantics.", "The Romantics", "American rock band The Romantics"]}, "target": "['The Romantics.', 'The Romantics', 'American rock band The Romantics']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang what i like about you originally?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Jacksons"]], "filtered_resps": ["The Jacksons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2e16d91b91bb2107ca5ee1cebdf943402d93a08dc767f2be9ba11bc6d75a9448", "prompt_hash": "79d1083a06b1cfc920727c3b2134ac3b7dfb08d4de369c92e5e36711f0f49264", "target_hash": "8058f7d394d78171fd5f08ea334a33e38e840577c9dcd03a45c7999d02a55abf", "exact_match": 0.0}
{"doc_id": 1297, "doc": {"question": "the complete collection of dna and genes is called", "answer": ["genome", "chromosome"]}, "target": "['genome', 'chromosome']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the complete collection of dna and genes is called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" genome"]], "filtered_resps": ["genome"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "996167f9ef2d6178e30940a1a16602f4d422cd660ccd9f06db648bf01de76329", "prompt_hash": "6bd00b13b48e2ba048c13acccb797be3521c972ebd7234656808df9a66b38cf5", "target_hash": "6117e308d080a69f33b170913094b62d8ba3cea8b1b0924d1d92949a01b41938", "exact_match": 1.0}
{"doc_id": 1298, "doc": {"question": "who plays big momma in big mommas house", "answer": ["Ella Mitchell", "Martin Lawrence"]}, "target": "['Ella Mitchell', 'Martin Lawrence']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays big momma in big mommas house?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bernie Mac"]], "filtered_resps": ["Bernie Mac"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9aca068a1f8bc3e5f837ff9ff94df1b527b31e05cfa6d73359621fff00c0c465", "prompt_hash": "f3123da9ebed1dfc19a9525dd9fea67c117e087e10c1e98a04f1cfa64b6f328b", "target_hash": "aa794b5e7881077fdf1cd2bbf57ad52412c80881b4a292c870d994ea3a2177bf", "exact_match": 0.0}
{"doc_id": 1299, "doc": {"question": "when were 7 books removed from the bible", "answer": ["1546"]}, "target": "['1546']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when were 7 books removed from the bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1546"]], "filtered_resps": ["1546"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4cf93cdc501a96a77dcd1f3a32c0261bbf06284074b4d9aa271fd9335dbcec12", "prompt_hash": "db296cceaee4bf8b965efb6d332307861962518941d06978ce5f5aed005b4cdb", "target_hash": "3c48c5ec508afe9af82105895d7011078b0a364119640c43313adde8c6bb55c1", "exact_match": 1.0}
{"doc_id": 1300, "doc": {"question": "what percentage of global man-made co2 emissions comes from worldwide aviation", "answer": ["4 percent cumulative effect"]}, "target": "['4 percent cumulative effect']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what percentage of global man-made co2 emissions comes from worldwide aviation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a255210d606ad4cfae2dc96ba2eb3a2f96cfe46fe3e4af31a7794bb579caccb2", "prompt_hash": "1699d1c4ac4de9b993fa7be83ab0a147cc77c0826f2aa60d7b85fd942aef144f", "target_hash": "7947afc58f4e474e53ea4e5891d0bad8b58b8b1886574d1e34b8a0d7e021a1e4", "exact_match": 0.0}
{"doc_id": 1301, "doc": {"question": "when did i'm coming out come out", "answer": ["August 22, 1980", "August 22, 1980"]}, "target": "['August\\xa022,\\xa01980', 'August 22, 1980']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did i'm coming out come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1991"]], "filtered_resps": ["1991"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "176f40d70150aa3642cf80fde3c4a1c5bcf1ecee8a78c0eecf8c2f68ca9f0fc3", "prompt_hash": "96b92aed2c890679151d61b536a1a7e70f197104ad046e8cc7720f009a7d8a16", "target_hash": "e2b727d81a3d2c1287cfab40496cfe0126bc19dcec3de752e3baf8be31bb873c", "exact_match": 0.0}
{"doc_id": 1302, "doc": {"question": "how long has tom brady been playing professional football", "answer": ["18-season career"]}, "target": "['18-season career']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long has tom brady been playing professional football?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 20"]], "filtered_resps": ["20"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06ec87183aa00c83cf7412dc8ec662e0990d111929aa15e512c017e3f82dec48", "prompt_hash": "fe151ff97e91ee37b10811c3e2630c494b768a67d5176775d15ce7c7eb930e2e", "target_hash": "ddab054db33f1d1662027605af8b0e92dbfd0596c45b1be66e1ad660ea38452e", "exact_match": 0.0}
{"doc_id": 1303, "doc": {"question": "what is one element a topographic map shows", "answer": ["relief"]}, "target": "['relief']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is one element a topographic map shows?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" contour lines"]], "filtered_resps": ["contour lines"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "430537ed186f98575b87f5bbf723e501c2566c54b88188431942e6280e00b160", "prompt_hash": "97c3e35e9ac921ff8121664e04f4cc6c8d0e6a8325f26ea4c3986422ec67efcf", "target_hash": "b75fb4f39045a03a756a8ba1d10940beacf380426341dee368535ea86675c163", "exact_match": 0.0}
{"doc_id": 1304, "doc": {"question": "spyder from once upon a time in venice", "answer": ["Jason Momoa"]}, "target": "['Jason Momoa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: spyder from once upon a time in venice?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Antonio"]], "filtered_resps": ["Antonio"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0f2b55b39fb341b7ba1370b04496c771ec87068d8de85a24a377aa2d537d6320", "prompt_hash": "5ff4e8076f9f9184a49f0a707076ab860f73a988ea52ceb08b4683ffa9121c16", "target_hash": "c61965a6ef8a11867c441013575452ed92fecc581dc4f0e4e8351eaccdb79285", "exact_match": 0.0}
{"doc_id": 1305, "doc": {"question": "who dies in season 6 of once upon a time", "answer": ["Hyde", "Jekyll"]}, "target": "['Hyde', 'Jekyll']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who dies in season 6 of once upon a time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Emma Swan"]], "filtered_resps": ["Emma Swan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "690ff9e11254f510ff78478a0992f788b84477a1e0463f80019075cd7027e6e2", "prompt_hash": "c9a3df19010915868d71cf8bb3495283bdfd9423940c0630490712d7b5d85354", "target_hash": "fa39dfe8f1429b761f95a7f98c9d7033677d703b3d9c9666fe7fcc9889a42d6d", "exact_match": 0.0}
{"doc_id": 1306, "doc": {"question": "who won mark and goal of the year 2017", "answer": ["Joe Daniher"]}, "target": "['Joe Daniher']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won mark and goal of the year 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cristiano Ronaldo"]], "filtered_resps": ["Cristiano Ronaldo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55702160617f3382ad10a4a0f040ef4fa369728369859b9f5d633fce81faf1d9", "prompt_hash": "604b2be839c5f701dd67946facc2ea7e4c4c0a8287adf1df69e90e3775a4c5bc", "target_hash": "862412eb4102cfd077d7b0f0b897c9a374dd8c094425e6d9e0d6ba335d56b2a2", "exact_match": 0.0}
{"doc_id": 1307, "doc": {"question": "who said i'll gladly pay you tuesday", "answer": ["Wimpy"]}, "target": "['Wimpy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said i'll gladly pay you tuesday?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bob Dylan"]], "filtered_resps": ["Bob Dylan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "80b466a9eac00c6d05ad339c7e107d50ce90a96d0e8e454ea0a6ddf574d6352b", "prompt_hash": "d54e50791db329258cfb02a5a9fc01c904ab2fd8dd6f9ebdbb5e54cf57f7f392", "target_hash": "d695f03539e87a4842085441d574578c569f65d1941c4c2a6387942fe884cd5b", "exact_match": 0.0}
{"doc_id": 1308, "doc": {"question": "where is the 7th game of the world series played", "answer": ["Dodger Stadium, Los Angeles"]}, "target": "['Dodger Stadium, Los Angeles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the 7th game of the world series played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" home of the team that does not win the 6th game"]], "filtered_resps": ["home of the team that does not win the 6th game"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "683a7fd7ae26418ac6effdb2ed555d17fe9ca0bdbfbcfdbe8a821b72de4391ab", "prompt_hash": "50a02099630f68cbe3433ad52b7ae38aba52ecb35cfcf2c0efb8f35f9a6d85be", "target_hash": "2a83cb36745279efaab9e9c0438503e61b4990f1ec5403e48a6b44b09fea7463", "exact_match": 0.0}
{"doc_id": 1309, "doc": {"question": "when does the next season of wynonna earp start", "answer": ["2018", "January 15, 2018"]}, "target": "['2018', 'January 15, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the next season of wynonna earp start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" July 2018"]], "filtered_resps": ["July 2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "326a185083ae6a62fe2d983a970c63e144a24e22b160e7eed83b3330ab7bcac5", "prompt_hash": "f279664cf703d96fa76e58c42019a9c6614b85a67e6176a196d2eec393a8b3d4", "target_hash": "30e898011e520711821adc999500077ccb879d6fad6d3bae4857c52905557b32", "exact_match": 0.0}
{"doc_id": 1310, "doc": {"question": "what is the emblematic rhythm of dominican republic", "answer": ["merengue", "bachata"]}, "target": "['merengue', 'bachata']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the emblematic rhythm of dominican republic?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" merengue"]], "filtered_resps": ["merengue"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c458857ef44cba65f14800f6590a3c3f7ff6dc2788324a23f04ee73d157dc49b", "prompt_hash": "638098a5e16e221cab501d18124f0665e1b7beab904bb87ce0b02c2400e9aed9", "target_hash": "f63bff706e480922d606cad176993bd687d0a786827848997eeedd2b8a735305", "exact_match": 1.0}
{"doc_id": 1311, "doc": {"question": "what side does us army go on acu", "answer": ["the right"]}, "target": "['the right']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what side does us army go on acu?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" blue"]], "filtered_resps": ["blue"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "afbcfa094b097b72e307d8547b09aa2a4b068ae766ded7f34cfa47d8b7604cb3", "prompt_hash": "9dd967766ba4130d98baca084bf911a1fb5eb3ba3b337b6ed9f184354546f49b", "target_hash": "0952df057cf97d08218f902ae5738e3df9d864eb76bbb1f85efa12f99696e123", "exact_match": 0.0}
{"doc_id": 1312, "doc": {"question": "who sang it's a mean old world", "answer": ["T-Bone Walker"]}, "target": "['T-Bone Walker']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang it's a mean old world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Platters"]], "filtered_resps": ["The Platters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a1e5b2bbf8f04e78db2a8683cd362348404e27d9c17a9e480cb28237b024894", "prompt_hash": "ce517c62c08da3417c85f2deabcae34a75720c2799db8103aaa20e6bda08ae18", "target_hash": "b2aec8a2c4a49486a87cb511218115499ebb0c9213619bfa668733d04fa8c9be", "exact_match": 0.0}
{"doc_id": 1313, "doc": {"question": "who did the astros play last time they were in the world series", "answer": ["White Sox", "Los Angeles Dodgers"]}, "target": "['White Sox', 'Los Angeles Dodgers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the astros play last time they were in the world series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dodgers"]], "filtered_resps": ["Dodgers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "364da95e2d3d64bf96eaaee8cde5f792b153fcca00a065caf5875dc466d56493", "prompt_hash": "b2d3035918ea39506b59c0f4b41a8f2bc80680570ad1f7f7fe1b1093d5ff469f", "target_hash": "ae35a8d99b4507e6a5aa9530005cb3594901f74524028758fdde3fdb87e8365a", "exact_match": 0.0}
{"doc_id": 1314, "doc": {"question": "who was the inventor who developed a way to record sound", "answer": ["Thomas Edison"]}, "target": "['Thomas Edison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the inventor who developed a way to record sound?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thomas Edison"]], "filtered_resps": ["Thomas Edison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4fe87fec60564e8ac8f1e05bc69646da2502c07ecf5cdfb531eae30da90d02e", "prompt_hash": "6e82997257ee33853b163b1e98e2cf1f0971883c057ef06964a7566dd8d6ee13", "target_hash": "384a41fb119f9a8711a168a4b175905a1fa5c28c1a11424c82a6b5bd551909b0", "exact_match": 1.0}
{"doc_id": 1315, "doc": {"question": "who became a leader of the indian national congress", "answer": ["Mahatma Gandhi"]}, "target": "['Mahatma Gandhi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who became a leader of the indian national congress?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jawaharlal Nehru"]], "filtered_resps": ["Jawaharlal Nehru"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a673d7b64e24f51ef7c87d79b312a65371e0d633e91e3611a58d7bea3c97a0dc", "prompt_hash": "833a511e540ce23b7a4520b6036b911f6e2b017bc9fad4dbcf1d7ddd9c2642ed", "target_hash": "372af63225cb4a0444c6b2f6514e66913e8cd939b248560fe0148412ccb4e3ea", "exact_match": 0.0}
{"doc_id": 1316, "doc": {"question": "who has the most sacks in college football", "answer": ["Mike Czerwien"]}, "target": "['Mike Czerwien']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most sacks in college football?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Derrick Thomas"]], "filtered_resps": ["Derrick Thomas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "802b3c9e5d1c246f87b8ce30eae096810a3308fb7aa8027f106be6fb3db5b853", "prompt_hash": "6e83224c6543d71eff7ec588cdee763fe2cc3321fe84a1147ad701b98cd85587", "target_hash": "9978947618366f292b29be246734dec464c977b61101948feea0d4b60df483d3", "exact_match": 0.0}
{"doc_id": 1317, "doc": {"question": "what does zonia receive from reuben in the play", "answer": ["kiss"]}, "target": "['kiss']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does zonia receive from reuben in the play?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" a necklace"]], "filtered_resps": ["a necklace"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f2fbbb98415f99f58e6090c9fb2cfdfd6fb43f64cf571886363d0a7d91966cea", "prompt_hash": "b835817dd61e651cd743902acaa6aff6b9c03c876ec05b9441f1850b73338967", "target_hash": "24f37b9e07ebbdcb50f0ec0c04ce9b4a2f1d9bc2b0d0a39c1576dae249f445cf", "exact_match": 0.0}
{"doc_id": 1318, "doc": {"question": "during the great depression in the 1930s the u.s. economy experienced a", "answer": ["economic recession", "recession", "an economic recession"]}, "target": "['economic recession', 'recession', 'an economic recession']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: during the great depression in the 1930s the u.s. economy experienced a?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" depression"]], "filtered_resps": ["depression"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "510bb80ea36830c72b10f1aa3ae52ba9cbe87bedba4675db85e1046b8c91fcd8", "prompt_hash": "ead6ccd7ce12cb61aff4b9e3b65b63026968db3bf943503f2f185b356d480c58", "target_hash": "860c040284aa32d4f8aeb7c8f92db2d13080ed670bb844d860c186cbcf01283b", "exact_match": 0.0}
{"doc_id": 1319, "doc": {"question": "who is the guy who walked across the twin towers", "answer": ["Philippe Petit"]}, "target": "['Philippe Petit']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the guy who walked across the twin towers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Kunst"]], "filtered_resps": ["David Kunst"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f85264aa6c100f065de1fcf2bb2b581bf6663c54d5037fc5fd7a1b17ee6d0cda", "prompt_hash": "a242c16c52a8474f01b93296ebf2e1bceaba9a557d5c8a825c16aa728902d0e2", "target_hash": "a02e3621f303b5e84a63b6e16b494ded6552b06b793388ea2167e3fecdf47fa6", "exact_match": 0.0}
{"doc_id": 1320, "doc": {"question": "turkish finnish and hungarian belong to which family of languages", "answer": ["Uralic languages"]}, "target": "['Uralic languages']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: turkish finnish and hungarian belong to which family of languages?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Uralic"]], "filtered_resps": ["Uralic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d96097a91ac8671d7e9310207fc58b908e2f7d1364221e8429b3bbde2384974", "prompt_hash": "18d443fbfa09feda09e3baec18a95f962be065fd84d896ba8289f96a8be20cc3", "target_hash": "61405e275c4b4acf6cf49dba6cbe74cef580fb985cccd40ea3d4a5f676eec310", "exact_match": 0.0}
{"doc_id": 1321, "doc": {"question": "who sponsored the phoenix open before waste management", "answer": ["Friedman Billings Ramsey"]}, "target": "['Friedman Billings Ramsey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sponsored the phoenix open before waste management?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mercedes-Benz"]], "filtered_resps": ["Mercedes-Benz"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb36949c5a6c9f72ce8001b7ff8964d4eedb0b7e546a591d390c96d6d5dac5b2", "prompt_hash": "21a35a92cd511424fd0cf8912ba39305eac40d15ef308da036e82fa202876320", "target_hash": "9bb87109d43b4102308fd34e476a454f9579e3a3a2b25fc6caf32582ff4b8df3", "exact_match": 0.0}
{"doc_id": 1322, "doc": {"question": "when was i can only imagine the song released", "answer": ["1999", "2001"]}, "target": "['1999', '2001']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was i can only imagine the song released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1999"]], "filtered_resps": ["1999"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f6cf81ee45a616d9ceb516332d47b7a799f8d2c0c123341fa288b9a9aacf2b0", "prompt_hash": "c812336cae73a126d37ad226b9dbd784fd481e8cffd8a02fb5515a7fce169bfc", "target_hash": "b80cf2ca83123640ecb49c62f4a610e9243622781dd438f9067d44511a875182", "exact_match": 1.0}
{"doc_id": 1323, "doc": {"question": "how much money did the film titanic make", "answer": ["$2.18 billion", "$2.187 billion"]}, "target": "['$2.18 billion', '$2.187 billion']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much money did the film titanic make?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $219"]], "filtered_resps": ["$219"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e5597d57caab477828e05868282fef6af1c8b25a7302195996853b3bcb765595", "prompt_hash": "473647709629b464547526a861e6285cbf132c03846e2320207cc3e8c8790b4c", "target_hash": "2206f2a7649092a8162014ee6cc99c94c5b5fdb5384ec1d2d4925631de4b6467", "exact_match": 0.0}
{"doc_id": 1324, "doc": {"question": "who came up with the theory of relativity", "answer": ["Albert Einstein"]}, "target": "['Albert Einstein']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who came up with the theory of relativity?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Albert Einstein"]], "filtered_resps": ["Albert Einstein"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "429e0336a564ac699a565bab5a0ac35f9f1c5cc8c099b35fa5cfa96d560d4e91", "prompt_hash": "b83bb5899dfcddbc7faef67d1e1753a246d91f4ee6e958a21de4d702b1d247ae", "target_hash": "4be75df4951c14d83b42e6553b3cbe3d737c3b2e0be5c73c502fd158ca4cbf24", "exact_match": 1.0}
{"doc_id": 1325, "doc": {"question": "where does prime rib come from on a cow", "answer": ["the primal rib"]}, "target": "['the primal rib']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does prime rib come from on a cow?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" rib A3"]], "filtered_resps": ["rib A3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "085b43d67df63593d4587842265aa617ae9fa238eaebd64662ada63427592f52", "prompt_hash": "69d607f6c106bdc7fc7c779d399518e1fa52cfea2b2cef2a15959a857755760f", "target_hash": "dda2fddb7b9a77e3105c02548160ae5d69b33d7291906bcbbb46c203f9399c5c", "exact_match": 0.0}
{"doc_id": 1326, "doc": {"question": "where is the meridian that is opposite the prime meridian located", "answer": ["the 180th meridian", "antimeridian"]}, "target": "['the 180th meridian', 'antimeridian']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the meridian that is opposite the prime meridian located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 180th meridian"]], "filtered_resps": ["180th meridian"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f92f76d880139f8a6fe5c8fabd73fbdb04ee75f3720d23177550d25ad7b535b0", "prompt_hash": "79f3946a31ae79afad1cff2fc7d3c250cc2948d7c0f7ef082397ee9de3c69ac4", "target_hash": "5fc26d029f4f1fc46503d8763326598679fd06e43480cb784368d453ece772d2", "exact_match": 1.0}
{"doc_id": 1327, "doc": {"question": "what does it mean to fuse the spine", "answer": ["joins two or more vertebrae"]}, "target": "['joins two or more vertebrae']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does it mean to fuse the spine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" kyphoplasty"]], "filtered_resps": ["kyphoplasty"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "de974289f772ea07bb70fe5984edc1e317e41d18048ace26cd791a56ca9d3cbc", "prompt_hash": "639096cde6bd8e6566b2d65b253d0535510f8b38e1375aa0adc07f445563bef3", "target_hash": "eb6a67829566a6c82e5e23a48fcb813b85dcaced1f691aaa20d29187e97cfec6", "exact_match": 0.0}
{"doc_id": 1328, "doc": {"question": "who took control of jamestown and made new rules that made everyone work", "answer": ["John Smith"]}, "target": "['John Smith']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who took control of jamestown and made new rules that made everyone work?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sir Thomas Dale"]], "filtered_resps": ["Sir Thomas Dale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad0f4ba09f600d7353d59569edd394c02ae2172585414d75a59518be3b34ad73", "prompt_hash": "cacd35dc78340c7dfd0d6f47a09c8ff4c31c1815477038348256100995d1bef9", "target_hash": "65d0b1255e503500a09e6596e29b56ee22fb706e59f6af95d2d4b5e56b1063a4", "exact_match": 0.0}
{"doc_id": 1329, "doc": {"question": "who presides over the highest criminal court in a district", "answer": ["District Judge"]}, "target": "['District Judge']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who presides over the highest criminal court in a district?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chief Judicial Magistrate"]], "filtered_resps": ["Chief Judicial Magistrate"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "95118d971dc44e042489c3bb87e1123219e73c505960288913adcdb72e21e8e1", "prompt_hash": "6d7b4871901efc09991581083d15d2d761838b7681c736f3d83ad1eb03606138", "target_hash": "ca109936977a0f8c994f7dd116d8e46435f939354cafc00452fd7070969c6f28", "exact_match": 0.0}
{"doc_id": 1330, "doc": {"question": "who played stonewall jackson in gods and generals", "answer": ["Stephen Lang"]}, "target": "['Stephen Lang']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played stonewall jackson in gods and generals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stephen Lang"]], "filtered_resps": ["Stephen Lang"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2526a58737dac4237d181913cb312d528f0205cb63837d3f47b258f14de436d6", "prompt_hash": "c231ab2ea854a9fafa3e6048ddd1b106af48559e9ec327600d30d049629bdd81", "target_hash": "838b5cd4841ca80a98e736827026c3fd49d2c34a0567cf20191b1f252ff5e3cf", "exact_match": 1.0}
{"doc_id": 1331, "doc": {"question": "how did early humans make use of stones during the prehistoric period", "answer": ["hunt", "start fires", "bury their dead"]}, "target": "['hunt', 'start fires', 'bury their dead']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how did early humans make use of stones during the prehistoric period?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" tools"]], "filtered_resps": ["tools"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "37b4913316fc42c97cbccc6389278dab58e8b1bf5a0bb9de9792e6b0f7c63972", "prompt_hash": "5f22149e3f595908f6bbf31c67cecca9aba4ecb51118434bc2d502533b48e1d9", "target_hash": "da7fe2ff1b76547fa0ea88ade79a3da159de551e5fff42abc30c62bedd6528d5", "exact_match": 0.0}
{"doc_id": 1332, "doc": {"question": "who won britain's next top model 2016", "answer": ["Chloe Keenan"]}, "target": "['Chloe Keenan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won britain's next top model 2016?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lauren Silverman"]], "filtered_resps": ["Lauren Silverman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "be5950af9b4c0e1b77faefd8d1acedb67ade6f4a01d347c2cdc161d94acffd2c", "prompt_hash": "bf7a46d49d2d65232930d2e4a1176a3bf1472badcc647f99f496b5fa9f513236", "target_hash": "ce154153a903f3132e4f39b2082b441ea106ba43dc15648dfe21091b5b052729", "exact_match": 0.0}
{"doc_id": 1333, "doc": {"question": "which state was returned to spain after the revolutionary war", "answer": ["Florida", "Menorca"]}, "target": "['Florida', 'Menorca']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which state was returned to spain after the revolutionary war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Florida"]], "filtered_resps": ["Florida"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "07e65a55ab61f3d7d3e12dcdec6077ffedca442865d86d86ce91f42978926363", "prompt_hash": "c93ba4c142f6e3b3455d51fdec0cff174db902307194954e31045b5b30c71734", "target_hash": "050aca2a5e17827fd5036465ae22a386a0f5d34aca1f55c5a0e7d3de324a6baf", "exact_match": 1.0}
{"doc_id": 1334, "doc": {"question": "where do they film take me out fernando's", "answer": ["Club Bijou on Chapel Street"]}, "target": "['Club Bijou on Chapel Street']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do they film take me out fernando's?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hollywood"]], "filtered_resps": ["Hollywood"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "61d02cdbc7407ae3c37f2652a75edcfe6b462eac01e8a665a1c48f2fa2bbcd44", "prompt_hash": "821964fc9226fbdab4b8b7b97e7b1e9d287cf154ccf5b3dffa30b237d5ee09ed", "target_hash": "277d91f7d8c5fc8995dd0cdf318b19e521b58ee8324179e28dcd9721e7a33eb4", "exact_match": 0.0}
{"doc_id": 1335, "doc": {"question": "who played booster in jingle all the way", "answer": ["Curtis Armstrong"]}, "target": "['Curtis Armstrong']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played booster in jingle all the way?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Candy"]], "filtered_resps": ["John Candy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2ecb4a0e41dc20f7a4c5cdfe8c5eb6b939374d91829c7a60123044260e68e68b", "prompt_hash": "df1e0883b4fac354b47c59123f264a25fe53ebeb18575e9c1c83f519f635605c", "target_hash": "ca030afd09cd7e05dec502de5c21f7dad1f925a6614929a925a5e25f04735dd9", "exact_match": 0.0}
{"doc_id": 1336, "doc": {"question": "dendrites and cell bodies are components of what type of matter found in the brain", "answer": ["gray", "Grey matter"]}, "target": "['gray', 'Grey matter']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: dendrites and cell bodies are components of what type of matter found in the brain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" neurons"]], "filtered_resps": ["neurons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c40e442c8f963dc0111896a73b882afd56abfb2da7aa43bd8c1a952e5fba1a03", "prompt_hash": "48e71b6e6a864229df07b5849caaf00b6d4a02b8234d36a1a39ee72956ac4e4b", "target_hash": "1e2d8beba8f0ca0232bfeea6d566de37cb0326303bfb4d3a98078fdb4528533e", "exact_match": 0.0}
{"doc_id": 1337, "doc": {"question": "what was the united states foreign policy after ww1 started", "answer": ["neutrality"]}, "target": "['neutrality']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the united states foreign policy after ww1 started?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" neutrality"]], "filtered_resps": ["neutrality"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b47cec31c273bdbd1a7db49b681d8346f910c5e64fc59c4a7b13603152b3ef78", "prompt_hash": "1ff23b1c65453bbb35290eaa16c3e012db0117d0a43ffb8e6d63f1c242116067", "target_hash": "e2e6777f0ac32d935fde249df331179a9660cc7303c8093de61a37db5058c2b7", "exact_match": 1.0}
{"doc_id": 1338, "doc": {"question": "which nfl coach has the most superbowl rings", "answer": ["Bill Belichick"]}, "target": "['Bill Belichick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which nfl coach has the most superbowl rings?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tom Brady"]], "filtered_resps": ["Tom Brady"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b1cd54ab5fa12b231be54e8521718d3a6926ab68009b4e87b54ab8b708b85de5", "prompt_hash": "98162a9a30cb1cdb33e50b9623b4985ab70ea1f37043e8a77495aac63eba9e88", "target_hash": "1dbfd7bf1dcb26eac75e7ca6eb608b35dee12ff7ceb5fdbb5518ab0752f2b060", "exact_match": 0.0}
{"doc_id": 1339, "doc": {"question": "when did radio shack change to the source", "answer": ["April 26, 2005", "2005"]}, "target": "['April 26, 2005', '2005']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did radio shack change to the source?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2000"]], "filtered_resps": ["2000"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "695fa1978af4c023e04db28dbdf5c2929b4f6a0f80938aef78e508347a38cce0", "prompt_hash": "a5d436c7e4d98686741c2a0b875a1d9ce0cd14b096d4c10da65611aca05c5099", "target_hash": "dcbd415f285cfd0eb7299cf90ba670a74e6acaf227606252cb79a2047d5a19cb", "exact_match": 0.0}
{"doc_id": 1340, "doc": {"question": "who wrote the theme song for the winter olympics", "answer": ["Leo Arnaud"]}, "target": "['Leo Arnaud']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the theme song for the winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Williams"]], "filtered_resps": ["John Williams"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d762fd84a5f1c6aaa4149ce1822daad8312965186701611fc5d04be28ef858c", "prompt_hash": "97ad504e4777f8e82878018c95f4fcccffbf8b47faaf7071b086ed691e65f55d", "target_hash": "f61c6832a51d807e77ca2292679945eafc8d83cfece1c16e472c5c5cddd5697d", "exact_match": 0.0}
{"doc_id": 1341, "doc": {"question": "when did little polveir win the grand national", "answer": ["1989"]}, "target": "['1989']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did little polveir win the grand national?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2011"]], "filtered_resps": ["2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b67cb69a8c9db65010b78cdbf858ea74d1f779c30093094881cb19169fb89ce0", "prompt_hash": "d119150a5a1bd9221225cbd0db7749b4f163090168c4267cb2847e29961bbe5f", "target_hash": "f29cbf85fafcf80ace6273501b873d82d249a9b85142c12822207ef416c8728f", "exact_match": 0.0}
{"doc_id": 1342, "doc": {"question": "most assists in an nba all star game", "answer": ["60", "22"]}, "target": "['60', '22']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: most assists in an nba all star game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 14"]], "filtered_resps": ["14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "227165244dcb73af50c9c76992e458f273afd1ffbcfe885585bade1e92dcf272", "prompt_hash": "8a8fc5a4eb591cb7778cfa1762f87f73936e360cbd9dc27c540d6c36be6f8fcb", "target_hash": "ded942cfdce55e49f9dc1f5645e0a01d4bb710e3f3b18a1bca61f8892789f382", "exact_match": 0.0}
{"doc_id": 1343, "doc": {"question": "when is the next episode of flash airing", "answer": ["May 8, 2018"]}, "target": "['May\\xa08,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the next episode of flash airing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 10"]], "filtered_resps": ["October 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e6fb7b4552d7b412db5ae5097df0c958039feff3cd7d460be9893a706703dccb", "prompt_hash": "e0a49b6a3988b59a123ff675127ab3552941fe55189f61285a1b6cd1ebaebc23", "target_hash": "e49792e63770fc23a2b7475b401e34a964977da9efad0ae57377dacebcaf585b", "exact_match": 0.0}
{"doc_id": 1344, "doc": {"question": "when did the democratic party change its name", "answer": ["the 1830s"]}, "target": "['the 1830s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the democratic party change its name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1844"]], "filtered_resps": ["1844"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "43424710c92971c1180738134044c73e7cf7c6133b1b6a26513625999af712e4", "prompt_hash": "6f9f7e9229f3aee85f287b44e24f4efa811ff22a1c61df697abe32ee390e6ea7", "target_hash": "8a166230da3f8655fe93a79ef9dc72ca5b91def2a2276d6fc21573c2a148b106", "exact_match": 0.0}
{"doc_id": 1345, "doc": {"question": "who did michael showers play on vampire diaries", "answer": ["The Man"]}, "target": "['The Man']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did michael showers play on vampire diaries?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stefan Salvatore"]], "filtered_resps": ["Stefan Salvatore"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a7b9c7741df47f885ea6d9c48ed2908cbbfab283815803be52dc8685f231fc33", "prompt_hash": "7c685e4696040dacd0a16e12304f36ac4eab33cbfa662c4e741aaa073a03630b", "target_hash": "008c6dd41497291f70865abcad486d807e7e1e0a5485665b21d5ba326b066b8e", "exact_match": 0.0}
{"doc_id": 1346, "doc": {"question": "where does the largest amount of federal tax revenue come from", "answer": ["California"]}, "target": "['California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the largest amount of federal tax revenue come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" income taxes"]], "filtered_resps": ["income taxes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ec3f4533d64798821c0b519253204bfd75de3dd0fc3a3e0a4b5ba24d2ff8f16", "prompt_hash": "49c743b467f355cc824f9b7188cf606a433cbf07a3c9a0a5e6a1f2179c457cee", "target_hash": "06cf4562a4f6489654c6ea5952a99bbceb81c4e34e8de327d8bb4734b81035c5", "exact_match": 0.0}
{"doc_id": 1347, "doc": {"question": "where does wild caught sockeye salmon come from", "answer": ["the Northern Pacific Ocean"]}, "target": "['the Northern Pacific Ocean']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does wild caught sockeye salmon come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alaska"]], "filtered_resps": ["Alaska"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ecc159df3a38b49c7b167d090c3566dd638e130698fb6b597d2112ee63c2546c", "prompt_hash": "4495d52e8c5596e763579a80acf78d5f70cdf1e263b75fe42e4274a4dcf5a583", "target_hash": "45de389fa8119eb86d4a8574a3b12515bd6094ccd7390bd4c58c31c9970ba643", "exact_match": 0.0}
{"doc_id": 1348, "doc": {"question": "who won the mvp for the national league", "answer": ["Stanton, Giancarlo", "Giancarlo Stanton"]}, "target": "['Stanton, Giancarlo', 'Giancarlo Stanton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the mvp for the national league?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christian Yelich"]], "filtered_resps": ["Christian Yelich"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "68b076fb65abbe2f651d271c8ced02645bea9a9e95fe743262c5db0678139056", "prompt_hash": "d69303c5d6e8b0fe0f9e288baf3749e424699e123e5b16e0216c13ea53473d60", "target_hash": "837f175e684426763c896634649b8c112ba1939c0f77160ccb98d572bad63506", "exact_match": 0.0}
{"doc_id": 1349, "doc": {"question": "what is the latest edition of the intelligent investor", "answer": ["Fourth Revised Edition", "2003"]}, "target": "['Fourth Revised Edition', '2003']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the latest edition of the intelligent investor?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2004"]], "filtered_resps": ["2004"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "84596cca23604d6661af324a2623a4159692ad3df96888695f687bc3a0e268ab", "prompt_hash": "67885e4bc9660b9821233aa9af5599b77de31dc7078607668431529421094e2c", "target_hash": "cd58316cf67957548050fe687f44cbd6cbf142066ef634905ab5b286574ed57f", "exact_match": 0.0}
{"doc_id": 1350, "doc": {"question": "who is the girl that played darlene in roseanne", "answer": ["Sara Gilbert"]}, "target": "['Sara Gilbert']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the girl that played darlene in roseanne?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Julia Louis-Dreyfus"]], "filtered_resps": ["Julia Louis-Dreyfus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b4130844058823ddf48411b8cafd21b95d749a7fc5d4d939e26c6c9c5bf0824", "prompt_hash": "281cc5d61e74a5bbc0a2f9ac6eec4e3a4a4260baae7f8e04e9dba7d9e2ff101b", "target_hash": "c7c79abb8e43207a511d97cb1a1dad7944f6ed53c55de9ecf79288d54f83a771", "exact_match": 0.0}
{"doc_id": 1351, "doc": {"question": "who does tyler end up with in you get me", "answer": ["Ali"]}, "target": "['Ali']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does tyler end up with in you get me?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sasha"]], "filtered_resps": ["Sasha"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b61414ea9799cbc8a324bb1485a7de19845dab6c44151ee3e5de7f6a217f1b57", "prompt_hash": "f222d0035ff37ca6deb5eb6baa3dde498e9d6aa6e14af59d60635e5d6af3ef79", "target_hash": "59037b4805fc16d02aeaff3e4c212c3ab509cdc14d794a49e6ead67daa0a3ad0", "exact_match": 0.0}
{"doc_id": 1352, "doc": {"question": "big bang theory season 11 how many episodes", "answer": ["14", "12"]}, "target": "['14', '12']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: big bang theory season 11 how many episodes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 13"]], "filtered_resps": ["13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "873e624437ba0f5a79b63fd890745beac19e32f44d3c984b73b8511fb2bf3d0d", "prompt_hash": "86849ff1a8a17f8d3109b19c61d01b2744b4420c4537992092dd4d109aa9287d", "target_hash": "6120216fcf1f89535a87a4c6379b9b06b4dc575eaa6e766e2f9027c201ade74e", "exact_match": 0.0}
{"doc_id": 1353, "doc": {"question": "when was i look at the world poem written", "answer": ["30–31 October 2000"]}, "target": "['30–31 October 2000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was i look at the world poem written?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1917"]], "filtered_resps": ["1917"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f664a58b6debb6db392af7e53cd23e713c25426173802f1aa62b6da93ba8cb05", "prompt_hash": "ed78694ac8ea552a4db56f010964d425cfa06d372545f867ab59302ad72613fd", "target_hash": "f1dc1b0d81c98ad97d7c1a3672de9aa4bebbccfcc5c601296ac021b5cb6a5947", "exact_match": 0.0}
{"doc_id": 1354, "doc": {"question": "what is the year round weather in dubai", "answer": ["tropical desert climate"]}, "target": "['tropical desert climate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the year round weather in dubai?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 25 °C"]], "filtered_resps": ["25 °C"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "193fadb1af121e0429b6e2e88facd4002507fa22483e970fdc8a85811f12d5e8", "prompt_hash": "18b2292e23dbdc44496cd6c386723e5894f5a0525a331c47973f639e038cd688", "target_hash": "2a6b51ba32624f380986651a44168d65e46ed23594dca3108627899851572900", "exact_match": 0.0}
{"doc_id": 1355, "doc": {"question": "who played g baby in the movie hardball", "answer": ["DeWayne Warren"]}, "target": "['DeWayne Warren']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played g baby in the movie hardball?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Megan Fox"]], "filtered_resps": ["Megan Fox"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ce7e0c85ac36ce8a7a0b95bda10348e4ae38c5fc6234c6d072ac37c120265c5d", "prompt_hash": "c0dcfedf399e8cf7c72acd199babf8fb6a8ab5a51904661ad11f749bda2e4593", "target_hash": "ac4919f72dfa5240e4c55fc3b5acff85a16b98267560ece234d244f17d87b036", "exact_match": 0.0}
{"doc_id": 1356, "doc": {"question": "when was where have all the flowers gone written", "answer": ["1955"]}, "target": "['1955']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was where have all the flowers gone written?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1962"]], "filtered_resps": ["1962"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3fec628201eaaf722cb660e32cb900dcd5670cf853b69ed95bcec4b414324392", "prompt_hash": "1adf334dbf79143405029f55778bfc6570d7fad4fb52973e9af494527bdfe678", "target_hash": "9ee1eda5a9ba920ab68d54fbc606ac85ae2616870d988971f77543ff0d64ccf8", "exact_match": 0.0}
{"doc_id": 1357, "doc": {"question": "when was the wheel introduced to north america", "answer": ["about 1500 BC"]}, "target": "['about 1500 BC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the wheel introduced to north america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1521"]], "filtered_resps": ["1521"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b5cdeceab5cfcaf38539caed22a44c1a2ac700c78382df6523b9dec4539d4709", "prompt_hash": "5d784a9a90a83727dda4b830303866867a6a73ae5c6803a6b1a5f45026613469", "target_hash": "c7fa1a516949091c17769e0016c40cd0f8dcc252943c8a9f581718fe4e8079e0", "exact_match": 0.0}
{"doc_id": 1358, "doc": {"question": "who was named african footballer of the year 2014", "answer": ["Yaya Touré"]}, "target": "['Yaya Touré']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was named african footballer of the year 2014?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ahmed Musa"]], "filtered_resps": ["Ahmed Musa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3346d08cf9445215d02a5b76ded2b63918c37737492fe0c7a0094917e0fdcf78", "prompt_hash": "f7e6f08eae3e659f70c3ea717f151ab947a356b51618d48ef9efd76f798f4c3b", "target_hash": "5f58c3f28e240a0a4edf13eebca9269f5688712df85c3fecca36984fe8fd13a1", "exact_match": 0.0}
{"doc_id": 1359, "doc": {"question": "where does the last name cintron come from", "answer": ["Portuguese", "Spanish-French"]}, "target": "['Portuguese', 'Spanish-French']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name cintron come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Puerto Rican"]], "filtered_resps": ["Puerto Rican"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4f4226fe95a96693958cea858a1691ee29a12db92da20c4375fc0ddd94829f0b", "prompt_hash": "3c5050cdde477c2b6db21e5b4ee6b8436992325bd0f9210daabebe668b16729e", "target_hash": "ccc2afeb16bdf8887320de7fec5de4f5660e50f202bc2f848079e03003266cf5", "exact_match": 0.0}
{"doc_id": 1360, "doc": {"question": "who formed and first came to the colony of maryland", "answer": ["the English"]}, "target": "['the English']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who formed and first came to the colony of maryland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cecil Calvert"]], "filtered_resps": ["Cecil Calvert"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "46ef531dbb0bbeec24313f1e83a47d9217daf7a0616d7a45d756ed72b8ba12ed", "prompt_hash": "0671dc320c52549afe7717b7d3478d47bf03eb0545fc7ed774935a9f81f80bf0", "target_hash": "b3d240c11cc128fc5246395e6efce6e6beaee90a083a00610a5fffb49558d9b5", "exact_match": 0.0}
{"doc_id": 1361, "doc": {"question": "what is the biggest town in west virginia", "answer": ["Charleston"]}, "target": "['Charleston']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the biggest town in west virginia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Charleston"]], "filtered_resps": ["Charleston"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e1cc30640117dc4fb2a2eb55baec966a26a9829e9fac907475999efcdcbf6cfb", "prompt_hash": "e6242bc756e3253c96a090a845a459f3615a4274d9c4162b8ecdea8153770fef", "target_hash": "b5741f38c53cc4dd3209dd96fbf561b90b4ca67c57eb9fba2f8ab6e628e80140", "exact_match": 1.0}
{"doc_id": 1362, "doc": {"question": "what are the 5 prohibitions of yom kippur", "answer": ["No marital relations", "No wearing of leather shoes", "No eating and drinking", "No bathing or washing"]}, "target": "['No marital relations', 'No wearing of leather shoes', 'No eating and drinking', 'No bathing or washing']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the 5 prohibitions of yom kippur?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fasting"]], "filtered_resps": ["fasting"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c8fb87888c0dac5cd937fa44322636f5f92454e6d781af955f357a90b27d585c", "prompt_hash": "333a64775149f6f7ad24a2af1c0a3775dd70231436ad10826792694281f6b8a4", "target_hash": "6a0ae80937e7d96e74b7a96f9595f9e9f718615e82ebac471dc33bfc68504f8a", "exact_match": 0.0}
{"doc_id": 1363, "doc": {"question": "who died in the first episode of stranger things", "answer": ["Benny"]}, "target": "['Benny']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who died in the first episode of stranger things?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Eleven"]], "filtered_resps": ["Eleven"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "23e13de8ce2dfdb2a2c015d013811d284995f934258a215f81a8f616590e74fc", "prompt_hash": "134c2aed2e1958a9fb0cb7eda6d1528db66b012b272d1214277a9a325e744b13", "target_hash": "c64d701bab3b116e7b708a405318be3d9aa716a03fae1a91eaefff0a3f975961", "exact_match": 0.0}
{"doc_id": 1364, "doc": {"question": "who plays zoey in i love you man", "answer": ["Rashida Jones"]}, "target": "['Rashida Jones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays zoey in i love you man?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Anna Kendrick"]], "filtered_resps": ["Anna Kendrick"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "94e856a2b90d189c4046907f2cea6fa6a11559ebbb442da62f028940217785f8", "prompt_hash": "3d5787df16ac1d265a7657dd01774506bfba476c37f9db308a3ead5eb9a08bc9", "target_hash": "8fe4694852f4a805b50867a6be1b458ffb1af170e0a4160316cf59c9e783c226", "exact_match": 0.0}
{"doc_id": 1365, "doc": {"question": "which is the default file extension for an audio file in windows media player", "answer": ["Windows Playlist (WPL)"]}, "target": "['Windows Playlist (WPL)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the default file extension for an audio file in windows media player?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" "]], "filtered_resps": [""], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2404851f01b94f47131bdc14206280344f17a5141dfaac4c7963b1bcc83f36ea", "prompt_hash": "fda317021093e7bc8ad7967e73e6f053ccb33c667bd16da6c696b561779f550a", "target_hash": "3320e95784789a712e653e58953f059716d7886fb9f777931ace3fc9706fa8a9", "exact_match": 0.0}
{"doc_id": 1366, "doc": {"question": "who owns the dower house in the archers", "answer": ["Lilian Bellamy", "Caroline Sterling"]}, "target": "['Lilian Bellamy', 'Caroline Sterling']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns the dower house in the archers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nigel Jeffries"]], "filtered_resps": ["Nigel Jeffries"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d185e120d900a5ea67d67d77435123a7af2c82091e38f687ea2ed7c76b085993", "prompt_hash": "cb07a0fb6195f28846f6f3be43271869f6ddfd089179bea44ae95dec993ceafa", "target_hash": "ff22ec8afc50bbc0e967cb28cf914a80ed790f19267dea75310d674e6849169e", "exact_match": 0.0}
{"doc_id": 1367, "doc": {"question": "to aru kagaku no railgun s episode 3", "answer": ["Project Radio Noise"]}, "target": "['Project Radio Noise']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: to aru kagaku no railgun s episode 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to aru kagaku no railgun s episode 3"]], "filtered_resps": ["to aru kagaku no railgun s episode 3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "85e3c9582b180260c7d68a84c947bce8f048c9ab54bc182db8555fa4df5afc4a", "prompt_hash": "7f290bfcbe146d4a2f7a99580c36b8b5ffdc834543f5d0c5b023cbda0de7c9ae", "target_hash": "fabe91500b8c3fd6ed7219e58407837d5d4ab36f87071c38f7b82c1d9133008c", "exact_match": 0.0}
{"doc_id": 1368, "doc": {"question": "1 the bureau of land management manages over acres of land in the united states", "answer": ["247.3 million"]}, "target": "['247.3\\xa0million']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: 1 the bureau of land management manages over acres of land in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 245 million"]], "filtered_resps": ["245 million"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0707b51e65750ef0390dc5f62d6be0fa26bcd47cbeffc436392b066efb8a01e0", "prompt_hash": "8f8771facf1b46f44e2c4c3d67ee3ebe5ae48eddd98f39e904f48d1b29b1a736", "target_hash": "75f81d99accc51f1029c40cb052c69ce22f99bb81c9e7d10f6de095a147fe57f", "exact_match": 0.0}
{"doc_id": 1369, "doc": {"question": "what cities are in san joaquin county california", "answer": ["Lodi", "Stockton", "Lathrop", "Manteca", "Ripon", "Tracy", "Escalon"]}, "target": "['Lodi', 'Stockton', 'Lathrop', 'Manteca', 'Ripon', 'Tracy', 'Escalon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what cities are in san joaquin county california?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stockton"]], "filtered_resps": ["Stockton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "78e1bddc366c1b77d147244c8fc40b3865582c5f538e357675df030e773ee0a4", "prompt_hash": "9ddf8c124a5e025b9742833750a5bb852568bff8520009cc499420ba76cc4c07", "target_hash": "454bba1030c39d85684a352811866433937d494a6b17618818bd563464d5f881", "exact_match": 1.0}
{"doc_id": 1370, "doc": {"question": "when does the mlb season start and finish", "answer": ["March 29", "October 31, 2018"]}, "target": "['March 29', 'October 31, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the mlb season start and finish?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" April"]], "filtered_resps": ["April"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c3f46ad9936319bc1f13373318efd17127402f53a408706f7addeaff7b6ade1", "prompt_hash": "199007e1d75e83fef781e464e34463d214b89ec420b15ac57f95d0257eba4900", "target_hash": "9ba687955dbc2dc3ee7151c93928308e5133c7f4c347f1973c4afabe5e3af594", "exact_match": 0.0}
{"doc_id": 1371, "doc": {"question": "who has won the 2018 formula one australia grand prix tournament", "answer": ["Ferrari driver Sebastian Vettel", "Sebastian Vettel"]}, "target": "['Ferrari driver Sebastian Vettel', 'Sebastian Vettel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the 2018 formula one australia grand prix tournament?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sebastian Vettel"]], "filtered_resps": ["Sebastian Vettel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ad40eebdf6600f16fa18056d0281444d2799dd76ff460feb3511001c3e20625", "prompt_hash": "4f1dc492a145307aedd813d63fbfdabb5007f282c2eda2ccccde7ebf84886e2a", "target_hash": "79d161ce466a15ee65df7db461c924f1aae90913518287a02e69ec623e8a0370", "exact_match": 1.0}
{"doc_id": 1372, "doc": {"question": "who said everyone will be famous for 15 minutes", "answer": ["Andy Warhol"]}, "target": "['Andy Warhol']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said everyone will be famous for 15 minutes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Andy Warhol"]], "filtered_resps": ["Andy Warhol"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8bf423af3d57c376c35c25ec415702a0b1103163f4be2d81b36593271189963", "prompt_hash": "9066d7d6a0a9989c5411970e5254f0cae98b097d3678f64c94d0251859d90b37", "target_hash": "2a9db2f90629a31b4e5490349277ccacad2886f890921f062a6d6708a91cc42c", "exact_match": 1.0}
{"doc_id": 1373, "doc": {"question": "when does the new adventure time come out", "answer": ["April 21, 2017"]}, "target": "['April 21, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new adventure time come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 15"]], "filtered_resps": ["January 15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9af8063a200085ff46c52e7fee614ea032059364a9e8fa2352edd4f9782e5a3", "prompt_hash": "5cb30510daf49cd084c9ac847da9b9a0ea7c3b6512ef8da40a8b429ece0f467b", "target_hash": "9ba0c21fff8f8236c14c5774944cb80934d8276c3d51ae133152c1c516fd4ed8", "exact_match": 0.0}
{"doc_id": 1374, "doc": {"question": "dynamin is associated with which type of vesicle", "answer": ["newly formed vesicles"]}, "target": "['newly formed vesicles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: dynamin is associated with which type of vesicle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" endocytic"]], "filtered_resps": ["endocytic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18a6d3197301cf3d6149194dd82bf7e6eb3735ca63bcc922b402d392d75f769b", "prompt_hash": "d076f43ebf9bcda50f8cb998dcb1bb1f5b5686e1df7f180d7a45421e26689d04", "target_hash": "bccdc152da1b09fb12b6b2656ce132f6ec506aa724cdfa8ec29baf60d9b92b09", "exact_match": 0.0}
{"doc_id": 1375, "doc": {"question": "when did the revolution begin in the united states", "answer": ["1775", "1765"]}, "target": "['1775', '1765']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the revolution begin in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1775"]], "filtered_resps": ["1775"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "719e88571c3dc52f06836be80e8f9eb70bdb2a6f3a4aca6d0b5547465be40533", "prompt_hash": "4bcce9f401980b2ce45440baede410ae50e32bf14bb74c0a7ae519b72ebaca1d", "target_hash": "b64715076f227cb7de68dc048536037da70e3121ac783b71dba5fdad6e17c0bf", "exact_match": 1.0}
{"doc_id": 1376, "doc": {"question": "who played tom on as the world turns", "answer": ["Jason Kincaid (1984)", "Jason Kincaid", "C. David Colson", "Peter Galman", "Scott Holmes", "Gregg Marx", "Justin Deas", "others", "Tom Tammi"]}, "target": "['Jason Kincaid (1984)', 'Jason Kincaid', 'C. David Colson', 'Peter Galman', 'Scott Holmes', 'Gregg Marx', 'Justin Deas', 'others', 'Tom Tammi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played tom on as the world turns?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Ruffin"]], "filtered_resps": ["David Ruffin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "af8f3e89e3c0164d320bb6a18b21f092b1ab4d923dbda5648e72dbe2309b2c78", "prompt_hash": "d496f48cfda6e1902e497648784c99471b4aa94905de21c92c9b0b0669d2bdc8", "target_hash": "68d44b3c57733f2998bed859eef0fad1932e04f52d339580e001266a44ee2c68", "exact_match": 0.0}
{"doc_id": 1377, "doc": {"question": "who won the ncaa women's championship last year 2017", "answer": ["South Carolina Gamecocks", "South Carolina"]}, "target": "['South Carolina Gamecocks', 'South Carolina']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the ncaa women's championship last year 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Duke"]], "filtered_resps": ["Duke"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d0856c33e2f5b699d05697533c28437d57b086dc9fbf8f643c8de2a0f14ae79", "prompt_hash": "bde7687430d2c1e50558da65914a702148d40ff9700f0306b2de1b5144116e6d", "target_hash": "f8d2dde629f72fa732115c5a843d2a2519b2d4b7decf5e6a958af1532d1ad4b8", "exact_match": 0.0}
{"doc_id": 1378, "doc": {"question": "who made the nuclear model of an atom", "answer": ["Ernest Rutherford"]}, "target": "['Ernest Rutherford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made the nuclear model of an atom?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Niels Bohr"]], "filtered_resps": ["Niels Bohr"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b2345a2e729598e60f4681b58c9b236d5b284464bf26f9ece82e2f581734ea0", "prompt_hash": "4eff8936ac8cc00abdca8904472fc02c7b94c1d480a2c28004583e3c33cb7f77", "target_hash": "6e216ee2968bde52cc73dfc1bfc7840c6261ac5b799a5ef5282d2d39c0cf25ad", "exact_match": 0.0}
{"doc_id": 1379, "doc": {"question": "when does the little couples new season start", "answer": ["September 19, 2017"]}, "target": "['September 19, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the little couples new season start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f07d4eddccf20ba9136c02038dbe9ef521447be1cd38985d0d24d9fbccce486", "prompt_hash": "a479a296799bcca84152e1363d5c1e2271d0cf814ad151143f87677d175917bb", "target_hash": "66406d9f150dc16e8daebd5bab91d7527106613c89bea2a0df932de65d6973d5", "exact_match": 0.0}
{"doc_id": 1380, "doc": {"question": "what is the common name for gravitational force", "answer": ["Gravity", "Gravity, or gravitation"]}, "target": "['Gravity', 'Gravity, or gravitation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the common name for gravitational force?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" weight"]], "filtered_resps": ["weight"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92d6bd9c1a132339815e03eec1c17a2f2d7cf85d84aebbe8e8e46a09f939dd01", "prompt_hash": "39ee4166c44537aa7226c6516743b90a5b36f91424a0ba9ae08ee443532d4639", "target_hash": "03cf5e8c2cf56e40c95005ed3d576e9aeba60778ac92a451c56447f5894635ed", "exact_match": 0.0}
{"doc_id": 1381, "doc": {"question": "who played mrs warboys in one foot in the grave", "answer": ["Doreen Mantle"]}, "target": "['Doreen Mantle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played mrs warboys in one foot in the grave?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dawn French"]], "filtered_resps": ["Dawn French"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ec85b5d65cca9a34512b65d67d83bded266cecb624f1b34620816fe22291463", "prompt_hash": "9a735cb2ec4b61fa264923ded47d7e2d46692877fab02134bb027711ff1b6a01", "target_hash": "ee12b0f27766733df76893d468d7d2bba0e973262a215b639be931e020921293", "exact_match": 0.0}
{"doc_id": 1382, "doc": {"question": "who does tony end up with on skins", "answer": ["Michelle"]}, "target": "['Michelle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does tony end up with on skins?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nicola "]], "filtered_resps": ["Nicola "], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1231013ccdfd641825fa95a3a42e6024160d2f827b1ee4af038f39ea3116f47d", "prompt_hash": "fe0ef8c71b232cbb134f22a85e6de13f938a1eb742687c7559984e82f5fe2ac7", "target_hash": "54673345df160691966c747c22446150063821aa919d972e6877b5e0253337dd", "exact_match": 0.0}
{"doc_id": 1383, "doc": {"question": "what is the population of fayetteville north carolina", "answer": ["204,408 in 2013"]}, "target": "['204,408 in 2013']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the population of fayetteville north carolina?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 83"]], "filtered_resps": ["83"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5f9174466040566415ce1322ed2e5da009a909e4c907bde4aa7f70d051217bfc", "prompt_hash": "ad4a6c267609ffd6489c027fcc720836f439264fedd3c690d85e0bf65219c8be", "target_hash": "baf792297721f169bb243bec970ff26ad74a2f54c4d1a2642a2979054eee3b09", "exact_match": 0.0}
{"doc_id": 1384, "doc": {"question": "when was harry potter and the philosopher's stone made", "answer": ["14 November 2001", "1997"]}, "target": "['14 November 2001', '1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was harry potter and the philosopher's stone made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2001"]], "filtered_resps": ["2001"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "54a6df1727ed6f63c351403b0b209b11e3c14c1b936cfd3a87835e6666a2d8a3", "prompt_hash": "63283eef0d40e3ecf6a141e0abec55160f3a8b72f2ec3f294ee235b914b93e8d", "target_hash": "4512e0884ddd86062fb65dc07ed53a3b00f4579a63407ffbc5045e240d0bd204", "exact_match": 0.0}
{"doc_id": 1385, "doc": {"question": "who were the nationalist in the spanish civil war", "answer": ["the CEDA", "the Falange"]}, "target": "['the CEDA', 'the Falange']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the nationalist in the spanish civil war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Republicans"]], "filtered_resps": ["Republicans"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d0d17f5d33ad4601da75404ac76514dca2e6658386548e90062b25e914a480e", "prompt_hash": "4f0d0f023c7a0c125f618bf80ab053a78a626f4302758c300c6890e1c0e2e200", "target_hash": "780fb88dde1bdce461256c9edf8955ca5b7e8a3f5f8ab8b8d10261af7bbe8aa0", "exact_match": 0.0}
{"doc_id": 1386, "doc": {"question": "who owns the biggest house in beverly hills", "answer": ["Anthony Pritzker"]}, "target": "['Anthony Pritzker']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns the biggest house in beverly hills?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Geffen"]], "filtered_resps": ["David Geffen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d9e7680e87810dd248aa74c2f7923d8ed6df5a5d9c08788798fc8ac39f1380b", "prompt_hash": "b9f6d867f7a8ca7e323c524f32c960b0a96e9cfaa4cf2ab6edc3ab3840d51542", "target_hash": "fa1a5edc689db6df77de56273a795348732e99bcdd49c6969bec070cdbf4e3fa", "exact_match": 0.0}
{"doc_id": 1387, "doc": {"question": "when is the flash coming back after christmas", "answer": ["January 16, 2018"]}, "target": "['January\\xa016,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the flash coming back after christmas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 10"]], "filtered_resps": ["January 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "544480e97a88cfabaa499377543e8de9d4fcfe3f7deb63bfa2f02d44e08a168a", "prompt_hash": "0fab12258b8ed1f1464a2b151149ffd653edc405a3d5d3c30a1b0ad2939f49b2", "target_hash": "6a644b2d7afdb8951dffbb3276fa7504d1125a5bfe1442be7a01f5609a5ac8a4", "exact_match": 0.0}
{"doc_id": 1388, "doc": {"question": "where do the signals for apoptosis come from", "answer": ["from other cells"]}, "target": "['from other cells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the signals for apoptosis come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mitochondria"]], "filtered_resps": ["mitochondria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "69aa2f90ecb562649ebed3d7bb90e2f3a8e5cda6cf072d9031c792ba3aea08cb", "prompt_hash": "91ac56e48d2404abe5427c08a15ad1363a8d8d23301869573279989b87c58203", "target_hash": "2b85be9c3ba7e7c00134a0cbf0f349eaf1c7fb202ab3810928e085648cc46130", "exact_match": 0.0}
{"doc_id": 1389, "doc": {"question": "when did the battles of lexington and concord happen", "answer": ["April 19, 1775"]}, "target": "['April 19, 1775']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the battles of lexington and concord happen?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" April 19"]], "filtered_resps": ["April 19"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e410005f9b945c748521c8cb47acf1397de0a64481838457b5f1c35e13b2a81", "prompt_hash": "61f163e88222da045113ecbc263848d30cd5059bc25e2dc4dc4830e4024e8f03", "target_hash": "8c83325072ab4a53316c51e9dd87e5d2d5a48b93104c1a50094c30526219d732", "exact_match": 0.0}
{"doc_id": 1390, "doc": {"question": "who has the most number one single hits", "answer": ["The Beatles"]}, "target": "['The Beatles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most number one single hits?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Beatles"]], "filtered_resps": ["The Beatles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7cb3469856726db821ec88c50cc4dd5842a597af88aa62a17ba91af4306c9525", "prompt_hash": "d74f765b41c33758ed2ff3ef1397ceefaa3e5909901555ea8ecb0f3b9a2c7a24", "target_hash": "1656edfec581107e0ad591f64a6c0a50abf71367761f6330cd410765c366acaf", "exact_match": 1.0}
{"doc_id": 1391, "doc": {"question": "what do you call a person who officiates a wedding", "answer": ["vow master", "solemniser", "marriage officiant", "\"vow master\""]}, "target": "['vow master', 'solemniser', 'marriage officiant', '\"vow master\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what do you call a person who officiates a wedding?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" minister"]], "filtered_resps": ["minister"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d8f3e4412031545af94c8f1df56c6a32cc1611885e5158a528fd9929651bd880", "prompt_hash": "30e7e6a188638ccf6a9c9ebd4cceed7f8804cdb6e53aec89b05807a3b34bcd9f", "target_hash": "31196a00256ab9dff3d24a8b809dce7e3c966b4a8b5cc460ba2eba6c6c0a4b9e", "exact_match": 0.0}
{"doc_id": 1392, "doc": {"question": "where did the river thames start and end", "answer": ["Thames Estuary, North Sea"]}, "target": "['Thames Estuary, North Sea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the river thames start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oxford"]], "filtered_resps": ["Oxford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d060bf4bffb1f64e02983b9357e54f03e8f3cba6eec5625edadecfc7670f09b", "prompt_hash": "cd0f5a4493624dd4c45bd5ead83ab6a08291b07ed55bdc953846bf03d504ae78", "target_hash": "c38e82f2fd15ad007de56e090cc2a8e381ae4f8bf753a6e62103fedab101fabd", "exact_match": 0.0}
{"doc_id": 1393, "doc": {"question": "where is a simple gear train used in real life", "answer": ["Automobile drivetrains"]}, "target": "['Automobile drivetrains']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is a simple gear train used in real life?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" bicycle"]], "filtered_resps": ["bicycle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f199eaad45190e3a17f6f39d4678b7f04d47b19823d88c315df7035f9b157f10", "prompt_hash": "d5a954bbf2f1cc86cff22593eb677f5478f4ebd0b70e5169430f51a53d8c66b7", "target_hash": "eb62f924485e730ee22e6c2c1bf539d1878218928d4a480fd9d6ebd96fefc53b", "exact_match": 0.0}
{"doc_id": 1394, "doc": {"question": "when was how deep is your love released", "answer": ["1977", "September 1977"]}, "target": "['1977', 'September 1977']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was how deep is your love released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6dbd2e97f7bb73527b3a47f182fe9489b852d363fa16afcaca408d50973e4247", "prompt_hash": "3c2b1d458e0d214ed9df8f22a89a6273de2ebbb4c6d5fe39b6d19e870e9adcd2", "target_hash": "cc43e781cfb81fd1fb4bc22e233bb55fae180cf42df1329bcfe811004a77e1a2", "exact_match": 0.0}
{"doc_id": 1395, "doc": {"question": "which apostle spoke at the council of jerusalem", "answer": ["Paul", "James", "Barnabas", "Simon Peter"]}, "target": "['Paul', 'James', 'Barnabas', 'Simon Peter']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which apostle spoke at the council of jerusalem?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Peter"]], "filtered_resps": ["Peter"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dff723515a83df108b81fc96716c102996786712769256501eae5bf753e66dcb", "prompt_hash": "d55716f3c3b436091e2287da5f7e527cf58559146eaab3ebfe0d66840c7502cd", "target_hash": "1de70cf7ac7b45b2ff5a73db2398611010d331811d4d3467289f7fca1e8c591b", "exact_match": 0.0}
{"doc_id": 1396, "doc": {"question": "when was the last time the seattle seahawks won the superbowl", "answer": ["2013"]}, "target": "['2013']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the seattle seahawks won the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2005"]], "filtered_resps": ["2005"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a95754ffdda526a9ffb143e61d6f9725dd30e3b24f53c0f1cbb0d87155f4dae", "prompt_hash": "e56dd89277c77114dd99067e9bd4aaf1c8c8b5a95740d8c8c192b81d697bbb27", "target_hash": "00de2d7e2b1eca89d609387ec691632f88d9b1686ecb42268bbf0e5127cc375f", "exact_match": 0.0}
{"doc_id": 1397, "doc": {"question": "who said one man's vulgarity is another's lyric", "answer": ["Justice Harlan"]}, "target": "['Justice Harlan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said one man's vulgarity is another's lyric?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Faulkner"]], "filtered_resps": ["William Faulkner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "af4facdfadf1ff8a7a487e0adebb6fd7c6e4dc8725b232006203a9f5fae5c753", "prompt_hash": "b5faaef9c1b339c398d5be8eb529b9d097a468fe222e067f3c3afb3109a63ff4", "target_hash": "348df3bcadf3772f0555622b856eb19e4603addbb27ce3c903b143ceefcee5ea", "exact_match": 0.0}
{"doc_id": 1398, "doc": {"question": "when did men's curling start in the olympics", "answer": ["1924", "1998", "1998 Nagano Games"]}, "target": "['1924', '1998', '1998 Nagano Games']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did men's curling start in the olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f9e0451fdfb4156b8ce3e4dcd34ec59838b208a5304bc14b5cdc3079ce93885", "prompt_hash": "db1dded2cfd1ed41ef6b4e52e11eadb7018a50da64d84b14d9d3b6e43572a0c1", "target_hash": "8eeebe68f49928495bb0832268b169ca66610581b9b91381566731ff7d5bd8d8", "exact_match": 1.0}
{"doc_id": 1399, "doc": {"question": "who does claire from mcleods daughters end up with", "answer": ["Alex Ryan"]}, "target": "['Alex Ryan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does claire from mcleods daughters end up with?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jamie McLeod"]], "filtered_resps": ["Jamie McLeod"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91cced18edf54586aa7be11fd01ea861b06292ae8be06472cbec72eed32ca8c6", "prompt_hash": "845803e23704aaac117c275cfee774d5038060cd1ead3cc28e735d930df58e1f", "target_hash": "c8a9d07d0e9cc5bff4159b9d974a63fc87bf751045cff76b6bf5d7bc01fc686e", "exact_match": 0.0}
{"doc_id": 1400, "doc": {"question": "what's the dwarf's name in game of thrones", "answer": ["Tyrion Lannister"]}, "target": "['Tyrion Lannister']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the dwarf's name in game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pyp"]], "filtered_resps": ["Pyp"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "544efc37134741103cdabcadf37e8a8abcc5ec752732bffce47849d155312479", "prompt_hash": "72d4ff1f0d69efcd227354be3a81280dc255085309a24a37cabb7eadc06d689f", "target_hash": "9e753f123037d4e334cb45ba799935891e990eaf95f04492bade7af0eb93589f", "exact_match": 0.0}
{"doc_id": 1401, "doc": {"question": "who plays the queen in victoria and abdul", "answer": ["Judi Dench"]}, "target": "['Judi Dench']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the queen in victoria and abdul?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Helen Mirren"]], "filtered_resps": ["Helen Mirren"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aa0c050f493f4ccec6aa6bb708a527b7a83a52ce457b50b9d9b90f2788a65e7a", "prompt_hash": "2026eb4fa8e231d43393dc5eae50418e3063ec56fe07df6aa8804bfd8fe051d9", "target_hash": "6e4715d2a4d8d51a995323fadc4fe4a18861967dfd74787e01979beb7561b123", "exact_match": 0.0}
{"doc_id": 1402, "doc": {"question": "where does the grapes of wrath take place", "answer": ["Oklahoma", "California"]}, "target": "['Oklahoma', 'California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the grapes of wrath take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" California"]], "filtered_resps": ["California"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "822df28766c62b2a9b7bed5d777ac2e1ee24e041a8290a454fcfe5fc71d7c5eb", "prompt_hash": "72e82526c931ab6adcba1e555d69da10c1ee5811f196cf63f477828de94e9fea", "target_hash": "34c67e50bddfd2e73733bcf8ed270a788627c214c260b6fd9cc912fc1146a696", "exact_match": 1.0}
{"doc_id": 1403, "doc": {"question": "when is last time the astros won the world series", "answer": ["2017"]}, "target": "['2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is last time the astros won the world series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a17cfbcf3c84ed9e3d5c5981c494c09ac6df2485970a35f75f295c263cc6edd6", "prompt_hash": "8d269f1acbbfce83b61a21567e2d1dff5fe27e2d6af605f18959cf1a4e73adb1", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "exact_match": 1.0}
{"doc_id": 1404, "doc": {"question": "when did drop it like it's hot release", "answer": ["September 12, 2004"]}, "target": "['September 12, 2004']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did drop it like it's hot release?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1999"]], "filtered_resps": ["1999"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "04c8ddd590c8fd17ebfbd3baba6abc4939a24be18c19639c3b4e9757804d9e84", "prompt_hash": "498b27c66817eb25500cb0d47887a2a0eff7235e9ba36a693283b83994a6517e", "target_hash": "0730d7db281f556ba3c60610696d0740f039fb52c124347e0d5fd9da8493f191", "exact_match": 0.0}
{"doc_id": 1405, "doc": {"question": "when was south african youth day first celebrated", "answer": ["1976", "June 16"]}, "target": "['1976', 'June 16']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was south african youth day first celebrated?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1995"]], "filtered_resps": ["1995"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "45781141bbecfb73ca6daf5cbceadb753a4684e2538fa3a73342c9530c11dc55", "prompt_hash": "900bccb88ea6db5775970cbe6f887c855d940985b108dc009022331fc35c0601", "target_hash": "73580bbc5d9e18f51695d60cbb11cc576bce4414a29a896f025e88ceffb8755f", "exact_match": 0.0}
{"doc_id": 1406, "doc": {"question": "china in chinese is called zhongguo which means", "answer": ["Central States"]}, "target": "['Central States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: china in chinese is called zhongguo which means?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Central State"]], "filtered_resps": ["Central State"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5bb8b486a5ea2e40b9608c0e27b969c3995d39cbca2563210b2da7e9371348b1", "prompt_hash": "d2dba34212f05e8d8c50e5e31ef0189884586dc4e5138cd5a780629d4768e288", "target_hash": "a14a42ab3f5728a89e7eb6fcf0f8a51376e29239bf69b5902f37210363930736", "exact_match": 0.0}
{"doc_id": 1407, "doc": {"question": "how many counties does the state of georgia have", "answer": ["159", "159 counties"]}, "target": "['159', '159 counties']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many counties does the state of georgia have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 159"]], "filtered_resps": ["159"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5145500c93d4efca185e69c9b6753655c7b61122bb8f14270a063d682fd23437", "prompt_hash": "62780bef63666fe2b0f290a8b0aee5f85f8e46430fffe60725b21dd18a3c78bc", "target_hash": "417b9c2ce716dc534aa3418fa7a84d530f5b2462f9596e475d70dcaed4299a9b", "exact_match": 1.0}
{"doc_id": 1408, "doc": {"question": "what is mercy mercy me by marvin gaye about", "answer": ["sorrow regarding the environment", "the environment"]}, "target": "['sorrow regarding the environment', 'the environment']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is mercy mercy me by marvin gaye about?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" poverty"]], "filtered_resps": ["poverty"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbb2c25914356e9294b39a3cb40d12f15631304040d9dc58ef29bcdc54b9de40", "prompt_hash": "f4807a7df3c92529ff02310160bf8d7e27e412fd023fdb877cf66fba957cb07b", "target_hash": "39d39eec6ef5991ae26f014b870afbf08ff60a2b4df8d91f27badd62798db0b6", "exact_match": 0.0}
{"doc_id": 1409, "doc": {"question": "what is the name of the compound p4010", "answer": ["Phosphorus pentoxide"]}, "target": "['Phosphorus pentoxide']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the compound p4010?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" tetraphosphorus decoxide"]], "filtered_resps": ["tetraphosphorus decoxide"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d43166cdc0f344f9bf973c0f1b9ebfaf0633f12b9504c42afd5f1cccab3d72f0", "prompt_hash": "23fc43857916b4085046a3a943881848ae9e6ddbc53715e03db8cd645db53a0c", "target_hash": "8880e3b2787877380f8e5bb38c7497f0a965e1506ae45bb082c65b80c5d4b2f8", "exact_match": 0.0}
{"doc_id": 1410, "doc": {"question": "what does the msc in msc cruises stand for", "answer": ["Mediterranean Shipping Company S.A.", "Mediterranean Shipping Company"]}, "target": "['Mediterranean Shipping Company S.A.', 'Mediterranean Shipping Company']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does the msc in msc cruises stand for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Maritime and Science"]], "filtered_resps": ["Maritime and Science"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b14172598e9e6adf77bcf476f2ff45f4d8b32b3367352967582e273f60657ef5", "prompt_hash": "62495b5bbfb70e29280d3786f238b11b89e929f6c63bbaf73466045f5034ba36", "target_hash": "64988dcaaa28787f9685bb8c61f3d29b19a89f588cf56d6f62660946f49e8126", "exact_match": 0.0}
{"doc_id": 1411, "doc": {"question": "who played joey's agent estelle on friends", "answer": ["June Gable"]}, "target": "['June Gable']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played joey's agent estelle on friends?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Aniston"]], "filtered_resps": ["Jennifer Aniston"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4f61f95e94a25e4b9e20eb9a508340cd0a70fb0e17e5de7801269ec11d26e870", "prompt_hash": "5745383fb24c57d915cf3009e97ed00a573176af366f1160d8e28a6eaf14e9e2", "target_hash": "c2f562e34b79d9c25a2f06db3f2fb0772cbbd717a9c855b64e5e96b68a4faed5", "exact_match": 0.0}
{"doc_id": 1412, "doc": {"question": "who is the sixth president of the united states", "answer": ["John Quincy Adams"]}, "target": "['John Quincy Adams']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the sixth president of the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Madison"]], "filtered_resps": ["James Madison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fe0df693b9151b1c45c78af1bae88b1a27c5dfcb1cd136310cef0b2cabb9ce8a", "prompt_hash": "59ca18fbabb5abf0baf6dd8a7e92a27afa33640c546e94ac8574602f42941ce6", "target_hash": "b65798a4383fc49e2af6d3fe6f13feb3a724f88fde281089b54258e0d43b649e", "exact_match": 0.0}
{"doc_id": 1413, "doc": {"question": "who won the food network star in 2016", "answer": ["Tregaye Fraser"]}, "target": "['Tregaye Fraser']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the food network star in 2016?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Berberet"]], "filtered_resps": ["Jennifer Berberet"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e8cfa3c43091c23ee1923edf2ae3275aec6b4ad34838f00ca3436be09f7d6456", "prompt_hash": "44a9afd1807a44dc26bc19339c81c49a11a7bb162a62747d1300085c116895fd", "target_hash": "d19536f16d80b763a3182ba9422b4cc84c178d89ed44418d864eac39a0ebfbb8", "exact_match": 0.0}
{"doc_id": 1414, "doc": {"question": "what kind of bird is in the lion king", "answer": ["red-billed hornbill", "a red-billed hornbill"]}, "target": "['red-billed hornbill', 'a red-billed hornbill']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of bird is in the lion king?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" eagle"]], "filtered_resps": ["eagle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0ce888c2e28a21fe31d34f1f96c184a7f4aeb0a4be3a7100911b13cd132f2bfd", "prompt_hash": "bbac8cacfe3c77f0614da9800fd4935f5ce93ab31ef984005116a6b1b9b01a45", "target_hash": "a0665d2a6349b08483579e4642db1b094bd06cba25834329ec10c806e742ee1f", "exact_match": 0.0}
{"doc_id": 1415, "doc": {"question": "who were the judges on dancing on ice 2014", "answer": ["Jason Gardiner", "Ashley Roberts", "Robin Cousins", "Karen Barber"]}, "target": "['Jason Gardiner', 'Ashley Roberts', 'Robin Cousins', 'Karen Barber']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the judges on dancing on ice 2014?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bruno Tonioli"]], "filtered_resps": ["Bruno Tonioli"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "23725120799fd3ae06013662b18a2174266915b3bc8b40f48df9a2420c85ca3d", "prompt_hash": "4eb38b789c8f74d45b5b4b3cca5c6b45ef5fe9aa1171e705a18a94b1ce7ebc47", "target_hash": "df4ce13e52c27aaf6f88fd80adc3833de714f9567b598b9a0b8a0417f97adbeb", "exact_match": 0.0}
{"doc_id": 1416, "doc": {"question": "where did this case of product tampering occur", "answer": ["the Chicago metropolitan area"]}, "target": "['the Chicago metropolitan area']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did this case of product tampering occur?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New Hampshire"]], "filtered_resps": ["New Hampshire"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cbf8ff073f284cb09dfa6a588da8ef819eb71fed768c6b4dd4615e0db0957800", "prompt_hash": "954bcb32013461b254ce7a343b475e37c4eb138b28ae4ea0d9942c80ce9dde39", "target_hash": "e3829d6e019ec7194e25fbde42fc5d2fd282112374f30fb02163b62c11f3e34d", "exact_match": 0.0}
{"doc_id": 1417, "doc": {"question": "what happens to water that infiltrates the soil if it is not absorbed by the roots of plants", "answer": ["runoff"]}, "target": "['runoff']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what happens to water that infiltrates the soil if it is not absorbed by the roots of plants?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" groundwater"]], "filtered_resps": ["groundwater"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d4d43013962dcef1fbde6e15560e7ac0d43e9485654743dc8ead1c8161f447d6", "prompt_hash": "e9d72915707e521e014600cbd297364ea94da54f459b1dc93e6d899d2341584f", "target_hash": "e1f2bef68fbf4a4801c8fbb8dfb563880059b80cd929d05f70c50bf293a527ce", "exact_match": 0.0}
{"doc_id": 1418, "doc": {"question": "who starred in the pirates of the caribbean", "answer": ["Johnny Depp", "Orlando Bloom", "Geoffrey Rush", "Kevin McNally", "Jonathan Pryce", "Jack Davenport", "Keira Knightley"]}, "target": "['Johnny Depp', 'Orlando Bloom', 'Geoffrey Rush', 'Kevin McNally', 'Jonathan Pryce', 'Jack Davenport', 'Keira Knightley']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the pirates of the caribbean?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Johnny Depp"]], "filtered_resps": ["Johnny Depp"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ebc10fb7a47c5f996376124a8694db72f9a542280090e241e6aaf8d23404baa7", "prompt_hash": "365d8bfa7b5948d62a8358a79e65eb27373034202bfd64ebb132c39bff7cdccc", "target_hash": "7dc7d7c310e1e7b0463cf8099d547d6869bced018050f2235d7a9068e401f340", "exact_match": 1.0}
{"doc_id": 1419, "doc": {"question": "who plays bianca in that's so raven", "answer": ["Erica Rivera"]}, "target": "['Erica Rivera']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays bianca in that's so raven?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Raven-Symoné"]], "filtered_resps": ["Raven-Symoné"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "54ba309e772a858ce9f6f90ac085a852a7f3a98467dd9f08e2e067defb34fefa", "prompt_hash": "9e5b1729e51fc7045a2d27a20b496bc89cbfc681d6de53ee37bb99b96a8a373d", "target_hash": "9ff7c98cef0a5cea52cd28d4fc52c5efcf4a9a07b3fbe3e87e902a80d85db236", "exact_match": 0.0}
{"doc_id": 1420, "doc": {"question": "when did we find out the smoking man was mulder's father", "answer": ["Later in the sixth season", "in the sixth season"]}, "target": "['Later in the sixth season', 'in the sixth season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did we find out the smoking man was mulder's father?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Smoking Man's true identity as Mulder's father is revealed in the episode \"Talitha Cumi\""]], "filtered_resps": ["The Smoking Man's true identity as Mulder's father is revealed in the episode \"Talitha Cumi\""], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e5a3c878a14652a8b71a40014622f46fe6c3d1ee7f5d7067f5f9ec47b6ce492", "prompt_hash": "c06c679ffad419c0cbd56037c34173f048fdf127e822e67da0bcaa5327301226", "target_hash": "bf3da7ae194a2e603d4d7e3a2323ab414dc1d6aabb5dbfe5506152fa2783c507", "exact_match": 0.0}
{"doc_id": 1421, "doc": {"question": "what causes cracked skin at the corners of your mouth", "answer": ["allergies", "infection", "irritation"]}, "target": "['allergies', 'infection', 'irritation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what causes cracked skin at the corners of your mouth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" angular cheilitis"]], "filtered_resps": ["angular cheilitis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5dbd9e488f7c8af76c2008e0a97074b1a781824b43847e7072d3bdf3d0b5fdff", "prompt_hash": "bb61e05afca7dd9110a5c3d8d0a545291b85ce45ae4903b8249446bc9ea2d123", "target_hash": "e6b5db15733323e551a87ca970e47e71066ef662ace9d3406c1ab7c33808d7b2", "exact_match": 0.0}
{"doc_id": 1422, "doc": {"question": "the south west wind blows across nigeria between", "answer": ["till September"]}, "target": "['till September']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the south west wind blows across nigeria between?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" June and September"]], "filtered_resps": ["June and September"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0af9bdcc3ac5146b14ecf77f8763976cc95f85d24223c7a535e14cd69ad2eae9", "prompt_hash": "c28d0d7ade23dff82c284e71a00fd726a402e48f027f11f0791131f7000d4aa0", "target_hash": "9e93f4e7e18417b30642ede056eb650da15148528326ef39fbcf597371d0141d", "exact_match": 0.0}
{"doc_id": 1423, "doc": {"question": "who is the lead singer of collective soul", "answer": ["Ed Roland"]}, "target": "['Ed Roland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the lead singer of collective soul?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jim Witter  ANSWER: John Rzeznik"]], "filtered_resps": ["Jim Witter  ANSWER: John Rzeznik"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1e3ca98a36124503ce5c7392fa505c533f45d0c836370b2c13b8f4e6a11cc3b1", "prompt_hash": "b5d2ebda6a24a13d3bffde6b63bfc4e40be3a5fbba336f46e88ff87bb25d2a1d", "target_hash": "5b92605db605333745c21cd4837fc3224deca1e050b190beddc5122de4bc1fc3", "exact_match": 0.0}
{"doc_id": 1424, "doc": {"question": "what are the top five wine producing states", "answer": ["Washington", "Pennsylvania", "Oregon", "New York", "California"]}, "target": "['Washington', 'Pennsylvania', 'Oregon', 'New York', 'California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the top five wine producing states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" California"]], "filtered_resps": ["California"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03e0fcca9f0b9279f81ee0e46235bf4c9e2f58b81fac1510f7c17a6b626a1f59", "prompt_hash": "9e6932908f2eb146029728d8f8b2ec3c844f9000e97b573b6d09e383b4d96cd5", "target_hash": "10c03c8ef3397ced54f4df1944e439cd8e900b1a67fc85450d32003307b2e502", "exact_match": 1.0}
{"doc_id": 1425, "doc": {"question": "who is known as the father of indian constitution", "answer": ["Bhimrao Ramji Ambedkar"]}, "target": "['Bhimrao Ramji Ambedkar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is known as the father of indian constitution?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" B"]], "filtered_resps": ["B"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9ee00a45d706b55de2a1d74c9637f8b65976966f356ad482218dd9362939ff5d", "prompt_hash": "4067fadf4790f4b5975d12c41111b0239db11181ccbd7089f252304ec582e67d", "target_hash": "c6088d03c7160b9ea7292419ebb89ff695acd200323d35bca16f445994bdd0b1", "exact_match": 0.0}
{"doc_id": 1426, "doc": {"question": "what episode does lori die on the walking dead", "answer": ["\"Killer Within\"", "Killer Within"]}, "target": "['\"Killer Within\"', 'Killer Within']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what episode does lori die on the walking dead?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 101"]], "filtered_resps": ["101"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d5791c73ed3f65bcba45b5df110ae457d09f65dc3984aea214ab6b53e5413f6", "prompt_hash": "daa07965e148c3a731f02a5b3c5c7f88caa1d9f2ee18462454db4e21f9de6f10", "target_hash": "c477e4217bcb960ebc79c0197bf7cfb23f58daf46022618f85515321c2012448", "exact_match": 0.0}
{"doc_id": 1427, "doc": {"question": "who penned the famous indian epic the ramayana", "answer": ["Valmiki", "the Hindu sage Valmiki"]}, "target": "['Valmiki', 'the Hindu sage Valmiki']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who penned the famous indian epic the ramayana?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Valmiki"]], "filtered_resps": ["Valmiki"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d25654ceead9e4cd0fdc24df158b70c49d4ded08fea1e737a8850acc710a7c04", "prompt_hash": "46b5540516b3caa1fccc675a1cbfe190a024a2ce5b04448401ca3ad940bd2aeb", "target_hash": "ad4aaf04a5403f5db9f6cbc00b55d22590ca0ebf19cb25c62ea7f19b243ce010", "exact_match": 1.0}
{"doc_id": 1428, "doc": {"question": "when did the united kingdom entered world war 2", "answer": ["1939"]}, "target": "['1939']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the united kingdom entered world war 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1939"]], "filtered_resps": ["1939"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "14c14e67bfd6cd095bca8b30e877088042173602f1f14e3e98c04bd3592c7c53", "prompt_hash": "44e4474c8322bab14cd2fc84545f94e76ad448f0cbdde38433c023d59ec6b369", "target_hash": "38e0bf9ab20f85bb53523831e786bb1307fac89ce51efdab993a89423bae7a09", "exact_match": 1.0}
{"doc_id": 1429, "doc": {"question": "where does jack ryan live in patriot games", "answer": ["Baltimore, Maryland"]}, "target": "['Baltimore, Maryland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does jack ryan live in patriot games?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Seattle"]], "filtered_resps": ["Seattle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3861aa8c7416f6a644246dd01ef990116085c2d3c49e9241ae970e2bcd32d706", "prompt_hash": "4240fa756c4831478593f07e1dd1a4b1235f9cadd3aa7a6aeb9249688dd9cc0e", "target_hash": "a51f28f3233f79bf90335bec020b587139038e6594b05c685a653d003ce985f6", "exact_match": 0.0}
{"doc_id": 1430, "doc": {"question": "when does season 8 for blue bloods start", "answer": ["September 29, 2017"]}, "target": "['September 29, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 8 for blue bloods start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" September 25"]], "filtered_resps": ["September 25"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2d53c44723f1a9ea91e62e3aad9e0211eff452eb7f8741a121dc689d8d9c4f92", "prompt_hash": "3b11506ec38b184f21b028d98d24438284eed22a6e4b9eda87df12068ed4c099", "target_hash": "dfa2d9567f46b0aaa1a45e1ab8f613de86eb4368ca3e1941dcbb25ced7ed0eb5", "exact_match": 0.0}
{"doc_id": 1431, "doc": {"question": "nobel laureate who began career as accountant in calcutta", "answer": ["Amartya Sen"]}, "target": "['Amartya Sen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: nobel laureate who began career as accountant in calcutta?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rabindranath Tagore"]], "filtered_resps": ["Rabindranath Tagore"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7348babaf31cc85e756763a2cc0a520989eabdf84093fc1abd360341bfbdf2ee", "prompt_hash": "316b63ac0a84e62eaf2c553a75619ef27df5bba60abe4f7a54a44318485a7a69", "target_hash": "bf17c04c2c441a4b2af36a7f4d4b480b5091659493ea06a26b83e4c18f83bfe7", "exact_match": 0.0}
{"doc_id": 1432, "doc": {"question": "what order do the captain america movies go in", "answer": ["Captain America: Civil War"]}, "target": "['Captain America: Civil War']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what order do the captain america movies go in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Captain America: The First Avenger"]], "filtered_resps": ["Captain America: The First Avenger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "25389031a77470aeb1d8f94fbf6361e6c767e62cf808fa5eefb5493cb3498c60", "prompt_hash": "7b1b6e98727d1fa4bedf6b2c2541cbb42733559eb1528d6bf6813722746bcc97", "target_hash": "e5b97b552d6052f397fee0313f29eddd7e85cca6fc63a4c27303c8008d37feee", "exact_match": 0.0}
{"doc_id": 1433, "doc": {"question": "who did virginia cavaliers lose to in basketball this year", "answer": ["Virginia Tech", "West Virginia", "UMBC"]}, "target": "['Virginia Tech', 'West Virginia', 'UMBC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did virginia cavaliers lose to in basketball this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Duke"]], "filtered_resps": ["Duke"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a1191b6e5ee27507645a002dc51be244e4379a62e74f2f753a9d099ea1f49362", "prompt_hash": "fa172fd5dde2b11827c558bb4a73b32c696383bfb0e56ceb05ae76b1994a4572", "target_hash": "971d44816297a4e80d855aafd1c26aa2ebdb02bc469eee3ad09b4ebeffcab5f3", "exact_match": 0.0}
{"doc_id": 1434, "doc": {"question": "how many times have the golden state warriors been to the playoffs", "answer": ["nine"]}, "target": "['nine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many times have the golden state warriors been to the playoffs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 15"]], "filtered_resps": ["15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c83acfcb553635ecd50ac3633659f7a35e3ccb87a5f146e68fb4987fcf3d01f5", "prompt_hash": "8657f2ed429ff21ae2ba7b59e6ae6e71f7f1d72c9ea79fa0f95bb0169bf9c1f2", "target_hash": "ed09870c5f3a538b0b7284264077e0c307c8e19bb7d3d74ad6d4049cf3a4c998", "exact_match": 0.0}
{"doc_id": 1435, "doc": {"question": "who plays emma in air bud world pup", "answer": ["Brittany Paige Bouck"]}, "target": "['Brittany Paige Bouck']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays emma in air bud world pup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bridgette Barrera"]], "filtered_resps": ["Bridgette Barrera"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d1ea7be5e99125cecdb4214ce070ac5d43da321b90701a3a87bf23f14f6af3ea", "prompt_hash": "718ed6d9609e53c92a8a96e342e73833d44a31202f9d2c048ee93bf1a2ce66cc", "target_hash": "2b273828b5117ad5e555caf3a7c1c91714fd1532ea3d809928849ddde27600f6", "exact_match": 0.0}
{"doc_id": 1436, "doc": {"question": "who was the qb for the saints before drew brees", "answer": ["Aaron Brooks"]}, "target": "['Aaron Brooks']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the qb for the saints before drew brees?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Aaron Brooks"]], "filtered_resps": ["Aaron Brooks"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e5fc15534588e46901c68dbf38df9306ec89b5394ab373dca4818e9b5f4e9551", "prompt_hash": "3e492671658568225ec07f5f7695675d4a13974c9c352b63d92c6ac487a6a981", "target_hash": "f5fb2413c46adb92458b64d2462ac4917da833a2cd1aae628bf5bf3258ce1d33", "exact_match": 1.0}
{"doc_id": 1437, "doc": {"question": "what type of government is in north america", "answer": ["Federal presidential constitutional republic", "federal republic"]}, "target": "['Federal presidential constitutional republic', 'federal republic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of government is in north america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" federal republic"]], "filtered_resps": ["federal republic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e83ed2a6651805a65acde0929cbee046148c79468652a0331a09ad6957fbebf4", "prompt_hash": "844bd4f8a62e7a1f0dd7230450814eddea0b6f5c5039ada524351c6b7a1ea872", "target_hash": "2ec89a570d83c83e138bda82fd32f0b9bfd0330e60493ebf5cb4b822d87a67d4", "exact_match": 1.0}
{"doc_id": 1438, "doc": {"question": "when did the first episode of that 70s show air", "answer": ["August 23, 1998", "August 23, 1998"]}, "target": "['August 23, 1998', 'August\\xa023,\\xa01998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first episode of that 70s show air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8677a3564ee4fc7321c8f438baa2b8beffe97a78f165c87ca32747106676c8a3", "prompt_hash": "313bca00a9b270398c32b858b3b95081d3a4a0cc520ec3c1860f8d49eb7374eb", "target_hash": "b3abe67a59388e0dcc7028de443fd81f54b1c3f72a6647a912f79024aa6aa767", "exact_match": 0.0}
{"doc_id": 1439, "doc": {"question": "what is an example of an americanized mexican food created by tex-mex cuisine", "answer": ["fajitas", "nachos", "hard tacos", "chili con carne"]}, "target": "['fajitas', 'nachos', 'hard tacos', 'chili con carne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is an example of an americanized mexican food created by tex-mex cuisine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" chili con carne"]], "filtered_resps": ["chili con carne"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "24b7ea7edec8ef3eb6129d75a364998647213112b8a379e69db80b429216470b", "prompt_hash": "e5b020159ba46ed6d5675ed0b3974f1a52a0c649d02b38242123fd8c6b9d1985", "target_hash": "7753476ed273844ffa40578a8c24fd0ef464731757198e1013eb909c9f4d1cc2", "exact_match": 1.0}
{"doc_id": 1440, "doc": {"question": "where was the film manchester by the sea filmed", "answer": ["Lynn", "Swampscott", "Beverly", "Gloucester", "Manchester", "Essex", "Middleton", "Salem", "Tewksbury"]}, "target": "['Lynn', 'Swampscott', 'Beverly', 'Gloucester', 'Manchester', 'Essex', 'Middleton', 'Salem', 'Tewksbury']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the film manchester by the sea filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Massachusetts"]], "filtered_resps": ["Massachusetts"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cfa46a5d06ee875f273a839f49bb2c6b62f8aa4332bf8dcac02276f1aa1331cd", "prompt_hash": "66659e844c76a6a77fef18443d6cf8f4fd3d4bad02c08aa6a021ceabc1901937", "target_hash": "1327121071181b274d0205051683e22774e2c882744cd62ade2250f1be585642", "exact_match": 0.0}
{"doc_id": 1441, "doc": {"question": "when did the nba add the three point line", "answer": ["1979–80", "1979–80 season", "the 1979–80 season"]}, "target": "['1979–80', '1979–80 season', 'the 1979–80 season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the nba add the three point line?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1979"]], "filtered_resps": ["1979"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8cd68060409c55c8f6fd4f11df5f8896c7213a6f2f6d4250ab8171a9fd187ce5", "prompt_hash": "1a7c4b6d29ed6c02174d75b0f4f13d545af00a7c67fa495896e91e61605a8178", "target_hash": "b8834fc9a57faff625a32ae1c26add35926fdbe68dd6c024a9464246d87122fc", "exact_match": 0.0}
{"doc_id": 1442, "doc": {"question": "when was the last time someone won the super bowl back to back", "answer": ["2004"]}, "target": "['2004']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time someone won the super bowl back to back?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2003"]], "filtered_resps": ["2003"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d62d73b655059a90e963c141ecc1fe79c91db94a6d23cfea28d65d23aac1c67e", "prompt_hash": "b7ce162a6944e9aa0201d5cf9784a7d6212c50cbc380c2e531ef4c5d621650f6", "target_hash": "eb91df4202b3210a842f8a3b8934224aa5db066b63905c74326c0b1e796d6ee7", "exact_match": 0.0}
{"doc_id": 1443, "doc": {"question": "who has the best nba record this season", "answer": ["Houston Rockets"]}, "target": "['Houston Rockets']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the best nba record this season?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Houston Rockets"]], "filtered_resps": ["Houston Rockets"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06786316514f4a61821292066a67c5f66e2486f1d47b4c3e0cc48237d706027b", "prompt_hash": "4d9709a150055004fd49375438cf1681a194c55987254b639a812673eb12a558", "target_hash": "fdf02a059d36cc7cbceec22a24b06f39dad3f983261197f00f18811f284cd641", "exact_match": 1.0}
{"doc_id": 1444, "doc": {"question": "who was the main character in their eyes were watching god", "answer": ["Janie Crawford"]}, "target": "['Janie Crawford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the main character in their eyes were watching god?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Janie Crawford"]], "filtered_resps": ["Janie Crawford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f7e576ee3a04fbd85ecc964749df78b2830d7cd7c3cbe1ffcf9dd7c8caffca0c", "prompt_hash": "1a22faf9c2ccebcb20eda72d418d80b1af09fa069a6aec305536e97f484776d6", "target_hash": "510188b3138c058e298609475d73973319aeacaf459d8c06893b8966b15c2d1c", "exact_match": 1.0}
{"doc_id": 1445, "doc": {"question": "who bowled the last over of the 2007 world twenty20 that india won", "answer": ["Pakistan"]}, "target": "['Pakistan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who bowled the last over of the 2007 world twenty20 that india won?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Harbhajan Singh"]], "filtered_resps": ["Harbhajan Singh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e6fa1f277d5d07d93813c8c348bc21c1fa22a04ef36cabd953420006009c23b7", "prompt_hash": "219cd07b92171fb7bac1ecdae2d8aa854b5ac845de904c319d4611ecb2bef548", "target_hash": "3d291c4b54e6edc7fdfa6b5ca636d65ecacf962508dda37b5449539ffbb58b72", "exact_match": 0.0}
{"doc_id": 1446, "doc": {"question": "who was the youngest president to be elected", "answer": ["Theodore Roosevelt", "John F. Kennedy"]}, "target": "['Theodore Roosevelt', 'John F. Kennedy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the youngest president to be elected?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Theodore Roosevelt"]], "filtered_resps": ["Theodore Roosevelt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "10ab21b2c9e5210aaea3836ceceab89d292685dc8b5d7290217505dfae88d5b8", "prompt_hash": "e200b46563de1b0e4ab61c733ea5c436db13d2ecb316d8a13c2c1a1e39dc621b", "target_hash": "8c8d79e5929f6d03dfe048e659e0ba7540f2f900b62a832aab32e96c8b653b52", "exact_match": 1.0}
{"doc_id": 1447, "doc": {"question": "when is the last time wisconsin missed the ncaa tournament", "answer": ["1997–98"]}, "target": "['1997–98']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the last time wisconsin missed the ncaa tournament?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2013"]], "filtered_resps": ["2013"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e839bf8b385839ec171dca64944d67e2af51bf5c59e5579b461e6657932b73aa", "prompt_hash": "a8e942ac63408f18f2236a7393e6d44ccd444474bc5c6ee6e329a8c63a39bf76", "target_hash": "a483c74d35fa58e93a276404208d7f08a7948f1af8bb5a4bd78f9a203f97135a", "exact_match": 0.0}
{"doc_id": 1448, "doc": {"question": "mention the chemical change that proinsulin undergo to be able to act as mature insulin", "answer": ["post translational modification"]}, "target": "['post translational modification']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: mention the chemical change that proinsulin undergo to be able to act as mature insulin?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" folding"]], "filtered_resps": ["folding"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "04e782aedfb1fd834ee8e2c951b9ebe7cffad27f52afc767caab94d2d528cd6b", "prompt_hash": "064d399b52ead810f44b541b388997d65da9edbeb8dfefaa34ef7d2f3aa0c2bf", "target_hash": "b6c9204006744caf8c1521834f6926535651205ef79869826ab7e98dbfece07c", "exact_match": 0.0}
{"doc_id": 1449, "doc": {"question": "how much venom can a king cobra produce", "answer": ["420 mg", "up to 7 ml", "200 to 500 mg"]}, "target": "['420\\xa0mg', 'up to 7\\xa0ml', '200 to 500\\xa0mg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much venom can a king cobra produce?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 500 ml"]], "filtered_resps": ["500 ml"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "204b1e6289429f7e23c40dbd818b6105c40fea5774651a4d2ddcfbd422903b7d", "prompt_hash": "9617a8a9c67f0c3a0bd73bb699487925cc1538bb8836810ecd650d9cfc4321c0", "target_hash": "24a818550c52db44f69333d448a3a7cf0cbfec8f0ecdec095b0e6c320471137c", "exact_match": 0.0}
{"doc_id": 1450, "doc": {"question": "who is the girl in the hinder video lips of an angel", "answer": ["Emmanuelle Chriqui"]}, "target": "['Emmanuelle Chriqui']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the girl in the hinder video lips of an angel?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Britney Spears"]], "filtered_resps": ["Britney Spears"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6eeabbd6e117276ea4bec60368b8b741a287c23433864160e7be1b7e91a09db6", "prompt_hash": "f1431f95fffbabf1db57177fb607619fe0289e3a30a21c76639ff22aecee9abd", "target_hash": "fc585a406ce03846dad8b027b27b4e3b6360d2d9ebeeb2fa40b2902dee96aa97", "exact_match": 0.0}
{"doc_id": 1451, "doc": {"question": "when did the name of bombay change to mumbai", "answer": ["1995"]}, "target": "['1995']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the name of bombay change to mumbai?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1995"]], "filtered_resps": ["1995"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4378962858162cd4c82ca792450840f1342e734042eaa7d93519747d77d3d998", "prompt_hash": "03f80b2b41af56a01d437ef9315f8eaa5b24931a3ee426c729d7443be220736f", "target_hash": "fa78c62118c7a21177eaba595abbbd9aac5656a594a0bdac7cd33675edf1399d", "exact_match": 1.0}
{"doc_id": 1452, "doc": {"question": "when does season 7 game of thrones dvd release", "answer": ["December 12, 2017"]}, "target": "['December 12, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 7 game of thrones dvd release?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" March 31"]], "filtered_resps": ["March 31"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3666cc1ace1fe1f226d03889bd3ca8c230445bc5ad4ecf61116ea1f1e8aa74e3", "prompt_hash": "ad81c36c5b135d025bbc6c92cea55e733e91d80c3d0f32bf5b83359a52833c2b", "target_hash": "46188d8c0789dbba3d98d63e8f264c3099cad0941344ee190c5b713bc3cde036", "exact_match": 0.0}
{"doc_id": 1453, "doc": {"question": "when did the twenty one pilots hiatus start", "answer": ["November 2016", "July 2017"]}, "target": "['November 2016', 'July 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the twenty one pilots hiatus start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0eddd5ec90a0d6746737f3cf2d538a2916c8a94e447e0d08bc5bf1bbbf3b359e", "prompt_hash": "27f3715664feaa6c56cd7a7e88af94a5b7ee6d993c68b47d82572cf4076f0d8a", "target_hash": "183bdd1723c8fc9934b93cb95256b4b52e8a380f7432207bdca96867095fe9e1", "exact_match": 0.0}
{"doc_id": 1454, "doc": {"question": "who becomes chief resident on grey's anatomy season 7", "answer": ["April Kepner M.D.", "Kepner"]}, "target": "['April Kepner M.D.', 'Kepner']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who becomes chief resident on grey's anatomy season 7?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Derek Shepherd"]], "filtered_resps": ["Derek Shepherd"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f7466b3fc3023d80f4796bd1f7248018c83a6cd5fd8798e33fee478e8b6529d8", "prompt_hash": "8ced666b2448858fb7450c0f77bf2e36895888f9eea2692099d9db7ed17b04a0", "target_hash": "721586649d07749ebe55774e3d6cc0bf15a260fcbe805b06a3fdf57b4f8020ad", "exact_match": 0.0}
{"doc_id": 1455, "doc": {"question": "when does the heart develop and begin pumping blood", "answer": ["by week 4 of development", "week 4 of development"]}, "target": "['by week 4 of development', 'week 4 of development']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the heart develop and begin pumping blood?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" during the fourth week"]], "filtered_resps": ["during the fourth week"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b9074a394191a4bcbed076490eae79a6901f6cd8c57362928049f36d9cdd3d2", "prompt_hash": "c6ce29ed0e410d19b617806d17613a17f4a35debe7827d26d6e5a7e2b03dafa8", "target_hash": "3331a0f4d4727143944c7014627187e9591b672d9766604234e3bdb9fc922741", "exact_match": 0.0}
{"doc_id": 1456, "doc": {"question": "who won the fifth season of america's got talent", "answer": ["Michael Grimm", "Soul singer Michael Grimm"]}, "target": "['Michael Grimm', 'Soul singer Michael Grimm']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the fifth season of america's got talent?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Heidi Montag"]], "filtered_resps": ["Heidi Montag"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "123f015574c07fdb9d72bebbd10d27ccfa2b608dfed2c15f6f7f1231562ff91d", "prompt_hash": "4810efb28e129a073eb59b215f553190c1b9040d5113e98f409293d8fe6ccd9c", "target_hash": "8837587655d9b2e8cca8e996a461aeeae066ab89445114be168090e2be8f98f6", "exact_match": 0.0}
{"doc_id": 1457, "doc": {"question": "what is the origin of the word celsius", "answer": ["the Swedish astronomer Anders Celsius"]}, "target": "['the Swedish astronomer Anders Celsius']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the origin of the word celsius?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Anders Celsius"]], "filtered_resps": ["Anders Celsius"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f50926fa543250236c9e84ba959f146a642f0f09ca3be3df111fc7e64f7cc95f", "prompt_hash": "5b289bc0e859ac66ddcda69c6bb3a6c6d12976928fdae810e4007cd8e5c4ed69", "target_hash": "542103f73e5dae24bfc07328356d46df10207c19283c561a77f2d1cb61ae1ebb", "exact_match": 0.0}
{"doc_id": 1458, "doc": {"question": "what is the official slogan for the 2018 winter olympics", "answer": ["Passion. Connected."]}, "target": "['Passion. Connected.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the official slogan for the 2018 winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" \"Together in Sport\""]], "filtered_resps": ["\"Together in Sport\""], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3db14977f4e4a6c8c2a01f469e23e8cdb05c54db8088aa274599627a0c5073f6", "prompt_hash": "e3a20250c3c388d686772de6dce2a73be8d26f82b642d003dc7a817badd033da", "target_hash": "7b007c9d3a11cb2de85a8fc343db5c75cefd7038853e7ade85a6baf9817ff1cd", "exact_match": 0.0}
{"doc_id": 1459, "doc": {"question": "who starred in the film walk the line", "answer": ["Ginnifer Goodwin", "Reese Witherspoon", "Dallas Roberts", "Joaquin Phoenix", "Robert Patrick"]}, "target": "['Ginnifer Goodwin', 'Reese Witherspoon', 'Dallas Roberts', 'Joaquin Phoenix', 'Robert Patrick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the film walk the line?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joaquin Phoenix"]], "filtered_resps": ["Joaquin Phoenix"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a8c4d36a61980b69dde7d581015d167f6a612e6938d072547d42f45897fd951", "prompt_hash": "337aeea7a44917e5d54fe657dbc0dc0a68974eb374e4566e07d72a913ae82db9", "target_hash": "6bec7833eeb2e8648d28f689726312178451351a81bd23dcfe5bf241f108c719", "exact_match": 1.0}
{"doc_id": 1460, "doc": {"question": "who sang the song brother let me be your shelter", "answer": ["Needtobreathe", "American Christian rock band Needtobreathe"]}, "target": "['Needtobreathe', 'American Christian rock band Needtobreathe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song brother let me be your shelter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Eagles"]], "filtered_resps": ["The Eagles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c0b50384b510f8cbf013ea595e87f1d93e13bbbaa327366767b35e07660998dc", "prompt_hash": "b2a9e7e4ec5d569ec8c9d1e44aa24c5a8fed75bea86ced974e496303727b5f78", "target_hash": "c2588d6d4fc5d07e26503570cb62e5645202523e4bf816942662a7106f28840b", "exact_match": 0.0}
{"doc_id": 1461, "doc": {"question": "who plays justin on the haves and the have nots", "answer": ["Nicholas James"]}, "target": "['Nicholas James']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays justin on the haves and the have nots?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew Knight"]], "filtered_resps": ["Matthew Knight"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c416b7d4e578896590dfca508c54b3d5e09dae199a7eb2d0f35c0d9ead7468b5", "prompt_hash": "f88e43711096c506cd98870b8c7557a6c25960bee6032f3f447744880c32d577", "target_hash": "cc1667d3052f2b572ed8ce168ed0f8d44c00aefc09589e38deb1c407a8fa0a8c", "exact_match": 0.0}
{"doc_id": 1462, "doc": {"question": "who gave a speech to the democratic national convention in 1984", "answer": ["Mario Cuomo", "Jesse Jackson", "Rev. Jesse Jackson", "New York Governor Mario Cuomo", "Senator Gary Hart", "Gary Hart"]}, "target": "['Mario Cuomo', 'Jesse Jackson', 'Rev. Jesse Jackson', 'New York Governor Mario Cuomo', 'Senator Gary Hart', 'Gary Hart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who gave a speech to the democratic national convention in 1984?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walter Mondale"]], "filtered_resps": ["Walter Mondale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ffbfeab92e7b7da904a0363635c3f44f9479a26fe5b7a36d11d31a2ac93a0f9d", "prompt_hash": "c7626e30659b3df78176739857f3da9705c880759e89478899b23bd253041041", "target_hash": "106e356b1393c728676b917889d95dc4b3848d78308f63669fa08452a40d4eb7", "exact_match": 0.0}
{"doc_id": 1463, "doc": {"question": "who sings what the world needs now in boss baby", "answer": ["Missi Hale", "Steve Mazzaro", "Hans Zimmer"]}, "target": "['Missi Hale', 'Steve Mazzaro', 'Hans Zimmer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings what the world needs now in boss baby?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Hudson"]], "filtered_resps": ["Jennifer Hudson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a1677e1effb5acc95d340d7b49fdb091c8f34a2ff558e197d930445c93580b8", "prompt_hash": "ac125facd2770c9cca7a9faf97838b2bf486f2c0ab7e849d4cc349d52fb4a078", "target_hash": "cc0bdf54313899575da1a8a02771aae26130b689385b438b0d329ec2469d6114", "exact_match": 0.0}
{"doc_id": 1464, "doc": {"question": "who sings only love can break your heart", "answer": ["Neil Young"]}, "target": "['Neil Young']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings only love can break your heart?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c997fdbb00341b456218f5b2045e65909c3eab26cbfa30962dc5812ef0841c18", "prompt_hash": "6268987744afd6d72408027461041b4397ae76dea77011940eb6be6677ffe2a6", "target_hash": "4ac8c0097dc85ada79f85a01d2bd1e83241e090e04378185b4a81a7711105d56", "exact_match": 0.0}
{"doc_id": 1465, "doc": {"question": "who sings ive loved you for a thousand years", "answer": ["Christina Perri"]}, "target": "['Christina Perri']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings ive loved you for a thousand years?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Reba McEntire"]], "filtered_resps": ["Reba McEntire"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "646b5e9743de9da93b1e8ebaa767b517d4118f45234826990218a288acc40c5c", "prompt_hash": "0f08098010c4aa3f4ae123b3de151ac445c2f075e2e348f01d8ff60e7033140a", "target_hash": "8f030fa2296709b854a1f2775b57df0e7a5afccd8511ac7f218d1f63cea75267", "exact_match": 0.0}
{"doc_id": 1466, "doc": {"question": "when was the first case of alzheimer's diagnosed", "answer": ["1901"]}, "target": "['1901']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first case of alzheimer's diagnosed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1906"]], "filtered_resps": ["1906"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "46d5bcc08830eb1976f61fbcb68beb75bdc9216decc5e771cb14f3656c875095", "prompt_hash": "850c77e9b1f9720126f31155b52a816a80bb59c27f68cadf3b799e735923ff05", "target_hash": "c6193553236bb50f0ac2512e85a08ec906fb9a91f8b824604f528f5cd1e0846b", "exact_match": 0.0}
{"doc_id": 1467, "doc": {"question": "who is the founder of google and when was it founded", "answer": ["1998", "Sergey Brin", "Larry Page", "Larry Page and Sergey Brin"]}, "target": "['1998', 'Sergey Brin', 'Larry Page', 'Larry Page and Sergey Brin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the founder of google and when was it founded?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Larry Page"]], "filtered_resps": ["Larry Page"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d9617c072744b69d4274ab4709e2b52988709f94b571a57a70b74e8ac5f0303", "prompt_hash": "91fc1c863c423da26cbb0c283d6ba97b5ef29f8736e8274a35e2efeb3bb071d2", "target_hash": "d3f258bfc7b5cd6c79073aa40f267828eabeb237c492a9097c5d05169443f7e8", "exact_match": 1.0}
{"doc_id": 1468, "doc": {"question": "who is the longest serving chief minister in india as on jan 2015", "answer": ["Jyotirindra Basu"]}, "target": "['Jyotirindra Basu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the longest serving chief minister in india as on jan 2015?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shivraj Singh Chouhan"]], "filtered_resps": ["Shivraj Singh Chouhan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5dd259097d71fb78434b8c64ed0457e722ec3e4cfec82d1504f14ea00e10b253", "prompt_hash": "face934956e12ae52a3fe9bfd00564ba2a798fd6d44257fb9461a249aeeb1ffb", "target_hash": "e3f5993f73b0688326225591ae0466e4ddcd8e2d3ffd07f7bd3668e51dabdef2", "exact_match": 0.0}
{"doc_id": 1469, "doc": {"question": "joined mexico and the united states to form nafta", "answer": ["Canada"]}, "target": "['Canada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: joined mexico and the united states to form nafta?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Canada"]], "filtered_resps": ["Canada"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5a72e8f5c2910ef6e864d52441001b2c56ac4f7d5fcea2b94df988feeb433eab", "prompt_hash": "0477b7fd29f43a696aa7d31d80476e3e673abd3fb2113dedd8a8cefb7f9529e8", "target_hash": "b6331bd59b4f4c96bb93005fbd31f398481003ab682305e364aab6a46846811a", "exact_match": 1.0}
{"doc_id": 1470, "doc": {"question": "who was the chief guest of 2014 republic day", "answer": ["Prime Minister Shinzo Abe"]}, "target": "['Prime Minister Shinzo Abe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the chief guest of 2014 republic day?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Narendra Modi"]], "filtered_resps": ["Narendra Modi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "90f1e98162670ebeeb3985eae0a0ae443c8141502772f952aea870e8c93fc6ca", "prompt_hash": "1e109080759f0e8fd6a73a1fe529252023fd4ae02ea93ec6b0fadfd9a5d334a7", "target_hash": "75842d8f804acfeb1b2018b22ce7c22567c070ac84948b46e6b806979ca64c5e", "exact_match": 0.0}
{"doc_id": 1471, "doc": {"question": "how many episodes of ray donovan series 3 are there", "answer": ["60", "12"]}, "target": "['60', '12']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes of ray donovan series 3 are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 13"]], "filtered_resps": ["13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a70351c08ff0b5e6d0ae925867c38fff4fcef08339fc9daeeeee13f61f2ecab5", "prompt_hash": "35238fe79cf8245af47ac1e989ea27e5eb071c052859d9ec586674ad3b10b61c", "target_hash": "c819629dacae15cca01fe3e5eb2ce163163a7b57cc5ebfc74de4e0cffa4fe022", "exact_match": 0.0}
{"doc_id": 1472, "doc": {"question": "when do they put the rockefeller tree up", "answer": ["mid November", "late November or early December"]}, "target": "['mid November', 'late November or early December']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do they put the rockefeller tree up?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" December 2"]], "filtered_resps": ["December 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7d5cea5adae219931862d479aaabd9ee8cde595f38f6fad424fb0af6e3f6ee5", "prompt_hash": "c1e179bd93fff3f56d4fb7012b7b273de6dc8db4bc538b825910a4bac7a7314a", "target_hash": "aaf89139bae846c8ae0b55a4bf2fc9202d7107ebddb9e9c02b44fd83489fd38e", "exact_match": 0.0}
{"doc_id": 1473, "doc": {"question": "when does synapsis of homologous chromosomes occur in mitosis", "answer": ["prophase I of meiosis", "during prophase I of meiosis"]}, "target": "['prophase I of meiosis', 'during prophase I of meiosis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does synapsis of homologous chromosomes occur in mitosis?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Meiosis"]], "filtered_resps": ["Meiosis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cfa82b3dfda645e7b45fff050f5fe6cd80f4b8c59cb63ef25523d4d4c7e119d6", "prompt_hash": "16f8de498620c84756079572051fda7279b629bfc1ffd08f6f3dbd91d0ed0b28", "target_hash": "6f3c08b3414ec5a20043648e435f986143bba39ec018c5ca707bc66a1104533f", "exact_match": 0.0}
{"doc_id": 1474, "doc": {"question": "who does the civil rights act of 1964 protect", "answer": ["all Americans"]}, "target": "['all Americans']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the civil rights act of 1964 protect?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" African Americans"]], "filtered_resps": ["African Americans"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0530f4022b88f110534f71cb4d1b0e65ccf0ce3bb6755d910da4d04123ef3739", "prompt_hash": "ed9eeea070e57e94a375d9d0e01f67284490a6d35c2e2f1c2cfe011ee4daac0d", "target_hash": "b245b183dd9897ebd90897fffa1435823d9db9243a4d658bce7f249f8eb7335c", "exact_match": 0.0}
{"doc_id": 1475, "doc": {"question": "when was the last time america hosted the summer olympics", "answer": ["Salt Lake City", "1996"]}, "target": "['Salt Lake City', '1996']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time america hosted the summer olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2002"]], "filtered_resps": ["2002"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "71e8d8e8a6b4d2bd4e2461beb729c9f6aeba1753a338db57ebdd46f4042d1dda", "prompt_hash": "b3a34c0c8d3779860c4b7ca33b17dc3c5502b7a0a6aa5bf724698047ce62da6e", "target_hash": "13205548b075856375ccb51ea9347d7b5fbbfa742acfde262ac70ebddece893b", "exact_match": 0.0}
{"doc_id": 1476, "doc": {"question": "when does season 8 of shameless come back", "answer": ["November 5, 2017"]}, "target": "['November 5, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 8 of shameless come back?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 2017"]], "filtered_resps": ["January 2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "48118a2f1ae966bd8c79b54453619b0a445fd5179f2fe356914f02d97e4330be", "prompt_hash": "be2fceefa29a29bafb577575c6ae950eb143a3c848bb3e12f40f253031eb7800", "target_hash": "760275f6ba6e238ebad173aa4854e3d3183e3cf008f140b743e32241509a5fa1", "exact_match": 0.0}
{"doc_id": 1477, "doc": {"question": "the heart muscle is stimulated to contract by electrical impulses which are generated where", "answer": ["the sinoatrial node"]}, "target": "['the sinoatrial node']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the heart muscle is stimulated to contract by electrical impulses which are generated where?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sinoatrial node"]], "filtered_resps": ["sinoatrial node"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "98204aeca259d8dc45aa867afd88140c58ad8f04a9b60ae73319c13ee3f5fa44", "prompt_hash": "e23319e6e32f45b7ae4cf21b5a2f4309b7ec312b4cd33ae5cfcc1be65436410d", "target_hash": "743a604553072484dbb512a94499df4c573ddfbf3afb7b481b6b825e28216b8e", "exact_match": 1.0}
{"doc_id": 1478, "doc": {"question": "who scored the most points in their nba career", "answer": ["Kareem Abdul-Jabbar"]}, "target": "['Kareem Abdul-Jabbar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who scored the most points in their nba career?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kareem Abdul-Jabbar"]], "filtered_resps": ["Kareem Abdul-Jabbar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "532df6671ce64463edd0b625031c2f7b06896c7aec901ee1f7b2b2eea1a8ebc6", "prompt_hash": "0c97fea4fbf97a06e7c91151af65439f29f7f6c0ad29e69247c04578f60d3c43", "target_hash": "d953184597acb7a70d0d0cd3f9418219906cbc9eb2f304e619c6487486496bb2", "exact_match": 1.0}
{"doc_id": 1479, "doc": {"question": "what is the meaning of the name gomez", "answer": ["man"]}, "target": "['man']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of the name gomez?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gomez is a surname"]], "filtered_resps": ["Gomez is a surname"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7add74795b7c6fc60a5cc6203293defd92e71d349aa3029011db46c1b820e32e", "prompt_hash": "c9f594e5a14919435ba81a917c20c17fd974951ac17395d621c9c84f05e92af5", "target_hash": "c58bf5527af7568562786ab18d4e2b04831262cb66638e00892a7636fdbc193a", "exact_match": 0.0}
{"doc_id": 1480, "doc": {"question": "where was the first t20 cricket match played", "answer": ["Lord's"]}, "target": "[\"Lord's\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the first t20 cricket match played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" South Africa"]], "filtered_resps": ["South Africa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a3f1c3f2837e3e56d35f79d5dceeeb3545dd74ff889353063798c2ffa0e92012", "prompt_hash": "3707ec47f21f3d807b9cabf49074f54816a67fa7dddc219221332a39631aea2d", "target_hash": "a2572aa07426d1c7713799e95c7b72fe23fa7eb6c37c9a2eb4f4db1a979996e9", "exact_match": 0.0}
{"doc_id": 1481, "doc": {"question": "who has won the most masters green jackets", "answer": ["Jack Nicklaus"]}, "target": "['Jack Nicklaus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the most masters green jackets?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jack Nicklaus"]], "filtered_resps": ["Jack Nicklaus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "968c30fc884a116e3d982a154a1f3ff37453f775472656dc07c26e31a85d1927", "prompt_hash": "51cf7298c6d7fba489d132b472bf1522538dabb06acff491aa6785f5ac587333", "target_hash": "fd9d674e4da6426305f7d1a648a06ed0a9e7eccc4b0156022c68d3d935269b9d", "exact_match": 1.0}
{"doc_id": 1482, "doc": {"question": "who was appointed to manage the national recovery administration", "answer": ["Hugh S. Johnson"]}, "target": "['Hugh S. Johnson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was appointed to manage the national recovery administration?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hugh Johnson"]], "filtered_resps": ["Hugh Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aaca46c4442f98e7552c4812d22867a039eee070896b11aa1b140351980fc3b2", "prompt_hash": "bd2104e380a7cf58156c7909d66a8fdde9960e84066186f53d107c873cff09db", "target_hash": "03a84309f2859060c4d621890ae141679514a7dd09e4414cb8e63e44808bad54", "exact_match": 0.0}
{"doc_id": 1483, "doc": {"question": "who want to be a millionaire calls his dad", "answer": ["Carpenter"]}, "target": "['Carpenter']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who want to be a millionaire calls his dad?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul A"]], "filtered_resps": ["Paul A"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "04010c8b9a7f11c09d08738fe123425184ea19128f50a6cf29bd604eca74a087", "prompt_hash": "73b7f22c93d4699e1f2922a38346c6afb3eda96ca20c3d29be8719337ba04092", "target_hash": "d1b297944b909b1c6febb6a364509a43b328994956ac88daf9368b4b73dc773b", "exact_match": 0.0}
{"doc_id": 1484, "doc": {"question": "when was the suite life of zack and cody made", "answer": ["March 18, 2005", "September 1, 2008", "2005"]}, "target": "['March\\xa018,\\xa02005', 'September\\xa01,\\xa02008', '2005']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the suite life of zack and cody made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2005"]], "filtered_resps": ["2005"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ba23c48800c5ef790964efa23bad19c8185c7574b121dbf7dbceb07d5572b95b", "prompt_hash": "62f8273d754ba696a15f192f9c178d6c33bac295a1fa80ab97849ad61e98b678", "target_hash": "6098665438cd24d3e4615b6cbfb870da2e8138e34c2d8dedc69cc5960622ac33", "exact_match": 1.0}
{"doc_id": 1485, "doc": {"question": "who played shaggy in the new scooby doo movie", "answer": ["Matthew Lyn Lillard"]}, "target": "['Matthew Lyn Lillard']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played shaggy in the new scooby doo movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew Lillard"]], "filtered_resps": ["Matthew Lillard"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "74bde0d66cb903bde212bbd1f3565b1b0101035bf3c3c4c67926245da244c328", "prompt_hash": "eb4947a3d267a1222583f4d5156188c9a81d6af7732af5991029c3a6df7a112d", "target_hash": "0c8f23c9252a805222c00c251c06de8bc62de4ad4605da8568f9740636c18869", "exact_match": 0.0}
{"doc_id": 1486, "doc": {"question": "who is the song diamonds and rust about", "answer": ["Bob Dylan"]}, "target": "['Bob Dylan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the song diamonds and rust about?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Neil Young"]], "filtered_resps": ["Neil Young"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2df0078caf0ca8b971b5f686bc48a159573cd30026ecde9da82d78c8aa956b8", "prompt_hash": "601a624140063ff029511059a56f394552519e1a6ae65bc3d27b612bc265bb9d", "target_hash": "183d2854d32090092acfd7e78bc5499b9b3fb04b27e2963a1269b70bf1812be7", "exact_match": 0.0}
{"doc_id": 1487, "doc": {"question": "the resting stage of the cell cycle is", "answer": ["Gap 0", "Interphase"]}, "target": "['Gap 0', 'Interphase']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the resting stage of the cell cycle is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" G0"]], "filtered_resps": ["G0"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cbf576aa4cba1178b3f6b00269e48598980fa7bdfbd41114568b2050198c591f", "prompt_hash": "8198cf8b4694174a65ab604ff5dbabd2a3f707ad3838510e4345f1d48e1f0f82", "target_hash": "203ab38348d0a872ddad1020a34f1000317f37a459f6b8d4bb752e1be4a30bd3", "exact_match": 0.0}
{"doc_id": 1488, "doc": {"question": "what do you call initials that stand for something", "answer": ["acronym", "An acronym"]}, "target": "['acronym', 'An acronym']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what do you call initials that stand for something?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" acronym"]], "filtered_resps": ["acronym"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "743421480ab2b29b8edc98f508bf535dd3e8c97fdc98f28f10b677922205ed16", "prompt_hash": "c1b9a8e4fd3c41a2d5a3a7ea8f4df4e01250d229bb673fd9e4966b49e6df46d5", "target_hash": "a3b0ff8631ae2670a0624b0e5a05dcaff7d3e8dd17d133ac085d8c9d30c1d8c9", "exact_match": 1.0}
{"doc_id": 1489, "doc": {"question": "who sang oh annie i not your daddy", "answer": ["Kid Creole and the Coconuts", "Kid Creole & The Coconuts"]}, "target": "['Kid Creole and the Coconuts', 'Kid Creole & The Coconuts']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang oh annie i not your daddy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Toby Keith"]], "filtered_resps": ["Toby Keith"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6c45ae516d0691bb94579983f7f75d5e5ebdeb4b6cba7b4ea399547c1af5205d", "prompt_hash": "e3de9e209c39897764fda2b41dd68d39741d139342e91ab22b2d10816ff7cffd", "target_hash": "3bd5d7acfd9943fb71e4855270726b0715b1588d23c7f2b7c3cc9cd615e1c302", "exact_match": 0.0}
{"doc_id": 1490, "doc": {"question": "who played king theoden in lord of the rings", "answer": ["Bernard Hill"]}, "target": "['Bernard Hill']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played king theoden in lord of the rings?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bernard Hill"]], "filtered_resps": ["Bernard Hill"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b5f2f0b435eb7c10a7e95ae49db768ac44cd83a8b697f812b7a5f3fbc0ad765", "prompt_hash": "16d30c4d104d8e7b3d1982b17f3b719138a9b7f2681b8dfe912236b728c1b49d", "target_hash": "729eedac56ce03ff2d13534e70055b130b85cd73a008834dc74dfca6c4aab069", "exact_match": 1.0}
{"doc_id": 1491, "doc": {"question": "what country shares borders with both belarus and romania", "answer": ["Ukraine"]}, "target": "['Ukraine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what country shares borders with both belarus and romania?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Moldova"]], "filtered_resps": ["Moldova"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "97bad44c0b9510a6774f74fe8027ad620d67497764bb44f5a4833283bdb702b2", "prompt_hash": "2697ae55b1ff5526a9cd6c637be41d0bb13cdfdd7e1259b50b531d0b3b1ff1be", "target_hash": "1f13d4522512b4308c35808882b9194be36f5b06d71a41af6e815c35846c9580", "exact_match": 0.0}
{"doc_id": 1492, "doc": {"question": "where was harry potter and the deathly hallows part 1 filmed", "answer": ["on location"]}, "target": "['on location']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was harry potter and the deathly hallows part 1 filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Leavesden"]], "filtered_resps": ["Leavesden"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b56384f95d1d725adf43263c533b88d51770940f6b65c5596e5a98697fb29b1e", "prompt_hash": "99513c920097fc5ebaf59dd35b26c3d13fa3acd24b6c3c5414a1451e8a2bef1b", "target_hash": "920e72b2c3cc80a9991f5a14e8c5e48966cc31fa113200507605d1f37cae95ae", "exact_match": 0.0}
{"doc_id": 1493, "doc": {"question": "how many farmers are there in the usa", "answer": ["3.2 million"]}, "target": "['3.2 million']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many farmers are there in the usa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "767a7208f0aa5f2cfb52c4c6cb98d133da62ecba9e5806867c81c1125f3d2b05", "prompt_hash": "550dc28d1f359bb34cc670c282ba553c12ef2fe5084e1b2edfa1268d01725dcd", "target_hash": "902de50911494744fe56eec0c0a70b58c5b3aa048a5f7297c90cda510fa25894", "exact_match": 0.0}
{"doc_id": 1494, "doc": {"question": "what are the 3 layers of the meninges", "answer": ["the arachnoid mater", "arachnoid mater", "pia mater", "the pia mater", "dura mater"]}, "target": "['the arachnoid mater', 'arachnoid mater', 'pia mater', 'the pia mater', 'dura mater']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the 3 layers of the meninges?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dura mater"]], "filtered_resps": ["dura mater"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0ce1dc3aa2d76654cd56323eed76216783a1a7eb9c16c6b62ffdaf596e0d5e99", "prompt_hash": "f38c672d025765c2de2ae8dc33e18d7353dd64c385effbe8c93762d57533ff94", "target_hash": "511e33aa4c4f9e8a2487d7fcf3b57086ec16ff0319014add8cd88b7c1bb3c804", "exact_match": 1.0}
{"doc_id": 1495, "doc": {"question": "which of the following helmiths feeds on blood", "answer": ["Necator americanus", "Ancylostoma duodenale"]}, "target": "['Necator americanus', 'Ancylostoma duodenale']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which of the following helmiths feeds on blood?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" hookworm"]], "filtered_resps": ["hookworm"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9d644d7da674efac5468db343645d9245bc10854e17c3be4f535e08883bc64fb", "prompt_hash": "815a639f97715134a364665f550c74ccce9ac869c432fc7c40f95a0dde099e51", "target_hash": "f151e8a6567d5d9224349e03306b4586401698e96333d232bb524f6a2ab8518c", "exact_match": 0.0}
{"doc_id": 1496, "doc": {"question": "what culture region is germany a part of", "answer": ["Central Germany"]}, "target": "['Central Germany']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what culture region is germany a part of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Central Europe"]], "filtered_resps": ["Central Europe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "97bf2ba13482dc3b9577291724dcfde7dd68ea5cb0494597ab43ad999504b152", "prompt_hash": "552135c62b7e7e6b280178a51fc71df5941e13918567142f0aed04fe24b88c55", "target_hash": "42a32d180c916ed8eb3891f420f9f4753c58a02e445015bb34a369d000ac643a", "exact_match": 0.0}
{"doc_id": 1497, "doc": {"question": "what was guantanamo bay before it was a prison", "answer": ["Guantanamo Bay Naval Base"]}, "target": "['Guantanamo Bay Naval Base']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was guantanamo bay before it was a prison?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" naval station Guantanamo Bay"]], "filtered_resps": ["naval station Guantanamo Bay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3edb7c00b41b5b4e7ead218528635c5c051dc38ad684e92bbd557725344dbf24", "prompt_hash": "edc82b3976cdeee18d26e939e0d5852ef3457320dc1cfefed1f8c8b2e10f5bed", "target_hash": "7a05a5d3cc9997237f9269138b5e6f9ff68c55e527c05ea8bfe3eeb5c737d2d0", "exact_match": 0.0}
{"doc_id": 1498, "doc": {"question": "who wrote most of the declaration of independance", "answer": ["Thomas Jefferson"]}, "target": "['Thomas Jefferson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote most of the declaration of independance?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thomas Jefferson"]], "filtered_resps": ["Thomas Jefferson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "04cf58b645c3944a3882d01a9b517e8c8d2aacae7a88e45a909a788a3063e9c4", "prompt_hash": "57aed02a9a1b85beaee7ff4f4f6ac04699c8bc1b10eb85c2dc6a3e1690bb85cf", "target_hash": "42475f6d3851560323eedeea820f3a8225fc5fb4cfaa94bd56cadfe6d3c6c397", "exact_match": 1.0}
{"doc_id": 1499, "doc": {"question": "in what part of the digestive tube do you expect the initial digestion of starch", "answer": ["the mouth", "the minor salivary glands"]}, "target": "['the mouth', 'the minor salivary glands']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in what part of the digestive tube do you expect the initial digestion of starch?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mouth"]], "filtered_resps": ["mouth"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c9e021a063ecbebb18dcb7db9aa693a589f288e0759345edbea87501ef972bd1", "prompt_hash": "765b58bdaf80098d88f3774efa380da5c454679e43833871d193eb6ef52caa2d", "target_hash": "567a3e3420a58735d55839b9a897eb40119a4b7478597b9e5775eadaf39abfcc", "exact_match": 1.0}
{"doc_id": 1500, "doc": {"question": "where do kidneys for kidney transplant come from", "answer": ["living-donor"]}, "target": "['living-donor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do kidneys for kidney transplant come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" deceased donors"]], "filtered_resps": ["deceased donors"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6e2c9fde9861807d6aab4e914c455092dffe6bc7927dbf92b8dd31146009120d", "prompt_hash": "3542aca6f6fd0d70439f743e52d8da1b18be25d11e94f3d4a0736b3331103a19", "target_hash": "732ec6095231403ba808bb68bbb313c8b2dcb8476e69f5ee2bbe709b3430ec30", "exact_match": 0.0}
{"doc_id": 1501, "doc": {"question": "what song did the titanic band play when it sank", "answer": ["\"Autumn\""]}, "target": "['\"Autumn\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what song did the titanic band play when it sank?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nearer"]], "filtered_resps": ["Nearer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7839d75d7d6fc6843ad9630d709bd99fa5f1cd08edbd0c3bc16e73abaf8a49b", "prompt_hash": "4b95c8446e009d9357f2d958b688614128be9c19a10bc0fdeca9b68b5a797f2b", "target_hash": "2bdba9c3ff3c1d44a3d0baaccd5b80fcb311c106f3978d8791099b9105dc01d9", "exact_match": 0.0}
{"doc_id": 1502, "doc": {"question": "how tall is the tallest building in las vegas", "answer": ["The Drew Las Vegas", "1,149 feet"]}, "target": "['The Drew Las Vegas', '1,149 feet']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how tall is the tallest building in las vegas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 385 ft"]], "filtered_resps": ["385 ft"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "680921fef7e064a8cdd0e18ca099fc8bb73b1020bf19fc110cf6d8b646c6f395", "prompt_hash": "55c93c44f23c83237cfb6c8d1b55bb0cc7aa9deeb6ab861e07209d0b8c99fb80", "target_hash": "c3765c7c29b5f83482e373471fd1da4a3f7427fc8d09637a2503cf685d5ab568", "exact_match": 0.0}
{"doc_id": 1503, "doc": {"question": "when is the new series of keeping faith starting", "answer": ["13 February 2018"]}, "target": "['13 February 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the new series of keeping faith starting?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f19d508ecc69adde07c8dee2fc15d1300205fff70c7a51f14bdc4f95da5bb649", "prompt_hash": "27a29b7a90cef7593f74efb54f811a9158402bafafc71f3d7c73d36de209fa7c", "target_hash": "faf0be6a4c5e81cad84093b6996b9b36eedab9982de1e7c5bbaee44a64eadd98", "exact_match": 0.0}
{"doc_id": 1504, "doc": {"question": "who wrote trust and believe by keyshia cole", "answer": ["Jessyca Wilson", "Darhyl Camper, Jr.", "Guordan Banks", "Keyshia Cole"]}, "target": "['Jessyca Wilson', 'Darhyl Camper, Jr.', 'Guordan Banks', 'Keyshia Cole']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote trust and believe by keyshia cole?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Keyshia Cole"]], "filtered_resps": ["Keyshia Cole"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "854ce317cb8d2baae32accc529f759e294b2522007233f1016cfc26a1fa58d9e", "prompt_hash": "58b18f28a54900bb00e36529d06137be573f9d9b044fe03ba0bb478749a2412a", "target_hash": "8273c87ead8ebf4f3573f110bc194fcce06445ffff3d0f1fa1e28e5fa7c07bf6", "exact_match": 1.0}
{"doc_id": 1505, "doc": {"question": "who has the oldest team in the nba", "answer": ["Sacramento Kings", "New York Knickerbockers"]}, "target": "['Sacramento Kings', 'New York Knickerbockers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the oldest team in the nba?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Boston Celtics"]], "filtered_resps": ["Boston Celtics"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9733d5546b7e31506d13197d7d1d1fa40107d3db1b7a2766b400bd7a2ab2bdd5", "prompt_hash": "b247edda7b23d1c97684ae2b25c9b26f6ee632eda07100ebb59a9027527f5088", "target_hash": "bac61a91e6451b7fb9983af0f0c73b41ef7c39a029db02b36f96f909bd7669f5", "exact_match": 0.0}
{"doc_id": 1506, "doc": {"question": "when was the electronic funds transfer act signed into law", "answer": ["in 1978", "November 10, 1978", "1978"]}, "target": "['in 1978', 'November 10, 1978', '1978']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the electronic funds transfer act signed into law?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1978"]], "filtered_resps": ["1978"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cff2afe347a79ee1ba2d5bf900adaa12fa16e98fea9366eb41f660beb88615f6", "prompt_hash": "34e1a9db6ef7031d8d86187c7d3b08b9c298d23f7120e8dfcd732e195f3656da", "target_hash": "093666c3aec784cc3ad5440fc1bfbc4c5090f9911ecd1426ea31b938e68477ce", "exact_match": 1.0}
{"doc_id": 1507, "doc": {"question": "who won the election for mayor in boston", "answer": ["Marty J. Walsh", "Marty Walsh"]}, "target": "['Marty J. Walsh', 'Marty Walsh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the election for mayor in boston?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Marty Walsh"]], "filtered_resps": ["Marty Walsh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55e395bce73d8e246b84aecac73a3875bc07c0888708deebf6347ff6d60d6452", "prompt_hash": "b76401338a2255858c65c31df80bd212ae5f37b01c8320348914ae66f09a032c", "target_hash": "09272cce9d965c029eb79aea8f4c9f31753d7142a57f1e652f98ca89ae9b069f", "exact_match": 1.0}
{"doc_id": 1508, "doc": {"question": "who won the first medal in olympics for india", "answer": ["Norman Pritchard"]}, "target": "['Norman Pritchard']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the first medal in olympics for india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Karnam Malleswari"]], "filtered_resps": ["Karnam Malleswari"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4eeca73957b6b4fe6749ad7441da451b586c524acc47448000238dee8d801e8d", "prompt_hash": "8e170943b2db4ff07805c1091a73704ea2fa17d91535a81303536016faef2ca8", "target_hash": "9bb50ea51d24b52b2c8e90953b8d9b571af03604f3f1508d39b7cd8bac50bf0e", "exact_match": 0.0}
{"doc_id": 1509, "doc": {"question": "who wrote the music for christmas story live", "answer": ["Pasek and Paul", "Pasek & Paul", "Justin Paul", "Benj Pasek"]}, "target": "['Pasek and Paul', 'Pasek & Paul', 'Justin Paul', 'Benj Pasek']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the music for christmas story live?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Deacon"]], "filtered_resps": ["John Deacon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "416b2254229fc4c941c438c0e2b98ac90f859ebf3dc059a0c09701d989d0fea4", "prompt_hash": "5e028901f8eba4371b0aa0ed27a9f4585c6a7914b5070befd9efb8425697163d", "target_hash": "0013803dbc6afba9858c95ae8dade8839e9ae7593f1574955af6355e16750a3d", "exact_match": 0.0}
{"doc_id": 1510, "doc": {"question": "first jnanpith award was an autor of which language", "answer": ["Malayalam"]}, "target": "['Malayalam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: first jnanpith award was an autor of which language?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bengali"]], "filtered_resps": ["Bengali"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "753393296f0d7ff68f4c91da91e31ea032b8e292f8050dfa0915ced0d1d6cf6d", "prompt_hash": "4e668d74231f36c6f4c68a87a2bc1238f3988b8510d3c892f07c447621c6cf50", "target_hash": "38226e0b39a7a324771d83a4873bca5dfbfbe7469264cb5e5e9c48259f142c31", "exact_match": 0.0}
{"doc_id": 1511, "doc": {"question": "which is the ring finger for male in india", "answer": ["the left ring finger", "left ring finger"]}, "target": "['the left ring finger', 'left ring finger']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the ring finger for male in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pinky"]], "filtered_resps": ["pinky"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d164b7d8f48c441a72c3353a38c6ba2b3a81cad22984c24cbc82692088e8e00", "prompt_hash": "e90605825419e85dddb293ffb2ce17c41e78ac11de9a0303036630c4323905c5", "target_hash": "e031489b73a9fa7768a9fc13a34b986f3058b22bc1651f007c2b6eb99945dc3b", "exact_match": 0.0}
{"doc_id": 1512, "doc": {"question": "which condition would most likely require nutrition delivered through tpn", "answer": ["bowel obstruction", "ulcerative colitis", "high-output fistula", "short bowel syndrome", "very severe Crohn's disease", "prolonged diarrhea", "gastroschisis"]}, "target": "['bowel obstruction', 'ulcerative colitis', 'high-output fistula', 'short bowel syndrome', \"very severe Crohn's disease\", 'prolonged diarrhea', 'gastroschisis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which condition would most likely require nutrition delivered through tpn?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" liver failure"]], "filtered_resps": ["liver failure"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f84ee41d6a222f0afbbf0adc8cf4b683ae3c66890cc6393eb41873ebdc5d2713", "prompt_hash": "f27fceb67e8e621e33ec3af06867a5c3dd48830f08c9699dae3801bc7f0a7ad8", "target_hash": "6d78be4617526e88f3b99cad1aceed22037312b0c4aa4a6eb092bd7550089a8b", "exact_match": 0.0}
{"doc_id": 1513, "doc": {"question": "who does the voice of stewie family guy", "answer": ["Seth MacFarlane"]}, "target": "['Seth MacFarlane']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of stewie family guy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Seth MacFarlane"]], "filtered_resps": ["Seth MacFarlane"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0b07f93f7756e845a8ee46196f52d50f2ccbf25f8ab6b0742a379ceb9da2cd29", "prompt_hash": "b97460b180a0ce9ba7ca277e2a4f540aea0c13c9b860cf24aa44bdd554e93872", "target_hash": "03cb497c67b8073ade363bf704d949c1307809f7569340cc720f85bac921726a", "exact_match": 1.0}
{"doc_id": 1514, "doc": {"question": "what is the name of the restaurant in seinfeld", "answer": ["Monk’s", "Monk's Café"]}, "target": "['Monk’s', \"Monk's Café\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the restaurant in seinfeld?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cosmo's"]], "filtered_resps": ["Cosmo's"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f278ba7ef643e24b8dd8a8ebbb6ce05ee347121079621595b58d3f3e050def9a", "prompt_hash": "c1f01e04f40f48edda040965457a3deb072521db325da76a0d8ae7f4a4d275e9", "target_hash": "a11226e4711ffbfd166828c25d1a3ec015466ff3ad59e21bc8ab96606ff9266b", "exact_match": 0.0}
{"doc_id": 1515, "doc": {"question": "who played david on the assassination of gianni versace", "answer": ["Cody Fern"]}, "target": "['Cody Fern']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played david on the assassination of gianni versace?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Steven Van Zandt"]], "filtered_resps": ["Steven Van Zandt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "920c1511b391d6b599ba15ed869ad7bcd59ce7752a7f7a768e6d93ac71f2322a", "prompt_hash": "ce552f92f2288c1b672ff024f7f6c9616a72030487b5a680ac236c247ecb2dc2", "target_hash": "a85657b87c5397df998d89cd3779f4c1518d74fd68af2bb00ca4c3289c0f006f", "exact_match": 0.0}
{"doc_id": 1516, "doc": {"question": "what is the democracy of the united states", "answer": ["federal republic"]}, "target": "['federal republic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the democracy of the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" representative democracy"]], "filtered_resps": ["representative democracy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91bf319ece18ae20b8a61ca15b603a63fac7b6387b50336ade4ace434a6a3040", "prompt_hash": "a2f1f488b275f35081aa6a27b2334de63a1bd3983aeeff9fb7c695c776cc8bf4", "target_hash": "6c9b0d7ed71e985d45a913998e63d7e2b06c82cf822fbe054651ab4f7c602402", "exact_match": 0.0}
{"doc_id": 1517, "doc": {"question": "who plays grace in the secret life of the american teenager", "answer": ["Megan Park"]}, "target": "['Megan Park']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays grace in the secret life of the american teenager?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lauren Graham"]], "filtered_resps": ["Lauren Graham"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9ad2f0005d27389c959bb1dfa428bcb7e8686ff817e72658ea978ae5a71e775", "prompt_hash": "eb5850a42108997ea776fd4df24a852fc2eda497bc1c0fcbfc1eace17e4b04f0", "target_hash": "ef785a5bfaeddf1200060230a8c9aeb1160ac65ca231347a2cd56bb838148fa0", "exact_match": 0.0}
{"doc_id": 1518, "doc": {"question": "where did the rulers of the qing dynasty originate", "answer": ["Manchuria"]}, "target": "['Manchuria']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the rulers of the qing dynasty originate?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Manchuria"]], "filtered_resps": ["Manchuria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "da03ff0252b562c8fa7269881d91218e98ea2277667c04c20543719d9c82eed9", "prompt_hash": "543469ad627fd5005a4cbb45d9452303fa13adc0f7429bb00621b824f063da03", "target_hash": "6b4a6ddfcb97a095ce525ba9f241b33ff21b91a76c513522ba406502ed03b7ed", "exact_match": 1.0}
{"doc_id": 1519, "doc": {"question": "in what state was the united states center of population gravity located during the period 1790", "answer": ["Maryland", "Kent County, Maryland"]}, "target": "['Maryland', 'Kent County, Maryland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in what state was the united states center of population gravity located during the period 1790?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pennsylvania"]], "filtered_resps": ["Pennsylvania"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7962d24fcbcc04debb2ce03d91681e59ef14b79b99f4baa1274ad211eaf46bda", "prompt_hash": "591f3b0ffaa737450954efd112392b43a90ba173308049827314608126c1cc82", "target_hash": "b4f0b0c3241ffe76a5a37f7614fbd81b2850ff0695219c3754114f420d20b9d8", "exact_match": 0.0}
{"doc_id": 1520, "doc": {"question": "where do the royalties for winnie the pooh go", "answer": ["the Slesinger family", "Slesinger, Inc."]}, "target": "['the Slesinger family', 'Slesinger, Inc.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the royalties for winnie the pooh go?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Save the Children"]], "filtered_resps": ["Save the Children"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b63348e9efb50074fa933fcb6de0c3e4ee9cf7fc8365c5764214541b53ccb79", "prompt_hash": "ae051ed05f08caf26cc43a28fc32767b34662d9ed9aa90e1494ef7235a8ddba4", "target_hash": "533f2f0471b33f9e7c093289c02090002d98685f940bfd2ec8fd4a891f4ec527", "exact_match": 0.0}
{"doc_id": 1521, "doc": {"question": "when is the new season of chicago pd", "answer": ["September 27, 2017"]}, "target": "['September 27, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the new season of chicago pd?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 10"]], "filtered_resps": ["January 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "93fe659614a6b574da7c8e5a9cc44869b3413cf5aef174aa30fc271bf658dbcf", "prompt_hash": "f8e8d891781f873d88e4b3ca3d47ed0c3a512405ce0eb5c9f9f2c6d1addb3c83", "target_hash": "5865d73804e6229eb21de1665d8c77848d9ab9b6383c5c40a2dd429d2f31e471", "exact_match": 0.0}
{"doc_id": 1522, "doc": {"question": "who owns the majority of the us debt", "answer": ["China", "Japan and China", "Japan", "the public"]}, "target": "['China', 'Japan and China', 'Japan', 'the public']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns the majority of the us debt?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" China"]], "filtered_resps": ["China"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e46159caa7dcbf73a3cd411fc7f3790aced0d4a583db24b438914c5d0211719d", "prompt_hash": "ce11940180a4765805b7047048ba9c804567704850d1f416be6b9e3092ac9afc", "target_hash": "20ed4e71435e80befd7a27dada4551fdb464305715a9fd405d113736dce5e80b", "exact_match": 1.0}
{"doc_id": 1523, "doc": {"question": "who won the most on who wants to be a millionaire", "answer": ["Kevin Olmstead", "David Goodman"]}, "target": "['Kevin Olmstead', 'David Goodman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the most on who wants to be a millionaire?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Regis Philbin"]], "filtered_resps": ["Regis Philbin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a1178e24a8d7ddb00dfc151fc4893cb86756ca43dfefec7163f8a0c19bba237", "prompt_hash": "fe9b9a033641ae91bb8ed0fc388c2caae3e1bc5d661e1de82e1cde4e862099bf", "target_hash": "cb4f89eabec081c5359531d6f1e67649fcdb0f79107b99c0369f83fe8dda8c21", "exact_match": 0.0}
{"doc_id": 1524, "doc": {"question": "who does the voice of ash in sing", "answer": ["Scarlett Johansson"]}, "target": "['Scarlett Johansson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of ash in sing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew McConaughey"]], "filtered_resps": ["Matthew McConaughey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6c11e870db08d65bc11d3555416d4960fae63c0de923f3bbf4946306832c614d", "prompt_hash": "7fa8e5714cfed2b299e187662d787d32b4696534f45f0a01ceceab8c4c9d89bf", "target_hash": "bd499f55a8893eb44eea7b7ee1cfd26a76b6ed54dc3833e929e6cf981ced6ec9", "exact_match": 0.0}
{"doc_id": 1525, "doc": {"question": "who was tammy from basketball wives married to", "answer": ["basketball player Kenny Anderson", "Kenny Anderson"]}, "target": "['basketball player Kenny Anderson', 'Kenny Anderson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was tammy from basketball wives married to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jason Telfair"]], "filtered_resps": ["Jason Telfair"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6fd7ce2a8938f0dbb20487891cd84522a8fbe9101575eec67eb091d0a7b85eb9", "prompt_hash": "f8c26716fcc7a0f289a93cccd7996f79bea4ded17eb8c82a58aecc39eed07883", "target_hash": "226370c5d15765222afcba4aa02595d4d97c95863d1fc92a76921ab45dd53de4", "exact_match": 0.0}
{"doc_id": 1526, "doc": {"question": "who sang original i want a hippopotamus for christmas", "answer": ["Gayla Peevey"]}, "target": "['Gayla Peevey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang original i want a hippopotamus for christmas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gayla Peevey"]], "filtered_resps": ["Gayla Peevey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "45cde050e5d1d283d5a5d593de51a9676c3cceb06174660ff702f4bf1f762abb", "prompt_hash": "a39b603a7318735d404a7d3ab9546c5b3715c1c699bacdf413976c8a745c8896", "target_hash": "29340be44afbf20afc8d1b95e5d3eb1fd1f62c3c0eef18bae2ee35d6672842fa", "exact_match": 1.0}
{"doc_id": 1527, "doc": {"question": "roman god of the underworld also called orcus or pluto", "answer": ["Dis Pater"]}, "target": "['Dis Pater']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: roman god of the underworld also called orcus or pluto?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dis"]], "filtered_resps": ["Dis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18656803db50bd2ca253ee007de5e898c3357ec41fc281c2c14526ea50c13c63", "prompt_hash": "84105fd29fdd75188f48ead45241b353db7edbcf83cbced09b5aea1d5948c57d", "target_hash": "a59999b2dff7bbaed315e2263f787f734b47a1d526684b95bb006f09200e8d76", "exact_match": 0.0}
{"doc_id": 1528, "doc": {"question": "where does the donkey talk in the bible", "answer": ["Numbers 22:28"]}, "target": "['Numbers 22:28']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the donkey talk in the bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Egypt"]], "filtered_resps": ["Egypt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "924aaf678acb280605a35febd8e2af6ef328d647ce4ea7bcb2a5d6701a304025", "prompt_hash": "a07a88092bfc5594673caaf0735c90f1ecf4a0a442cb26c439ba674390ca281d", "target_hash": "244f942c1e01899ac056c5322a4b702b53d12e922dff043bdcc217af0e038b25", "exact_match": 0.0}
{"doc_id": 1529, "doc": {"question": "when is chinese new year and what year is it", "answer": ["the year of the Dog", "Friday, 16 February", "year of the Dog"]}, "target": "['the year of the Dog', 'Friday, 16 February', 'year of the Dog']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is chinese new year and what year is it?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2016"]], "filtered_resps": ["2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "703d51790ff35b31f6486fcb78eb251a1c7885ab824b2053181705d2bf4340b7", "prompt_hash": "e1c95fc0bfe0f04942c7a03373a58396c0d84847095bb4eaaa0e64efe9984f4a", "target_hash": "d7261871360f6983035814144d957db73b4b2c168bbb6336bf9675f9252c02de", "exact_match": 0.0}
{"doc_id": 1530, "doc": {"question": "when did korea separate into north and south", "answer": ["1945", "17 August 1945"]}, "target": "['1945', '17 August 1945']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did korea separate into north and south?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1945"]], "filtered_resps": ["1945"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3ee5bc689889067ef0b8919f5f5102d61be7bc4ba903dd4f073ad3d87dea08b", "prompt_hash": "20533f95a7e0c6ee1532fc5520af22c7b890f378775bd7d21b452fd5c8e7f9b9", "target_hash": "d5ec28af1a152fb17d901965af7aaacf96784148a443b070493c10a58f7eb837", "exact_match": 1.0}
{"doc_id": 1531, "doc": {"question": "who wrote if i were a boy reba or beyonce", "answer": ["Toby Gad", "BC Jean"]}, "target": "['Toby Gad', 'BC Jean']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote if i were a boy reba or beyonce?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Reba McEntire"]], "filtered_resps": ["Reba McEntire"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c666568932fe8e5108c8f8c1cdaba509dc9c9e71fe2b5815484b8db1babaecdd", "prompt_hash": "f3b891c3197636bfd515a07812d704e1652ee45ab038e2d3678a36a5aeb67529", "target_hash": "36940a6431286970142ee75c0e08fdf04dc62512dd540c164e0abe133fa7793f", "exact_match": 0.0}
{"doc_id": 1532, "doc": {"question": "what color is the golden gate bridge in san francisco", "answer": ["orange vermilion called international orange", "red", "International Orange"]}, "target": "['orange vermilion called international orange', 'red', 'International Orange']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what color is the golden gate bridge in san francisco?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" International Orange"]], "filtered_resps": ["International Orange"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "495532f8c0df5139c39433df06eadd7ca2f96eb305fc4d0ea3b586b04439e5d4", "prompt_hash": "3f73d5e3e05a455283c530e32a9654b683b7dfc9151091c5da53e7af6a379c27", "target_hash": "8031bfb9d14e41705247d8b060fab7b1d19cb92b42c0f6ce818920aae65e1f14", "exact_match": 1.0}
{"doc_id": 1533, "doc": {"question": "who played jennifer in back to the future", "answer": ["Claudia Grace Wells"]}, "target": "['Claudia Grace Wells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played jennifer in back to the future?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Parker"]], "filtered_resps": ["Jennifer Parker"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "81c0afe013f2d0effe9171559d69bf084fb65e74e20817008bcc9993d5c4cd53", "prompt_hash": "0ffc3bb122d51cbe0d1ee5bcdeb2fecc23be81469d484c5966767f05c869433f", "target_hash": "f64274b0b06f51784218074d9d72783d8d3618356d0f21c868f7defeb847e43b", "exact_match": 0.0}
{"doc_id": 1534, "doc": {"question": "where was the italian episode of everybody loves raymond filmed", "answer": ["Anguillara Sabazia outside of Rome"]}, "target": "['Anguillara Sabazia outside of Rome']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the italian episode of everybody loves raymond filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Italy"]], "filtered_resps": ["Italy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "87728d1de64c79988edceb7b1f158539ce4bb07e142e7e9e0308b6843e73940d", "prompt_hash": "d1b6779331b0d1e2e2e2b6674465fdb2dbe2bff0b4d120134da8b2530d4cae0a", "target_hash": "fd672d4b0f1d9404d359b087e3c6b723c1f40e486d12774dea0032a04d735863", "exact_match": 0.0}
{"doc_id": 1535, "doc": {"question": "when was the last bear killed in the uk", "answer": ["c. 1000 AD"]}, "target": "['c. 1000 AD']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last bear killed in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1928"]], "filtered_resps": ["1928"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e393c5b8e82b12a0ab312d49792f7c2824948de9e1aa486502f94a6f213b82bf", "prompt_hash": "9eddb9b107045f3459d4064782096bc2b2721e3e33bed3a0b730771d5517e25b", "target_hash": "e09bf893f14df76ba9682d78c54f1db009c12370d21b6bbbe8fc79cf72576d63", "exact_match": 0.0}
{"doc_id": 1536, "doc": {"question": "who wrote the song stop the world and let me off", "answer": ["W. S. Stevenson", "Carl Belew"]}, "target": "['W. S. Stevenson', 'Carl Belew']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song stop the world and let me off?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Beatles"]], "filtered_resps": ["The Beatles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ccb4f209ac593658c2d2034f4f5d21b93a8fa6d772a7da08058f925c606025f3", "prompt_hash": "899d25474a0ca50f92fb1ba30217bfcb5ec762084e895b8c51c4b5c7e470cbb2", "target_hash": "a85d126179cca7c27731666ab8d3b4cec7bf09b2c3a8314b1f9368cfbcac677f", "exact_match": 0.0}
{"doc_id": 1537, "doc": {"question": "what was the book wealth of nations about", "answer": ["what builds nations' wealth"]}, "target": "[\"what builds nations' wealth\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the book wealth of nations about?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Adam Smith"]], "filtered_resps": ["Adam Smith"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e4b388f1051d48a1c0f625469b55f7e66d60094a9d92e79b8d79bfbd629f8684", "prompt_hash": "bcc3a75e663def453617480446808a6f8088a905b78df69e2d0a7a750cc06d39", "target_hash": "f05e7d27cd8cf5a561b101798e2033b8df0ccc5e80590eb7284144a460e41470", "exact_match": 0.0}
{"doc_id": 1538, "doc": {"question": "most consecutive games with 20+ points - nba history", "answer": ["126"]}, "target": "['126']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: most consecutive games with 20+ points - nba history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kobe Bryant"]], "filtered_resps": ["Kobe Bryant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d66844471f6206f15c261615179d52ecfd27ac6d379e53c9349a007dde904f3", "prompt_hash": "a9497eaf91c024960de1072b3be133ef00f0649f92897dd6931e8f00b3656ddf", "target_hash": "de3fa1c63e7075302dd0acabd7db750dc18a0f1e9e9c60c3b01056ab7c276098", "exact_match": 0.0}
{"doc_id": 1539, "doc": {"question": "who in germany signed the treaty of versailles", "answer": ["Gustav Bauer", "colonial minister Johannes Bell", "Foreign minister Hermann Müller"]}, "target": "['Gustav Bauer', 'colonial minister Johannes Bell', 'Foreign minister Hermann Müller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who in germany signed the treaty of versailles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthias Erzberger"]], "filtered_resps": ["Matthias Erzberger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df19f12be7352bae61f4a646fc6a889d18a675bfa1a092cf97afdb376de27488", "prompt_hash": "c6630ee633d412ffff6f14060c605242da68a42d5953b0775a76b58923b4d256", "target_hash": "ad2c8c9f55eead65dc7fc775d52482045800b6c49917abc42d1cd906e6019fd2", "exact_match": 0.0}
{"doc_id": 1540, "doc": {"question": "who was the first president of the constituent assembly", "answer": ["Dr. Rajendra Prasad"]}, "target": "['Dr. Rajendra Prasad']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first president of the constituent assembly?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rajendra Prasad"]], "filtered_resps": ["Rajendra Prasad"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ca91bde976eb51a35f673f07f0779ead3312557a57a8ca1acda4a7b5644ccadc", "prompt_hash": "75e0857164ad1c6daf2f8bddd9fe706a6241e2848fd34f6fb3edc8fd2a932f91", "target_hash": "f8ea3473e951287049bc32dd1c24f67a3ba7c7178e39a8e76025e565608fd024", "exact_match": 0.0}
{"doc_id": 1541, "doc": {"question": "what is the purse for the senior open", "answer": ["$2 million in 2011"]}, "target": "['$2 million in 2011']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the purse for the senior open?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" £1"]], "filtered_resps": ["£1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5a130913adb3502583b45a8a77d751c6d7748ea292e9e19377a7657f866caf1", "prompt_hash": "6e5f845324fab7d956774a4c0714c6e369bba22aae0eb2440ee25f3fcd6b189b", "target_hash": "8fa17c0a22c67e2fc9592b557aeacd8daaa629ed422c3de9436f7134a832b756", "exact_match": 0.0}
{"doc_id": 1542, "doc": {"question": "when did american idol end the first time", "answer": ["April 7, 2016"]}, "target": "['April 7, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did american idol end the first time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2010"]], "filtered_resps": ["2010"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8422ffc4461c8c95a40daca1a0fc134c48c0c669cade972f8edd492df57cb4fc", "prompt_hash": "85a54be836cfc91a5a972f8ccbacf6ce7c5c26c88c27e2c412131028fb9901fd", "target_hash": "b691a680661a79cbec271182ff8df845a3faba6485d1e23866e5692a3b3a3559", "exact_match": 0.0}
{"doc_id": 1543, "doc": {"question": "who played anna in once upon a time", "answer": ["Elizabeth Dean Lail"]}, "target": "['Elizabeth Dean Lail']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played anna in once upon a time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Morrison"]], "filtered_resps": ["Jennifer Morrison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "590676f0b5415961f44e1974525c912a2b91dbd262ec6311494dc094d8172bb8", "prompt_hash": "f7b3a8c993032c0e734d9919928ea025b94a9980bb6c881f5bc4d1139f892c47", "target_hash": "a4b68ba06feb65612210ec917380e72779a10d21bebd42492a7b312efd911459", "exact_match": 0.0}
{"doc_id": 1544, "doc": {"question": "who sang the original song send in the clowns", "answer": ["Glynis Johns", "the actress Glynis Johns"]}, "target": "['Glynis Johns', 'the actress Glynis Johns']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the original song send in the clowns?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stephen Sondheim"]], "filtered_resps": ["Stephen Sondheim"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "af77bad90e6cea776cff3da8c424b648fc8267cf733ff26c95c2db40d05b5e07", "prompt_hash": "3a3907e0b007f879726da5e6a97025a15a78a5c33a315a8c4a97e8bc745ccea1", "target_hash": "7a8274521ced816ae534c7de04a3185b2db38962a52497115770718bbe26be88", "exact_match": 0.0}
{"doc_id": 1545, "doc": {"question": "when was the drinking age set to 21", "answer": ["By mid-1988"]}, "target": "['By mid-1988']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the drinking age set to 21?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1984"]], "filtered_resps": ["1984"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5c4c63e0a2e89bacbf9823c890fd54c6f9eac98dbc42c3629a0eedd0ac992c73", "prompt_hash": "e295bef6223ae88601f28a95e184805940c6bdccbd99abd1a6e70057a29ac1eb", "target_hash": "3902568cf2af728287c85895e1e8d90a1539223efe1406189e367a07b06af0d0", "exact_match": 0.0}
{"doc_id": 1546, "doc": {"question": "who produced the movie i can only imagine", "answer": ["Pete Shilaimon", "Raymond Harris", "Daryl Lefever", "Kevin Downes", "Cindy Bond", "Joe Knopp", "Mickey Liddell"]}, "target": "['Pete Shilaimon', 'Raymond Harris', 'Daryl Lefever', 'Kevin Downes', 'Cindy Bond', 'Joe Knopp', 'Mickey Liddell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who produced the movie i can only imagine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tarsem Singh"]], "filtered_resps": ["Tarsem Singh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fa8c98798fb70297fb629f9b1a34f4137fd86f3fc151d6197bb380e7a63391fd", "prompt_hash": "6eaf05b540282fa44874bcc0a9923289abbe9fd0286de7fccce8496c3fd6d7a6", "target_hash": "f0d1c19ed5109cb06e37f7e10cf3e2bbe85b01f2a0c0034f06deb9fe5a811861", "exact_match": 0.0}
{"doc_id": 1547, "doc": {"question": "what is the meaning of auv in cars", "answer": ["action utility vehicles"]}, "target": "['action utility vehicles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of auv in cars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" autonomous vehicle"]], "filtered_resps": ["autonomous vehicle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4bafd19b749120c42a09b7c453269cb67e57b703ccb2be9b939268280f8e45c3", "prompt_hash": "f2bdd2360d86dec1ed92e74781fb4dc741de98097c5621552b763b317b57a4be", "target_hash": "71438b49a55ca0b99d91f12629b733f4ba99fd0904d6789079bf1ad119315ede", "exact_match": 0.0}
{"doc_id": 1548, "doc": {"question": "who monitor the recovery of the location during a disaster", "answer": ["management team"]}, "target": "['management team']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who monitor the recovery of the location during a disaster?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" FEMA"]], "filtered_resps": ["FEMA"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a65567ab4804833fcb7e8e33b476313c07d94466e287e19914d40a7a5b66d23", "prompt_hash": "9d3f7a521c4570209908462492a24af48412c5ac0093c16fd6f938eb299e2005", "target_hash": "c1401c00b73853fe4ee22b082f58266ba3026211acc391a55b0885d65126e9f4", "exact_match": 0.0}
{"doc_id": 1549, "doc": {"question": "who wrote the treasure of the sierra madre", "answer": ["B. Traven", "bilingual German author B. Traven"]}, "target": "['B. Traven', 'bilingual German author B. Traven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the treasure of the sierra madre?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Robert Louis Stevenson"]], "filtered_resps": ["Robert Louis Stevenson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a34b114a5856de95c5734efa6c8771d4258b779d6df71a729d80bcc7cf05d55b", "prompt_hash": "092e9d26c7d7ef0b93e9b2cf71646e7b6f0cd2df41f0f5ddac7f7d1111b86566", "target_hash": "34cc8a747e197e878a655fa266e00d226f230c82c4bd93ed1302b8c150fde7d4", "exact_match": 0.0}
{"doc_id": 1550, "doc": {"question": "the first line of http request message is called ____", "answer": ["A request line", "A status line", "the status line", "status line"]}, "target": "['A request line', 'A status line', 'the status line', 'status line']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the first line of http request message is called ____?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Request-Line"]], "filtered_resps": ["Request-Line"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ac38add8c2e0a347f80674909ca77fc31a45bf82af791308ed335e0acbf4ed30", "prompt_hash": "b34b3ad2c576e962327fdcabd4d2f687eb7793eb7de7fbc200399ac9d15744cf", "target_hash": "67b586f65e44da65c8487190e4189f4895d0736c0deeb5465713215dd647ca4a", "exact_match": 0.0}
{"doc_id": 1551, "doc": {"question": "who hit 6 sixes in an over in test cricket", "answer": ["Ravi Shastri", "Garfield Sobers"]}, "target": "['Ravi Shastri', 'Garfield Sobers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who hit 6 sixes in an over in test cricket?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stuart Broad"]], "filtered_resps": ["Stuart Broad"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a964141e90b29d74f806e52649a74998bde582ebb9d2122f5b71439b4f7a5416", "prompt_hash": "008b57ae576846b03c1fdf628c9b65f4602b76c5a13e95804e82135810a337b9", "target_hash": "d5cf0f818fdac4c56a898b2bc2b26bb34f941de859de301035d58b1d3e4e6028", "exact_match": 0.0}
{"doc_id": 1552, "doc": {"question": "when was the internet introduced to the public", "answer": ["September 1993", "1995", "in the very late 1980s"]}, "target": "['September 1993', '1995', 'in the very late 1980s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the internet introduced to the public?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1991"]], "filtered_resps": ["1991"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7d3ef44543c914bd405704ff94af473cfeb65e15653d39eda677a670965196fb", "prompt_hash": "809a52e94a3e5792cb2831339fcc61231810975f68212b15be89dcad953f9158", "target_hash": "27172f8b73121025810038d7f0e040fc6016ae11c3c161f73da17691cd226064", "exact_match": 0.0}
{"doc_id": 1553, "doc": {"question": "in texas how is the commissioner of insurance placed in office", "answer": ["Appointed"]}, "target": "['Appointed']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in texas how is the commissioner of insurance placed in office?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" gubernatorial appointment"]], "filtered_resps": ["gubernatorial appointment"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "10acb7a70ebee727755d98316713b63dec28005674ed59e849a68df537a3e724", "prompt_hash": "7d2d241272d2a11a7832a10586e52eb1d105fc3e99f7ba8284eb77e861a88a49", "target_hash": "f21bf0a7db49eba811804a3f05eaed31727c088246dc2dfe3d6d59383f8d3d03", "exact_match": 0.0}
{"doc_id": 1554, "doc": {"question": "when did the first fleet arive in australia", "answer": ["18 January 1788", "1788"]}, "target": "['18 January 1788', '1788']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first fleet arive in australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1788"]], "filtered_resps": ["1788"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6dc7585e7799a3d4f8ba6478e0598df44875292bc0e9a6533e7e8f0cf7ea0266", "prompt_hash": "57969541f8fb01c6f36d0a9a41b444ae70f3b022f9a66b65809c75142c692501", "target_hash": "0cbdf87324f742710b507530be3f79c7c5179a02423d3ea5af399c5f98e9b7a3", "exact_match": 1.0}
{"doc_id": 1555, "doc": {"question": "who sings so come and dance with me jai ho", "answer": ["Mahalaxmi Iyer", "Vijay Prakash", "Tanvi Shah", "Sukhvinder Singh"]}, "target": "['Mahalaxmi Iyer', 'Vijay Prakash', 'Tanvi Shah', 'Sukhvinder Singh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings so come and dance with me jai ho?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" DJ Snake"]], "filtered_resps": ["DJ Snake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1ca571eab89378259cc274780270f7c9bcabc28fd6a36a0dc0e77c1ae7443e31", "prompt_hash": "bf698e222046304d27de58aa4ad55f0c5a35046dfc78dfb4992964ae88dd838b", "target_hash": "f332a4b5de2444469421187a92209d64d13b806c215605faf4a7e1f7dd638790", "exact_match": 0.0}
{"doc_id": 1556, "doc": {"question": "this inventor co-created the film fred ott’s sneeze", "answer": ["Fred Ott", "Edison"]}, "target": "['Fred Ott', 'Edison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: this inventor co-created the film fred ott’s sneeze?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Eadweard Muybridge"]], "filtered_resps": ["Eadweard Muybridge"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7f2118bade4e9b70a17c4dc2ceeb302e73db7a27275c79044508e2ad306e0a2e", "prompt_hash": "1db83cbcb493fa8c7bbdc69de170048d1eb6ea5a96652f7e60d96a7c4a28824a", "target_hash": "fd58ef0eb0711635f9578c7e257ee3b5af88d8ce763c7905219ddde49f95c088", "exact_match": 0.0}
{"doc_id": 1557, "doc": {"question": "which city is called the manchester of north india", "answer": ["Ahmedabad"]}, "target": "['Ahmedabad']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which city is called the manchester of north india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kanpur"]], "filtered_resps": ["Kanpur"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "530cdf83a22f58c935aae54ac8345a5c33a1c766fff9842909ec84f2bdc802eb", "prompt_hash": "92b97dae6229ae3bbe658a9b74405518ef412baac4cfc674cd87402032711bcc", "target_hash": "52ae5827296d62e93ded21545046f1d5d0d52198561602fc08fc0e08e6bb6f23", "exact_match": 0.0}
{"doc_id": 1558, "doc": {"question": "what was the first video game for nintendo", "answer": ["Magnavox Odyssey", "EVR Race"]}, "target": "['Magnavox Odyssey', 'EVR Race']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the first video game for nintendo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Donkey Kong"]], "filtered_resps": ["Donkey Kong"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "575c967ab89c331c76f58d730e51bcc6de8dc85ea638af06e6f887ebb0fbd1a3", "prompt_hash": "c2fc5a5540fb9c3a2cc88e449916347168421056a1ff187c65fccb6a3900a028", "target_hash": "4a0cb738e6a750f4e23d52520f0baa6bc519a96d58ed4be21c48d133aa8d0fe3", "exact_match": 0.0}
{"doc_id": 1559, "doc": {"question": "who sang the american anthem at the super bowl", "answer": ["Pink"]}, "target": "['Pink']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the american anthem at the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "81c9b55757d0d4f53ba5d8ff7974f0ebf1a6cb0ecea52d8b3ad730dbf0ae1428", "prompt_hash": "ff2ef4574aef307d77f336eea8c7ccafa5d295a0547ab28a6fff92112e4f41f4", "target_hash": "491e226c43d39720230f8307c51a67ba28b0b68501d88129e96093803b7f227e", "exact_match": 0.0}
{"doc_id": 1560, "doc": {"question": "seven union territories of india and their capital", "answer": ["Chandigarh — Chandigarh", "Daman and Diu — Daman", "Lakshwadweep — Kavaratti", "Puducherry — Pondicherry"]}, "target": "['Chandigarh — Chandigarh', 'Daman and Diu — Daman', 'Lakshwadweep — Kavaratti', 'Puducherry — Pondicherry']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: seven union territories of india and their capital?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chandigarh"]], "filtered_resps": ["Chandigarh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1fb7a4d510365b40318420442c4f7fecdb1fdd97801226685dfb7b3853c236a0", "prompt_hash": "f6a37e72ea82ba4f364c7fdbb59b72a07b5402ca51d136da82cb6f0c452d4176", "target_hash": "62210b895b99b2030ae6e41256bb3b99d9055e31158e14e931393b54f65d5007", "exact_match": 0.0}
{"doc_id": 1561, "doc": {"question": "mount everest is part of what mountain range", "answer": ["Himalayas"]}, "target": "['Himalayas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: mount everest is part of what mountain range?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Himalayas"]], "filtered_resps": ["Himalayas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c21a0a7abfcf5660fe836c449e903118352ac1318c313e79ecf7fb2a2038acf8", "prompt_hash": "4ba84bc6750d88888f2a5952b6b1e92f444c8ab7a62182fa9e12c89a0eb80e5e", "target_hash": "91a817f89904ef652c2222747f3e7f766e78ef5254ccfc2af3c0a67c409591b9", "exact_match": 1.0}
{"doc_id": 1562, "doc": {"question": "how many us states are commonwealths and which states are they", "answer": ["Kentucky", "Pennsylvania", "Massachusetts", "Virginia"]}, "target": "['Kentucky', 'Pennsylvania', 'Massachusetts', 'Virginia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many us states are commonwealths and which states are they?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pennsylvania"]], "filtered_resps": ["Pennsylvania"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d3f818fd1b55145ccb10416d6b8cf7920024bc21fb2d3ab68f1e2fcb241f20f", "prompt_hash": "8b04bcfad140ebc07a6d948fb0de9fb9922c2daff42f9c554b4ebd618b0b5ccf", "target_hash": "673ea4c421e70a02b0bb9425039475ead7d1d312e2c1d7b0fb842d702a2d6e4c", "exact_match": 1.0}
{"doc_id": 1563, "doc": {"question": "where does the river irk start and finish", "answer": ["east of Royton", "the east of Royton"]}, "target": "['east of Royton', 'the east of Royton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the river irk start and finish?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Manchester"]], "filtered_resps": ["Manchester"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db7c870ea96fb221395d6598e0ac33f23d3d27e0294ea31ff6068c724c1beade", "prompt_hash": "91b641b1b9477c4fd58c5e9335d4a2a2b885ab69bc88787d3b62cea1042ab7a6", "target_hash": "c0852faaabb0b2684540d624d159aa7861b97cf1feee30c4e5b150034e08150a", "exact_match": 0.0}
{"doc_id": 1564, "doc": {"question": "what was the most important new religious figure of the new kingdom of ancient egypt", "answer": ["Aten"]}, "target": "['Aten']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the most important new religious figure of the new kingdom of ancient egypt?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Akhenaten"]], "filtered_resps": ["Akhenaten"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1b114d1adbf39fa4d9df895881e77f31b625056f86d83e9d9a1f753060d75681", "prompt_hash": "47a6002269ee055231e4f72ca851b7b1332d5f404ef6b27ace3587c52c8575f9", "target_hash": "2d260e47c19aace65565338138ed5e9e65450fd9bcad55c1501b61c663739f34", "exact_match": 0.0}
{"doc_id": 1565, "doc": {"question": "what is the average salary for a us congressman", "answer": ["$174,000"]}, "target": "['$174,000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the average salary for a us congressman?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $174"]], "filtered_resps": ["$174"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a58fa430f4c92c7b996742695dfae1db41330c158bf22d61558a83f375f29634", "prompt_hash": "12b0fb276da2d09412772103feb2d2186e915e8463998675ec8f259ff2f418a6", "target_hash": "09cb12d762738227a420c4cb5faee7d01136dea1370ba36640464794a4147d86", "exact_match": 0.0}
{"doc_id": 1566, "doc": {"question": "what was the full name of the titanic", "answer": ["RMS Titanic"]}, "target": "['RMS Titanic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the full name of the titanic?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" RMS Titanic"]], "filtered_resps": ["RMS Titanic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "73cc0d498986cf1b9a3ffb2e65cc8ab48509c5da843299d16b107556991b0419", "prompt_hash": "edfe905bfcdd35066eb3fb79277ba40db5051dfafb56680bb7c30fd034ffa2e6", "target_hash": "a24760e0af295f6ed51f76c9bd13e9d5c6e191e18fa1f598d4ef3648132a8a2d", "exact_match": 1.0}
{"doc_id": 1567, "doc": {"question": "who wrote the book my country my life", "answer": ["L. K. Advani"]}, "target": "['L. K. Advani']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the book my country my life?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Zhou Enlai"]], "filtered_resps": ["Zhou Enlai"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "325c31579de827bf89fc1c5906a21a16ee34671ea9a28cc54caa78d51d146690", "prompt_hash": "11fd614a8c8c4139a1bcc9cd68848eb42632fb7179902adc47f699a3409d6aa7", "target_hash": "2e78952916643a33b61dfa4572762dc5cdfb9939656213362126b148b31977be", "exact_match": 0.0}
{"doc_id": 1568, "doc": {"question": "winner of first series of great british bake off", "answer": ["Edd Kimber"]}, "target": "['Edd Kimber']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: winner of first series of great british bake off?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nancy Birtwhistle"]], "filtered_resps": ["Nancy Birtwhistle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bc57596efcf43c95e7d8cec878700e2eeccf8029206d658b16b064c597e0f47c", "prompt_hash": "f87ceece578071312d9d07a733e517ec805808bf479a7864e1149a0cbafa549e", "target_hash": "c29a42a4d347d3bce938569cc71034a22dd7c1bce9c9ce7b34f95fcf308646b1", "exact_match": 0.0}
{"doc_id": 1569, "doc": {"question": "how many rooms is there in buckingham palace", "answer": ["775", "775 rooms"]}, "target": "['775', '775 rooms']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many rooms is there in buckingham palace?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 775"]], "filtered_resps": ["775"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2fcc9bc0490ceab04bc3ee1d84849cf743a947b180de1633d2f2536e8becb3ce", "prompt_hash": "4c200acf670c3f23e488f3a45c89a389e9b3b3163d3ec118ccbf707d83404406", "target_hash": "a7d0fd2f0898de2e86af5f41fa3262b88ea48423da03c01a876031a6cca5bff1", "exact_match": 1.0}
{"doc_id": 1570, "doc": {"question": "who plays the dad in the new beauty and the beast", "answer": ["Kevin Kline"]}, "target": "['Kevin Kline']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the dad in the new beauty and the beast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kevin Kline"]], "filtered_resps": ["Kevin Kline"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6896744a6d4d1a614cb297734209cad29ee861091b5875c7ca4bdd0668b73836", "prompt_hash": "2b932ba9b86ba5ac8a2a98ef53de2aef2e201e3edeb16f0fde83db626d1a60ef", "target_hash": "a105d27051ddd855695bf28e628be7c8cd5d2bec066ef3c7e91fd0fe897e829c", "exact_match": 1.0}
{"doc_id": 1571, "doc": {"question": "when does season 3 of the riveras start", "answer": ["March 11, 2018"]}, "target": "['March\\xa011,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 3 of the riveras start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 11"]], "filtered_resps": ["January 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "24c766348b567d813372520914bd7aa59f91d6054fef4dce7f38ff8464307f9c", "prompt_hash": "058f4353a7caeace6a6d5f3edf1493e7a09e60aa05904a31f34b05dcc4777c69", "target_hash": "f591869194a7fc6ff2980a322adef639da74537dbc82e0d6d6f38cd95ccebb19", "exact_match": 0.0}
{"doc_id": 1572, "doc": {"question": "who has won more national championships michigan or ohio state", "answer": ["Michigan"]}, "target": "['Michigan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won more national championships michigan or ohio state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michigan"]], "filtered_resps": ["Michigan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c7f9da8fb0c6d6abff3a0b6cc2772a3e9e447474bb9ed3472d7aea50e7afff2", "prompt_hash": "1f976d6ab5cee68808628b052a75bb33fa40ad307499a8ccc44e631a187ca152", "target_hash": "e74655ceb00ecd29195097df2d77eb21e2220645ea21913b8e2d1335882f4c86", "exact_match": 1.0}
{"doc_id": 1573, "doc": {"question": "what happened to the actor who played frost on rizzoli and isles", "answer": ["committed suicide", "Lee Thompson Young"]}, "target": "['committed suicide', 'Lee Thompson Young']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what happened to the actor who played frost on rizzoli and isles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" died"]], "filtered_resps": ["died"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "51351340fc29e9b07ec5ca1f7ad5c6bd962d6d3bbdc059b6edb2a636933fb663", "prompt_hash": "2222f25d04196217bc663f379e87d17b6f21398e281a9451650d1a99c01b03fa", "target_hash": "935c6d15270557a238770a44fc40c63bf90ddfd4d2bc1a9c08cf9f9c11bc80d5", "exact_match": 0.0}
{"doc_id": 1574, "doc": {"question": "when did lynyrd skynyrd's plane crash happen", "answer": ["October 20, 1977"]}, "target": "['October 20, 1977']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did lynyrd skynyrd's plane crash happen?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 20"]], "filtered_resps": ["October 20"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cda091efbb8aceac9985701972000ad4362c8a9a11f7c09e8f1e47b46735f685", "prompt_hash": "44780262cbc76ed009c190daccbd498a5fe8ef5ff31e13f7212f153cd96409a0", "target_hash": "59ab0fe646b1f5d9e35fa58b7ed5f6a9f7397a21b61d22d35b8bd3dd76dafe71", "exact_match": 0.0}
{"doc_id": 1575, "doc": {"question": "how many countries in the world have scouts", "answer": ["216", "169", "216 countries and territories"]}, "target": "['216', '169', '216 countries and territories']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many countries in the world have scouts?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 160"]], "filtered_resps": ["160"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7764bdc612428b704cee79ede6464de4832f96842500702670725bd34599e967", "prompt_hash": "0fd257f3d61ad7383c5c23200022f8e5431c7d3e5216d792f1bee8e1dd8960fb", "target_hash": "9771ad04629b4ad1fba55aba957d808aef07e981d3c10ca52b6292c412def79e", "exact_match": 0.0}
{"doc_id": 1576, "doc": {"question": "what is the name of season 6 of american horror story", "answer": ["Roanoke", "American Horror Story: Roanoke"]}, "target": "['Roanoke', 'American Horror Story: Roanoke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of season 6 of american horror story?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hotel"]], "filtered_resps": ["Hotel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d28989d107f7e883f2abcaedd16ae5a967b3db702be1030e28a88f5aed66cf49", "prompt_hash": "370efb7a22a6704de204f0da9e33772a67fc19ec7a89ab4c7d024e0bca0a9a57", "target_hash": "87b2add700a897cfa7e25cada770e275cf30635aa57a7e107ef759539f815912", "exact_match": 0.0}
{"doc_id": 1577, "doc": {"question": "when did dragon ball super tournament of power start", "answer": ["February 19, 2017"]}, "target": "['February 19, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did dragon ball super tournament of power start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" March 2018"]], "filtered_resps": ["March 2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6be997b43de64ec8c373eb2fa0128e7bcb449120ff92f5c122753802ef48e197", "prompt_hash": "558eee6ad1f57790fe9133442b8f45f314b847fb6887d9956a59f193b51252f2", "target_hash": "96a67e927665bce3cde893fe5201561da7760bf068de308a8bc2042ee641c44a", "exact_match": 0.0}
{"doc_id": 1578, "doc": {"question": "who plays bongos on sympathy for the devil", "answer": ["Rocky Dzidzornu", "Rocky Dzidzornu – congas"]}, "target": "['Rocky Dzidzornu', 'Rocky Dzidzornu\\xa0– congas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays bongos on sympathy for the devil?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Watts"]], "filtered_resps": ["John Watts"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d56de173a9492fbbe8f676f448e061fc55761bf67584b5347a51bc4296a5f97", "prompt_hash": "8fa2a0fcb1908d68c4d51debe9a697c56c61f79748be1b728eb158b0d646ba17", "target_hash": "f13fa98c1db1657c00f9e0d2b5eb3f9cd8c7adfd88481a923dde516c778b9d5a", "exact_match": 0.0}
{"doc_id": 1579, "doc": {"question": "who said if a tree falls in the woods and nobody hears it", "answer": ["Philosopher George Berkeley", "Charles Riborg Mann", "George Ransom Twiss"]}, "target": "['Philosopher George Berkeley', 'Charles Riborg Mann', 'George Ransom Twiss']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said if a tree falls in the woods and nobody hears it?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Henry David Thoreau"]], "filtered_resps": ["Henry David Thoreau"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7edcaa7d7b8a00e59be3fd45b3bfe865a93b9b67fabdbf4a2c18c3e10314e5b4", "prompt_hash": "5114554b973f2b1355c6a9558ef5b5fe82b0a59de25200d975a2b7d62d049400", "target_hash": "076b1647bc5c2bd477b2695bbf125cfc99d36bc7c627ae933ab4d4bda5a84dd9", "exact_match": 0.0}
{"doc_id": 1580, "doc": {"question": "who formed the indian society of oriental art", "answer": ["Abanindranath Tagore CIE", "Abanindranath Tagore"]}, "target": "['Abanindranath Tagore CIE', 'Abanindranath Tagore']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who formed the indian society of oriental art?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rabindranath Tagore"]], "filtered_resps": ["Rabindranath Tagore"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "80ea260df6f199fc6af749a962562c77ab5a431b306bc2e14f84f9e786add6e6", "prompt_hash": "874e4d2cc85c12015dbedb51b6862561fd42fe89dcc8c9cce765014e4051b8f8", "target_hash": "65af63ae3dbe55e63b4cbc8afd98d9528f8b74327f38f7ebd89f2904972f4f9a", "exact_match": 0.0}
{"doc_id": 1581, "doc": {"question": "what is the meaning of lord shiva lingam", "answer": ["reminds of the Omnipotent Lord"]}, "target": "['reminds of the Omnipotent Lord']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of lord shiva lingam?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" phallus"]], "filtered_resps": ["phallus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e6612687673f66939e3e5c7ef0f5eb3e3d000430b2a8aea10c2b9edcfc69fe7f", "prompt_hash": "2ee10bfa225067728a6b2dd0873bc748a8791021a147fb7e8942f3eac6bd0711", "target_hash": "01d2930658cf70c39463aa7385c7f0f4d7b72bbd6771a564c5a17d2faac558a5", "exact_match": 0.0}
{"doc_id": 1582, "doc": {"question": "who is the queen of hearts in alice in wonderland", "answer": ["the primary antagonist"]}, "target": "['the primary antagonist']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the queen of hearts in alice in wonderland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mary Elizabeth Mapes Dodge"]], "filtered_resps": ["Mary Elizabeth Mapes Dodge"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "16ec5b4f32f3714af0d2e8a5e532d5bd070982ca303167ebab7ca13853c06f68", "prompt_hash": "fc11377d59cd0136dc5fe49ce7e22eb8233ce08c927cdf7699d49935a5f5669b", "target_hash": "ee402793d368582b7b7963b220a68681b8e3643ab7751f12648aff833cccd478", "exact_match": 0.0}
{"doc_id": 1583, "doc": {"question": "the octet rule states that in chemical compounds atoms tend to have the electron configuration of a", "answer": ["noble gas"]}, "target": "['noble gas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the octet rule states that in chemical compounds atoms tend to have the electron configuration of a?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" noble gas"]], "filtered_resps": ["noble gas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b567baf0a0e268a4d8686e63626fb52931787c72efefddf923a7c351a44b792d", "prompt_hash": "6a9855a2b99001b6b75649249abe39592d84fcc694022ca7de69f296ba46124d", "target_hash": "50ffcfe62038a763940e83f025e504b846e9b85d758a747494de84d8d8193d71", "exact_match": 1.0}
{"doc_id": 1584, "doc": {"question": "where is the majority of phosphorous stored in the environment", "answer": ["minerals", "rocks", "in rocks and minerals"]}, "target": "['minerals', 'rocks', 'in rocks and minerals']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the majority of phosphorous stored in the environment?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" soil"]], "filtered_resps": ["soil"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "71c36a5ddd3286107a578cae2f80bc87c377c2501f94c63ef5d2683d2e8bb83e", "prompt_hash": "e841f58eac682ed7c25ab6067059afcd4fae7d85d6bcb53b1a05b6e207685ff9", "target_hash": "44ebb7d78120bd210743d151f3012c04ab52c37c7cbdb581f6ffc878dccd7ee0", "exact_match": 0.0}
{"doc_id": 1585, "doc": {"question": "where is the hotel used in the movie the shining", "answer": ["The Stanley Hotel"]}, "target": "['The Stanley Hotel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the hotel used in the movie the shining?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Colorado"]], "filtered_resps": ["Colorado"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a1722619880ec2e3900a0003f582bb8d2c25f8a21b358e8d132d5358d225d06", "prompt_hash": "0c07297bdcc2a46dffb40838e24f26d709bc6a1f0b5f393baf8ac22d5582977f", "target_hash": "421871403a41fbe3021295748b7967d43a00f6a6f82faa561a5e2ce19d623a61", "exact_match": 0.0}
{"doc_id": 1586, "doc": {"question": "who was the father of the princes in the tower", "answer": ["Edward IV of England", "Edward IV"]}, "target": "['Edward IV of England', 'Edward IV']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the father of the princes in the tower?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Richard"]], "filtered_resps": ["Richard"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fd473c9c080bcc5fb5534b971b74a4402b283862ba11e6d9ceaf1473276c632b", "prompt_hash": "7fb97322c805ef57fbb7adfa2e867f2b125a13526531b18dea4621b691a8524d", "target_hash": "de997a754d29f497a69ceefb111e76bd9c2f7e35e23c8336841e94a26d71a0cb", "exact_match": 0.0}
{"doc_id": 1587, "doc": {"question": "when was the titanic started to be built", "answer": ["31 March 1909"]}, "target": "['31 March 1909']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the titanic started to be built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1909"]], "filtered_resps": ["1909"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d96b8a512914e94d9e97aafe256bb5676ff31ab052c5b0d82aeb03b9dd526fd9", "prompt_hash": "f4a1921e7bdc96d54738c18861d21124d1709db298e4072c2637398ceb46dcdf", "target_hash": "c622b9e32ec83abcd187d235299448ea7c5b4450fabb3fa1a44fd97e7f7d3bc5", "exact_match": 0.0}
{"doc_id": 1588, "doc": {"question": "when does the first nba basketball game start", "answer": ["October 17, 2017"]}, "target": "['October 17, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the first nba basketball game start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October"]], "filtered_resps": ["October"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f4c6da41e817f1d5c349de123ff1d2e53594c345fb00597ccd0b4f5cb824ee59", "prompt_hash": "361a7f6aac9185f4b0a0b16070da7aaf4902e7c483ee5dc555b43eeee2b27ef0", "target_hash": "815f8246747ac91dd8040d9dc3e0e18097b922bd6aea98e3ca57418347370689", "exact_match": 0.0}
{"doc_id": 1589, "doc": {"question": "the german princes who chose the holy roman empire were called", "answer": ["prince-electors", "the Prince-Electors"]}, "target": "['prince-electors', 'the Prince-Electors']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the german princes who chose the holy roman empire were called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Electors"]], "filtered_resps": ["Electors"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3fd70f5d67e707d8d6c9dbf56772ab1cd218150494bb9efc8cd0613bd63a2bc0", "prompt_hash": "350991b640f8c0457401d6041ca61ff7ed6bd7adb232fe361069386c5e3c4f4f", "target_hash": "51a7bef7d776b823cfbcfa85fbf7f96aae17d026c1da5831df6e255ecd589c12", "exact_match": 0.0}
{"doc_id": 1590, "doc": {"question": "when did refrigerator perry play for the bears", "answer": ["1985–1993"]}, "target": "['1985–1993']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did refrigerator perry play for the bears?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2014"]], "filtered_resps": ["2014"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b21bbd8112f0a85da8f80f23c7be3dd418b32aeace3811abc1fb9dc09d13d52", "prompt_hash": "8bda6af1a02cda41919d7a55b768b6a3847c5f4ff1df0e16903a5f51dfcf4515", "target_hash": "6f572de87526e61ce458f269fb29d16d642ea85e229c32811ff0731c167741fb", "exact_match": 0.0}
{"doc_id": 1591, "doc": {"question": "who came up with the idea of the transcontinental railroad", "answer": ["Dr. Hartwell Carver"]}, "target": "['Dr. Hartwell Carver']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who came up with the idea of the transcontinental railroad?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chinese"]], "filtered_resps": ["Chinese"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4fcd7a6df7a1f14eaab8aa8c3923e9bbc0fa4351359a471e7487057c3163bbb4", "prompt_hash": "130fcc27a6ec2db417cc675ddea079894caa354e8a7ec50434bab523eca0a7eb", "target_hash": "96feb67a8836f79d3bdc275dad327e2422189e72ba3262836ecdbbf89ec826d6", "exact_match": 0.0}
{"doc_id": 1592, "doc": {"question": "the elements in each period have the same number of", "answer": ["electron shells"]}, "target": "['electron shells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the elements in each period have the same number of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" electron shells"]], "filtered_resps": ["electron shells"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e1aa658ca77fc7421d84cef7852f24f7130cb52416188389dbf2c63a8cfc8bc1", "prompt_hash": "eacb7a0a857c0e123816a062990591a60ec92657ecc64b45c1eaa517c79aa9f4", "target_hash": "f3ef9533d79d71f3a68b50dd6de81b77725e984f1bd516cee94ee591d9751cb5", "exact_match": 1.0}
{"doc_id": 1593, "doc": {"question": "when is the new season of wentworth coming out", "answer": ["19 June 2018"]}, "target": "['19 June 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the new season of wentworth coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5c7d5115bacc519de2da3312d4dad21952ce84cb88a9d8780c8204b0c23750a1", "prompt_hash": "122c15be94d7cce0f1adde1ead8e25ba4ba664a87dffd14b1a6a3000699922f1", "target_hash": "92fad6550dddf70efb170d56575d982610dd9e2f6ea75032e5460086f4bc33a0", "exact_match": 0.0}
{"doc_id": 1594, "doc": {"question": "leader of carthage in the first punic war", "answer": ["Hanno the Great", "Xanthippus", "Hasdrubal the Fair", "Hamilcar Barca"]}, "target": "['Hanno the Great', 'Xanthippus', 'Hasdrubal the Fair', 'Hamilcar Barca']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: leader of carthage in the first punic war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hamilcar Barca"]], "filtered_resps": ["Hamilcar Barca"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fea6e502b656b1a151a2e6568eaf88c4d2bd12470b2070ee0b1fd56c82ec5427", "prompt_hash": "a12119b9be26e5907fd6abad423b19aaf5de2ffebb7735cbea755dc901243bb5", "target_hash": "b1968e70ccd612d64422ab2af92ec5c6039f2170cc35d63f8a8a6ba2716e3f19", "exact_match": 1.0}
{"doc_id": 1595, "doc": {"question": "who won latest america's next top model", "answer": ["Kyla Coleman"]}, "target": "['Kyla Coleman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won latest america's next top model?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kandyse McClure"]], "filtered_resps": ["Kandyse McClure"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "27b84671f6527052e394ee533487d1e178d42bbf2b011292398476946269a919", "prompt_hash": "413a36a5a7188e04298632488663d72f17186981cdd99ba0d8dacb2b31520cfb", "target_hash": "a5f1e24fa7202a2778fe5239fd6cb0ee0d6712a0e8da5b12995c7c07e6194921", "exact_match": 0.0}
{"doc_id": 1596, "doc": {"question": "what is the main use of the eiffel tower in modern times", "answer": ["visited paid monument", "paid monument"]}, "target": "['visited paid monument', 'paid monument']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the main use of the eiffel tower in modern times?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" radio and television broadcasting"]], "filtered_resps": ["radio and television broadcasting"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a341937a867d5d74cc0b516a3fc1cf2648cc21ace782a1a0ffb9b998ef2d68ea", "prompt_hash": "3e8def9585e5865c32dcf9fef67b174c53dfc00d443c27ca6534a8f8cef08c8b", "target_hash": "9ac77c6244aa4a131e4bc7fe4fb15ab26fe240e49e0e65cce337652009a71758", "exact_match": 0.0}
{"doc_id": 1597, "doc": {"question": "who sang smoke gets in your eyes first", "answer": ["Gertrude Niesen", "Tamara Drasin"]}, "target": "['Gertrude Niesen', 'Tamara Drasin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang smoke gets in your eyes first?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Platters"]], "filtered_resps": ["The Platters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3178784de21d99fd125ffa6603db30f9cc637d3980e8b0bc7ee38b6643851866", "prompt_hash": "403f144b7736d412d3496e6f142d2eb825e6d15bd444e7f0432f4414d46c555c", "target_hash": "960eefe81e33dd67a20a691e10f5b29c7bb59cd9d4e25d8fb037538e2f77aca7", "exact_match": 0.0}
{"doc_id": 1598, "doc": {"question": "when did the soviet union first gain control of parts of poland and the baltic republics", "answer": ["14 June 1940"]}, "target": "['14 June 1940']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the soviet union first gain control of parts of poland and the baltic republics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1939"]], "filtered_resps": ["1939"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e1bca1501777573810e2f77c691929051460beadb338be3ab6a59d0bfe099e1b", "prompt_hash": "1a472ff424efc709f2fa5bb1a6f298919164571532ef0b381b88c2dcdd5a9da8", "target_hash": "85ab31d4cae8c093c9355894d6d6ae19aa3a8b3b16a0e8afa4e30cb2f12a1fca", "exact_match": 0.0}
{"doc_id": 1599, "doc": {"question": "who played nathan scott on one tree hill", "answer": ["James Martin Lafferty"]}, "target": "['James Martin Lafferty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played nathan scott on one tree hill?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chad Michael Murray"]], "filtered_resps": ["Chad Michael Murray"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "57e394e37e83810ca7e764149912a41c4329c3e3c34d929758118f2b7032faab", "prompt_hash": "342ecd2f2203e49ca3843002345b955c4afe2773c07e934a5a2a6e8b2c43c217", "target_hash": "5bca68e925a19a4169da68604604ecc708d6315149e31efdf12dd2e999a378d8", "exact_match": 0.0}
{"doc_id": 1600, "doc": {"question": "when was the movie the wizard of oz made", "answer": ["1939", "August 25, 1939"]}, "target": "['1939', 'August\\xa025,\\xa01939']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the movie the wizard of oz made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1939"]], "filtered_resps": ["1939"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "545633d8d3ca31b64df03ea7f8fa33cd3fbf17edfcc442dce1b9c8a6357b6cbc", "prompt_hash": "13b0a88a26839a0dd84aec653cf8b9627566abd3109e772a5cf4dc13bf84c955", "target_hash": "cec347a51b9d0504d6e0bd4485c72956d84c8195a8d4894dd8bed5463bde6e69", "exact_match": 1.0}
{"doc_id": 1601, "doc": {"question": "who died doing the luge in the olympics", "answer": ["Nodar Kumaritashvili"]}, "target": "['Nodar Kumaritashvili']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who died doing the luge in the olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Antonin Svoboda"]], "filtered_resps": ["Antonin Svoboda"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9ce93afe668bac248385d15315a9e228c6ddc3ff94f64c62e7a7c19618556e3", "prompt_hash": "3995eabba8769bae90c5e8304181365a351a78b2cc3bffb88541aba33965da9b", "target_hash": "5fd80c5680ce8f8bfd967117f04ac457fdfbedfd1b0d1a4fc522396bf478d480", "exact_match": 0.0}
{"doc_id": 1602, "doc": {"question": "who plays dr. sloan on kevin saves the world", "answer": ["Richard Masur"]}, "target": "['Richard Masur']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays dr. sloan on kevin saves the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Love Hewitt"]], "filtered_resps": ["Jennifer Love Hewitt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5f4b3808ee27aad0e4b56f6d450cbafb9f1e039269d5151cb4eeeb74019c74c", "prompt_hash": "84b6999cfa7e644fb8c02dde1756a9d4ad4aeff56b7882525b5c6748b552050d", "target_hash": "a0c38263c781d8ba747b3d79371e45f0da09182c4ef398d6faa77c178f8fb023", "exact_match": 0.0}
{"doc_id": 1603, "doc": {"question": "when was clash royale released in the us", "answer": ["March 2, 2016"]}, "target": "['March 2, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was clash royale released in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 12"]], "filtered_resps": ["October 12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "12cee1f61dacf82cde188126f585c8acb20c86363505620427ec312b653dde6c", "prompt_hash": "27f47659ab54087c5e64400c68938af6d4ae5a51ae1a1253e31e2cb4f3a491c3", "target_hash": "92622893cf3d51b0bc4f285d45cd761d6c85d5b0894c7bfc96236857e707a91b", "exact_match": 0.0}
{"doc_id": 1604, "doc": {"question": "ranjit sagar dam has builded over which river", "answer": ["Ravi River", "the Ravi River"]}, "target": "['Ravi River', 'the Ravi River']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: ranjit sagar dam has builded over which river?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chenab"]], "filtered_resps": ["Chenab"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fce2baf15f94d90149ef65c4d16ceb42103f03540195485d57400ec54ab3f1e3", "prompt_hash": "9435634adf9283b4cf9bfa9c3349857279a5e9a536492408543650a4a713a7fe", "target_hash": "034de43690b9f023478baf97dff368efc32d0951d2211aa58ab2da03fe92c141", "exact_match": 0.0}
{"doc_id": 1605, "doc": {"question": "when does the sword art online movie take place", "answer": ["2026", "the year 2026"]}, "target": "['2026', 'the year 2026']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the sword art online movie take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" during the SAO incident"]], "filtered_resps": ["during the SAO incident"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5cd5912541b5ba400a2fe404979cb7d86425a41e50752eaebe011a13e8ccf5c1", "prompt_hash": "8971cac1db8dfb69dc0f4daaddbb659cb9f903bc4ebf82a05117ff8c7731b28a", "target_hash": "bf231f441c2ad344aeee89836ce1f91771e1aa49443b823cf7e59e3c17ed73de", "exact_match": 0.0}
{"doc_id": 1606, "doc": {"question": "what was vietnam called in the years before world war 2", "answer": ["French Indochina"]}, "target": "['French Indochina']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was vietnam called in the years before world war 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" French Indochina"]], "filtered_resps": ["French Indochina"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e982c85320c70366767f3e430af64623a71d3dee9b0e6a245182ff648d624825", "prompt_hash": "a78f65212eae5bbff566f9f3f62cc3c2d1b11e51fd6ae7538285c9bb568e2b4f", "target_hash": "60763cc825ab327bebde39c8b43dc3824fb67702ab99cde3e3fbc1edc6adaced", "exact_match": 1.0}
{"doc_id": 1607, "doc": {"question": "third president of the united states of america", "answer": ["Thomas Jefferson"]}, "target": "['Thomas Jefferson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: third president of the united states of america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thomas Jefferson"]], "filtered_resps": ["Thomas Jefferson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2d6ccaa970a960e800a93c886029facc5914ec9e6688abf8fc600fc586997453", "prompt_hash": "4c4675b713ea5f7cff49bd1cf8b0d261930982a7ca8fd23c75f3e932b696e6ca", "target_hash": "42475f6d3851560323eedeea820f3a8225fc5fb4cfaa94bd56cadfe6d3c6c397", "exact_match": 1.0}
{"doc_id": 1608, "doc": {"question": "who played stumpy in the movie rio bravo", "answer": ["Walter Brennan"]}, "target": "['Walter Brennan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played stumpy in the movie rio bravo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dean Martin"]], "filtered_resps": ["Dean Martin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55c9cf17f0015f61066a31764a471d6c01f658a4832989cd9637fba363310133", "prompt_hash": "e882358bc8a63d7bf7973cffe20762ae1621bb74de6dfae860996ec267785058", "target_hash": "51e9bcf725fc2a8e09059dc42eaf27a37a09a9318e41386f19e38256ef980636", "exact_match": 0.0}
{"doc_id": 1609, "doc": {"question": "what is the dogs name in wizard of oz", "answer": ["Toto"]}, "target": "['Toto']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the dogs name in wizard of oz?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Toto"]], "filtered_resps": ["Toto"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a570a3edf2a68a298cf3ac096ad5fa419079961e63d389c0c793f56ce5574a6f", "prompt_hash": "7b17cf6d25e14feff435aee534c08bc7ba2e1135e5d59f9ddbb652355d84683e", "target_hash": "ff4f5ecfd2c22fc36e982276ecf9feedc307fb9b435a87dfcf79eab3331e3510", "exact_match": 1.0}
{"doc_id": 1610, "doc": {"question": "when does the next warrior book come out", "answer": ["April 10, 2018"]}, "target": "['April 10, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the next warrior book come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a3f875cea02ef61bdb557f7cd7eeef3e0cb061381c23676081667a8423c49c20", "prompt_hash": "6a51f5e0312d4990a8faf551166cb58459787349ba74cc8ff09a1eaf9ec7da6e", "target_hash": "d3a1f6403d6d2819a15f96fb8fb7afe4b72ada66da40f306d3a1170193dc1d38", "exact_match": 0.0}
{"doc_id": 1611, "doc": {"question": "how many senators are there in the us senate", "answer": ["100", "100 members"]}, "target": "['100', '100 members']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many senators are there in the us senate?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 100"]], "filtered_resps": ["100"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "61d3d389b727d03bc0c3597e37d5df0e04c9b6bc9b7be5391d3d6a2c00d91dfa", "prompt_hash": "fdc17bd9657f0fcbf370dd6b27009846bb3e96bbf5e9a9c4e65045fb8b2e8f54", "target_hash": "ab7307da6e7b34176d09745040a056815a380811bc3c7a94843a2e664798f0f7", "exact_match": 1.0}
{"doc_id": 1612, "doc": {"question": "who is hosting e live from the red carpet", "answer": ["Ryan Seacrest", "Giuliana Rancic"]}, "target": "['Ryan Seacrest', 'Giuliana Rancic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is hosting e live from the red carpet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kelly Osbourne"]], "filtered_resps": ["Kelly Osbourne"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "132bede81614a49d6af9d0baa37b89fd79fdc96392c49a8d8fca771c39da7032", "prompt_hash": "d864e71c9e6f3c2ac5e7504a9adaf2a0b820c049f893d215d721562e5afadb36", "target_hash": "902471ec60719509e5cc45205dcdf3075224c1122792b7f39e4e4c7dedf739d7", "exact_match": 0.0}
{"doc_id": 1613, "doc": {"question": "where do the greasers live in the outsiders", "answer": ["Tulsa, Oklahoma"]}, "target": "['Tulsa, Oklahoma']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the greasers live in the outsiders?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" south side"]], "filtered_resps": ["south side"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8350d92711f8bf2dc4d787c32d60c0fc7ced5ea33ec8909186da4c30cda2c203", "prompt_hash": "ee3a20e0033fc79b25757f23222188836ef59e0c93e7175b335e4e50994a4409", "target_hash": "640d22ceb2550f94e4aca665af6c4139d6ccc1ed9768caef84547843a0f62454", "exact_match": 0.0}
{"doc_id": 1614, "doc": {"question": "how many pitchers does a major league team have", "answer": ["five starting pitchers", "seven relief pitchers"]}, "target": "['five starting pitchers', 'seven relief pitchers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many pitchers does a major league team have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 5"]], "filtered_resps": ["5"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d95af871230c7ba57e49b8499b603beac144be1bb61505a90f74c5f1d7cd896", "prompt_hash": "c40e9fb7a0949f570a388a58b48a4cffebeea7c32505474aee0f22a0f40a3dae", "target_hash": "c9b1a69f6a3fb8c22d3771463e8baf4cbc5eb5dbd276bfe88303df671cf7407d", "exact_match": 0.0}
{"doc_id": 1615, "doc": {"question": "when does the team find out prentiss is alive", "answer": ["In the season seven premiere", "the season seven premiere"]}, "target": "['In the season seven premiere', 'the season seven premiere']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the team find out prentiss is alive?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the finale"]], "filtered_resps": ["the finale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8c86af4ae0a564529f6ceabec5ff228b0363873799168095b6cc7cecb5e5de6a", "prompt_hash": "f12a6080dffd300d065a7fe579771d770c4d116c908048d16fb4b02dd00f90ad", "target_hash": "7b9dd945ff5559f07bbb4e167be7aef3619446ca4694c7889e930586d6845af0", "exact_match": 0.0}
{"doc_id": 1616, "doc": {"question": "how many steps does the cn tower have", "answer": ["2,579 steps", "2,579"]}, "target": "['2,579 steps', '2,579']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many steps does the cn tower have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1776"]], "filtered_resps": ["1776"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "13f19098ea34b5969dc1c809951097a34ae37fe988b505a4085c8133574f1c7f", "prompt_hash": "b645e24cf28b3490da48afdcaab91966de89b25e33b5b23c7d0eb52ad4ed0c4f", "target_hash": "7ca7cf43975426298bf4bbaacd96bfdfa74fed3d47ac8d282ecfea4d5b084cbc", "exact_match": 0.0}
{"doc_id": 1617, "doc": {"question": "trick taking card game name derived from spanish for man", "answer": ["Skat"]}, "target": "['Skat']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: trick taking card game name derived from spanish for man?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ombre"]], "filtered_resps": ["ombre"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d89d522b956c88594b32c0b2563fb48dbb0e63cd728324e9a11b4911559f1d8", "prompt_hash": "89d678e378ecc887e44cae335ddc1d1f032fb09319f6f47e5725df6c6e62b0ec", "target_hash": "1b106ac77c736ed83053bd02afb947c18fe1131c256f11990b6f51eebc635338", "exact_match": 0.0}
{"doc_id": 1618, "doc": {"question": "which two regions grow most of china’s rice", "answer": ["provinces along the Yangtze River", "provinces in the south"]}, "target": "['provinces along the Yangtze River', 'provinces in the south']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which two regions grow most of china’s rice?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" southern China"]], "filtered_resps": ["southern China"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0dc524bd6b7c559d6276348c93f7fe4caa3c02fa6a88a54313bc4f10fc9ca960", "prompt_hash": "d5bd650f1a7eb88ffc8ce2d1a3ac59cecbac4893d5b673fee9365d82868c0e74", "target_hash": "42325c98d29a0fa31a50a7ad64ad7e5e29db2162304093002ed2a503301820a6", "exact_match": 0.0}
{"doc_id": 1619, "doc": {"question": "from whose perspective is the story of all quiet on the western front told", "answer": ["Paul Baumer"]}, "target": "['Paul Baumer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: from whose perspective is the story of all quiet on the western front told?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul Bäumer"]], "filtered_resps": ["Paul Bäumer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f691b47b68bcfb267790a53955c098a246e5c956d2e6b2727fce0147cf5a6d9c", "prompt_hash": "34ddafc723803a993db97d8781363cf673d846d34146a00514fa6a8ec8e39dbf", "target_hash": "ab45e99a3871180c9ad95b24706b05f1f21affc536e7390f5751ee263f781c6b", "exact_match": 0.0}
{"doc_id": 1620, "doc": {"question": "what was the meaning of the song puff the magic dragon", "answer": ["the hardships of growing older"]}, "target": "['the hardships of growing older']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the meaning of the song puff the magic dragon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Peter"]], "filtered_resps": ["Peter"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3595a5062ab32069455dd238ae7d19e32dcd9cbfab851da986cd0499b20783df", "prompt_hash": "5f51549713df5d109e301af9e07ed09f0d018bdd6ca1f171177c00dabdf1e9d6", "target_hash": "96fb7bd520b5bdbdeee81a4ee5e129117cea211c998b0ba024e2bf6697540971", "exact_match": 0.0}
{"doc_id": 1621, "doc": {"question": "a player that can shoot pass or dribble is considered to be", "answer": ["Triple threat"]}, "target": "['Triple threat']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a player that can shoot pass or dribble is considered to be?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" versatile"]], "filtered_resps": ["versatile"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d2ecd282ba0ab5c8416639ef46f70f90fd00b0f2014d5b797d193eb76fd83b8", "prompt_hash": "f7363b7d64a14b415ded073a30abbdaf6a1cb3a4dd3e1a51a062a4bd299f527d", "target_hash": "3203e626bb2ab8899d60011c197809ab807215f0893e8a6d4ec17bf7be593687", "exact_match": 0.0}
{"doc_id": 1622, "doc": {"question": "who was the temple of vesta built for", "answer": ["Vesta"]}, "target": "['Vesta']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the temple of vesta built for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vesta"]], "filtered_resps": ["Vesta"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "249a25446086c217cfa32f3fe72f4f358334bec4f2fd6f904e21ebb4687fcfd9", "prompt_hash": "93e0c2d0019929604d2603c54768156aacb1cfc0ac47808663c8f5ca91267034", "target_hash": "79b3adced5e63fe0cd943dd83cb3c51f779ac839f72a250bdb12f4c481f17210", "exact_match": 1.0}
{"doc_id": 1623, "doc": {"question": "who invented the printing press and what year", "answer": ["circa 1439", "Johannes Gutenberg"]}, "target": "['circa 1439', 'Johannes Gutenberg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who invented the printing press and what year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Johannes Gutenberg"]], "filtered_resps": ["Johannes Gutenberg"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df75523114c6cd349bc4a3bebc5ec2bbb3d298b842d79f4bf30823a3b0378b8f", "prompt_hash": "739dd10b146f42081e216135b77d8f4e05f518bd91e4a1c9839ffabd5a19e1dc", "target_hash": "66f466a8c7e2d5a89c4b6e5ef53259faaac911cec2d8f3ca3552723cfc69d49b", "exact_match": 1.0}
{"doc_id": 1624, "doc": {"question": "when was the worlds first laptop computer introduced in the market and by whom", "answer": ["September 1980", "R2E Micral CCMC"]}, "target": "['September 1980', 'R2E Micral CCMC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the worlds first laptop computer introduced in the market and by whom?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1981 IBM A: 1981 IBM"]], "filtered_resps": ["1981 IBM A: 1981 IBM"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0acebaf2eff3e22e34af7bce907ad0f44f82634c7faecd133e83d99ae0ae0a87", "prompt_hash": "d8b90f93dffe3c0d168ec778fb755c2f783661e8f58bc3196bc9050d1829562b", "target_hash": "c4460198868801433e94980c7612c7dc066ff129460605aa61d8014d9b6754d4", "exact_match": 0.0}
{"doc_id": 1625, "doc": {"question": "how many states have a cracker barrel restaurant", "answer": ["43"]}, "target": "['43']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many states have a cracker barrel restaurant?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 48"]], "filtered_resps": ["48"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e106c8c268ba85c9169580aedf36f5d4c2366b4e64f798dbc08b7473b1f78942", "prompt_hash": "018e720a4681ef065d501f90020dbd272c0b8149f9ad1fd141aefffa8f9cd26d", "target_hash": "a1b09e2d08af6bd539e755f2ef69fa47657360ed4a2c24f636e6b426f21d2f4d", "exact_match": 0.0}
{"doc_id": 1626, "doc": {"question": "when did new york stop using the electric chair", "answer": ["1963", "1972"]}, "target": "['1963', '1972']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did new york stop using the electric chair?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2019"]], "filtered_resps": ["2019"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e2f5dfb470d2d9a2f059d9059288ab0e571c16683da03976819b23bbf016fa7b", "prompt_hash": "3cb100dc693201c95487d23c2e2a22eef184aceee3f6e6041787bd09aafbd22b", "target_hash": "36e2c5cd86c1ec074cff5e3e9178ecb1da0947ffca778545382d2ac2ebf27a21", "exact_match": 0.0}
{"doc_id": 1627, "doc": {"question": "what is the meaning of gorakh dhanda in urdu", "answer": ["You are a Puzzle"]}, "target": "['You are a Puzzle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of gorakh dhanda in urdu?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gorakh dhanda"]], "filtered_resps": ["Gorakh dhanda"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7e95116c3b002fe6ef6a8e38b6e8782f4f44266363f629abb93cc784cd5f74a9", "prompt_hash": "7f35e20b16e43737f7a2328463dbd7d3f2d75013fc47a5e90af3fae4b7bc4c25", "target_hash": "9c666b7be831c7d7428db96c8d4435228973ef42562a1f7c632eb1d2ce15252c", "exact_match": 0.0}
{"doc_id": 1628, "doc": {"question": "who was the first english child born in north america", "answer": ["Virginia Dare"]}, "target": "['Virginia Dare']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first english child born in north america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Virginia Dare"]], "filtered_resps": ["Virginia Dare"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6761ff6e2d5854dd82d29a0ff08f0499d5a8c6e63953296d44f7452164fb3cb2", "prompt_hash": "11e3ab71d6f9f9500f61d9eccfd72c1091dab6e6c2270f2cf3ee05af7eb2398a", "target_hash": "4b1d2b1a0f73b0a2bceafede64cd41a88e67a31401987a189c3a15adda6179ab", "exact_match": 1.0}
{"doc_id": 1629, "doc": {"question": "who killed barry allen's mom tv show", "answer": ["the Reverse-Flash", "Professor Eobard Thawne"]}, "target": "['the Reverse-Flash', 'Professor Eobard Thawne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who killed barry allen's mom tv show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Reverse-Flash"]], "filtered_resps": ["Reverse-Flash"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff8e7486c5b9d898c7d2ce016c139af1a8b31fcf4ba1ed013a3723c00dcbb264", "prompt_hash": "2548818e7272bc1a1f920d242d919278d994c05503ae456a2dae03e9b0f4ac92", "target_hash": "918aea9f46e08630652eae6a2ed7ba0ad11dc204f7284a009d95867ce886c46f", "exact_match": 1.0}
{"doc_id": 1630, "doc": {"question": "when were the words under god added to the pledge", "answer": ["on Flag Day in 1954", "1954", "Flag Day in 1954"]}, "target": "['on Flag Day in 1954', '1954', 'Flag Day in 1954']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when were the words under god added to the pledge?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1954"]], "filtered_resps": ["1954"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2cb33a873dcfa1b851109e10a3fd02552f05bed5b4872f380512db5caa288195", "prompt_hash": "9e6bbedf15a7468e96afed5e9f17b4f5c75dc92c915c64e282568e203d6f7498", "target_hash": "ed5a6f73a4c876f1d42556996da76661b46f327949a3c944495fe3ecee4bdd15", "exact_match": 1.0}
{"doc_id": 1631, "doc": {"question": "how do you say zest for life in french", "answer": ["Joie de vivre"]}, "target": "['Joie de vivre']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how do you say zest for life in french?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" passion pour la vie"]], "filtered_resps": ["passion pour la vie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d69aa2dd8e2ec12f3080ddf799691d698db7ff5fbacc74edd233905231421442", "prompt_hash": "1c807f9305fde1b7ce8f9d8eb66d52121a5871b0cda416bbdf0658afa992669a", "target_hash": "724aa65699a92055cea647b7d1462a9091308e231013748ac312f91c6221bc34", "exact_match": 0.0}
{"doc_id": 1632, "doc": {"question": "who sings the song you'll never find another love like mine", "answer": ["Lou Rawls"]}, "target": "['Lou Rawls']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the song you'll never find another love like mine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kenny Rogers"]], "filtered_resps": ["Kenny Rogers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff53ba004cd088bb30eee41a763079f94d9b84a7cf0999c1eec2ed2e9ba377c3", "prompt_hash": "3376c86b6315a62dd6e122d9934d058aeaa55749e6eb265f8213b5a14591a541", "target_hash": "da30894d09509b17154a7c6fde678a285b5140da6145351c33282e0a15fa2781", "exact_match": 0.0}
{"doc_id": 1633, "doc": {"question": "who is dylan's father in bates motel", "answer": ["Norma's brother, Caleb"]}, "target": "[\"Norma's brother, Caleb\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is dylan's father in bates motel?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Norman Bates"]], "filtered_resps": ["Norman Bates"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e0e8fa0f5d02faf2a1fc3807410b8c4a6095156f310573592036775b299bfa41", "prompt_hash": "8e03c150ca51904ce923cc22ed63eb291c037dd2e2a722b84302f2d33052cc1e", "target_hash": "a08dd0d309477d4f3861aa9d01d37a954d14ca8dbff18a49595d8e887737f98b", "exact_match": 0.0}
{"doc_id": 1634, "doc": {"question": "in which country bikram sambhat the official calender", "answer": ["Nepal"]}, "target": "['Nepal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in which country bikram sambhat the official calender?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nepal"]], "filtered_resps": ["Nepal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ece1a54fa5083023a97f9762cf0c82c8fd15acb7208a3928d31f32719466faf0", "prompt_hash": "79ba2f54c7f1e52e678595235a333fdf6d97af94b50a09bb936e60d85158d4c5", "target_hash": "5eae663fa5cc1e5ca7453952659a57f448caf997168f538173bed6349a8bb36d", "exact_match": 1.0}
{"doc_id": 1635, "doc": {"question": "who was suite judy blue eyes written about", "answer": ["singer/songwriter Judy Collins", "Judy Collins"]}, "target": "['singer/songwriter Judy Collins', 'Judy Collins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was suite judy blue eyes written about?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Judy Garland"]], "filtered_resps": ["Judy Garland"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "654e9af57825722b8b45cbe3d15e093cd84a9b1d7ca04d095f585a1014d59630", "prompt_hash": "556f3d2720428df0c8e2bbba21d704be58f6801e8e23ac27867c7d0fbed319f3", "target_hash": "75acbffd3be5d223e2e799028b81be49c160f4c537bca42d3abd4d50c4980587", "exact_match": 0.0}
{"doc_id": 1636, "doc": {"question": "when does middle-earth shadow of mordor come out", "answer": ["September 2014"]}, "target": "['September 2014']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does middle-earth shadow of mordor come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 3"]], "filtered_resps": ["October 3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f02f89f09ddfea8e2e398353d7c141b0fc89845c516d43751c74dad0b76576b2", "prompt_hash": "ac28e1cd2dc9953668e808a97bfc8cd262f27af9004adddd465d6e9c5d78b7ce", "target_hash": "b79fdd003142b05f84abe11fe6c7105095559d0cdd91205679de1920f00f1c27", "exact_match": 0.0}
{"doc_id": 1637, "doc": {"question": "where is the bread plate located in a formal setting", "answer": ["at each place"]}, "target": "['at each place']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the bread plate located in a formal setting?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to the left"]], "filtered_resps": ["to the left"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a1b6f069550a39ac429dcf0ec069522f38857fb50e11acdc2442606c7c81e65a", "prompt_hash": "eaf9a08dbfc89056ee3d1c53199aede99b0b34fdd621bcbf5cd2929056489bd0", "target_hash": "8ae55fbd2d24a133745d7ac155fe2a1f56ade1a0ee7fdb80bf0f448e466747e8", "exact_match": 0.0}
{"doc_id": 1638, "doc": {"question": "who was in dont worry be happy video", "answer": ["Bill Irwin", "Robin Williams", "McFerrin"]}, "target": "['Bill Irwin', 'Robin Williams', 'McFerrin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was in dont worry be happy video?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Bieber"]], "filtered_resps": ["Justin Bieber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff6074f37d4944b69cedc8e5a6f688bb12c2454b4d613080ba25a9848bd5d2c3", "prompt_hash": "e27cdc320ce8536f194f7a62b48a764108ec2f5f6918e97bef6b9617d79fe86f", "target_hash": "e8cfb92e085392a14100a1f2847981837b47f525b623c2b570545edc824b9657", "exact_match": 0.0}
{"doc_id": 1639, "doc": {"question": "where do the secretory cells of endocrine glands secrete their products", "answer": ["epithelial surface", "bloodstream", "directly into the bloodstream"]}, "target": "['epithelial surface', 'bloodstream', 'directly into the bloodstream']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the secretory cells of endocrine glands secrete their products?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" blood"]], "filtered_resps": ["blood"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18fa8b5a82f5824d891bd8c2a251819376bf36507251a3fe7edf665f00c928fb", "prompt_hash": "e1129eced6d8291608f51918a25f64bfc2fa33b7a29e95f9bd0fbc5e11c210e3", "target_hash": "540ad85dcac14d566241866437f4d1bc0ab046d36001ee9bf0e0641734efb225", "exact_match": 0.0}
{"doc_id": 1640, "doc": {"question": "when was the last time vancouver had an earthquake", "answer": ["1946"]}, "target": "['1946']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time vancouver had an earthquake?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1949"]], "filtered_resps": ["1949"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e04a0e0cb647ecde597ff243175a3d5c125a42dc6228ee332912d8626c879ea2", "prompt_hash": "c6b7bf5aa168528035d22ae4b99da30e47f125719b7b7c1181dd737dbbde0ecc", "target_hash": "acf4cfa805447028eebe632960b3f2cdb72316115fc3db603edf8f2b37e9caf5", "exact_match": 0.0}
{"doc_id": 1641, "doc": {"question": "when does canada switch to daylight savings time", "answer": ["the second Sunday of March"]}, "target": "['the second Sunday of March']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does canada switch to daylight savings time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the second Sunday in March"]], "filtered_resps": ["the second Sunday in March"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92d0f7a6bd3f22d56900b6bc19320d678e515d7393690414309738c199b80a4f", "prompt_hash": "5bbf3092ebf82de8faba4bb120f0451573af48a6647fc0991995e8ccd34c2e00", "target_hash": "b0b0d97486547dab6147329414c8eddbe2f0295d3a511d5d9f81e086bf92e312", "exact_match": 0.0}
{"doc_id": 1642, "doc": {"question": "how much of the world's diamonds does de beers own", "answer": ["33% in 2013"]}, "target": "['33% in 2013']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much of the world's diamonds does de beers own?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 30"]], "filtered_resps": ["30"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "554ae8bd96373ca78743ccde1af5bb5be91c2f28db23bfe54f3b479ed1d0f0c3", "prompt_hash": "c82a091ba737aa6a1504b73b638274040f2922282ab58d511a40588ebd3ac87a", "target_hash": "0fc9306fb09e5292564439f31082dcef6bbaa377dd5401fd9501dec99116c37d", "exact_match": 0.0}
{"doc_id": 1643, "doc": {"question": "where did the ashes from ash wednesday originate", "answer": ["Pope Gregory I the Great"]}, "target": "['Pope Gregory I the Great']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the ashes from ash wednesday originate?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" palm Sunday"]], "filtered_resps": ["palm Sunday"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e7913d0ca748865d6217262b5fd427d63d51e430ee8813c262de790dae4386f", "prompt_hash": "61cd9705a9b4fae46cdda1f3af2c5e389ee2d7d4f7882458f846825a9898045b", "target_hash": "a336537350380491d3506396f2c272ffab795d87a1ff6c078d629d054f01859b", "exact_match": 0.0}
{"doc_id": 1644, "doc": {"question": "where is the diaphragm located on the human body", "answer": ["the inferior thoracic border", "the thorax or chest region"]}, "target": "['the inferior thoracic border', 'the thorax or chest region']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the diaphragm located on the human body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" abdomen"]], "filtered_resps": ["abdomen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "52f3aada913c19b675ddc1a774c74c5449c4908a90b4b93af6df62b7a92d8105", "prompt_hash": "6b3cdf029ee6800797c2fe5fe5dfb6d44ecf964ceda5d174aa048afec6497b4d", "target_hash": "6b67f9e3e6eee1c418b02f10f29bb2ff6354a0e8ecf319f197cf25d47f094952", "exact_match": 0.0}
{"doc_id": 1645, "doc": {"question": "who drafted the document of the un declaration of human rights", "answer": ["John Peters Humphrey, Canada", "Charles Habib Malik, Lebanon", "John Peters Humphrey", "William Roy Hodgson", "Charles Habib Malik", "René Cassin, France", "Hernán Santa Cruz", "William Roy Hodgson, Australia", "Hernán Santa Cruz, Chile", "René Cassin", "Peng Chun Chang", "Alexander E. Bogomolov", "Eleanor Roosevelt", "Charles Dukes", "Charles Dukes, United Kingdom"]}, "target": "['John Peters Humphrey, Canada', 'Charles Habib Malik, Lebanon', 'John Peters Humphrey', 'William Roy Hodgson', 'Charles Habib Malik', 'René Cassin, France', 'Hernán Santa Cruz', 'William Roy Hodgson, Australia', 'Hernán Santa Cruz, Chile', 'René Cassin', 'Peng Chun Chang', 'Alexander E. Bogomolov', 'Eleanor Roosevelt', 'Charles Dukes', 'Charles Dukes, United Kingdom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who drafted the document of the un declaration of human rights?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United Nations"]], "filtered_resps": ["United Nations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5904eef30c075322636c347f0aec8fbb304864998b4879be12900cad607f73ad", "prompt_hash": "33794cec1f7fc82f3104a0794d7562eaf9bc4fe419295773d324983bde9cac66", "target_hash": "13b410cd5c0ad1d8a7ed2bfc083eb76a39480dfd91ce0da8c02f44472db8290a", "exact_match": 0.0}
{"doc_id": 1646, "doc": {"question": "when did all night long come out lionel richie", "answer": ["1983", "August 31, 1983"]}, "target": "['1983', 'August 31, 1983']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did all night long come out lionel richie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1983"]], "filtered_resps": ["1983"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7763cd16cd9d1fb01863ffa0bb7818eb4c2fcf653dd9ed6c1c7b3821b96036f0", "prompt_hash": "c6b162280cc4ba3e20844c790760276d1a218da8d39f6ab85adbeed0e418c689", "target_hash": "2e4edb5193becc646f25744334fe17f608dabb22e16585c695518c3a2242c691", "exact_match": 1.0}
{"doc_id": 1647, "doc": {"question": "how much square feet is a basketball court", "answer": ["94 by 50 feet"]}, "target": "['94 by 50 feet']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much square feet is a basketball court?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 4200"]], "filtered_resps": ["4200"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bafd16baa879b052ac6d922ce58983a99ef8708ad2fc44ec4e00742bdd51e33c", "prompt_hash": "e7d440f6cb71236885fa56df2606405b0f8313915bbb6bd5970da84904408271", "target_hash": "fb056522b39fc65876b03157246fb3bad1fd7d1fd83c3bb611a4cd9dffd9f27b", "exact_match": 0.0}
{"doc_id": 1648, "doc": {"question": "the cold dry winds that blow over northern india in winter are called", "answer": ["Northeast Monsoon", "Retreating Monsoon", "Northeast Monsoon or Retreating Monsoon"]}, "target": "['Northeast Monsoon', 'Retreating Monsoon', 'Northeast Monsoon or Retreating Monsoon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the cold dry winds that blow over northern india in winter are called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Siberian Express"]], "filtered_resps": ["Siberian Express"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d4556b05b205f1afad8660c14e8a187403cb3aeef9c2bda605658b674d9262f9", "prompt_hash": "4c9623505a006dc1712e71e31e3e74aa734e11cd74ca3b5ab27959348c84f290", "target_hash": "5d085fb749df99023af1b449bf521eb689bedc0a269281bbae9c266a7c32493b", "exact_match": 0.0}
{"doc_id": 1649, "doc": {"question": "who sings i don't want to be lonely", "answer": ["Ronnie Dyson"]}, "target": "['Ronnie Dyson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i don't want to be lonely?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "86e9734259d0e41b8549c85a2fb28f390dbcbd749e556e3acc399085686b26e0", "prompt_hash": "6339f19802db0b431cbf008dc89ccaac829e036d3eeac371a08d9c54aae8b77c", "target_hash": "f7f6dfc9f42c84e442737095049b3ecd4fe6f6ace2338dc057a8c8595a1b6d05", "exact_match": 0.0}
{"doc_id": 1650, "doc": {"question": "who has been chosen as the brand ambassador of the campaign 'beti bachao-beti padhao", "answer": ["Sakshi Malik"]}, "target": "['Sakshi Malik']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has been chosen as the brand ambassador of the campaign 'beti bachao-beti padhao?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Amitabh Bachchan"]], "filtered_resps": ["Amitabh Bachchan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e4d4cb64d7db50d7d5047cad17b41fce68f053e81d19fa1dbb8b8ad7a7431254", "prompt_hash": "674e5e04d62a013b53148ec212c789df16a765a5824b00df03a0a039a81fe85a", "target_hash": "3e6bf9549fc38e5c34fb386ca97c4b8c9aba86f7a722d3cde7a948b8714c264f", "exact_match": 0.0}
{"doc_id": 1651, "doc": {"question": "when is the second half of vikings season 5 coming out", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the second half of vikings season 5 coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 14"]], "filtered_resps": ["January 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4a254a21e7f2e92b0481b78dc0cd953b8c841d0835b332092335e680903efb9c", "prompt_hash": "f4648cc93433fb1969431c6b9538b1da3de00d7d191ddeeb552ddc4aec4a71e0", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 1652, "doc": {"question": "original cast of natasha pierre and the great comet of 1812", "answer": ["Amber Gray", "Blake DeLong", "Paul Pinto", "Shaina Taub", "Dave Malloy", "Nick Choksi", "Brittain Ashford", "Phillipa Soo", "Lucas Steele", "Amelia Workman"]}, "target": "['Amber Gray', 'Blake DeLong', 'Paul Pinto', 'Shaina Taub', 'Dave Malloy', 'Nick Choksi', 'Brittain Ashford', 'Phillipa Soo', 'Lucas Steele', 'Amelia Workman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: original cast of natasha pierre and the great comet of 1812?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Natasha"]], "filtered_resps": ["Natasha"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c859f182a591d28d9161b5403ac18c02489677782b905efc40ab0c862f9d313a", "prompt_hash": "13657e96c2795ec6c50d3475410024b7b5fcb01a0a9e97c1809c9e6250d6ac86", "target_hash": "08e95fcece44d097e3cdf193348875e61aaedc9d392db2f9b7933826163eac0e", "exact_match": 0.0}
{"doc_id": 1653, "doc": {"question": "what's the biggest country in western europe", "answer": ["Russia*", "Russia", "France"]}, "target": "['Russia*', 'Russia', 'France']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the biggest country in western europe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" France"]], "filtered_resps": ["France"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b0d06911276582240a2877dcd63917017b8b39a3e5ff12d878e7c7e497ba8ed", "prompt_hash": "b7a00ed54e054166fd53fc58432192d9b60f8f5e333591f815e4d2a43a5e0a53", "target_hash": "25bf739cde2d96f16fd3806bf17c988663db87602948d6c24f0588da3e8d041d", "exact_match": 1.0}
{"doc_id": 1654, "doc": {"question": "when do wood thrushes fly when they are migrating", "answer": ["in the winter", "at night"]}, "target": "['in the winter', 'at night']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do wood thrushes fly when they are migrating?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" spring and fall"]], "filtered_resps": ["spring and fall"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b6f1baaf54db105b9dda467d37914bbc762882bef9cd7fd5e4a94ddc5b7a908", "prompt_hash": "548fb050a1155b8ed1f3adeb4efcec3d37033ad6b70353d4f5fb0175b804e041", "target_hash": "7ef0d9a65df0f35da1fcc198dbfda48c6856007b9c16606d74891d283893abfc", "exact_match": 0.0}
{"doc_id": 1655, "doc": {"question": "what is the title of the person who runs the house of representatives", "answer": ["the Speaker of the House", "Speaker of the House"]}, "target": "['the Speaker of the House', 'Speaker of the House']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the title of the person who runs the house of representatives?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Speaker"]], "filtered_resps": ["Speaker"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3131dd7174639b74c7520a2c8bde418af8b8ff4260e474dd742ecbc3c726ddf3", "prompt_hash": "12dc8bd01e6752f896cd3c898b4e37f4bcdde177f292064721d57cc56f59fbdb", "target_hash": "e779a4d0a75775b62ae8919135a13f93938b37c6dcfec4ecfc491d1715e54c65", "exact_match": 0.0}
{"doc_id": 1656, "doc": {"question": "what is a whistle blower suit under the false claims act", "answer": ["qui tam"]}, "target": "['qui tam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is a whistle blower suit under the false claims act?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" qui tam"]], "filtered_resps": ["qui tam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fba1427d161c4ab8d45ae206bee9869613803ca70fc47a087232763ca01ffe7f", "prompt_hash": "d707cc9fe18e02a5c4092ed1769020de196c4a47e17e4e6b1730064c2529d8ad", "target_hash": "9c2d4f5410598449d32e4786d88291116968b3e93fe27c6571863d960c5d8ab0", "exact_match": 1.0}
{"doc_id": 1657, "doc": {"question": "where is the oldest house in america located", "answer": ["Taos Pueblo"]}, "target": "['Taos Pueblo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the oldest house in america located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Plymouth"]], "filtered_resps": ["Plymouth"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aa53887f937ad21cde62142904d4283156fc14fea4832b311059e1fb0dcad5e2", "prompt_hash": "6bc3bca66efdb11c63c3e9d14d95fba7ec1d249a08c44d8f2e601d82b69648ca", "target_hash": "e94d76be038d2688aa899fdbbfc2bd872facf86a4d5eb48fc1f5601ba4b26004", "exact_match": 0.0}
{"doc_id": 1658, "doc": {"question": "when was the land rover defender first built", "answer": ["1991", "1983", "in the 1980s"]}, "target": "['1991', '1983', 'in the 1980s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the land rover defender first built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1981"]], "filtered_resps": ["1981"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ac19f17880543d39db1f1feae421902406b2c71d213c260b1b6a1921d40fcc65", "prompt_hash": "63a28d1481e6d4e39506029ca0bcf3394eda5bec52bc5d76724b8a8c219f90e5", "target_hash": "cb34dd032a2ccc1f66f2dd451a2ddfdaece490722250b40c5a0a33a5f4e39ee0", "exact_match": 0.0}
{"doc_id": 1659, "doc": {"question": "what is the main source of nutrients in an estuary", "answer": ["sea water", "fresh water", "detritus"]}, "target": "['sea water', 'fresh water', 'detritus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the main source of nutrients in an estuary?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" freshwater"]], "filtered_resps": ["freshwater"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff479445ed08b6e72c8aa7f961c4b1df3f5fab3376cb88870cdf725279ddc3cf", "prompt_hash": "e2b038a3355cd40a729bea39a859008891ade8065692c429366962c46534743b", "target_hash": "d8d0254c2772c6a8f27f56036ed4ae9cf0ed7c66653485d29d56b8c4f1f520c3", "exact_match": 0.0}
{"doc_id": 1660, "doc": {"question": "what's the biggest nfl stadium in the united states", "answer": ["Michigan Stadium", "MetLife Stadium"]}, "target": "['Michigan Stadium', 'MetLife Stadium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the biggest nfl stadium in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Raymond James Stadium"]], "filtered_resps": ["Raymond James Stadium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "66fdd40f7a096fffd3d3ba2416f4f1804b0dd42ae5ff20d77df3d9bdf81a4754", "prompt_hash": "02e566a5f598fb025e827e7d8e86539c61ac30fa7ac3dda3ffeae4f82f60e076", "target_hash": "29015af727eb9a12a74abfe66435ea80384477d34c417248a604326e5de87201", "exact_match": 0.0}
{"doc_id": 1661, "doc": {"question": "who was the walker rick killed in the first episode", "answer": ["Addy Miller"]}, "target": "['Addy Miller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the walker rick killed in the first episode?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shane"]], "filtered_resps": ["Shane"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b0f58fdca2acec16936d82e98962c99d8053e4dae4ad317547c8ff63928a8013", "prompt_hash": "93bbc003f1ca3f341f53ee6bceb30642b1c29b82d36549106fe47709685aa630", "target_hash": "552277ebbadf5107a4c3240710697e5aa90fe4fbe13b1df1ada605bec33b6c07", "exact_match": 0.0}
{"doc_id": 1662, "doc": {"question": "where did aeneas go when he left carthage", "answer": ["Sicily"]}, "target": "['Sicily']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did aeneas go when he left carthage?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Italy"]], "filtered_resps": ["Italy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "887b4c8a8bc285364dbe96585386ed917a502d128a366072fff6238f9796ff88", "prompt_hash": "a010fe893e7749c089bb44ff8220bac511d63faa3e1461c80e0d7973ef61714d", "target_hash": "28a6619359d0ddc7d4aa18585999214d7d7394f106b90ea42dac6c0169207af2", "exact_match": 0.0}
{"doc_id": 1663, "doc": {"question": "vapor pressure of water at 100c in torr", "answer": ["759.9625"]}, "target": "['759.9625']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: vapor pressure of water at 100c in torr?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 760"]], "filtered_resps": ["760"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0b525e5886219cf28b310e44eb931a9e5796423235a3336d3e3116c7a2df98fe", "prompt_hash": "6827a22372862d8d3dbb57fe24630daaa21c9d39663691867aeaed949ec11b0c", "target_hash": "576897689285d9291385cbabfbb5164f2d10bb8779dc0a46eb75d3431da3b51f", "exact_match": 0.0}
{"doc_id": 1664, "doc": {"question": "when was the last time stock market crashed", "answer": ["27 Oct 1997", "18 August 2015"]}, "target": "['27 Oct 1997', '18 August 2015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time stock market crashed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2008"]], "filtered_resps": ["2008"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ada14d9943b26bf42727e829687170c24dc8fa135f03b0ad8810f261a33dce4b", "prompt_hash": "169ca6009ff62f66e4c2260f2374685bea27555e52f31cffde9bd25dc3ce0da9", "target_hash": "49897c0f94c6837fe685da05483d73f48bc1e7fc59bea4d3848a7ae25de45b47", "exact_match": 0.0}
{"doc_id": 1665, "doc": {"question": "during which season does cape town receive rainfall", "answer": ["winter", "summer"]}, "target": "['winter', 'summer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: during which season does cape town receive rainfall?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" winter"]], "filtered_resps": ["winter"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2e026881b58d672547c79505c5e496b17536a1b0afe051315fc29967b0eb71c2", "prompt_hash": "65f0f5525bd267fbc01c61c69f9895a6d879bb64bc66fe50f7b74b2051598a02", "target_hash": "9789760a205ede5080cae9bd3cc64ad0fc11ee02fb328397801a718afe58fb58", "exact_match": 1.0}
{"doc_id": 1666, "doc": {"question": "who owns the rights to rocky and bullwinkle", "answer": ["Universal Pictures"]}, "target": "['Universal Pictures']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns the rights to rocky and bullwinkle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hanna-Barbera"]], "filtered_resps": ["Hanna-Barbera"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "789afdd928cee073d56f0adff8ecf9ff03405fbbd733a1665d06657b858fa1aa", "prompt_hash": "1aae1d7618d21d94e9cf99521ff738bb736cf5b68e7f328a7927c01313e78737", "target_hash": "44aa5625e0521509acf3f529ac58549b5c81ca3bde41b46c010f8bb225fbe1ae", "exact_match": 0.0}
{"doc_id": 1667, "doc": {"question": "what state did they film daddy's home 2", "answer": ["Great Barrington, Massachusetts", "Framingham, Massachusetts", "Massachusetts", "Concord, Massachusetts"]}, "target": "['Great Barrington, Massachusetts', 'Framingham, Massachusetts', 'Massachusetts', 'Concord, Massachusetts']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what state did they film daddy's home 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Louisiana"]], "filtered_resps": ["Louisiana"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c125487e165fd776bca85eda59efb2c5b42a801892544a88a911526c74d4a97e", "prompt_hash": "e2006d6ce93cc0da99ac8fd6a3dd9ef69ba981437081ee6fcf0332b60121f21b", "target_hash": "1ef45248a9cac16b4400c984a87f888fc520f46eac4a7769f99aa07e1bc53ae1", "exact_match": 0.0}
{"doc_id": 1668, "doc": {"question": "first day collection of mission china assamese film", "answer": ["₹ 39.97 lakh"]}, "target": "['₹ 39.97 lakh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: first day collection of mission china assamese film?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d444f9a3d182214d44048e929daf7d7ac9d88c50a7466befcb8c60989b853446", "prompt_hash": "13ac5d9da1d6f03ebea0c5ebf0497f5cf3653059005a230b3811e53081755c05", "target_hash": "eaca6d5e194fc4e833ca1f395d5aa29ef50f7259103398a3f3af19589f376035", "exact_match": 0.0}
{"doc_id": 1669, "doc": {"question": "where do the florida marlins have spring training", "answer": ["Roger Dean Stadium"]}, "target": "['Roger Dean Stadium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the florida marlins have spring training?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jupiter"]], "filtered_resps": ["Jupiter"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "48200932529ad3f57ed9118cb2997e8f4c457b36e598142aad87a03064116fc3", "prompt_hash": "6075d600db08d9f679c9aa4f0b790a130511a984fc0ae529371a4f88d7868e06", "target_hash": "c99f7bb1eb920b8dd9f379de2134ad6b2f0d788a755b7fd3032d38fd53383473", "exact_match": 0.0}
{"doc_id": 1670, "doc": {"question": "who won french open 2017 tennis tournament in men's double category", "answer": ["Ryan Harrison", "Michael Venus"]}, "target": "['Ryan Harrison', 'Michael Venus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won french open 2017 tennis tournament in men's double category?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jamie Murray"]], "filtered_resps": ["Jamie Murray"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c09fd9291e33e6b0a50ff26b60867c5c05c0922f55007ea3526ad23a7f7ce046", "prompt_hash": "a54e6ddba2e67b08742c362c5347db1018718d840ac63e65d264f10d10ac7d8d", "target_hash": "b4280b4c45e6a946fdf115ae3acb4dc0692f4d69e96602074814fee3de089b9b", "exact_match": 0.0}
{"doc_id": 1671, "doc": {"question": "when did the united states became a nation", "answer": ["1776", "July 4, 1776", "In 1776"]}, "target": "['1776', 'July 4, 1776', 'In 1776']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the united states became a nation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1776"]], "filtered_resps": ["1776"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c9cddc1ae6dab9d15a0f7c2d762afa4d4c716d602d2c1667efe5d2177c3a92c", "prompt_hash": "371134c3a9ca64316d4f826c85a05c723116a58b23795c37455d344378d8d998", "target_hash": "215bf1bfeb08271ba69feaa865fe4fba96808d74f7d74c179d51f6f1809d4576", "exact_match": 1.0}
{"doc_id": 1672, "doc": {"question": "who played santa in the santa clause movies", "answer": ["Tim Allen"]}, "target": "['Tim Allen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played santa in the santa clause movies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tim Allen"]], "filtered_resps": ["Tim Allen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5cbb35cfd7a913d25be0628c11503c98a4625f61d0cd879176cfee01d25ad4a", "prompt_hash": "cf548c81643319666519f009cebbdeb5e7404f9b5b9eb4d26baf7daaaa9cc330", "target_hash": "279d40b5e191d4677611a5670a44f16cfcf723cde216ba2f4a8bfbb9187863c2", "exact_match": 1.0}
{"doc_id": 1673, "doc": {"question": "type of disappearing lake found in limestone areas in the west of ireland", "answer": ["turlough", "turlough, or turlach", "A turlough, or turlach"]}, "target": "['turlough', 'turlough, or turlach', 'A turlough, or turlach']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: type of disappearing lake found in limestone areas in the west of ireland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" turlough"]], "filtered_resps": ["turlough"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "526155b48eb9d779daf1d29d58dadd97af39c9f4bb8bb461fa2d0414e5730b92", "prompt_hash": "94bc15c9ed307695aaf86507ea20c92bb293d354ef9ba03030564c6ba1463edb", "target_hash": "0094da6776b850e1fd48fad6924846ee7e4a748faaee4fd81647e5e79a1fe572", "exact_match": 1.0}
{"doc_id": 1674, "doc": {"question": "who was the spanish civil war fought between", "answer": ["Nationalists", "Republicans"]}, "target": "['Nationalists', 'Republicans']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the spanish civil war fought between?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Republicans"]], "filtered_resps": ["the Republicans"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b6a086becf08a478fe6ea258f4759ffefa018da3d27bf810617d0bd5c961fbc9", "prompt_hash": "a30e23cfd4ef4727bde73cf35e855b2d5ec79224465f42db5881dfa62e6a46a7", "target_hash": "88a740d1e9a4fde1d7ef15103a8a5013a7dc40159b0592f14327192f359d780b", "exact_match": 1.0}
{"doc_id": 1675, "doc": {"question": "georgia out of angus thongs and perfect snogging", "answer": ["Georgia Groome as Georgia Nicolson", "Georgia Groome"]}, "target": "['Georgia Groome as Georgia Nicolson', 'Georgia Groome']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: georgia out of angus thongs and perfect snogging?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Princess Diaries"]], "filtered_resps": ["The Princess Diaries"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e4e35e526964d300f26021b21afdd3f874f7e3d871424a7ba9f9a8562bbd77cd", "prompt_hash": "42d1e152f2c76bb0242a21ba9251258a9e3f31f94709b1ce8a87194238891c54", "target_hash": "b73ec04dca27aaa25027bf46c58faf2568ef83f96e3dd903ffb7b31df09735b8", "exact_match": 0.0}
{"doc_id": 1676, "doc": {"question": "who issued gold coins for the first time in india", "answer": ["Gupta Empire"]}, "target": "['Gupta Empire']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who issued gold coins for the first time in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Reserve Bank of India"]], "filtered_resps": ["Reserve Bank of India"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a4e4d3c0fc287706ab28f65ae36c0a10c761c8cbf529e9e4d287b3a862d86f88", "prompt_hash": "58ea331fe1f3c78a5cb6568740acb27f1ea4d4c94b0717e45357d42d09c9b2ce", "target_hash": "3803a037e5cc815cab96e5e7c2b197550f057d5a4af3006c7552cd783c60f99a", "exact_match": 0.0}
{"doc_id": 1677, "doc": {"question": "which greek god flew too close to the sun", "answer": ["Icarus"]}, "target": "['Icarus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which greek god flew too close to the sun?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Phaethon"]], "filtered_resps": ["Phaethon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e02a8e171b24d8865cd8ac4dbff70d57c2978a918195e6ab455cacbf72229b1e", "prompt_hash": "8be078e864c66ebdd206d44d37a36599b9d1a850c2251a1dbc65d929b86968d5", "target_hash": "603e2a1c03dbede5efeb99fc1646c568613e71a6c6543c363df5d8db4d7d9a7e", "exact_match": 0.0}
{"doc_id": 1678, "doc": {"question": "where can carbon be found in the biosphere", "answer": ["other living organisms", "The terrestrial biosphere", "plants", "soil"]}, "target": "['other living organisms', 'The terrestrial biosphere', 'plants', 'soil']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where can carbon be found in the biosphere?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" atmosphere"]], "filtered_resps": ["atmosphere"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1505cb710457af5f8cf676437511321119866f9b2e219ac2f5c73313631cbf80", "prompt_hash": "3307457e051a73b1435d2a36396f227888d3b7fb6f50d4c1c3a58f60955897b2", "target_hash": "9740e5b83c7aa9b2847818e51e239e184e0a0ef2388327a7220ef0ff99d60d20", "exact_match": 0.0}
{"doc_id": 1679, "doc": {"question": "when does star trek discovery air on tv", "answer": ["September 19, 2017"]}, "target": "['September 19, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does star trek discovery air on tv?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "16d5d1848fe25b70286d13ce0119af5f8bff5f0a3f94ac901a04188c21f2fef6", "prompt_hash": "99ebec8412c44c3986fac4a6bd02946ae689a743af6fea856c0b165754695583", "target_hash": "66406d9f150dc16e8daebd5bab91d7527106613c89bea2a0df932de65d6973d5", "exact_match": 0.0}
{"doc_id": 1680, "doc": {"question": "when did earth's atmosphere change due to living organisms", "answer": ["around 2.45 billion years ago"]}, "target": "['around 2.45\\xa0billion years ago']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did earth's atmosphere change due to living organisms?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2893bebef84241989467d5b6a0a90d10c2aa33c7db8add855a35fcd0bd9456e", "prompt_hash": "acabad7b4d55c5d46653a6b71e50dea88d00dc464fe3eb04172dba37d7c73736", "target_hash": "3c59793a1fe44fc0d95a47da871e4d2e41a2f447eec3f1c90bd790d64606a784", "exact_match": 0.0}
{"doc_id": 1681, "doc": {"question": "what is the area code for phoenix arizona", "answer": ["602", "623", "480"]}, "target": "['602', '623', '480']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the area code for phoenix arizona?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 602"]], "filtered_resps": ["602"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f3cb246d124b59d9b28f52c1465fd466ba38839d71d6360796a4618c59ffe63", "prompt_hash": "fb796e858bb864d30202bd7b14fe6d28c2c7eedbb0d4472b67b84b8068bf4605", "target_hash": "63efff17f3485ede223fec0afe8a51204781be6982aa252a73e5102910a86b18", "exact_match": 1.0}
{"doc_id": 1682, "doc": {"question": "where in the bible are the 12 disciples", "answer": ["Acts 1:13"]}, "target": "['Acts 1:13']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where in the bible are the 12 disciples?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew"]], "filtered_resps": ["Matthew"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9597430e504f919d6d997205d7fba4aafc2983ad5f27723422455c8ba0142bc3", "prompt_hash": "ec1e209ab5faa88dba426707850e52333b39abf9410a876083e0a310f35ae2b3", "target_hash": "fb72d2d5a5b5565cf42fdf709ee36cc015c54f90008e63464616150a3afcf44b", "exact_match": 0.0}
{"doc_id": 1683, "doc": {"question": "who came up with popcorn at the movies", "answer": ["Glen W. Dickson"]}, "target": "['Glen W. Dickson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who came up with popcorn at the movies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Orson Welles"]], "filtered_resps": ["Orson Welles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a8850b045282e9562e159defe522b5f0c548b2cfbf1f44e4334636848c8b4ff4", "prompt_hash": "0ab6d297a540945bfa315ec3ceb5a803fed5e90f98b2507c1cd9d75ce0320863", "target_hash": "b498431144696959585b4b68acd68c4142476b3833843603bbc94e51eea19096", "exact_match": 0.0}
{"doc_id": 1684, "doc": {"question": "who is responsible for establishing local licensing forum", "answer": ["unitary authorities", "local authorities", "district councils"]}, "target": "['unitary authorities', 'local authorities', 'district councils']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is responsible for establishing local licensing forum?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the local licensing forum"]], "filtered_resps": ["the local licensing forum"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03323294884132241e023569f6006eebe95a6cb76b04b0f8c8988a4fbd678de9", "prompt_hash": "cf1151fb130345dced4d7b3dfde19a17048c72f9639c234ebbab271022c0813b", "target_hash": "9cde8fc887397387a9bc86f928adf3e56fa066c8aae8e7cc4ecdc23fcb6db17d", "exact_match": 0.0}
{"doc_id": 1685, "doc": {"question": "when did clifford the big red dog first air on tv", "answer": ["September 4, 2000"]}, "target": "['September 4, 2000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did clifford the big red dog first air on tv?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1969"]], "filtered_resps": ["1969"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96208ed528175c4113c0d0d249b80862b34a7c7796b32313b4051fb4961d3621", "prompt_hash": "244d830f633734766f991e18b27a5c39bd69e1ef03ce42abf9c98b2d5e7faf4d", "target_hash": "4ca5427a3ea3e655d2c3a2df018362cdadeff892bde9e6e0ab2396690834ab4f", "exact_match": 0.0}
{"doc_id": 1686, "doc": {"question": "when is episode 12 of miraculous ladybug season 2 coming out", "answer": ["30 March 2018"]}, "target": "['30\\xa0March\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is episode 12 of miraculous ladybug season 2 coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 11"]], "filtered_resps": ["January 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6daf61e16adfe024f5769887c22e638226d013ac0a56cc844b5b3641ceccee96", "prompt_hash": "3322245703f1632aa4b9957ccfff3cb39b3a5b0c030662fc501d6bb8d94db974", "target_hash": "fff28d9a70393efafbf224eacac262fa6a94b8576e13fb7f3851ae41598ab1cb", "exact_match": 0.0}
{"doc_id": 1687, "doc": {"question": "where is natural gas found in the us", "answer": ["Louisiana", "Pennsylvania", "Wyoming", "Arkansas", "New Mexico", "West Virginia", "Colorado", "Texas"]}, "target": "['Louisiana', 'Pennsylvania', 'Wyoming', 'Arkansas', 'New Mexico', 'West Virginia', 'Colorado', 'Texas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is natural gas found in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Texas"]], "filtered_resps": ["Texas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "97ac1a002c9abb72714466f896f41db280f8dbca880b23f076cbcd2475dac428", "prompt_hash": "9dca43268205b2c85e8e3ee19f7f1a206670a39f8b604727bc732895cf38b091", "target_hash": "b9d671a0c72c90eaf9957c585e8742c010cfebb9b5d5ae2a6bbb9d75dce361eb", "exact_match": 1.0}
{"doc_id": 1688, "doc": {"question": "who sang the most number of songs in the world", "answer": ["Asha Bhosle"]}, "target": "['Asha Bhosle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the most number of songs in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Elvis Presley"]], "filtered_resps": ["Elvis Presley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a765bee2fd01c8217be0bc19ea595179fb6023ba8f1667771a62966009eaeafe", "prompt_hash": "4cb1045c10cd7072f239bca34901578212f75602f127f43a3717e1223c816d68", "target_hash": "2740f37f57c7f2166398bbc9ab7f2bed63e8babd91bece36b1496a3eba23ad3c", "exact_match": 0.0}
{"doc_id": 1689, "doc": {"question": "who sings i will go down with this ship", "answer": ["Dido"]}, "target": "['Dido']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i will go down with this ship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christina Aguilera"]], "filtered_resps": ["Christina Aguilera"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "23945211242a53ffc5605311e8168ae2d18fb8268b7346c663a5a3f27bcc9ec0", "prompt_hash": "ca6a9aeeacda510cc1277c74c6b4e95aec2b3370e46fe79f9726a234fdc56dcd", "target_hash": "991d6d333de340865b00be5f745907f5023c1b3d0ea5aac40738532859c7b129", "exact_match": 0.0}
{"doc_id": 1690, "doc": {"question": "when did the right to buy scheme start", "answer": ["1980"]}, "target": "['1980']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the right to buy scheme start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "144cd898e00c1ffea149131663ea6e0aa890d9a1e1433e653d733ab2747dbf37", "prompt_hash": "1af31cd86b575e2fa6f58378fc8b3f67e27280576e0be27e391ba80619fcda2a", "target_hash": "982c196a07fa2fa33891f52e7ea12641498bc4165fd5d919e54ad9e46b9ee366", "exact_match": 1.0}
{"doc_id": 1691, "doc": {"question": "when did the wave hill walk off end", "answer": ["16 August 1975"]}, "target": "['16 August 1975']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the wave hill walk off end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1967"]], "filtered_resps": ["1967"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "56096c0c127973d8e85aa247b9f5ce479222f286b8f98a546011bf6f31b35de7", "prompt_hash": "453b925034964292428f2d83c5938ad7484fc8157f23819e89048931276f754b", "target_hash": "b6bbee0d613e7d3db951812811c9599854eec067aab03fda34538d303982f98b", "exact_match": 0.0}
{"doc_id": 1692, "doc": {"question": "when does the new episode of scorpion come on", "answer": ["January 15, 2018"]}, "target": "['January\\xa015,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new episode of scorpion come on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" September 25"]], "filtered_resps": ["September 25"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c1a3012237a6b3f12cdba8af3192609b8ee5eba5b29ba6439dbfb644cb29edf", "prompt_hash": "5b98c876b25daab473cd316a79baa80f9cd161e542800416e9e0ae996fd51925", "target_hash": "2df9773a2bdc26fcdb44096259cc72dc924fa2f45011359585ef12fcdfe112e5", "exact_match": 0.0}
{"doc_id": 1693, "doc": {"question": "who sings the songs in crazy ex girlfriend", "answer": ["Rachel Bloom"]}, "target": "['Rachel Bloom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the songs in crazy ex girlfriend?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Adam Schlesinger"]], "filtered_resps": ["Adam Schlesinger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "83fda890d6740560e43d297336165a218ff0ce0b1ec2796ed1bc51064d590b9d", "prompt_hash": "7f2ca91c88ee3445a81943cd830a8f5993af055836a9cee58504ef7aa7e24e1e", "target_hash": "a66d1f7b9d92c8c727570554d6af5d1d69f41a5644cebf71da8b39f71bce79c4", "exact_match": 0.0}
{"doc_id": 1694, "doc": {"question": "how many winnie the pooh books are there", "answer": ["four"]}, "target": "['four']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many winnie the pooh books are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11"]], "filtered_resps": ["11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9b0b5e759eec66f5943af5dbd93e185c6b3e558e95b80b2a82a5730f91ce9c85", "prompt_hash": "d5722e1a216d8906d243d369e22eb8114f2600d9b25803ef8ba7bdf2eb8d9943", "target_hash": "c0b12eaa48bad2eab0833b330f1f9a7a1caf1ef07c30f4fc2a1bd97e38e98618", "exact_match": 0.0}
{"doc_id": 1695, "doc": {"question": "wolf of wall street number of f words", "answer": ["569"]}, "target": "['569']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: wolf of wall street number of f words?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 112"]], "filtered_resps": ["112"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a526b935bb7f3975b8b5429db4ad5779b8ddc9af1141b589cae7101b8707b6bb", "prompt_hash": "cc115a1e688b04d80e1a8d89cc1b264872bd85e958b831ce6508a0f568429772", "target_hash": "ca8b1a8807278813fbb68df05f8a03d309d866766139239e0358e453ff7cb019", "exact_match": 0.0}
{"doc_id": 1696, "doc": {"question": "india south africa test series 2018 highest runs", "answer": ["286", "AB de Villiers"]}, "target": "['286', 'AB de Villiers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: india south africa test series 2018 highest runs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rohit Sharma"]], "filtered_resps": ["Rohit Sharma"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c884c6981ec987f9f2b32d2ecd87f5263ea839c6039d202964454dddb4648c7", "prompt_hash": "c7b18dd3af9adc0e282da5faf8121db3274812521a012d4610b0d64cf218832e", "target_hash": "72e9ee84db36900e603084af1494a6d46c32fb4d8b1ff487801637bb6f8938ab", "exact_match": 0.0}
{"doc_id": 1697, "doc": {"question": "when was the last person executed in florida", "answer": ["Eric Scott Branch", "February 22, 2018"]}, "target": "['Eric Scott Branch', 'February 22, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last person executed in florida?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2014"]], "filtered_resps": ["2014"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "59f5f404fdefc157f6ff1f9f2fce2460533f16531a9060da281aae34a0dc6e8e", "prompt_hash": "651a15804f69cf5a7d1d61c7395d3829709224cf995f8415ea2552788dc7eaac", "target_hash": "ec5689b22dcd6497906ccc19c6d07b6d28b33d10a1a730ff6bdcc66a85386b2f", "exact_match": 0.0}
{"doc_id": 1698, "doc": {"question": "who represented the state of tennessee in the scopes trial", "answer": ["William Jennings Bryan"]}, "target": "['William Jennings Bryan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who represented the state of tennessee in the scopes trial?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Jennings Bryan"]], "filtered_resps": ["William Jennings Bryan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4c12db4860cceadbf043c778580ee50ece06fb2dc877f844b75d52346ba4c5ca", "prompt_hash": "840084724c3a5f03b6bfa8d921d0da1f97e499dd705442395f2050b5b75a90d5", "target_hash": "18e5f4949fa264e7dcd2e916f1c3c944b8e833bb97bebdc5bc7974e64bd6fc47", "exact_match": 1.0}
{"doc_id": 1699, "doc": {"question": "who won the marathon in new york city", "answer": ["Geoffrey Kamworor"]}, "target": "['Geoffrey Kamworor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the marathon in new york city?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Geoffrey Mutai"]], "filtered_resps": ["Geoffrey Mutai"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "71144a3cfa9f1781b676877a798808cd6ea53be0ae3bd860b19604314e2a746c", "prompt_hash": "cc59aa972a9b9440b2658d71a0c2167f7be9729044c26e2105c2c56ff386099d", "target_hash": "36c3adc26a6288c1f3ece544a3af201132b6d15e5a6e6ba3946f7530dea06ee5", "exact_match": 0.0}
{"doc_id": 1700, "doc": {"question": "what is the setting of the story sorry wrong number", "answer": ["Manhattan"]}, "target": "['Manhattan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the setting of the story sorry wrong number?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d959aa70a1cb388d957bc548a1928bcefa5cff80f13a0672334f74740c28a8e5", "prompt_hash": "6adadb2a6f8208743e4a1826630c3b4af852632a1fdc750aa4e95d33a2aea3de", "target_hash": "f390a693fb8d4af20773a913896f103ed9e6b392b1d71d23b385583d81aa7755", "exact_match": 0.0}
{"doc_id": 1701, "doc": {"question": "does archie end up with betty or veronica in riverdale", "answer": ["Betty"]}, "target": "['Betty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: does archie end up with betty or veronica in riverdale?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Betty"]], "filtered_resps": ["Betty"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1f6afef51dce32f2695898fe8f91e85dbab0baebde5a28c34a61b5784fdada4d", "prompt_hash": "c383b3b9ccc1c22a37dd791e8dc7ce73fb8c792af9195f14432dc03f365b544e", "target_hash": "2ffafc9921f6381b9cc0b68f86d9ad2efcb75d1680832456dd80d6f852897b66", "exact_match": 1.0}
{"doc_id": 1702, "doc": {"question": "how and why were serial novels a phenomenon in the nineteenth century", "answer": ["improved economics of distribution", "technological advances in printing", "the rise of literacy"]}, "target": "['improved economics of distribution', 'technological advances in printing', 'the rise of literacy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how and why were serial novels a phenomenon in the nineteenth century?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cheap pamphlets"]], "filtered_resps": ["cheap pamphlets"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "238f1aed1cddfe792f1aad431247d3adca1d22b22eae377408a9972b579e6840", "prompt_hash": "908dddc9c9f63e017df70893a609dcd55971a0443de1a1ddde6b91e4ef5a5991", "target_hash": "eb8eacf80625ad5f3c09aa5df78ed63c1c0896412934e3dd020446ceebf5f61a", "exact_match": 0.0}
{"doc_id": 1703, "doc": {"question": "when did this season of american idol start", "answer": ["March 11, 2018"]}, "target": "['March 11, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did this season of american idol start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 11"]], "filtered_resps": ["January 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2036cf27fae747473e06c205f8eacf16a6c17ddd55487c3f55fe2293cc08c02", "prompt_hash": "55f909408727e6f18671df47a5f14bebb164c6dd97de1f3a1a7132f7e055b4bf", "target_hash": "05bf6f6df8b6ffd0d90eeb784c54b56d1feedf8f2598e6da445f6c4d10975e35", "exact_match": 0.0}
{"doc_id": 1704, "doc": {"question": "when did the newest macbook pro come out", "answer": ["June 5, 2017", "October 27, 2016", "June 5, 2017"]}, "target": "['June 5, 2017', 'October 27, 2016', 'June\\xa05,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the newest macbook pro come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 2016"]], "filtered_resps": ["October 2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e5e09029573db192abd5c56b31cbfb5318f17590ebfc1b6421e4cd8267b9a6e", "prompt_hash": "ac1f758911292bab364439ef0a25223e1b210e9715b6c79a3385594b90681d9c", "target_hash": "e544a351fd01dbf8d95401c8091f838cc017a113f3be8ff18769938117740e19", "exact_match": 0.0}
{"doc_id": 1705, "doc": {"question": "what was the outcome of the battle of san juan hill", "answer": ["Cuban-American victory"]}, "target": "['Cuban-American victory']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the outcome of the battle of san juan hill?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" victory"]], "filtered_resps": ["victory"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "af9e01a63d58c05e430f66252a36304ee3bab51ef8aaf39862608f45ef963faf", "prompt_hash": "a362e994b09ccc42db126558e81a98814140ce541632cf5cec72394ef0e14e42", "target_hash": "8c9127a7bf55e2b98aeceba1a1842a604abcfbcc874d2592cbe304c31c50c601", "exact_match": 0.0}
{"doc_id": 1706, "doc": {"question": "who sings the christmas song all i want for christmas is you", "answer": ["Mariah Carey"]}, "target": "['Mariah Carey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the christmas song all i want for christmas is you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mariah Carey"]], "filtered_resps": ["Mariah Carey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4379988a80532707137b191baa356eb6a1cdd6507b5d55002b2aa1898e05b96e", "prompt_hash": "c4b5f0c6e5855a373f8b07bee2a8ac223ffcc7c1489b17c8d1231ca40d8e7d76", "target_hash": "82b007b2c1b87c06d7f4314a3387cf3ec9b1b1ed6e309af36a8940be142f0e43", "exact_match": 1.0}
{"doc_id": 1707, "doc": {"question": "when was christmas first celebrated as a holiday", "answer": ["336"]}, "target": "['336']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was christmas first celebrated as a holiday?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 354"]], "filtered_resps": ["354"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ee38e23da37a2d10f781503171092fabfe475c4cba9a3481e4467723ea76c10", "prompt_hash": "d1f2d8e5f552499050cb5c20a1ef90c33290236dc60c9708ee503dfa82672787", "target_hash": "0fc001d62bcd5c4f22208d3a30f8d88e377f9b631e17892b8732ab8724f0274f", "exact_match": 0.0}
{"doc_id": 1708, "doc": {"question": "when was the last time the dodgers went to the world seris", "answer": ["2012"]}, "target": "['2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the dodgers went to the world seris?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "99bf3725bb61e963c03fdda96f92d7273b55c7527f7b3a3215f6a4e6ceed45e9", "prompt_hash": "a541ccd18d89e1bbb2fe8da07663927d0d7b3f191cac81040feef1115b09d5df", "target_hash": "04b09e138d9d7538745af7c282a2efd5708aae0ad9c106ae836329c5f0a7ce4a", "exact_match": 0.0}
{"doc_id": 1709, "doc": {"question": "when did teenage mutant ninja turtles come out", "answer": ["1984"]}, "target": "['1984']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did teenage mutant ninja turtles come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1987"]], "filtered_resps": ["1987"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb59f43e359ebe0559b7e1123c93666d33df18f79c7ef4d715cdc5ee68908bfc", "prompt_hash": "5b9d2f76befcb43d3b780d006e7f700e4b3443ad26a4683c0b130147769f8485", "target_hash": "8c1b2c2e658d9fadeeb5174a5acb23cdc07e29baf6e0886beeecb0ceac802b13", "exact_match": 0.0}
{"doc_id": 1710, "doc": {"question": "when were 2 dollar bills stopped being made", "answer": ["current denomination of U.S. currency"]}, "target": "['current denomination of U.S. currency']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when were 2 dollar bills stopped being made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1969"]], "filtered_resps": ["1969"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "110599084feb5a614616275e414bfde43f18868cb3396ed2ff355273a3af6611", "prompt_hash": "2f04a236482eee58a335e2c931abacf4448e2cc55f36251edd77e676ccfa52e4", "target_hash": "4b9a59b0ff941996d1f4cf52556161416955c385dca5d5b2851d10e550323de4", "exact_match": 0.0}
{"doc_id": 1711, "doc": {"question": "who is the leading african producer of wool", "answer": ["Sudan", "South Africa"]}, "target": "['Sudan', 'South Africa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the leading african producer of wool?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" South Africa"]], "filtered_resps": ["South Africa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "faaa975f4b47c698b22f487a6a1c419d900df145608908d31eefcdd07342ad88", "prompt_hash": "5277a37cb6aa387e9202dff6171dd43302d00efb05895e19c196bcaa2ad017e2", "target_hash": "024bcec55588437a55857241e8b53af89fe89ddc4398b854dd0df014a47d43ef", "exact_match": 1.0}
{"doc_id": 1712, "doc": {"question": "where does the president get authority to issue executive orders", "answer": ["Constitution"]}, "target": "['Constitution']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the president get authority to issue executive orders?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Constitution"]], "filtered_resps": ["Constitution"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "843b5466ae3c2b400c00f8b899d7374569b7200e8b729925afaf1c9cc58c3b4e", "prompt_hash": "1f373aaf5f707dd70473b3d7dad874b6019e42f626545b6e7c2ce2e015363305", "target_hash": "7b9819b56c1b1f3a6874a78601ace0825c8d8c5b13c19b37fee2f0d32b85482b", "exact_match": 1.0}
{"doc_id": 1713, "doc": {"question": "how long do you have to serve in the israeli military", "answer": ["two years for women"]}, "target": "['two years for women']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long do you have to serve in the israeli military?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" three years"]], "filtered_resps": ["three years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "43a1314d6ed75545c159e8338f3ad87b7db23d0e51f3d7824e4806955dcbd68f", "prompt_hash": "624fa1cd8b0206b2664b9dc6b7420842be5786e3a5b0400064d03a0055c593bf", "target_hash": "f825a30ad1d8aaf2afdc849e8aebfc605a6f82b8ec7ff302b269912eb01d74f3", "exact_match": 0.0}
{"doc_id": 1714, "doc": {"question": "dark parables return of the salt princess release date", "answer": ["September 2017"]}, "target": "['September 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: dark parables return of the salt princess release date?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2011"]], "filtered_resps": ["2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "21673403f39cb52ff7adf939a45fb718c2294707566fa79ade4347cf006b4fa5", "prompt_hash": "55e70df5a65f12b9f5c1c13e1e1bbce1a451df6848d26ea5cc51b10c889b41aa", "target_hash": "d19a62aa2c85e927a3c13a19308298458a7033d8d6885a0a2e72ada88ca65b38", "exact_match": 0.0}
{"doc_id": 1715, "doc": {"question": "when is the womens ice skating for the olympics", "answer": ["21 February", "21–23 February", "23 February"]}, "target": "['21 February', '21–23 February', '23 February']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the womens ice skating for the olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 10"]], "filtered_resps": ["February 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "807dfa30b44fe923efde7c7674d619785697efb5e88abd47080e9e0996461563", "prompt_hash": "ae486d87f2d7e182e33aee430b41c10d9d172b60a1a27cf0a9b6017d6d845c2d", "target_hash": "381a2976d10b1ab287b17341c60a4a97cf1516c837c761c7712fafc5089e6b6b", "exact_match": 0.0}
{"doc_id": 1716, "doc": {"question": "what is the most current adobe flash player version", "answer": ["28.0.0.137"]}, "target": "['28.0.0.137']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the most current adobe flash player version?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11"]], "filtered_resps": ["11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bd1635592db1a1c7ec305a5a11da416998ac899ae27524be5fb680ca273144f6", "prompt_hash": "e48cf7544efa018dbdb8249fd590788c6e3253cd027834707e542ed0e8144fd4", "target_hash": "f80967ade7ada8e944131a5320e108aa170f425c77de7e823c52b3e988f2e87a", "exact_match": 0.0}
{"doc_id": 1717, "doc": {"question": "how many players have scored 10000 runs in odi", "answer": ["11", "eleven"]}, "target": "['11', 'eleven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many players have scored 10000 runs in odi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9beed2cb1cdcc67dc0ad0c315e0fc1b0b473ae3fefd994beb21ec4c81110cb2", "prompt_hash": "84ec502ea61cd284a13259be149afa1365300c9280e6055c03737b3b26b3b9da", "target_hash": "c3b1647451f02b0165083a8e802e890f9b4cb3a5062cc374ff993cd988a79467", "exact_match": 0.0}
{"doc_id": 1718, "doc": {"question": "who plays lindsay denton in line of duty", "answer": ["Keeley Clare Julia Hawes"]}, "target": "['Keeley Clare Julia Hawes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays lindsay denton in line of duty?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vicky McClure"]], "filtered_resps": ["Vicky McClure"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2821b24d28d0d072cd4dc4dd97c6e626204c500bbafd1aeeacc48b5a2d78660d", "prompt_hash": "5a370e6527d76393c2bc2ead7446cac2ab0a63e8d4586b28a8219a2636249323", "target_hash": "0779f33c418d5d603ae8e59c9548d0c9de3d350b644c51722d9b1921b5780157", "exact_match": 0.0}
{"doc_id": 1719, "doc": {"question": "when did the first wireless beats come out", "answer": ["October 2012"]}, "target": "['October 2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first wireless beats come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2008"]], "filtered_resps": ["2008"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35f48676e53c43ccd201b13ec06991323fc71b29286d2a7222f71ce23192b5be", "prompt_hash": "00c087422f69578dd4a009f485c078d78f7e1630700a0de4398767ee8062e87b", "target_hash": "c816c0052fc1206def68c0f81037dc8d089a63e4efeea6d0bc9cc7f9ed3ae9d4", "exact_match": 0.0}
{"doc_id": 1720, "doc": {"question": "who discovered the atom and in what year", "answer": ["early 1800s", "John Dalton"]}, "target": "['early 1800s', 'John Dalton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who discovered the atom and in what year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Dalton"]], "filtered_resps": ["John Dalton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d1c6554fd5bc18bc23db2410374a8b38885c9923f46c00e80cb246fae389fbe", "prompt_hash": "59cc2413dba64d540efd52826091da7922a63fdab28bceab00d7842bed0229e2", "target_hash": "da710ef2c5933c667de896afc8fb2b74ffa38e9f641c52d9db0139962a2e65fc", "exact_match": 1.0}
{"doc_id": 1721, "doc": {"question": "when did the philadelphia eagles play in the super bowl last", "answer": ["Super Bowl LII"]}, "target": "['Super Bowl LII']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the philadelphia eagles play in the super bowl last?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff7881d53b11c44bb428730e3dcb43e0712f6e78771fb777055e2f6930098da7", "prompt_hash": "4be22fb14cac94b4093ae04cb563673f56d4d5b66c415365d49f62fb20bb3cd2", "target_hash": "4ce78f59d65d03e2f7288cbabcade727d1f49c50f9d5c06dece175258b25402a", "exact_match": 0.0}
{"doc_id": 1722, "doc": {"question": "what is the word for clarified butter in the balkans and middle east", "answer": ["smen"]}, "target": "['smen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the word for clarified butter in the balkans and middle east?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ghee"]], "filtered_resps": ["ghee"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0374229831beeded9c1460cf389d20adf71659ee1a305207bcd9e76a1e057b86", "prompt_hash": "d89c5598fe68fe406e3c6ef36cf374005e2f553e6228d5da39bbdf3b5c52c928", "target_hash": "6661fd01c02ce2b296165051ebc8b87a9cf8f564b0fcc043f34a8c8f1cae156b", "exact_match": 0.0}
{"doc_id": 1723, "doc": {"question": "where does the electron transport chain get its electrons from", "answer": ["electron donors", "donor molecule"]}, "target": "['electron donors', 'donor molecule']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the electron transport chain get its electrons from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" glucose"]], "filtered_resps": ["glucose"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "83fa4a3b4a61be4af559cd25914471fcf68940701d0f21ea0141b003487db675", "prompt_hash": "73eba7e35de3800c009941c1682a3fdef74f914542f6add5fbfe90bc0665a52d", "target_hash": "365cae2ea70df74de69fd7747b72f3e3c4d1130e9170efaeb82382f4817557b7", "exact_match": 0.0}
{"doc_id": 1724, "doc": {"question": "where is fight or flight in the brain", "answer": ["the adrenal medulla"]}, "target": "['the adrenal medulla']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is fight or flight in the brain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" amygdala"]], "filtered_resps": ["amygdala"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "454cc98598e4fc81338fe668a793e33bd7951b20bbb4dd652e155848f7affb56", "prompt_hash": "b4d23eb8ade0a88f7413fada3ee58a0a8dff5d2d32228582dd11807b808e93b2", "target_hash": "7e87246f5af298a75d24e6eefd14b35b7b3d802ab7f0cbfca7274be9afffeb7a", "exact_match": 0.0}
{"doc_id": 1725, "doc": {"question": "the road that connects the tombs is called", "answer": ["\"Spirit Way\""]}, "target": "['\"Spirit Way\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the road that connects the tombs is called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Via Domitia"]], "filtered_resps": ["Via Domitia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e0c5a9fb081f1cb0492b86504a54233fd47f667ffd8c4c6bcc8151a271ea7aa1", "prompt_hash": "3fdaa0b5e64c8108339465d0dc08fab8b84b6800abb165212cded805800f1e34", "target_hash": "81dd5e00150cfd6ffcc691cc7ed8c89ba775c96cb206cefdd1bd8d09357c1908", "exact_match": 0.0}
{"doc_id": 1726, "doc": {"question": "where did the idea of a unicorn come from", "answer": ["Greek writers of natural history"]}, "target": "['Greek writers of natural history']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the idea of a unicorn come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Greek"]], "filtered_resps": ["Greek"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e759d4406d58303b6ca3f72807fabb48726aebb477a233c51d8e771ea8719c9e", "prompt_hash": "d697f524bf3f7d94b72fc84484241857a724e5cb8e1d2618fa5db648123e6659", "target_hash": "ad8f583a7464b848ec0836c2f3ef4da94448dae5d31d98982b5e2c52ba316e48", "exact_match": 0.0}
{"doc_id": 1727, "doc": {"question": "which approach to psychology focuses on the body especially the brain and nervous system", "answer": ["neuropsychology", "Neuropsychology"]}, "target": "['neuropsychology', 'Neuropsychology']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which approach to psychology focuses on the body especially the brain and nervous system?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" biological"]], "filtered_resps": ["biological"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "537f4f714bdad0bba576da17af8eb8b498ae2a29be760102739265d8f6d73dd2", "prompt_hash": "2fed97499896a5594326693d12381dc7bcc57795892fea729ef1b3db50ef334a", "target_hash": "450c9874f9fdeb544a49932ada34431de8b79db2355918d3488923c461c674d0", "exact_match": 0.0}
{"doc_id": 1728, "doc": {"question": "who's the model on let's make a deal", "answer": ["Tiffany Adams Coyne"]}, "target": "['Tiffany Adams Coyne']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who's the model on let's make a deal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brooke White"]], "filtered_resps": ["Brooke White"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2bf4ed957e55f1b4894d5c128cff697a3d18d1af7bff6452c26208484d4556d3", "prompt_hash": "1f7bfcecacf65e0b3801da9aa882e0136d55cff619feae6f899746bd81b1526a", "target_hash": "60e0c458c8d4b631892d50df90de5d745de52d220cc72e6d0d3c6603ac7d55e9", "exact_match": 0.0}
{"doc_id": 1729, "doc": {"question": "uruguay is located along what body of water", "answer": ["Bay of Montevideo"]}, "target": "['Bay of Montevideo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: uruguay is located along what body of water?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Atlantic Ocean"]], "filtered_resps": ["Atlantic Ocean"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "86f4ef2258935a761d016b07dbd2a28ba5a55e168d7313c487f2a37ffb3efa57", "prompt_hash": "c61170f1d03c9cea23ec8069bde5ce5faa1671534af6620228d82b540fd51e22", "target_hash": "28ce2229a115d41e5cf925d913d5fc906fcaa98fdef767affd5e046ce64e5d6a", "exact_match": 0.0}
{"doc_id": 1730, "doc": {"question": "what drug does the mom use in requiem for a dream", "answer": ["weight-loss amphetamines", "amphetamines"]}, "target": "['weight-loss amphetamines', 'amphetamines']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what drug does the mom use in requiem for a dream?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" heroin"]], "filtered_resps": ["heroin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d6c87272689253a7efce99d2981ddc3c57ea32ca9cdb4834c4888e7e6b33d040", "prompt_hash": "561d2e9b9a726f1806f8ef0a3fc1fa385ecc02c109d2a97a0bbf21aa6f165b88", "target_hash": "9e62c4a23b842b2bf8555495e9da09834e6a6fd82831db70fd9a37bcc6fcca7a", "exact_match": 0.0}
{"doc_id": 1731, "doc": {"question": "when does the second part of vikings season 4 start", "answer": ["November 30, 2016"]}, "target": "['November 30, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the second part of vikings season 4 start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c53a56841ff0d26509e8f3b553a9206db3ec4b2cc1786f701a0840487457d99f", "prompt_hash": "ec533d091339cec731b1d7d66de12bf3f53bff2bc83dcde7bed68ed8fc1dcacf", "target_hash": "cf208d76e199f5d584c62fedeb2144bbdb0bca1d28edad4861fad076594b74f4", "exact_match": 0.0}
{"doc_id": 1732, "doc": {"question": "who donates a christmas tree to trafalgar square", "answer": ["Oslo, Norway"]}, "target": "['Oslo, Norway']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who donates a christmas tree to trafalgar square?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Royal Norwegian Embassy"]], "filtered_resps": ["the Royal Norwegian Embassy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fd7b4ef2b969dec0a69b8b409232c40ed004a8f21f75ae517fcf769c78648123", "prompt_hash": "8f6eed0c78f1cbb1879152d1c72e399cec04fcd01360556173cbffba9fb892ff", "target_hash": "7f695816c65d19ae17f07d17ba5352c0572fdf5bb61bc6f777f0dcf2a99c0f66", "exact_match": 0.0}
{"doc_id": 1733, "doc": {"question": "when was the original stephen king it movie made", "answer": ["1990"]}, "target": "['1990']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the original stephen king it movie made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1988"]], "filtered_resps": ["1988"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f785d695e249758239d25134fd3bd95f0469e5825a64ba6abd295ef273f538eb", "prompt_hash": "9b5d81a7ba3db1c189e0a49f649d7878ee30fb2ac7c69cbd18f8f9a86ed14666", "target_hash": "e0b02261abe31dfd5543c214570eaa30cdb8fabfb3f4f4830a9f11581c824889", "exact_match": 0.0}
{"doc_id": 1734, "doc": {"question": "who plays ser davos in game of thrones", "answer": ["Liam Cunningham"]}, "target": "['Liam Cunningham']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays ser davos in game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Liam Cunningham"]], "filtered_resps": ["Liam Cunningham"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "462b5769640d6aee76b3749030a11c56fb9c4135a199f7aef0b687bf278233fc", "prompt_hash": "755e6e536c880ee140bf7170d7fd2d5459f4eb7a96f927c28f911dd88b05fa7d", "target_hash": "576994c8042f0e02a272b3518c9d56ea3f3da415316c886fc11674ea0576db9b", "exact_match": 1.0}
{"doc_id": 1735, "doc": {"question": "when did the botswana currency first come into circulation", "answer": ["1976"]}, "target": "['1976']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the botswana currency first come into circulation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1976"]], "filtered_resps": ["1976"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "64801da65be874e43fae33b1a87963b7924ebd778fe81f9b2e9cab1abe326a38", "prompt_hash": "cb80d11df8980812c57e715e5c8ca1c763f0caaa46ca32fd00b21b5f008b0a8a", "target_hash": "4e14cc922f68c1714f06ccec7abcd10dacad618e3e0327b98e091ba06c92c99f", "exact_match": 1.0}
{"doc_id": 1736, "doc": {"question": "where is the largest salt mine in the us", "answer": ["Western New York", "Central New York"]}, "target": "['Western New York', 'Central New York']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the largest salt mine in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wieliczka"]], "filtered_resps": ["Wieliczka"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cef64d2d58b4f6563a07b9111d927c9b4e21a96ab9f7f3c7f466e49b44ecbb33", "prompt_hash": "3ab368e27c8aa125748c35296e1cd463fd3c270cf46bada9d5c7dfd083742cbe", "target_hash": "4acb270ae0cd0d5877326e81fcfd0cb93dc0dcd8ff1eedc0f8bf7d3a8da44825", "exact_match": 0.0}
{"doc_id": 1737, "doc": {"question": "who plays dylan in daddy's home 2", "answer": ["Owen Vaccaro"]}, "target": "['Owen Vaccaro']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays dylan in daddy's home 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Will Forte"]], "filtered_resps": ["Will Forte"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5dcf08da9448f32072544a34ad3854fadf0c2a2ad94874850158451fcd436d6", "prompt_hash": "d902d1ff04e62331712b7db6ef6f79d16a0c86bc7802529563fac667bba69255", "target_hash": "5fd8cde7ccdd0bb0e106fd14a53115da186dfbb4d18308ea205145e56d5d079b", "exact_match": 0.0}
{"doc_id": 1738, "doc": {"question": "when was the rock and roll hall of fame built in cleveland", "answer": ["1995", "September 1995"]}, "target": "['1995', 'September 1995']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the rock and roll hall of fame built in cleveland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1995"]], "filtered_resps": ["1995"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ec424fad38956849346eaab523419679899958bcd422d821a4a9e43a9ec9410f", "prompt_hash": "a60c2d220d49a499970302d8e74d925757c3db48b73b8a6df2272368e0b38d7f", "target_hash": "21a65f9685532726ba7a6785fb3a3a840474347aca36b72b9065ab15c5609ff4", "exact_match": 1.0}
{"doc_id": 1739, "doc": {"question": "who is nebula on guardians of the galaxy", "answer": ["Karen Gillan"]}, "target": "['Karen Gillan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is nebula on guardians of the galaxy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Karen Gillan"]], "filtered_resps": ["Karen Gillan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "768ccede46f3d286dc0e34720e17d71e41a824aa5cdfa3bb24d001757dc7d5f2", "prompt_hash": "069e050a8ae95cc8e41f340716713e576154de98b89524c94b866227daaeff70", "target_hash": "f4feb1a308ec9b9fc962286348532a0b4875c7c758589824cff057b504a2ab7c", "exact_match": 1.0}
{"doc_id": 1740, "doc": {"question": "what is the significance of the sahara desert", "answer": ["the largest hot desert"]}, "target": "['the largest hot desert']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the significance of the sahara desert?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" largest hot desert"]], "filtered_resps": ["largest hot desert"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b86627d1b44095a51a2aea4c944335dafa383424f0cbcdaedc0aeee24df8a3d4", "prompt_hash": "02213e42e3efca55673447f7ee3f44effbd7f2b5135b35d7d9eb07a431608671", "target_hash": "813fd76cbb3f68b38278e351969343197d3d090dd29ee3c527fd910a923c8f0b", "exact_match": 1.0}
{"doc_id": 1741, "doc": {"question": "who was the first coronated king of manipur at kangla", "answer": ["Pakhangba", "Pamheiba"]}, "target": "['Pakhangba', 'Pamheiba']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first coronated king of manipur at kangla?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Purushottam Singh"]], "filtered_resps": ["Purushottam Singh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ac441ab58ccf58a9b6839091365c5399ddef0e15d21e49fd4ce8a1f510867c4", "prompt_hash": "3f2fa503d4b6039f901eca3a88e1abac3db58c419662cfee2110a25ad5deab72", "target_hash": "70f347c30d6089f86168d131e3aa5d83e7f07dd3e712280276b5763f962568d1", "exact_match": 0.0}
{"doc_id": 1742, "doc": {"question": "who is the pastor of new birth missionary baptist church now", "answer": ["Stephen A. Davis"]}, "target": "['Stephen A. Davis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the pastor of new birth missionary baptist church now?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bishop Eddie Long"]], "filtered_resps": ["Bishop Eddie Long"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8aac5ddb9894b90d95975364a0753e2af3d323f26d4f92c9904c7358657875d3", "prompt_hash": "8a71144a0c913f5f9d876f19a0b92cd7ff5d3a8d84c65d1ea01f74ec1ea851cf", "target_hash": "38537b0b30464ef20ccc934eafed07af39feb7e058974939b965d5516c8393a9", "exact_match": 0.0}
{"doc_id": 1743, "doc": {"question": "where did saudi crown prince go to college", "answer": ["King Saud University"]}, "target": "['King Saud University']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did saudi crown prince go to college?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" University of London"]], "filtered_resps": ["University of London"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfb86c252c06880d6b46d156f4f1090c44aa4458f54cdbc9043b34447f529aeb", "prompt_hash": "ce96210bd1bb733f9f98f1a2e4c80687b47654fd627d01f08a6d749fa2713df5", "target_hash": "127e573b0eff848452fbf9a2232d4a345fdfa60679fc72e3a560f174a856cc69", "exact_match": 0.0}
{"doc_id": 1744, "doc": {"question": "what is the revolution period of venus in earth years", "answer": ["224.7 Earth days", "0.615198 yr"]}, "target": "['224.7 Earth days', '0.615198\\xa0yr']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the revolution period of venus in earth years?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 225"]], "filtered_resps": ["225"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fd2be8b124fcd46560b5cbc64b6deb908e97d53ad442d98f4ac935c0ec819937", "prompt_hash": "e97d1616a550b7d4a51750f0c262463a7ed65d4b69b2e5ef7f92be3efc92bb39", "target_hash": "98454a988c68b1abec2fe385d9e915f9353963329f2364f69118a97dd071aeb2", "exact_match": 0.0}
{"doc_id": 1745, "doc": {"question": "what is the definition of ph in water", "answer": ["potential of hydrogen"]}, "target": "['potential of hydrogen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the definition of ph in water?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pH"]], "filtered_resps": ["pH"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "723a4b3479ed775eaa3591b2e0c6e1348c6f82c7c3b2751ec38ae6bd017a694b", "prompt_hash": "1a8011446dc5c4568ea5ac19aacc28f5e9e40f1ad5ea6cccefd16c76b951a8e5", "target_hash": "7a36fa73641a1c05e1953f4a26a0ca65ebf0477a39c1d6d759eb8dbe680fc261", "exact_match": 0.0}
{"doc_id": 1746, "doc": {"question": "when was the national flag of india adopted", "answer": ["15 August 1947", "22 July 1947"]}, "target": "['15 August 1947', '22 July 1947']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the national flag of india adopted?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 22 July 1947"]], "filtered_resps": ["22 July 1947"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49dda10f35b1af2d6aa1fe40a6b161c7850a31700dcd809f573e6916ad5d3d5d", "prompt_hash": "4f5c60585d5d43bb892c208c010e7aae7f6a1d1420d4e8be5d8f81ea4863c942", "target_hash": "7faeb017ed6e8684ec5303bf8e6c2da63e8017bf50623f49d36fec3cdf03c720", "exact_match": 1.0}
{"doc_id": 1747, "doc": {"question": "ajay devgan preity zinta and madhuri dixit movie", "answer": ["Yeh Raaste Hain Pyaar Ke"]}, "target": "['Yeh Raaste Hain Pyaar Ke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: ajay devgan preity zinta and madhuri dixit movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hum Aapke Hain Koun! Champola"]], "filtered_resps": ["Hum Aapke Hain Koun! Champola"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e4aaf32587d5641ae89714ac89316fffb1629062633c32ff657d758fe9eaa7e1", "prompt_hash": "10a500461f58a9acca27ffab4e35d7157a86e80e0b21b490f831224bb0bc27b5", "target_hash": "b0012e22e5a5e572e736d4e409d5a31e7897ba5230e84e11504ab5b5d7fc21da", "exact_match": 0.0}
{"doc_id": 1748, "doc": {"question": "when did virgin australia (formerly virgin blue) commence flying in australia", "answer": ["November 1999"]}, "target": "['November 1999']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did virgin australia (formerly virgin blue) commence flying in australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1999"]], "filtered_resps": ["1999"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eb8372f92f412a3f9a4ad891ce2b746b60c933a55660f2473387dac5080c5a07", "prompt_hash": "9440fcf6f382f8c0bdf3b9a86c247d418ffbfc8a49b73fd949728fdeb4be8589", "target_hash": "72ffd10a32d3bfe8bd0f35b84b054e141cb021b4aab2f672a5391c93f80bfc87", "exact_match": 0.0}
{"doc_id": 1749, "doc": {"question": "who created separation of powers and checks and balances", "answer": ["Montesquieu in the Enlightenment"]}, "target": "['Montesquieu in the Enlightenment']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who created separation of powers and checks and balances?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Montesquieu"]], "filtered_resps": ["Montesquieu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e565ec598ed2ec421915ae88abc2648fbdde4887e22d8d2177105cdc84fdb2de", "prompt_hash": "6035abae314dfdc2904476383f22416cd8afc9360b3aa13132e048d9f9a80f69", "target_hash": "4dccebaed881d5a9049376b538cf533cc5d699e56fdb352c93a6c1ae3d1ce817", "exact_match": 0.0}
{"doc_id": 1750, "doc": {"question": "how many episodes of sabrina the teenage witch are there", "answer": ["163"]}, "target": "['163']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes of sabrina the teenage witch are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 100"]], "filtered_resps": ["100"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a508471df484db577ece6bccfbbc2fa4e99a7415fed2774233619260fa3ba8e2", "prompt_hash": "dde81a27dcb8552035d925225a862fc2e50c6d80f839595ffc601de7c395632b", "target_hash": "0f79dc7e3c5754f75e3703e9a2d834cb11c9f0e8b2f2f7aabc94ce6a41f03dd3", "exact_match": 0.0}
{"doc_id": 1751, "doc": {"question": "1. what was the precursor to the present day internet", "answer": ["the ARPANET project"]}, "target": "['the ARPANET project']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: 1. what was the precursor to the present day internet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ARPANET"]], "filtered_resps": ["ARPANET"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "27c44c97d87a276e6032d56e5bbfc8fa21dc5338c45cb91ddd609c8d243149ae", "prompt_hash": "b86e9cee2bee6de7a33a513edbd1f2d0b8512cf27a0637e79fa7ca0e4993087d", "target_hash": "472a2fcd5189c5dc19f281521a32f2983b4457020d7ae1a983ee3a8bc32aefe5", "exact_match": 0.0}
{"doc_id": 1752, "doc": {"question": "who was the first british team to win the european cup", "answer": ["Celtic"]}, "target": "['Celtic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first british team to win the european cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Liverpool"]], "filtered_resps": ["Liverpool"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e444bb0078a94f5d10c23f34ddff8a6ab0ab089519ec99ac6815bb4ae3704e4f", "prompt_hash": "01d94982c68071643f98eabfc5cfda2b640743415ab025d28028bbac52371406", "target_hash": "deb47a8e42d5fac08c4457ae42caf5c21afcf8634966ffd81abdd3630799de8b", "exact_match": 0.0}
{"doc_id": 1753, "doc": {"question": "who inaugurated 'world teachers' day'", "answer": ["UNESCO", "ILO"]}, "target": "['UNESCO', 'ILO']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who inaugurated 'world teachers' day'?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" UNESCO"]], "filtered_resps": ["UNESCO"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "305fc6e9adfe9cd0e166282e90c95beb88c967dfcce6edc95a7378863d0ea38c", "prompt_hash": "727aeca24da08536ab364ee8e41d715cc32121bb41b95a310b4d7042f1cf0816", "target_hash": "88cb146ecab5f9718ad32142ff349c57f7634944ed1f831427fc13c5224d6637", "exact_match": 1.0}
{"doc_id": 1754, "doc": {"question": "how many house of representative do we have in nigeria", "answer": ["360", "360 members"]}, "target": "['360', '360 members']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many house of representative do we have in nigeria?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 360"]], "filtered_resps": ["360"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9a7db6dd8b6fa85a20eb483d39b616d45868ec57752cecf88d899859866c34d", "prompt_hash": "b4a75829e0640166be29c339e79763b736ac105216930bfdf3ff97df1864053f", "target_hash": "122763006af4b61c98dfc2dfb840dda8276d8ba0284bc89cf68e1a286c78c494", "exact_match": 1.0}
{"doc_id": 1755, "doc": {"question": "which type of fire detector uses the effect of smoke on a light source and photo-electric cell", "answer": ["optical smoke detector"]}, "target": "['optical smoke detector']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which type of fire detector uses the effect of smoke on a light source and photo-electric cell?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" photoelectric"]], "filtered_resps": ["photoelectric"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "11847ae058b1a93d818d79958a836aa56e8e68477e2b2c9202228e3659a58081", "prompt_hash": "4cbff3262891027128392b0d8f3bc18eff960bf7ad996b36a27d13b72388dba7", "target_hash": "a9e2aed189a98ff2b3c9b6672c656397f03ce6c0849cf30ded5357047ca3f491", "exact_match": 0.0}
{"doc_id": 1756, "doc": {"question": "how many votes to approve supreme court justice", "answer": ["A simple majority vote", "A simple majority"]}, "target": "['A simple majority vote', 'A simple majority']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many votes to approve supreme court justice?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 51"]], "filtered_resps": ["51"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b5d330518bad06c92c0b804fd379eeecaad05fbf16ece64965faf7f56fe20ad9", "prompt_hash": "13376c8bb1d345dbc53f29e8b4cba8d459628be4c650f8cd0d8043a25c63d234", "target_hash": "9aa6b6653c66bd5fd7e3ac4bac04a76fe5dfa64baff6bfa8ce5cd7437b5cf27a", "exact_match": 0.0}
{"doc_id": 1757, "doc": {"question": "who do you meet at the gates of heaven", "answer": ["Saint Peter"]}, "target": "['Saint Peter']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who do you meet at the gates of heaven?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the devil"]], "filtered_resps": ["the devil"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4751ee212a1ac6aa1f6915904f583f5aea37996c538a1c8a48852feffb045a2f", "prompt_hash": "58ed01c8974f2810f40939371e7b00d9d103f7a61604b396f375756a0c088b55", "target_hash": "fbb4aac4c751344235dd9dda13a24f63d3a47f4b9f54db3ef0529b792c6f97d0", "exact_match": 0.0}
{"doc_id": 1758, "doc": {"question": "iupac name of element with atomic number 107", "answer": ["Bohrium", "bohrium"]}, "target": "['Bohrium', 'bohrium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: iupac name of element with atomic number 107?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bohrium"]], "filtered_resps": ["Bohrium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cce1808ed4d9e30b7317a6059b3d6470b827bdd96645e037b9cf345ec9c19a69", "prompt_hash": "5b303c8f884c324b352808d13d880e57b5c3bc7b986f261bd8976cc8dbfab9ad", "target_hash": "bfcf4836b381bb1c06cc177c687b4ecd88b9df9a16016180c69825b0e903c75a", "exact_match": 1.0}
{"doc_id": 1759, "doc": {"question": "where is the setting for beauty and the beast", "answer": ["Rococo-era France", "France"]}, "target": "['Rococo-era France', 'France']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the setting for beauty and the beast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" France"]], "filtered_resps": ["France"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "218c8ac46aec1c1bbccfb911a55563b75a624a18ca6cc8ed0e47038d52b08fa9", "prompt_hash": "2890ab5b832bfb34dc0221c05089433614ddee6d872b2028cc2be175730b19a1", "target_hash": "7dbc41b406bafa8bd15bac7f70690d882db808274d5e02edc93f33cc0a614cd5", "exact_match": 1.0}
{"doc_id": 1760, "doc": {"question": "when was the last time nebraska voted democrat", "answer": ["1964"]}, "target": "['1964']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time nebraska voted democrat?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1956"]], "filtered_resps": ["1956"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1639f0acd502296ad124f7d3dce36df38349f5b9fffa98a1e49baf05efedf1de", "prompt_hash": "fcb416bffd3405b1fbddc333bed022be14e9376a54756eea4f75d692cd1276e8", "target_hash": "c9a01a6932afd568501e3941640cda5d65b96094322fe667a5f573eaf47b64d3", "exact_match": 0.0}
{"doc_id": 1761, "doc": {"question": "winner of worst cooks in america season 5", "answer": ["Amber Brauner"]}, "target": "['Amber Brauner']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: winner of worst cooks in america season 5?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer and Jennifer"]], "filtered_resps": ["Jennifer and Jennifer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "20e9f4e63df85bfda7f3b9a0663bc613fcf477a586b884266904ebf67ef21bd5", "prompt_hash": "a195524386d2cd58b8e1366a1c9220629be67f045cd172b60359d8ff71894e10", "target_hash": "84450e9067bf4a1cc8249a98c83ef47b0df3554431fcf7c5750773519de15b05", "exact_match": 0.0}
{"doc_id": 1762, "doc": {"question": "when does the movie jeepers creepers come out", "answer": ["September 26, 2017"]}, "target": "['September 26, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the movie jeepers creepers come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2001"]], "filtered_resps": ["2001"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4a0915b2841acd9ea5211446c734a806578527397412479e645e2cce80945e3e", "prompt_hash": "17c5f17d01b6f5da0a1dbe76fed8699968ccd1761f4e97d2a19a27d7938550d8", "target_hash": "50e247ed19e2b5bce3561d8256ec1bb1fe5f0133519d43d5f12b78e5450d695a", "exact_match": 0.0}
{"doc_id": 1763, "doc": {"question": "who got the first nobel prize in physics", "answer": ["Wilhelm Conrad Röntgen"]}, "target": "['Wilhelm Conrad Röntgen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who got the first nobel prize in physics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wilhelm Conrad Röntgen"]], "filtered_resps": ["Wilhelm Conrad Röntgen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c4fb1c762ba82d268f94aebbbcfbb2c6d9d7dccc3d762d6ecf975b17c94b1863", "prompt_hash": "e614e481bf972c3b0ccb479058ff0418ad1cc92540224344e092bf9a2c18713c", "target_hash": "7744541d47dd8419ce7314ff4e57ebe4e2566f0623c4ea35ab28c624db630178", "exact_match": 1.0}
{"doc_id": 1764, "doc": {"question": "who's the guy in call me maybe", "answer": ["Holden Nowell"]}, "target": "['Holden Nowell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who's the guy in call me maybe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Adam Sandler"]], "filtered_resps": ["Adam Sandler"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9ee37d26fed26b744b7f0a803ff9841e0ddad0c58303b7e051aef31031fe77e", "prompt_hash": "bbb6c314dcbb5f9a0f324bfe4a794cd8ab5a6b62f5a796b63b9a9baa40cf1da2", "target_hash": "d0c8b113d3e19e17bb23a710631296244ec7328d7d999191b45cb385b7da5ab5", "exact_match": 0.0}
{"doc_id": 1765, "doc": {"question": "what is the longest river in the united states", "answer": ["Missouri River"]}, "target": "['Missouri River']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the longest river in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mississippi"]], "filtered_resps": ["Mississippi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "146b59460293fcf4a4711b1d5b4466619ae57a7046fe37c607fd9d48c3b0c92d", "prompt_hash": "dfe643ec7b565ff5cd8f4a61acc8768e8eb3e210b1195df3ca1db71cd6457263", "target_hash": "1e98f8ca3642d1a437059d990eb2409da36d7a7441e5ff98b5e80af44a01f973", "exact_match": 0.0}
{"doc_id": 1766, "doc": {"question": "when was the japanese videogame company nintendo founded", "answer": ["23 September 1889"]}, "target": "['23 September 1889']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the japanese videogame company nintendo founded?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1889"]], "filtered_resps": ["1889"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c2022499163cacb7ea1b4116556d087659ad816fac9aebc6c5dbc10282d42763", "prompt_hash": "821f0650139542328bc1887672a0c25a2702e46d4a5034001cc7dca530e3574b", "target_hash": "9d021359bea030e7cce7411501e5197e0ff47971f770e4d06b49b1930089f0d6", "exact_match": 0.0}
{"doc_id": 1767, "doc": {"question": "who plays the evil doctor in wonder woman", "answer": ["Elena Anaya", "Spanish actress Elena Anaya"]}, "target": "['Elena Anaya', 'Spanish actress Elena Anaya']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the evil doctor in wonder woman?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Danny Huston"]], "filtered_resps": ["Danny Huston"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6126b38b503dd5b7d4bb9c557bd375f83d8a8f9fe124dfa7bb5b649efb49dd9f", "prompt_hash": "8cc06403371cc34f25326c1bcc64859331cc5f8c8a24dcdfe76516b2f57f2a57", "target_hash": "7da1b3514a4809cd4e6243e2c0506c1b16c59317cc05c69459d348fc7d422871", "exact_match": 0.0}
{"doc_id": 1768, "doc": {"question": "how old do you have to be to get a gun licence in nz", "answer": ["16 or older"]}, "target": "['16 or older']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how old do you have to be to get a gun licence in nz?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 18"]], "filtered_resps": ["18"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b0a79d5fff5839cdaa8c52b6b6455dba7d9f05de07fd7d751d394575e4a99ebd", "prompt_hash": "d0e8b2898d016e08db6285fbc13001f891233f623696f92637c574ea663edbed", "target_hash": "63e92a82b839d18834f52b0c257b828fc2d1419caaf49b6370a3afd22e0e5e6a", "exact_match": 0.0}
{"doc_id": 1769, "doc": {"question": "who is ishani in lies of the heart", "answer": ["Siddharth Arora/Vibhav Roy", "Nalini Negi"]}, "target": "['Siddharth Arora/Vibhav Roy', 'Nalini Negi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is ishani in lies of the heart?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Priya Apte"]], "filtered_resps": ["Priya Apte"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f6f5187ca11c9aa483c0df7c0d39fb7a12aa8f5532c596b4833e37a47ba6cd51", "prompt_hash": "b091714fcbbeb85cfe1d016b14ffb3ba3fc05e7218001e581cb97767c4cd6f8e", "target_hash": "b2ed8ce1debe2419765b0f7251b25e4ebd18466ba33a3fb003944c088084170f", "exact_match": 0.0}
{"doc_id": 1770, "doc": {"question": "when did indoor toilets become common in america", "answer": ["After World War I"]}, "target": "['After World War I']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did indoor toilets become common in america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1930"]], "filtered_resps": ["1930"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a3c7da37f2965114432c098827da6cade599b5955de046974624b0116fe7322", "prompt_hash": "840a75fdbc653319408c7ef990e9c722cff8ebbf5ee4b19f554d790e60027397", "target_hash": "c1db123f27a2ffdfd9df023d5abc76b63529f74a08999875150f1fe68bf069a0", "exact_match": 0.0}
{"doc_id": 1771, "doc": {"question": "who won the king of dance season 2", "answer": ["LAAB Crew From Team Sherif"]}, "target": "['LAAB Crew From Team Sherif']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the king of dance season 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Lopez"]], "filtered_resps": ["Jennifer Lopez"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "950d2feb40df77bf659e0b068a850b9d535bd2af4cf1200040c9fb5ae86c27e5", "prompt_hash": "fbf07d3081aab49a23b55e5a6a5849a03f8fb7818e47272f0eae59b55249af0a", "target_hash": "d81080d63dc81a12f4ee362f536dd93b3717536331817e5cfbed8f6f65881264", "exact_match": 0.0}
{"doc_id": 1772, "doc": {"question": "only player to play for manchester united manchester city liverpool and everton", "answer": ["Peter Andrew Beardsley", "Peter Andrew Beardsley MBE"]}, "target": "['Peter Andrew Beardsley', 'Peter Andrew Beardsley MBE']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: only player to play for manchester united manchester city liverpool and everton?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Peter Schmeichel"]], "filtered_resps": ["Peter Schmeichel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "70f6531bbe055ba5ff0eabca1bc8874db1228bfab4063ca7346c62052fa2d2b5", "prompt_hash": "ad306e450edf4ead2e0c5dc752995fd2e609b706d6d392252e09962d8d5cfc1e", "target_hash": "bcb3686eb3fe99ecf0ddcbf5a778140fb9f5b3081098e68c5dc8df2f2015478f", "exact_match": 0.0}
{"doc_id": 1773, "doc": {"question": "when did france and belgium invade the ruhr", "answer": ["11 January 1923", "January 1923", "1923"]}, "target": "['11 January 1923', 'January 1923', '1923']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did france and belgium invade the ruhr?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1923"]], "filtered_resps": ["1923"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c6bea18177321f706c61257d2269a073bb81a7391af91543bd196ae2fce7e304", "prompt_hash": "bfcfee38ebe22f540347871f95118fa1aa1ad93eaee2d761db8bcf74f753c781", "target_hash": "c02daa69f5a25776c1a59d31d1e2506e62d29eda0efffc3554f4b5a09e83c743", "exact_match": 1.0}
{"doc_id": 1774, "doc": {"question": "who plays the first nfl game of the season", "answer": ["the New England Patriots", "the Kansas City Chiefs"]}, "target": "['the New England Patriots', 'the Kansas City Chiefs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the first nfl game of the season?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dallas Cowboys"]], "filtered_resps": ["Dallas Cowboys"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d54e2d4b2b90290728f4e826d1af8f0d708fecb0177209b9ad0cce008c44f62c", "prompt_hash": "9bc10b3039a5d00985965905dd2ade85199512c9ab505d851fd811641bb56167", "target_hash": "9397f24830ae8d56e35586d3c9f494f54a3acb5f3da12b2ea89a799c42e89757", "exact_match": 0.0}
{"doc_id": 1775, "doc": {"question": "who sings in walk hard the dewey cox story", "answer": ["John C. Reilly"]}, "target": "['John C. Reilly']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings in walk hard the dewey cox story?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John C"]], "filtered_resps": ["John C"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d83a19516adcc618035f5a890de7f3cd0124707ebcdd875e8e0eb01f3584ca4a", "prompt_hash": "6ad7252517e6fe7b7be473b03fbd3db51850967bf7841be0d216b7bbd34fa743", "target_hash": "5881f02ca9b9e922d4176b13168bf2a3d3cdda4135161f78e901e33c458a88ee", "exact_match": 0.0}
{"doc_id": 1776, "doc": {"question": "who won the american league pennant in 2017", "answer": ["Houston Astros", "the Houston Astros"]}, "target": "['Houston Astros', 'the Houston Astros']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the american league pennant in 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Houston Astros"]], "filtered_resps": ["Houston Astros"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c69a54b1749a8ba53294e45ccf77f006a615a2f702bbe141f48ee0286f99d11", "prompt_hash": "15a26f4cdf37bbb1faac307491cf9c638096a56c224d85d7f623dfc298a66445", "target_hash": "5671c4cc49111281b09501f5a1e2f7b673a43dc5a489f6c0fcc84bc83320be07", "exact_match": 1.0}
{"doc_id": 1777, "doc": {"question": "where are the san juan mountains in new mexico", "answer": ["northwestern New Mexico"]}, "target": "['northwestern New Mexico']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the san juan mountains in new mexico?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" southwestern"]], "filtered_resps": ["southwestern"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3bf3060a2298b63cb7e0d1c4d15316ba65cce835f1ff5d16d878b3d3ba593eb2", "prompt_hash": "ee1c2e6c20bde172afe601a82d805b0e04bb45b790129a8a9a5d6d78eb8adaba", "target_hash": "e5d69a3410e438f195216ed3212d1cee9d3b1ea3585ede687d86add8e9b88153", "exact_match": 0.0}
{"doc_id": 1778, "doc": {"question": "who won last year's grand national 2017", "answer": ["One For Arthur"]}, "target": "['One For Arthur']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won last year's grand national 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Silviniaco Conti"]], "filtered_resps": ["Silviniaco Conti"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a7a39b8ff7c49b25cc32c0609ee8cd8d655452ebbc66440e722e5bf77d44fb92", "prompt_hash": "364dd1bf20ca8e1e3e234f0d25ee4da9f8830a639841e122ddb6af56b963f5ae", "target_hash": "07d4303700c95797409234a28983f42c9694ea0c5e9c680b254608b0ff5bba8e", "exact_match": 0.0}
{"doc_id": 1779, "doc": {"question": "who played daisy duke in the new dukes of hazzard", "answer": ["Jessica Simpson"]}, "target": "['Jessica Simpson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played daisy duke in the new dukes of hazzard?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Catherine Bach"]], "filtered_resps": ["Catherine Bach"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5446ec1aeb1da4d06a5927d25e5f7795ba30f0fa3d3a6cec61aeef5414385f45", "prompt_hash": "18f7b5e075150fc04d1dc7825a402f0be9a02b4a3effed2ef5f1e621031b8f86", "target_hash": "ad7afafc392cd1ba0569c1f2a3299783e38fc16df6a0ab9601ba92119dcee7df", "exact_match": 0.0}
{"doc_id": 1780, "doc": {"question": "who plays the dad in drake and josh", "answer": ["Jonathan Goldstein"]}, "target": "['Jonathan Goldstein']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the dad in drake and josh?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Josh Peck"]], "filtered_resps": ["Josh Peck"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c616b1f73e734bd1ef866c5208c9a9f9eabc6b471a6d712c769bbfb449db2ed6", "prompt_hash": "cfc5ec88a8a2111cf4ad57f27f7a6186fc054116be556aa305683c785e80d20d", "target_hash": "b2b970c784f14978e21fc5f0233c77ff31da5fd2a9c2d55d83048d13b519a37d", "exact_match": 0.0}
{"doc_id": 1781, "doc": {"question": "what do mobile numbers start with in the uk", "answer": ["0"]}, "target": "['0']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what do mobile numbers start with in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 07"]], "filtered_resps": ["07"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9def4a31b5b69456b47da2233f95e474a6f47144386af30dc7e469a1c1589ce7", "prompt_hash": "816b223f4334bb3ce33a3a3eca7b9873247a6cc79f5739a2090df58913b692e3", "target_hash": "e2c74822c1cc5dce364c34c308ab4af572f4452d7535f1db214df3d53ec362e3", "exact_match": 0.0}
{"doc_id": 1782, "doc": {"question": "where is the lost dutchman's mine supposed to be located", "answer": ["in the southwestern United States"]}, "target": "['in the southwestern United States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the lost dutchman's mine supposed to be located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arizona"]], "filtered_resps": ["Arizona"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "671196e6318149717e05d65214c5fd6894ff941b21a15c31d3643d8a015f1f23", "prompt_hash": "0c9e9ffedc681f05b07da692356bffca83c63fdee7834ec02f819c2e012ebd0c", "target_hash": "3ca9be40e2df0fd277a84f36d3945c46f0618d02602742e1a6cffeea27a3240d", "exact_match": 0.0}
{"doc_id": 1783, "doc": {"question": "when did hollywood become the centre of the film industry", "answer": ["1912", "By 1912"]}, "target": "['1912', 'By 1912']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did hollywood become the centre of the film industry?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1910"]], "filtered_resps": ["1910"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e430ae6bcbd2ea1230fffd562649e0b4c12721e4b46cc0e60fd45048e9d28d7", "prompt_hash": "3670f5ef8fba0367938ebd716674683f094b09535f0d6bc34f6d08aaaeefb204", "target_hash": "a836a83dd06196896db3c8adaa821b4a40c2391e91df79c1e88952dc6034293d", "exact_match": 0.0}
{"doc_id": 1784, "doc": {"question": "who is known as father of green revolution in india", "answer": ["Mankombu Sambasivan Swaminathan"]}, "target": "['Mankombu Sambasivan Swaminathan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is known as father of green revolution in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" M"]], "filtered_resps": ["M"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3a8008dedc87edec9d80ee61e3dfa9563bbc7bfc55528a719fb5ec6f8d8ba55", "prompt_hash": "c68cd0d9eb56ba24574722a56ac81d1528567f2d5040e42763f35ee17f5fa217", "target_hash": "84480408a855b021d5c8263e18d44133a05675dfc478b775a1d2c0356e88c063", "exact_match": 0.0}
{"doc_id": 1785, "doc": {"question": "who brought the idea of castles to england", "answer": ["Norman invaders"]}, "target": "['Norman invaders']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who brought the idea of castles to england?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William the Conqueror"]], "filtered_resps": ["William the Conqueror"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bac2bbc6652e1d9acd8cfaa7528163aa68b2e665c08fc687c80298cf2c25c18b", "prompt_hash": "8811236b27e6fed0347cc0f60ae79bb7a168e39856a1f735cb7b4acd19b4ae67", "target_hash": "505e576b5c6ff25ee867e84bbcafd7b5599e8a12d49363f6f812cd3ce9a6de82", "exact_match": 0.0}
{"doc_id": 1786, "doc": {"question": "when do willow and tara get back together", "answer": ["Entropy", "Seeing Red"]}, "target": "['Entropy', 'Seeing Red']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do willow and tara get back together?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 5"]], "filtered_resps": ["season 5"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4da91273f9c92b283be5ac1d77bc9109b7a17d73e5ccfc84d77f85ccec26ee5", "prompt_hash": "73b0cc929f141173ec7006cbcb4a393eb99122cc520a8445be5f6af424852793", "target_hash": "e95fa93bb6e366ac4c3ef1b9722f2545bb65a79f3961dd750de892a458651936", "exact_match": 0.0}
{"doc_id": 1787, "doc": {"question": "what type of artwork was created in the safavid empire", "answer": ["architecture", "metal", "ceramics", "gardens", "book", "glass"]}, "target": "['architecture', 'metal', 'ceramics', 'gardens', 'book', 'glass']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of artwork was created in the safavid empire?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" miniature"]], "filtered_resps": ["miniature"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb2a3f1a267261cfeeb9861721e6a2747e08f45e84472846acd0cc34b45d4ecb", "prompt_hash": "4869c02110385149c6bfe232215d6b6cda5808359a460843dbfc304fe9e19da4", "target_hash": "c0091d7b1b31c49022fdd635b3d00df8d425ffaa78e7c3c4c7b1b1d4b6f8f8ce", "exact_match": 0.0}
{"doc_id": 1788, "doc": {"question": "what is the current mac os operating system", "answer": ["10.13.4", "macOS High Sierra"]}, "target": "['10.13.4', 'macOS High Sierra']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the current mac os operating system?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" macOS Mojave"]], "filtered_resps": ["macOS Mojave"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b67cdf57e330acde443b05bc1e44ecd115a6bc7ed6fb04482ad6f8a75e435963", "prompt_hash": "05fccb9e811b67c7d8e645c2dd9af9bea6c8669e6966187bcba992c4202921fc", "target_hash": "e6fb3e31f1f2dbec4fd5475f71e9fed85a1f8cecc12f026f22223fdb8bae9dfb", "exact_match": 0.0}
{"doc_id": 1789, "doc": {"question": "who won the fountain of youth stakes at gulfstream park", "answer": ["Gunnevera"]}, "target": "['Gunnevera']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the fountain of youth stakes at gulfstream park?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bayern"]], "filtered_resps": ["Bayern"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a979ff071503c74a678c92e24e96d401d2b6d7c2cbcc245017bd37c6b6c483b", "prompt_hash": "688472f987c6ce84e3bf8048c8d1f8c7a73af1f5c5bfe59375c6a7089620a90d", "target_hash": "44478fa27f41f20ba5f246f68684166fca98c033bf025b70c0d623a8beff9ce4", "exact_match": 0.0}
{"doc_id": 1790, "doc": {"question": "who was the nfl first draft pick 2017", "answer": ["Myles Garrett"]}, "target": "['Myles Garrett']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the nfl first draft pick 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carson Wentz"]], "filtered_resps": ["Carson Wentz"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6203de8546c9ffc396e6e95fa86c133e777173bf94d2bcd93b2c38238a41ae0f", "prompt_hash": "acaf96f2cfb82163fab6214df363d5fe7efed100ab502c89fa3ea91fda1156b9", "target_hash": "2dbffc14a08add048a9f0bfb46df5a25f58b047ac8a0c4eaa956a54406dbe4c3", "exact_match": 0.0}
{"doc_id": 1791, "doc": {"question": "why wasnt agent matthews in miss congeniality 2", "answer": ["relocated to Miami", "gets relocated to Miami"]}, "target": "['relocated to Miami', 'gets relocated to Miami']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why wasnt agent matthews in miss congeniality 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew Broderick"]], "filtered_resps": ["Matthew Broderick"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c8347494392e3ddf4927dc897cb1c484bcbfb4b7575680fe2d0cb5246a40b7f0", "prompt_hash": "fd1c781ce82c857926c00470d8005d37190802c2f59bd97b25765f05ca1a019a", "target_hash": "3c2467eb6f3f3e4315b6e6a0d68157e30e5141b1af934d2a64b7c6d47fc08e58", "exact_match": 0.0}
{"doc_id": 1792, "doc": {"question": "what type of dog was laika the spacedog", "answer": ["mongrel female"]}, "target": "['mongrel female']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of dog was laika the spacedog?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Soviet breed of the Ovcharka"]], "filtered_resps": ["Soviet breed of the Ovcharka"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "36345e211495dd54aeb74dca1d5ebbdf64efe951d02d6b79431bb9261e08d2ad", "prompt_hash": "b292bec96c6d92df4776149f7309ac3e156a73caf3d787c5ef93e9f6ca4accdf", "target_hash": "fb37b93837507cc30ec1d117c5ab0efa4be030d6621cca2081851f1c224155f3", "exact_match": 0.0}
{"doc_id": 1793, "doc": {"question": "who signed the declaration of independence for maryland", "answer": ["William Paca", "Samuel Chase", "Charles Carroll of Carrollton", "Thomas Stone"]}, "target": "['William Paca', 'Samuel Chase', 'Charles Carroll of Carrollton', 'Thomas Stone']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who signed the declaration of independence for maryland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Paca"]], "filtered_resps": ["William Paca"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cdc1f20051bbbfd317db1dcc7d6b790256755488b29667bf630fb1b7e7906a85", "prompt_hash": "805efd8f387fc2b65e7ac03f281459cb2be6f0fff3f9075394eba39effd361dc", "target_hash": "a5fa2114d5bbf723bd93448f1773d1f48ec4b2c7a6847681382fcd263ad68163", "exact_match": 1.0}
{"doc_id": 1794, "doc": {"question": "who expanded the territory of china during the qing dynasty", "answer": ["Qianlong", "the Qianlong Emperor", "Prince Bao"]}, "target": "['Qianlong', 'the Qianlong Emperor', 'Prince Bao']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who expanded the territory of china during the qing dynasty?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Emperor Kangxi"]], "filtered_resps": ["Emperor Kangxi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5afe1266c432f089aec9582b050c9053e2a49bca7199f31e3c93143f600d188f", "prompt_hash": "43f5647ac5e5150cca06d97285b50eafecedf921a32917ffb59b6f78f6214ce9", "target_hash": "10f9f37b7ba3842a0b8c0bf385a231aeceada40523e03fa1cf8a619b7215d2bf", "exact_match": 0.0}
{"doc_id": 1795, "doc": {"question": "who has scored more goals in the premier league", "answer": ["Alan Shearer"]}, "target": "['Alan Shearer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has scored more goals in the premier league?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alan Shearer"]], "filtered_resps": ["Alan Shearer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "09ed0b4b44b913c4dea1171c33707358065bc33f1711ca88c211b0c0057dff84", "prompt_hash": "ff876676980d509be4d1c886ee2b6c0a72ae1fd36ec8c504f03432509ab71005", "target_hash": "3bcf64e1710861821786a86b59ce3238b267e2f1886b12cfa7734590af990912", "exact_match": 1.0}
{"doc_id": 1796, "doc": {"question": "who plays kevins shrink on kevin probably saves the world", "answer": ["Richard Masur"]}, "target": "['Richard Masur']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays kevins shrink on kevin probably saves the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Early"]], "filtered_resps": ["John Early"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ccadd8f82c7ab020cbf9190c817163b286d682bff77de7d47da59d4a435d54a0", "prompt_hash": "021e1917b7d35470c7929fd905ab057d862b7e95c06467aab43a09a8a071efe7", "target_hash": "a0c38263c781d8ba747b3d79371e45f0da09182c4ef398d6faa77c178f8fb023", "exact_match": 0.0}
{"doc_id": 1797, "doc": {"question": "where was the 2015 rugby union world cup held", "answer": ["England", "Wales"]}, "target": "['England', 'Wales']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the 2015 rugby union world cup held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" England"]], "filtered_resps": ["England"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "45e784e5f3c76bd4f8cce0afa88829143445345567864ec72eca34db5823ffd0", "prompt_hash": "2ba19b1dce0650c63d0d99c8ad2e6ec7e2c404aa053882c36fede0254029bf60", "target_hash": "6ff33155311ef16502044ada36077111a1def51eb8674780390533b345893b9f", "exact_match": 1.0}
{"doc_id": 1798, "doc": {"question": "who won an oscar for schindler's list", "answer": ["Steven Zaillian", "Michael Kahn", "Steven Spielberg", "Janusz Kamiński", "John Williams", "Ewa Braun", "Gerald R. Molen", "Branko Lustig", "Allan Starski"]}, "target": "['Steven Zaillian', 'Michael Kahn', 'Steven Spielberg', 'Janusz Kamiński', 'John Williams', 'Ewa Braun', 'Gerald R. Molen', 'Branko Lustig', 'Allan Starski']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won an oscar for schindler's list?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ben Kingsley"]], "filtered_resps": ["Ben Kingsley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0adb7b87bf94660a084bd01984ffb1384661601d621de4bcea383a40e3bf1339", "prompt_hash": "298c3fa10f27187dd43f741071d0c3a7789b783755990bbd767abc9792cf03c7", "target_hash": "815a52a72b756a46ea386ba93c7c549834986f00d5e83ff94734e4f172ba7a59", "exact_match": 0.0}
{"doc_id": 1799, "doc": {"question": "when did britain set up east indian trading company", "answer": ["31 December 1600"]}, "target": "['31 December 1600']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did britain set up east indian trading company?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1600"]], "filtered_resps": ["1600"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7b873b31f6b8226e7e17aced96dcce9d66fcf7fe6cb96720e162fd5567fad5d", "prompt_hash": "f31a8af395181d77ea56ef695bae17f40fef6791b768e8bafd140337a3a07347", "target_hash": "f99023224feea35e25b9f61d03aece87e561c8d2aacb02cdeae61b9904b22e6b", "exact_match": 0.0}
{"doc_id": 1800, "doc": {"question": "where was the movie jeremiah johnson filmed at", "answer": ["Utah"]}, "target": "['Utah']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the movie jeremiah johnson filmed at?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Montana"]], "filtered_resps": ["Montana"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1ba5bb0221cf1c4a2128d98a6b9460f4b2450af110740a61d68f372806dc9acd", "prompt_hash": "c548644e35fe8774d1c543109a4565d0efe3ed369f3efb08323fb4a4b16028b0", "target_hash": "d664615b3920dc2184da808616877db474edfaa89160d657c8ef41abaf63db1b", "exact_match": 0.0}
{"doc_id": 1801, "doc": {"question": "where does a wrinkle in time take place", "answer": ["Connecticut"]}, "target": "['Connecticut']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does a wrinkle in time take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the universe"]], "filtered_resps": ["the universe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1776da9f3c9f222f9df7dc000ad70431716e426b9d82c6813c9020247a6fa48e", "prompt_hash": "5a491b87f5f52dd730b2b7c4b97119fb4a7ecf961b88e6450627676dfc109180", "target_hash": "01bf8a42f693ce33cb54b8c9373becb19cef2928f33ec5cd64f57097f47290c6", "exact_match": 0.0}
{"doc_id": 1802, "doc": {"question": "where did the potter's wheel first develop", "answer": ["in the Near East", "the Near East"]}, "target": "['in the Near East', 'the Near East']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the potter's wheel first develop?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Greece"]], "filtered_resps": ["Greece"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "782f6295e6a99256ec9c464be7a43c75780d2fc307858bd7c37b9600b5d9673a", "prompt_hash": "3eebd5ac2704c9b8e19350d36adbf0435a9ee19fb3639cc127e33b6e31d7ff8f", "target_hash": "b665bdd40db5576722642aa1a7f442d427af1667efa9383cef4db32820a51288", "exact_match": 0.0}
{"doc_id": 1803, "doc": {"question": "when did where are you now come out", "answer": ["February 27, 2015", "February 27, 2015"]}, "target": "['February 27, 2015', 'February\\xa027,\\xa02015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did where are you now come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3bc11aef622b923b102acffbea66469b301311783c3166d790a786c127f533ac", "prompt_hash": "76dbd16044c04ca2f620221c922e1c4ff9724ea030f6fe53324634868fb97588", "target_hash": "56369bc516e1a390f62c10c5adde0d9a0525ff9a8edb66ad5e8052b95290c254", "exact_match": 0.0}
{"doc_id": 1804, "doc": {"question": "who are the two teams in super bowl 2018", "answer": ["New England Patriots", "Philadelphia Eagles"]}, "target": "['New England Patriots', 'Philadelphia Eagles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the two teams in super bowl 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Philadelphia Eagles"]], "filtered_resps": ["Philadelphia Eagles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a5eaf86582cfbf57f424052d903739c61072f3a2951f399909d6a8d8c8de7e2", "prompt_hash": "fb24e2e69bc9de42815d12597d02c0b9021e3babd2c27525507f3355fe61cd6b", "target_hash": "fc2e301f6c33e56921994f8928b9a092707583cc7db389fe60b1ce3429700abd", "exact_match": 1.0}
{"doc_id": 1805, "doc": {"question": "what is dj's boyfriends name on full house", "answer": ["Steve Hale", "Steven \"Steve\" Hale", "rich kid Nelson", "Viper"]}, "target": "['Steve Hale', 'Steven \"Steve\" Hale', 'rich kid Nelson', 'Viper']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is dj's boyfriends name on full house?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jesse              Edit A: Joey              Edit"]], "filtered_resps": ["Jesse              Edit A: Joey              Edit"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4a1eaaa66896a1dd779d3170e387b84a59c77c935b369246a8535c6abb6e2724", "prompt_hash": "d8eeb51eaa67aac677ba15780c9589b181a735630c1060e78fdfaa62993ecde6", "target_hash": "c23af4008ff24612b062386818e97105ec6eeaa85830a8f0979efe3bdf3fa0c9", "exact_match": 0.0}
{"doc_id": 1806, "doc": {"question": "when did end of the road come out", "answer": ["1992", "June 30, 1992"]}, "target": "['1992', 'June\\xa030,\\xa01992']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did end of the road come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2008"]], "filtered_resps": ["2008"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d9e68ee1cd75c1d2ac2656196b17551a0877255036f4e98bc5575d2603aee15", "prompt_hash": "9a926bc236f04c39d5ed5d808999608cebdf20d1d4af0de545b2efb11f5d237f", "target_hash": "a2e6859592b6771b0444a1379d5ca88defbcdc8468719ecdda1a5e2956281e28", "exact_match": 0.0}
{"doc_id": 1807, "doc": {"question": "what year did the golden state warriors win their first nba championship", "answer": ["1947"]}, "target": "['1947']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what year did the golden state warriors win their first nba championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1975"]], "filtered_resps": ["1975"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c74e6059d43ead9f2e09af83a3d6d6478156a73af5ac1b1a70b817fd5f824c8c", "prompt_hash": "fefb1cbf5f63ce239fcb269325236ff1b56f2d92b7a94a736d1d363372d39164", "target_hash": "ce89c117e7809addaa509799a7df858149b33432c76a4f26e923945b8dccde98", "exact_match": 0.0}
{"doc_id": 1808, "doc": {"question": "who sings i want to be a rockstar", "answer": ["Nickelback", "Canadian rock band Nickelback", "Billy Gibbons of ZZ Top"]}, "target": "['Nickelback', 'Canadian rock band Nickelback', 'Billy Gibbons of ZZ Top']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i want to be a rockstar?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Story So Far"]], "filtered_resps": ["The Story So Far"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "24bb61505d121432f28a75798668a43df581b312b4c8e2bb8d66ab09ac1da397", "prompt_hash": "162ce058ba99d6bd7c27be02af2350b389102c9297d25c89c55360e3398ca5d6", "target_hash": "5c01756b5cbd4c8c76f74780430e8b1117ad965bab7c207356836fc94baaa6f4", "exact_match": 0.0}
{"doc_id": 1809, "doc": {"question": "when was the last time new england had a hurricane", "answer": ["Hurricane Hanna"]}, "target": "['Hurricane Hanna']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time new england had a hurricane?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1938"]], "filtered_resps": ["1938"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b13409e7b42269278bd1dbd8d9408c14200f5a623f589afe68d39a6549c469d8", "prompt_hash": "fc7bf39f746144c60e1a753508b10f2b0dc463c8c9b44c4d605b8552dd5207b9", "target_hash": "f5419ae48e8d525c0865896f264992245579c8e89dd289da46732127bc4dbacf", "exact_match": 0.0}
{"doc_id": 1810, "doc": {"question": "who won every men's biathlon event in the 2002 winter olympics", "answer": ["Norway", "Ole Einar Bjørndalen"]}, "target": "['Norway', 'Ole Einar Bjørndalen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won every men's biathlon event in the 2002 winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ole Einar Bjørndalen"]], "filtered_resps": ["Ole Einar Bjørndalen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "40ddf1853cc75dbf350cf9448beee3929f01089bada14e31323cf978853e486c", "prompt_hash": "e9632fbe519c570def5b18ed3dd661cd93ca60ed20aa4c4d2fb3259953f5489b", "target_hash": "db192727ff77b743ef77bc98cdb8471c5665acafe3e354712cc41facd6e54eb6", "exact_match": 1.0}
{"doc_id": 1811, "doc": {"question": "who does dwyane wade play for right now", "answer": ["the Miami Heat", "Miami Heat"]}, "target": "['the Miami Heat', 'Miami Heat']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does dwyane wade play for right now?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Miami Heat"]], "filtered_resps": ["Miami Heat"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dfd56b506c1a642fea714ec692ae511c121a3c17e4d65b16dd29c92ed9d8375e", "prompt_hash": "b5f4e91664b1fb3e4de679366bf5c7fb9365c7babb9bb9faa541b237e622c489", "target_hash": "70d367152e1496c84bad411446c7baf73eb3edebccf6081f46a9e67af697174a", "exact_match": 1.0}
{"doc_id": 1812, "doc": {"question": "who played the nurse on andy griffith show", "answer": ["Julie Adams", "Langdon"]}, "target": "['Julie Adams', 'Langdon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the nurse on andy griffith show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jean Anderson"]], "filtered_resps": ["Jean Anderson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "79c0c48f774da5c05789167d1acdab492204605083e9f5a5873bb57c313579c8", "prompt_hash": "68c26efa0405e1699e4a4c0d6e693c99171163dde620553ea5aaae967d9fe7d0", "target_hash": "57c9b610b6052af5ba9c7ec68fca24659ae63002fffeb1e243f962de1dfe259f", "exact_match": 0.0}
{"doc_id": 1813, "doc": {"question": "what is the form of mozart symphony no 40", "answer": ["G minor", "minor key symphonies"]}, "target": "['G minor', 'minor key symphonies']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the form of mozart symphony no 40?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" G minor"]], "filtered_resps": ["G minor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18427c6322901037735d135358c2bece8a456b84174e7f97d7e34aeaa9ae76c4", "prompt_hash": "ed22e26691b9df4d63988951702e9704ff7e0a7770512c527c6f283b4c0c22c8", "target_hash": "58641aa6636ee485c69ba1e8fcd8d2223e7b7835e4550d905a9516ab1fdd58b6", "exact_match": 1.0}
{"doc_id": 1814, "doc": {"question": "who is the head of parliament in uk", "answer": ["The Lord Fowler", "John Bercow", "Elizabeth II"]}, "target": "['The Lord Fowler', 'John Bercow', 'Elizabeth II']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the head of parliament in uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Speaker of the House of Commons"]], "filtered_resps": ["Speaker of the House of Commons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7be03e475cacf9a1a0c3814bf4b87cfe8e7abfa3fa73f9849dfb2550b33a3f14", "prompt_hash": "78c7caab4e7736673370b6d94c8efd4e6a8fcbffb53e970b59b09c20738c00c8", "target_hash": "9bbd9d15c1c74e847f4284df3242dc18d8e8179d80fedade7e55f645d04023c0", "exact_match": 0.0}
{"doc_id": 1815, "doc": {"question": "where does the sciatic nerve run in the foot", "answer": ["on the posterior aspect"]}, "target": "['on the posterior aspect']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the sciatic nerve run in the foot?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sole"]], "filtered_resps": ["sole"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "317f3a78e36fa5fc8beaba31dcc7b48676d1f1775eee4b4e441cd0df2f2bc2df", "prompt_hash": "fad2d8d4c39f3fb3d55cebe99615f59ccf1b0468eb900f052c9aeee3aed7ade0", "target_hash": "efeddac9c85178d8a685cf63b042db850d63bcd0a69e010b0f2297715ead08e4", "exact_match": 0.0}
{"doc_id": 1816, "doc": {"question": "who won the workd series of poker 2017", "answer": ["Scott Blumstein"]}, "target": "['Scott Blumstein']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the workd series of poker 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Daniel Negreanu"]], "filtered_resps": ["Daniel Negreanu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4896d6f97ec074c621b2510e36d658093ab991e283bd9a09a11c7a17ce6eba9b", "prompt_hash": "550a87e8e50eda7848790ea28a37967716d2047872278acd35fe617b2b60c1d8", "target_hash": "a6586f488963029239f685285fe1130338091a1b1bef0014ef073637111186b1", "exact_match": 0.0}
{"doc_id": 1817, "doc": {"question": "the creation of human beings in the kumulipo happens during which wā or period of creation", "answer": ["In the ninth wā", "the ninth wā,", "the ninth wā", "the ninth"]}, "target": "['In the ninth wā', 'the ninth wā,', 'the ninth wā', 'the ninth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the creation of human beings in the kumulipo happens during which wā or period of creation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" wā kaona"]], "filtered_resps": ["wā kaona"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ddcb6d35c6031f9ebd9bcf3c1e9fdafa5c4857857f09dc167032d2d06eca55d2", "prompt_hash": "ff23b129a938398712f99796f7a963001ae72266d43e9ac24fb2e9446425bb2f", "target_hash": "efbeda7e11febdf6fa5d2c82bab265cf66fd7eeff590d98ed2a5db4b39f284c0", "exact_match": 0.0}
{"doc_id": 1818, "doc": {"question": "who wrote hit me baby one more time", "answer": ["Max Martin"]}, "target": "['Max Martin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote hit me baby one more time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Britney Spears"]], "filtered_resps": ["Britney Spears"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fcf9e030607cce7c33b87774d6ba42d218b8e6dd0a0500758ad1dc5b7ddba05e", "prompt_hash": "bcfaead30bcc0e8db3c44069ad76428a9e1ef040b1d04159ca643260a38a8b0a", "target_hash": "468129cbb18428e45e933a702c2e661f2dd3b09aef8c6dee7ab70a0e931665e9", "exact_match": 0.0}
{"doc_id": 1819, "doc": {"question": "what is cost of first class mail stamp", "answer": ["49 cents", "47 cents", ".49"]}, "target": "['49 cents', '47 cents', '.49']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is cost of first class mail stamp?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 0"]], "filtered_resps": ["0"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b98789e5ff7037d47ee02cc64974860b078c0e31403c8c3b0238cdf51f0caaa0", "prompt_hash": "26fe797b34ec567bec7a86c3cdf0262e1b4ac9e458ab8b758d1e65405a6b7dbc", "target_hash": "0a5fe9bcfa02421061201034046fd0d4c603f4bb06f72c7a5a19f1e6ee2db056", "exact_match": 0.0}
{"doc_id": 1820, "doc": {"question": "who wrote the book the origin of species", "answer": ["Charles Darwin"]}, "target": "['Charles Darwin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the book the origin of species?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Charles Darwin"]], "filtered_resps": ["Charles Darwin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42385eaa1d4e3ebf13573797d6d026a569801cb1e0754ffd9ca82917bb6afe15", "prompt_hash": "0cdfd196898ec47759df8669063c0485481f7e79cdad8356dc54db94cfdb5513", "target_hash": "63f3c04c83e03302bd7af0f552252ca49c4871e3e7b3e5c78c390233789533fa", "exact_match": 1.0}
{"doc_id": 1821, "doc": {"question": "what's in a beam me up scotty", "answer": ["phencyclidine", "cocaine"]}, "target": "['phencyclidine', 'cocaine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's in a beam me up scotty?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" photon beam"]], "filtered_resps": ["photon beam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fa5dca15b989b0681ee8b4ced1ae7d3f0c34878274851204a690c0baddef06fe", "prompt_hash": "bca8aa7717a01b19c08b5364a5d0f1ebfec2120dfaa76e3568693e32163f89a0", "target_hash": "aa2b8392db004ce011bcf5f62f664e6726c906b84adcd15975ff11779e2c9b92", "exact_match": 0.0}
{"doc_id": 1822, "doc": {"question": "where does the red river start and end in louisiana", "answer": ["the Texas Panhandle"]}, "target": "['the Texas Panhandle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the red river start and end in louisiana?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Texas"]], "filtered_resps": ["Texas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff08ab6a008565fd3e43b9fd75b7a499d321dfc9c65ecd34aa74ef23ecc3b291", "prompt_hash": "07a33429f38d1f01ee39e1622b04522b079b0fd101d8199f9bfebcff71bfa790", "target_hash": "cef894c02d8b64d9210523b085908c9afb6d736a0e1d0de8474bbe81e8370257", "exact_match": 0.0}
{"doc_id": 1823, "doc": {"question": "what is the official symbol of the carnival of quebec", "answer": ["Bonhomme Carnaval"]}, "target": "['Bonhomme Carnaval']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the official symbol of the carnival of quebec?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the maple leaf"]], "filtered_resps": ["the maple leaf"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6c1179cb580b4c5c23f372a8e28090ffcaa78ed7ea4ab94d014404c84e3ea835", "prompt_hash": "eb62e8a73213f5f450b5fda6c67ce683c78f3d68257fc4a56e33979523b072d2", "target_hash": "d11e4fcca1b958ae358e5c05018d5f623f2acecd374ca3a3560de9fc267ff63c", "exact_match": 0.0}
{"doc_id": 1824, "doc": {"question": "what is the main mineral in lithium batteries", "answer": ["Lithium", "lithium"]}, "target": "['Lithium', 'lithium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the main mineral in lithium batteries?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cobalt"]], "filtered_resps": ["cobalt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55b5ce7892c4cb9f07faede4c9f50e74ff718594dcae85659684a9041e46f0ca", "prompt_hash": "f9ebcddedb3f7cc0a702044285968651a09b474dc0914831521af03c273eceb7", "target_hash": "80c419670f9cb1cebf43cebd04dd4d4751d85d6789c1fba621fd5881f31f8705", "exact_match": 0.0}
{"doc_id": 1825, "doc": {"question": "who played the middle sister on full house", "answer": ["Jodie Sweetin"]}, "target": "['Jodie Sweetin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the middle sister on full house?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mary Kate Olsen"]], "filtered_resps": ["Mary Kate Olsen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "816ce44372c2476910cd93e0c50d90e102d0f3e51fe0847022a2a5bbebc68fb5", "prompt_hash": "79b2f7df6323de89c1ec8760dcc4fe2a670cf91f148647ed809fa85e1e77e01a", "target_hash": "042dd31a47085ed6d491a748ac84732a338c016e904b5f14b403b863b51fbc0b", "exact_match": 0.0}
{"doc_id": 1826, "doc": {"question": "who plays caesar planet of the apes 2014", "answer": ["Andy Serkis"]}, "target": "['Andy Serkis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays caesar planet of the apes 2014?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gary Oldman"]], "filtered_resps": ["Gary Oldman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ac5a3753423105d4edf2271208d753cb0c74962227de7c2f11fd2b43e81bb18e", "prompt_hash": "a6d34016c9aca73eee7ea90aea75fa15143647806600bb209fc6942fbc2bb49d", "target_hash": "68a452147a514270edb8f4be4410f8bc9fc99d58d1fb52d680dd7201717ca54d", "exact_match": 0.0}
{"doc_id": 1827, "doc": {"question": "what type of car is a jeep considered", "answer": ["off-road vehicles", "light utility vehicles", "sport utility vehicles"]}, "target": "['off-road vehicles', 'light utility vehicles', 'sport utility vehicles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of car is a jeep considered?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" SUV"]], "filtered_resps": ["SUV"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "894683a5441b2db869e7f0645dc498b873859e28e1eb2740b4222e8963a7c1c6", "prompt_hash": "ffc0aeaf74ff3003e08457adf66586e55cefdaf6bc9902abf3a74fe72a3dbf73", "target_hash": "9576410c0a679d1eae3a93960291a85173bea572a59202788da119eab1769e90", "exact_match": 0.0}
{"doc_id": 1828, "doc": {"question": "how many us states currently use capital punishment", "answer": ["31 states", "31"]}, "target": "['31 states', '31']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many us states currently use capital punishment?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 31"]], "filtered_resps": ["31"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5aa7f8517b389b1a1e590893e6318612ba9a0762970aaa870d6d177c7e6be626", "prompt_hash": "8cde08b44b86bda4d59666dc5128e7d66dfd603c4c387a733649f68ab1ec16bc", "target_hash": "2e9d566262040b1c2d7540f1c8dcbd47efa02bd4cc5c875843958db6d6e66cc2", "exact_match": 1.0}
{"doc_id": 1829, "doc": {"question": "when was the last time dallas cowboys won the super bowl", "answer": ["1995"]}, "target": "['1995']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time dallas cowboys won the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1995"]], "filtered_resps": ["1995"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4f6210c9c21694f6677c7a91760b73fe4346a5b6b6bd65997455d0ff932ce155", "prompt_hash": "49cafed31f237197198d082aa1c244716b4452f305713107251652f9f30a5658", "target_hash": "fa78c62118c7a21177eaba595abbbd9aac5656a594a0bdac7cd33675edf1399d", "exact_match": 1.0}
{"doc_id": 1830, "doc": {"question": "where is salt mined in the united states", "answer": ["Avery Island, Louisiana", "Houston, Texas", "Avery Island", "Saltville, Virginia", "Central New York", "Hutchinson, Kansas", "Syracuse", "Grand Saline in East Texas", "Detroit, Michigan", "Western New York"]}, "target": "['Avery Island, Louisiana', 'Houston, Texas', 'Avery Island', 'Saltville, Virginia', 'Central New York', 'Hutchinson, Kansas', 'Syracuse', 'Grand Saline in East Texas', 'Detroit, Michigan', 'Western New York']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is salt mined in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Utah"]], "filtered_resps": ["Utah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5f9967e3bdaac5439998e3d191abcfef0efe8ecf1b5f1ed94c9f18132135639a", "prompt_hash": "e303afa1259fb5c5e58a00861c42ede3c30bf6469d57b1e67821fffb12b6a28c", "target_hash": "2e955efd1cbba66b51b7c053f51e9b2916d4acdd6398157fd1b3753988e04dcc", "exact_match": 0.0}
{"doc_id": 1831, "doc": {"question": "when does the new episodes of supernatural start", "answer": ["October 12, 2017", "May 3, 2018", "October 12, 2017"]}, "target": "['October\\xa012,\\xa02017', 'May\\xa03,\\xa02018', 'October 12, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new episodes of supernatural start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 11"]], "filtered_resps": ["October 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "54be4e48de86e6bf29ff01fc971c9f01241fb952e49001926c92634c6d5942e4", "prompt_hash": "985c9efc8cfd104f9c2ffddce157ec46a0b8e2e0f450bb5364a2c75f9c422e2e", "target_hash": "94abb82f307bd68ae6fab4a67b1de61f6d0a071e8062940f22057a3baf43dfea", "exact_match": 0.0}
{"doc_id": 1832, "doc": {"question": "who played pink in pink floyd the wall", "answer": ["Kevin McKeon as Young Pink", "Bob Geldof as Pink", "David Bingham as Little Pink", "Bob Geldof", "David Bingham", "Kevin McKeon"]}, "target": "['Kevin McKeon as Young Pink', 'Bob Geldof as Pink', 'David Bingham as Little Pink', 'Bob Geldof', 'David Bingham', 'Kevin McKeon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played pink in pink floyd the wall?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bob Geldof"]], "filtered_resps": ["Bob Geldof"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42e92931e5abfc9068fabd7f46fcf0c9a3f4ce228d68cef01c22a353a7c8cf4b", "prompt_hash": "0f13afe049685cc812962c0d4f08f574b5916f02a9f75bc46917345e82c1494b", "target_hash": "19893dfebdf8a2ba2e4df27eee65116e2befab268bb87934dc1ded6e8908857d", "exact_match": 1.0}
{"doc_id": 1833, "doc": {"question": "when did goku first go super saiyan 4", "answer": ["Dragon Ball GT"]}, "target": "['Dragon Ball GT']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did goku first go super saiyan 4?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dragon Ball GT"]], "filtered_resps": ["Dragon Ball GT"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "14aaa4eb9facbc8ce5b019663615e6f007663381c702ebcdd73efdb4f683efe6", "prompt_hash": "24e1e93651826d19ba82665820fb8917014040d3007c1eab2b0cbd6c7aa27f24", "target_hash": "5309ad307f1d71d16a72a900b7458afaa084818fc4d9ba82c9f1affe02f1c856", "exact_match": 1.0}
{"doc_id": 1834, "doc": {"question": "where are the giant redwoods located in california", "answer": ["Humboldt County", "Del Norte County", "the coast of northern California"]}, "target": "['Humboldt County', 'Del Norte County', 'the coast of northern California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the giant redwoods located in california?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sierra Nevada"]], "filtered_resps": ["Sierra Nevada"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d9e89430d4e649bbd78a80e5d6312abcaecb704bb184d834d237b4f0f626f8a", "prompt_hash": "694bb68dc6b85a2d4f208b43fd1a82301977d8650758bedd851768c056fc4ba2", "target_hash": "b4f2a4b772b6a9a85006d4505e6b589771637bf84a16c9bbef4b712d334bfc49", "exact_match": 0.0}
{"doc_id": 1835, "doc": {"question": "who was the first woman to win who wants to be a millionaire", "answer": ["Judith Cynthia Aline Keppel"]}, "target": "['Judith Cynthia Aline Keppel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first woman to win who wants to be a millionaire?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Viji Jayaraman"]], "filtered_resps": ["Viji Jayaraman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "61655df93ebfd49c6b8106f1b48eefdc9c07e8adf541721bb7edbc166736e21c", "prompt_hash": "56261818e3abfa17b71683ce69e9ca43747b7501740cafdbac6a17be4c5b829c", "target_hash": "27d8ae5b58cdbd8ae3b05d7f41d1896a00083b6903fc505f6a6c14f40e53cbcd", "exact_match": 0.0}
{"doc_id": 1836, "doc": {"question": "who sings for the beast in the new movie", "answer": ["Dan Stevens"]}, "target": "['Dan Stevens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings for the beast in the new movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Adam Levine"]], "filtered_resps": ["Adam Levine"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "977137c18695cfaaa237d88e0570634fcd41641217789974850d7df8f3e5370b", "prompt_hash": "0cc2c8d7d4d59c372f35d89d6c485de0941ffca9144b98384fd93d90b0f7d268", "target_hash": "6a04d8fe5396ef286d29fe0326bf1b89ad9f548594e9c7200756d5277e71c444", "exact_match": 0.0}
{"doc_id": 1837, "doc": {"question": "industrial city in germany on the rhine herne canal", "answer": ["Henrichenburg", "Duisburg"]}, "target": "['Henrichenburg', 'Duisburg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: industrial city in germany on the rhine herne canal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Duisburg"]], "filtered_resps": ["Duisburg"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b914d67d362f65d3ac4549e66c5dc3b14a5f38c9c87dc1a21d41792e1acb64e1", "prompt_hash": "e9d5622d80cbe22bc3b01003265c1614b52ff1bcd3760b9ea675b41823884122", "target_hash": "df31936dfdc29f6983ea6c12056955d15837496b910d481a41a3762664446ae0", "exact_match": 1.0}
{"doc_id": 1838, "doc": {"question": "who declared war on the united states four days after pearl harbor", "answer": ["Fascist Italy", "Nazi Germany"]}, "target": "['Fascist Italy', 'Nazi Germany']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who declared war on the united states four days after pearl harbor?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Japan"]], "filtered_resps": ["Japan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5032ea2627b90242dc2c61d89f17f4e2bf7cdc1b338ca692f0bf66be597b50f4", "prompt_hash": "772083ddaf3c08317e90d67aaac8d947b148dfbc7a4f6329a5f84b7025bab6f1", "target_hash": "724839ad2d741e460cb8e5d9d3b8b4ab10374857c1afa4e7415578a1f88154b3", "exact_match": 0.0}
{"doc_id": 1839, "doc": {"question": "premier league players to score 5 goals in one game", "answer": ["Alan Shearer", "Andy Cole", "Jermain Defoe", "Sergio Agüero", "Dimitar Berbatov"]}, "target": "['Alan Shearer', 'Andy Cole', 'Jermain Defoe', 'Sergio Agüero', 'Dimitar Berbatov']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: premier league players to score 5 goals in one game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alan Shearer"]], "filtered_resps": ["Alan Shearer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b758994d9b2529c2c0cfd60dc130549636c3be0bd65b81b64d9aa69f58aa8bbf", "prompt_hash": "6686a2893af00c34652c37f7bfc59ea232318e5cb380e45d4467a0056c49b6c1", "target_hash": "756536212556de20c9bcf9c98fbc3a1dd4a1e1c1e13dc9d513beee22a74db6bc", "exact_match": 1.0}
{"doc_id": 1840, "doc": {"question": "an edge that is between a vertex and itself is a", "answer": ["loop"]}, "target": "['loop']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: an edge that is between a vertex and itself is a?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" loop"]], "filtered_resps": ["loop"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f7d4e15728cb737a3127b4e37a90ff550d56e4bb237adf36bb32904b8c644c2d", "prompt_hash": "2284414aba336166701e5e328104385afd96af82f034cb3b66a1b2516d7db435", "target_hash": "4531e1ea442c561420496a7b9548a6106d9532e0e4efdad61cc49298a9d2f5ff", "exact_match": 1.0}
{"doc_id": 1841, "doc": {"question": "who took part in the american civil war", "answer": ["nationalists of the Union"]}, "target": "['nationalists of the Union']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who took part in the american civil war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Union"]], "filtered_resps": ["the Union"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d3d7a48d43acf9aeb9d7265a0a7413d7d1abec92f02237d6d8f7e5b80deb520", "prompt_hash": "3e5342eb288008d43186f8c75913679c06c3595e18fb1093ac6e4d020c300241", "target_hash": "4da510f177a95eb417e83306e4400c96ee3630424ca72a5bc26640d5aac78e8a", "exact_match": 0.0}
{"doc_id": 1842, "doc": {"question": "when was penicillin first introduced to the public", "answer": ["After World War II", "1942"]}, "target": "['After World War II', '1942']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was penicillin first introduced to the public?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1942"]], "filtered_resps": ["1942"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c157dd3403f3877a73afbbdbd4b4c0a17fdd6bedfff312a7b56a639572d42f55", "prompt_hash": "8687fd4b40b5ae4f1113b50df4554d97f6703d2f0282aeb73714ad028cb3c6fc", "target_hash": "3a82894972892ab562c74d3aebfb5a16268ce07d2123c9d04d3acf1a2afe7218", "exact_match": 1.0}
{"doc_id": 1843, "doc": {"question": "what ethnic group celebrates its contribution to america in october", "answer": ["Filipino Americans", "Filipino American", "Filipino"]}, "target": "['Filipino Americans', 'Filipino American', 'Filipino']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what ethnic group celebrates its contribution to america in october?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hispanic"]], "filtered_resps": ["Hispanic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f4ee0af43c8c791cf52ded9d4ec534ae073aa778d06b6f1dca45e2169863a5d", "prompt_hash": "00b225eeef2a73809e78d956a661f1b7c52ac65f9d518e687fbde075ccf4b426", "target_hash": "ebb342fdda460313a2b277ac789f92cb5b190ef6ce3c5b93e19154b60e682d4f", "exact_match": 0.0}
{"doc_id": 1844, "doc": {"question": "when was the last time a hurricane hit massachusetts", "answer": ["November 3, 2007"]}, "target": "['November 3, 2007']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time a hurricane hit massachusetts?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1938"]], "filtered_resps": ["1938"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dccb8d92332706021784fbc0067a5f91b55a19e1d004dd43b1f6b99ae57c9dec", "prompt_hash": "3dbfb6a1e90b22b54224e6f373da2525085381d5730ae60b6e2188d1de751ca2", "target_hash": "9b2dedf300598cbd7f950a9f8d828839aed8ceb6d53c24442a9ab84f44a51302", "exact_match": 0.0}
{"doc_id": 1845, "doc": {"question": "who heads the executive department of west virginia government", "answer": ["the Governor of West Virginia", "the governor of West Virginia", "Jim Justice"]}, "target": "['the Governor of West Virginia', 'the governor of West Virginia', 'Jim Justice']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who heads the executive department of west virginia government?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Governor of West Virginia"]], "filtered_resps": ["Governor of West Virginia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77377e07e42d6e848a056814d5f771de73595c51cdd72c5a54b1683ffee5f225", "prompt_hash": "dbd2f60e65250c9bd578ab0752d81e83580d99bc0f451a8a2d4c03f69d60e7b0", "target_hash": "a711bc0a755e6fc6f71cdbfebafb9e50ea7ea4c7df7c1ea72a03fc2589665950", "exact_match": 1.0}
{"doc_id": 1846, "doc": {"question": "who owned the colts when they left baltimore", "answer": ["Robert Irsay", "Irsay"]}, "target": "['Robert Irsay', 'Irsay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owned the colts when they left baltimore?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Robert I"]], "filtered_resps": ["Robert I"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9bbc65236639f728a4ec347cc2ab88c38789e0d8c9f3ca84814ecb13ff4e695", "prompt_hash": "03d4e3fd8491cf032dd59d4a434214547285cc43f881520e467a88e58f0645b9", "target_hash": "9b5def6f8e588550d2199cead86d51eeff8336323188c74231585038eaa08b60", "exact_match": 0.0}
{"doc_id": 1847, "doc": {"question": "who plays elsa's aunt in once upon a time", "answer": ["Elizabeth Mitchell"]}, "target": "['Elizabeth Mitchell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays elsa's aunt in once upon a time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ginnifer Goodwin"]], "filtered_resps": ["Ginnifer Goodwin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "699ff87efce72a802b81254f01a6de47ec178eb1b1f93d2d264190c36756c3b1", "prompt_hash": "04e0ea49f5354456e2e39885934b1689260acc0fe45e1b1d83d8d66fc07b26ae", "target_hash": "3b6a992ca8f009c10216dfed517d8d4dcaa3a57c5ff77264b51f8dfe93d1d66c", "exact_match": 0.0}
{"doc_id": 1848, "doc": {"question": "which president supported the creation of the environmental protection agency (epa)", "answer": ["Richard Nixon", "President Richard Nixon"]}, "target": "['Richard Nixon', 'President Richard Nixon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which president supported the creation of the environmental protection agency (epa)?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Richard Nixon"]], "filtered_resps": ["Richard Nixon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d3b346424394aeb8a8ec6ff5b431400f22706aad7da2ec211eace70eb960da2", "prompt_hash": "dbd976c92340fbd35e6315b70c6f46d1d3178e5d5129b8effadcbba4ccb0fa57", "target_hash": "1450901b16326429eeeebbca508508d679fe5418b685f3a965330852667e45c0", "exact_match": 1.0}
{"doc_id": 1849, "doc": {"question": "who played taylor on the bold and beautiful", "answer": ["Hunter Tylo", "Sherilyn Wolter", "Sherilyn Wolter (1990)"]}, "target": "['Hunter Tylo', 'Sherilyn Wolter', 'Sherilyn Wolter (1990)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played taylor on the bold and beautiful?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Taylor"]], "filtered_resps": ["Jennifer Taylor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c4deb776c81ab36897208aed6fc61621c33e8f09e79383c4bae1391bec8d3354", "prompt_hash": "229919ca13c3ee1f2362bf3e877dabacbacb807efb36e1461afbe7c6a6f4a83d", "target_hash": "3d8788ee5070ec82a049e1be1512300ea82afae8915083cd8d77d699de6ca4ab", "exact_match": 0.0}
{"doc_id": 1850, "doc": {"question": "where are trigger points located in the body", "answer": ["muscles"]}, "target": "['muscles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are trigger points located in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" muscles"]], "filtered_resps": ["muscles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ce0ac84635fc1c83466c46df956bbf12a7025753196b2f8576ff92bbe7e63065", "prompt_hash": "476b51c615ec2fb8b575846ddee3c5bfbeb2b80b6c4c066d4df6f14541b8f701", "target_hash": "5856122f2a1a8073fcc6320b2418e665c1f3ed3b31de14df335bbd1ec42611b4", "exact_match": 1.0}
{"doc_id": 1851, "doc": {"question": "who sings that aint no way to go", "answer": ["Brooks & Dunn"]}, "target": "['Brooks & Dunn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings that aint no way to go?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kanye West"]], "filtered_resps": ["Kanye West"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0ebcb429a07c28697ee099d16ccec98838fb9f9a640543b3bd7a2fe23998e3a1", "prompt_hash": "8100f0cd86e02c9593018f42b9751935a4121a33706ebed5f7dc7a53db16ff52", "target_hash": "b4836cec96616078f3bae5bc9f1d12941a8d83b9196fe904186fa3989cc15668", "exact_match": 0.0}
{"doc_id": 1852, "doc": {"question": "where did they get the tree for rockefeller center", "answer": ["State College, PA"]}, "target": "['State College, PA']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they get the tree for rockefeller center?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New Jersey"]], "filtered_resps": ["New Jersey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4a175b7dcbc22660cb08e2ba55eb8d0708318d4878e001ce570d7fb3b5b7fada", "prompt_hash": "33131e987ad0b2c7816b6ee5f54c73122256e112f40b897d35b71d87c1985fbf", "target_hash": "86775487e908fa2190ebaa74e33dcfef65a887600d8ec5e83c65815f1b18989c", "exact_match": 0.0}
{"doc_id": 1853, "doc": {"question": "when does bill nye saves the world season 2 come out", "answer": ["December 29, 2017"]}, "target": "['December\\xa029,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does bill nye saves the world season 2 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 11"]], "filtered_resps": ["January 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "09dc4c20a7445818b3ed5e67418ea7ba33e48ce5bfd1f978bbc3a305eeec33ad", "prompt_hash": "c9c286e4191f43b39a40d992433a299707ac2ad41f6561ef57e32cbff0ee3b2b", "target_hash": "fc79d35a3b825b252cea6b3d1dbf603ca8507ba5e15ff3f250da4edfe3c30f6f", "exact_match": 0.0}
{"doc_id": 1854, "doc": {"question": "when did the song holiday road come out", "answer": ["1983", "June 24, 1983"]}, "target": "['1983', 'June 24, 1983']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the song holiday road come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1970"]], "filtered_resps": ["1970"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bdf02414edde7239624601e545abb503a6fac600763f1830ffe43c6d716fab4d", "prompt_hash": "05b973ef24cf78bb7b9b3363c19ab1420355c8340ca8b20463389aa22b0eddae", "target_hash": "6ebea96318b7eaf49a27195ef3cb16c4dbee8fe968f4ac42bc2ff2d1eef0d1cb", "exact_match": 0.0}
{"doc_id": 1855, "doc": {"question": "what was the purpose of the bantu education act", "answer": ["enforcing racially separated educational facilities"]}, "target": "['enforcing racially separated educational facilities']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the purpose of the bantu education act?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to provide a separate and inferior system of education for black South Africans"]], "filtered_resps": ["to provide a separate and inferior system of education for black South Africans"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a1b974064bbe096b80d199b5e0dda2db03faddb13ff1d1bf2c7f2f5c6762f2a", "prompt_hash": "995c6fc6d76ff1664c359ab1cf6151bc4e9726b5dae6dbd9d0b5b7a3270b2d9a", "target_hash": "a82ccf04e3a958b2a52d83e3bbd50bc4d9f04a1b68fd06bf08e99790fecac342", "exact_match": 0.0}
{"doc_id": 1856, "doc": {"question": "who is the longest railway station in india", "answer": ["Gorakhpur Junction", "Gorakhpur railway station"]}, "target": "['Gorakhpur Junction', 'Gorakhpur railway station']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the longest railway station in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Howrah Station"]], "filtered_resps": ["Howrah Station"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7f7e8823b01293f4343573682b1d7a0cdb27dfbb83fb79da5fb7b0453e3262d", "prompt_hash": "c56bcf63c17bdec8d419da2f1f573799143802b6df4c4fa32def104c9286feff", "target_hash": "a177377fb1b90e7335776935a2c6fdbafa1a331d948e5440c2743f4679fe7153", "exact_match": 0.0}
{"doc_id": 1857, "doc": {"question": "who has played raoul in phantom of the opera", "answer": ["Tim Martin Gleason", "Hadley Fraser", "Steve Barton"]}, "target": "['Tim Martin Gleason', 'Hadley Fraser', 'Steve Barton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has played raoul in phantom of the opera?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Crawford"]], "filtered_resps": ["Michael Crawford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5cc70c6bb6b774f6551da5974ea456473e294942840b1751b730d585ad2e3368", "prompt_hash": "0b255bd0893ac60de9f8551baf7ef529ce8e245ee4e49771b5935175b04ccc0c", "target_hash": "9c03725eb46e51ce5703052baf963c12b933fd40a340ad5301e25235cda8f2f5", "exact_match": 0.0}
{"doc_id": 1858, "doc": {"question": "fast & furious 8 release date in india", "answer": ["April 12, 2017"]}, "target": "['April 12, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: fast & furious 8 release date in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 14 April 2017"]], "filtered_resps": ["14 April 2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3ad315325df4e2a69fa544ac20350be91e787f87d0abe62dc4765aaa789f5304", "prompt_hash": "7321a0399aecef0ba54a77ec94a99af370346320983ab6e3dc59024732f98638", "target_hash": "4d2dd68be8575bb4a671d20eb410fbc3f26568def1bb836a3584fc78165df44a", "exact_match": 0.0}
{"doc_id": 1859, "doc": {"question": "who wrote the song be thankful for what you got", "answer": ["William DeVaughn"]}, "target": "['William DeVaughn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song be thankful for what you got?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kenny Rogers"]], "filtered_resps": ["Kenny Rogers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18194bca63a9e4cb5b3645eadd86526171ff240bca488c2673553fce938194f4", "prompt_hash": "2e1e334476f927cb88ed316508c33fbe226bf94c604924a2277f34c036aebb9d", "target_hash": "20e2259d159077f9d29de99a110e07f11b84322c28363388250cf0285b2b4596", "exact_match": 0.0}
{"doc_id": 1860, "doc": {"question": "where did iowa state football coach come from", "answer": ["Massillon, Ohio", "Toledo"]}, "target": "['Massillon, Ohio', 'Toledo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did iowa state football coach come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oklahoma State"]], "filtered_resps": ["Oklahoma State"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f0444334814a090839c17b8be56eabd7d96942e35420f4fe75e51f02da55ce4d", "prompt_hash": "236058cdb6c0e58caf26786b54bb9ee419b7164c19c000d45639a5a74cc3d51c", "target_hash": "0899dd35eaa0501ca929fe04262d625783a3276bfeb64fa8cc67b275d5b87490", "exact_match": 0.0}
{"doc_id": 1861, "doc": {"question": "when did scotland last qualify for world cup", "answer": ["1998"]}, "target": "['1998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did scotland last qualify for world cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf71860f7ae71baa82978e7ab1f3bd543cd30ce09c6faf90ae04bd4032dfb1b3", "prompt_hash": "a5e4d07ebf0fdac33b909abfe57212846cc821163c775c295f75eb3e6edbe376", "target_hash": "af1dd11d685d07f55fbfc760446204424f6189f35a956c9f3b09a3b25cbebbea", "exact_match": 1.0}
{"doc_id": 1862, "doc": {"question": "who sang in the movie i walk the line", "answer": ["Reese Witherspoon", "Tyler Hilton", "Waylon Payne", "Johnathan Rice", "Shooter Jennings", "Joaquin Phoenix"]}, "target": "['Reese Witherspoon', 'Tyler Hilton', 'Waylon Payne', 'Johnathan Rice', 'Shooter Jennings', 'Joaquin Phoenix']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang in the movie i walk the line?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joaquin Phoenix"]], "filtered_resps": ["Joaquin Phoenix"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3db313cbf49c0d579a1b9ec389320126430d6be403d6706d6434e1ed3ea95c3d", "prompt_hash": "c7ccabd424b4a4d095684b21e255146c07b85e38b313686de9dff40ccdf60e63", "target_hash": "5d70f50a07d17239f9d085a479aeb916cbabcf10148bc4a85cc6314d4020d319", "exact_match": 1.0}
{"doc_id": 1863, "doc": {"question": "what kind of food did the woolly mammoth eat", "answer": ["grass and sedges"]}, "target": "['grass and sedges']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of food did the woolly mammoth eat?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" grasses"]], "filtered_resps": ["grasses"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a88efe5e4ff5b7c93aecc15e18be6845abcd92997a127f30488e3163ff1f8bd3", "prompt_hash": "257768d5ac5faf72ca6dd04dfd96578b36f58c34ea624ab130605155c797d8c3", "target_hash": "90d13d13200188511e51072e4f70d587ff5c270fcd61c0243a5373907572d66e", "exact_match": 0.0}
{"doc_id": 1864, "doc": {"question": "what's the average lifespan of a basset hound", "answer": ["Median 10–12 years", "10–12 years"]}, "target": "['Median 10–12 years', '10–12 years']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the average lifespan of a basset hound?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10 years"]], "filtered_resps": ["10 years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96ea11a71b20599dd55599e8301f71c93f345885b2bdf2f11a8751cb33a4814e", "prompt_hash": "c7c34a1d6260acad4a3d4befcedf3d28474f5c98492029601ff35a79d632a253", "target_hash": "a00f0f74925245eff934229a94fc1e0998d207698fce04ab818f9fea0b7cc1b6", "exact_match": 0.0}
{"doc_id": 1865, "doc": {"question": "when did the great plague of london start and end", "answer": ["1665 to 1666", "1666", "1665", "from 1665 to 1666"]}, "target": "['1665 to 1666', '1666', '1665', 'from 1665 to 1666']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the great plague of london start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1665"]], "filtered_resps": ["1665"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ba4ba96a6f2e5e3f43ffc2c6aae9b5f265f51b2f0733bb770cd8e06fe22d9ecb", "prompt_hash": "1e221719a8b08d063d25d0657d89c0105c69eabdfb04494afc667e4319235b4b", "target_hash": "39b6093c9fc05d41d15ee963f28a7a7ddcd4df7171e887e8e1bd0e623fd243d3", "exact_match": 1.0}
{"doc_id": 1866, "doc": {"question": "when was the movie the king and i made", "answer": ["1956"]}, "target": "['1956']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the movie the king and i made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1956"]], "filtered_resps": ["1956"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f4702eedd2f1f0105fb2c2377aaa9d00de350d1e7b7c280334609d1de4c71051", "prompt_hash": "8edc2918a6460aedbecdb8f5270eb112760cdf38c410d894c553fa44a8817f00", "target_hash": "f4e43d0c734bca10f33b9479507b5eccfe906edaacc1fe192ffac1b5564ec538", "exact_match": 1.0}
{"doc_id": 1867, "doc": {"question": "where did the first persian gulf war take place", "answer": ["Israel", "Persian Gulf", "Saudi Arabia", "Iraq", "Kuwait"]}, "target": "['Israel', 'Persian Gulf', 'Saudi Arabia', 'Iraq', 'Kuwait']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the first persian gulf war take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kuwait"]], "filtered_resps": ["Kuwait"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4c4e3cc3f857c4b3d03bb25f3a29995ca2dc3c107d1f09efa72717988ec2676d", "prompt_hash": "55c10e2c4682a323f1e6cc213796f8de80d044e08c76d286632623dd7caf39b5", "target_hash": "a7a5c553104910364ce99583d8f7360b7c81ab51d6f159c838e2d83d210a6823", "exact_match": 1.0}
{"doc_id": 1868, "doc": {"question": "who is considered the father of modern behaviorism", "answer": ["John B. Watson"]}, "target": "['John B. Watson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is considered the father of modern behaviorism?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John B"]], "filtered_resps": ["John B"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3be797e0de20a7c323d55b1284c7bb7fb9543a02e8fae0bf433bf6bce9e5614d", "prompt_hash": "adf2c35cd78064a7dd8240ac3b28ce93468cfa50fe99c9959ea8eaa89b081e24", "target_hash": "efc2961641a37c1c66b4bc6f8d9ef302ff2d67fbf0fe8907d5af4e20a44fe164", "exact_match": 0.0}
{"doc_id": 1869, "doc": {"question": "when did to kill a mockingbird become popular", "answer": ["1960"]}, "target": "['1960']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did to kill a mockingbird become popular?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1960"]], "filtered_resps": ["1960"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d60d5241981a2146ad3ced4b9e61d8ab5f765b573d6d09b3858d998da8a4c1cf", "prompt_hash": "ad5fd5cf9c7783a25946f77766d6f64cf23c62c035015bf7ef528b4458e6a355", "target_hash": "e6eb59589fc11b4a06dcdbd45c76c091d0b77679983b180d59f554396ab8d2a8", "exact_match": 1.0}
{"doc_id": 1870, "doc": {"question": "who played the first nfl thursday night football game", "answer": ["Denver Broncos", "the visiting Denver Broncos", "the Kansas City Chiefs", "Kansas City Chiefs"]}, "target": "['Denver Broncos', 'the visiting Denver Broncos', 'the Kansas City Chiefs', 'Kansas City Chiefs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the first nfl thursday night football game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dallas Cowboys"]], "filtered_resps": ["Dallas Cowboys"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5ba3c346eb8f345c19aa096adcf24fb87ae2ee263970c169d5c9aa8fa43b5e7", "prompt_hash": "574dfaedb31bf8aaaeb61a0ba0680c201f1f17042792d140094cc7f80b40363b", "target_hash": "357c964ae2fe2db3b31eea5f5fd2ff0105fe465293d028bbbed64ccf034fa039", "exact_match": 0.0}
{"doc_id": 1871, "doc": {"question": "what language is the olympic anthem sang in", "answer": ["Greek"]}, "target": "['Greek']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what language is the olympic anthem sang in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Greek"]], "filtered_resps": ["Greek"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "39deac2d54291a825c073636838211dcc4346051297c45795699302c441af30b", "prompt_hash": "2452d2ad3d9fca5c4153f008e0feb98d7088ff7698a26cd5611f88a935bff5b5", "target_hash": "821b984859122e9e40100b761fecf18f084ab23178f7c09eb89e2da10c08b6f0", "exact_match": 1.0}
{"doc_id": 1872, "doc": {"question": "is jaffa cakes a biscuit or a cake", "answer": ["Cake", "biscuit-sized cakes", "cakes"]}, "target": "['Cake', 'biscuit-sized cakes', 'cakes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: is jaffa cakes a biscuit or a cake?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" biscuit"]], "filtered_resps": ["biscuit"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aac0978d175dfe47f1edab90c9244a545f13a2783f45254a0d5a6735f5576364", "prompt_hash": "64e6fd0fc7b4a86833fccb1660d11f5125e963975db4262768b027ae9359e98a", "target_hash": "964cb60b786a914deb530dbc96d48d5c9b32c569c76d163dac39544f7ac54206", "exact_match": 0.0}
{"doc_id": 1873, "doc": {"question": "what's the difference between salisbury steak and hamburger steak", "answer": ["ingredients"]}, "target": "['ingredients']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the difference between salisbury steak and hamburger steak?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Salisbury steak"]], "filtered_resps": ["Salisbury steak"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0bea8542bf4e43196febdd6abe2d42800e5035fccae1b35de934c2006da1c6ab", "prompt_hash": "0d401c4576fb412c4100bc08c36918d84953e4068f547f9d3fbde35303a02a83", "target_hash": "0cb24600331cac234f973f20b8cfbd3b3a5660aa57cd105a078ae7b3680584bb", "exact_match": 0.0}
{"doc_id": 1874, "doc": {"question": "where did chocolate originate the americas spain asia", "answer": ["Mesoamerica", "the Americas", "Americas"]}, "target": "['Mesoamerica', 'the Americas', 'Americas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did chocolate originate the americas spain asia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Asia"]], "filtered_resps": ["Asia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d2914a5993e66e39299c8c0f2ee02fa7336422f92dadbe02fab4190017d6cfc", "prompt_hash": "b10ed83c282505fdd22c95e51c2bb32305b59f2760512207c550943413fdd6be", "target_hash": "749d9c52976d7dded5f7004c5138a992d69486cb6418a0ee6b8abd12943bf222", "exact_match": 0.0}
{"doc_id": 1875, "doc": {"question": "who received the most (but not a majority of) electoral votes in 1824", "answer": ["Andrew Jackson"]}, "target": "['Andrew Jackson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who received the most (but not a majority of) electoral votes in 1824?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Quincy Adams"]], "filtered_resps": ["John Quincy Adams"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a734b585fe67fc12e358f39289fc1a5288e82ceab1748965f9c07d9bbf0cc53c", "prompt_hash": "b3ca0b8bfd23f9f93a44c90fcd615ed98a7b39aeafa12e1f9984cfb2d944cee9", "target_hash": "6c769d9fd06251f012e6b75f50793a6bca7a03cd2c860cb97e159340945fe826", "exact_match": 0.0}
{"doc_id": 1876, "doc": {"question": "where does the papillary layer of the skin lie", "answer": ["adjacent to the epidermis"]}, "target": "['adjacent to the epidermis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the papillary layer of the skin lie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" epidermis"]], "filtered_resps": ["epidermis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c7af9b760339713a7d18aea4c08f6b998260d15b3d7cf9e40043a1fe4a57d875", "prompt_hash": "03d475ddd02758974e3ff77448ad57d8b59c923ac9607e5d41d1200c97d970c8", "target_hash": "900c3f267514115a2adbe46a15d1f5430ebf428414a9c16443b9f6f396f03c26", "exact_match": 0.0}
{"doc_id": 1877, "doc": {"question": "who is the original singer of you re going to love me", "answer": ["Jennifer Holliday"]}, "target": "['Jennifer Holliday']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the original singer of you re going to love me?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christina Aguilera"]], "filtered_resps": ["Christina Aguilera"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ede6e91d024823293c85cfd223c95db68478915e308c72563b4214e70e7b7539", "prompt_hash": "ee10b0ac81b134816ca2fe1611103602af8204b8812969c7dfa9a14f9513efe0", "target_hash": "ac63672170c8453a5a058832ce66e603de46052e5b6199b41cbfb9fdd495440d", "exact_match": 0.0}
{"doc_id": 1878, "doc": {"question": "what type of song is what a wonderful world", "answer": ["a pop ballad", "jazz", "pop ballad", "Traditional pop"]}, "target": "['a pop ballad', 'jazz', 'pop ballad', 'Traditional pop']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of song is what a wonderful world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" jazz"]], "filtered_resps": ["jazz"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "71b000faa13ef9a63dc1c95e6fc589683582e14dec022768530beeb0194ceebb", "prompt_hash": "b843d7b0f632320d2b7d0bffe344f925c89c50b7f18103e2c083d9d530d2440e", "target_hash": "09c92edfb02c957955bc822524ce2a80f2f0f5df0f16d0d87ffd042543a52154", "exact_match": 1.0}
{"doc_id": 1879, "doc": {"question": "suffix applied to the end of the name of enzymes", "answer": ["-ase"]}, "target": "['-ase']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: suffix applied to the end of the name of enzymes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" -ase"]], "filtered_resps": ["-ase"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4850c96ed79e6a0dece5f60f6078ae500c6668c9b7a647a91c69e7cee59f8009", "prompt_hash": "2f67c33c019cb93a5b7f5f73859b864f9a3963e2869fc7c32b4f514e145ba95e", "target_hash": "bfbde55d3f74f90b41518f20800ba2a493f8e064e5f7a91c9cd16bcaabf3021b", "exact_match": 1.0}
{"doc_id": 1880, "doc": {"question": "what type of business is chamber of commerce", "answer": ["a form of business network"]}, "target": "['a form of business network']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of business is chamber of commerce?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" trade organization"]], "filtered_resps": ["trade organization"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "554ebc3d699fa942ed324928109cab36407a8ec29c815ae956511e4ffcac6de1", "prompt_hash": "3d4164063f86dd208114e831e84579e1ddceff9004795e63832b81c75930021f", "target_hash": "00d8c5aa551fad3094b5b7a12c574a988f50651245dec97e074d865df0fa17f0", "exact_match": 0.0}
{"doc_id": 1881, "doc": {"question": "who directed the best years of our lives", "answer": ["William Wyler"]}, "target": "['William Wyler']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who directed the best years of our lives?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Fred Zinnemann"]], "filtered_resps": ["Fred Zinnemann"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d386b514c1f54844d92a3d317864261bf56d5bce2b36c1b85ca734128e9a256d", "prompt_hash": "ec93df351c542a7bdc2cd27399ced992792c2f16a84f4afa4f6ce9644646c2fb", "target_hash": "12f81ec38bbb515f2f5d2fff0dae0752a5054a58fbccf80564ac2163918ac534", "exact_match": 0.0}
{"doc_id": 1882, "doc": {"question": "the assassination of gianni versace american crime story season 2 episode 2", "answer": ["Manhunt"]}, "target": "['Manhunt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the assassination of gianni versace american crime story season 2 episode 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" \"American Crime Story: The People v"]], "filtered_resps": ["\"American Crime Story: The People v"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "10a39ba16a70da36e7d9175c9d77c7cad34a993073f66450ffb4836653568e42", "prompt_hash": "a4caaf0a3276be5847d6aa488708d5ea33e0cda76a67369cf910f49334973b9a", "target_hash": "1ba0637622a9c74bac57221b68c503ea24fe8ce38652c7abcb5dade390e513e8", "exact_match": 0.0}
{"doc_id": 1883, "doc": {"question": "when's the last time the philadelphia eagles played the new england patriots", "answer": ["in Super Bowl LII"]}, "target": "['in Super Bowl LII']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when's the last time the philadelphia eagles played the new england patriots?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a10d36d537ee6da8bf857d4571e7fc8210a26719e36d3a3877daf63297f3989d", "prompt_hash": "77552180841ae3dbf729d7e233d7955663b69fde08ac749edd3a0c14d5c547c2", "target_hash": "01475deec092b87ec8cccd0712faf5ae33fafc38c748663df09ce2ee312bd569", "exact_match": 0.0}
{"doc_id": 1884, "doc": {"question": "who was the first indian to be appointed as a judge in the international court of justice", "answer": ["Sir Benegal Narsing Rau"]}, "target": "['Sir Benegal Narsing Rau']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first indian to be appointed as a judge in the international court of justice?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" K"]], "filtered_resps": ["K"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "798f819dfb75f01d599a16cbfdc17ef1e340adba3ac9c1876b414ee4849ee394", "prompt_hash": "1469da52eb8b03b595dfc076e039adf5145234a4cf938ce7a34bc079320beac3", "target_hash": "bec1c57363f53f55a99d7af2bf73d8e8edbeb629523c2dfcfeb69cb2d9d38224", "exact_match": 0.0}
{"doc_id": 1885, "doc": {"question": "what type of tale is the pardoner's tale", "answer": ["a moral tale", "an extended exemplum"]}, "target": "['a moral tale', 'an extended exemplum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of tale is the pardoner's tale?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" moralizing A: satirical"]], "filtered_resps": ["moralizing A: satirical"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "75f5e5e0436b8407e7efc5340441686ad9092ddef14471f4e7f77d5099dde50a", "prompt_hash": "66da294cfdcb10f044e0da18c1bcfed72b0dadf9718367bb1cbbee440630a042", "target_hash": "f5423b620677a69b5b90412e32b46150e248fa391d98d2ac600e21b7d835724e", "exact_match": 0.0}
{"doc_id": 1886, "doc": {"question": "who is the all time leading scorer in ncaa tournament history", "answer": ["Pete Maravich"]}, "target": "['Pete Maravich']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the all time leading scorer in ncaa tournament history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jim Barnes"]], "filtered_resps": ["Jim Barnes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ea2d505b6367e414fb9758ed53e60db5bccd705864e6ae087743b2967ce8f5b", "prompt_hash": "934dce24db09ae847a00fd8629600fbcfb1eb9938694a2c609583e2124d763f1", "target_hash": "368cd98a21ba5ba20800593e2e85b7b76ca55eccee93d6cc4c654569766f3990", "exact_match": 0.0}
{"doc_id": 1887, "doc": {"question": "how many times has lake placid hosted the winter olympics", "answer": ["twice"]}, "target": "['twice']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many times has lake placid hosted the winter olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d1322a8fdf0bb8c3d04cf1a3e96df8cbe248d33545a6871d5f11a53565540524", "prompt_hash": "af326b04dea862d729ea76cbd3988af14a1ed6b23b55dc257b50d3650cba3f08", "target_hash": "f345aa7e79537313ac33e250865b4496dcb50cc9f207656fe55b79188fe146f0", "exact_match": 0.0}
{"doc_id": 1888, "doc": {"question": "where do some of the natural air pollutants come from", "answer": ["wildfires", "Volcanic activity", "Dust", "Carbon dioxide", "Vegetation"]}, "target": "['wildfires', 'Volcanic activity', 'Dust', 'Carbon dioxide', 'Vegetation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do some of the natural air pollutants come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" volcanoes"]], "filtered_resps": ["volcanoes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4aa13aaee0517ad5669f1d579d2381a848ee96aa6996190831264e2528a7bde3", "prompt_hash": "67667cfb3e70e647291fa07af677ba6801d3e6829317849f8b996292d923b836", "target_hash": "7201c8a2f8a598b65ebeb668df61010a90ceda36502924d8e6a612e81b99e1b7", "exact_match": 0.0}
{"doc_id": 1889, "doc": {"question": "what is the name of the shape with 100 sides", "answer": ["hectogon", "hecatogon", "hecatontagon"]}, "target": "['hectogon', 'hecatogon', 'hecatontagon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the shape with 100 sides?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" hectogon"]], "filtered_resps": ["hectogon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "604dc8a6e9a7315510e44e541d373f1a234782f0430132732ceb9823c4f33ac3", "prompt_hash": "910b1ee68dd27cbade0be432bccfbffaf070826dd7a1cd759e19962c2ac60f0d", "target_hash": "a81ca9fe7b4ce7cb60c52c89b0ab1d6489eb8efa451f3293c25a6dce90ce3af5", "exact_match": 1.0}
{"doc_id": 1890, "doc": {"question": "when is i can only imagine coming out", "answer": ["March 16, 2018"]}, "target": "['March 16, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is i can only imagine coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "191974f8561a5193dd6f27bec7cb4d1acb208a120189bac9112a2ea70cae1881", "prompt_hash": "2ed57f6447022f8b4b7b36c2adf14da0a11b06725d52d7bfc6f8df4b3dfd05e4", "target_hash": "639c7514cc16f734bcb3764a511c765b410be13fa605c8cb5414d851837bc0a3", "exact_match": 0.0}
{"doc_id": 1891, "doc": {"question": "who is the robot in lost in space 2018", "answer": ["Brian Steele"]}, "target": "['Brian Steele']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the robot in lost in space 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Will Smith"]], "filtered_resps": ["Will Smith"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1207bcc7f04e26a5ecbbc9e84bf9b87257ab933cc8247f8379d6657438c501ad", "prompt_hash": "55054cea4bbf22a2aee9c90ee07478b1edfca8b2b7daf240bb0e7f2a3122cdcd", "target_hash": "4c2cdb74eba0bfca66a39bfe6468ff0ccad5741aae82c5c4f0e9dd471d4c2647", "exact_match": 0.0}
{"doc_id": 1892, "doc": {"question": "who is recognized as the founder of islam", "answer": ["the Islamic prophet Muhammad", "Muhammad"]}, "target": "['the Islamic prophet Muhammad', 'Muhammad']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is recognized as the founder of islam?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Muhammad"]], "filtered_resps": ["Muhammad"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3ea55afd98de9a4535bfd11abd21aa41d291c0108bd92dcee3d2bb0c370a38e", "prompt_hash": "4c5cef507fdb90b3de4588748b05b65954c45222b063c444ef034a341d67ef09", "target_hash": "dfa4d56d2967d2d91b8116c2369e7b4642a994c35f4965ec50228bc05566ba76", "exact_match": 1.0}
{"doc_id": 1893, "doc": {"question": "when does the new mlp movie come out", "answer": ["September 24, 2017", "October 6, 2017", "October 6, 2017"]}, "target": "['September 24, 2017', 'October\\xa06,\\xa02017', 'October 6, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new mlp movie come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0ed402eb1de8585ace9d2b0d9a0e3cf807699bf20920bb45a8d038d2daaf6285", "prompt_hash": "00b1923456143b7d3ed4b17c3311c515ff2e70751a631b0942d9d94ac3fe694b", "target_hash": "955bf8742b97aab21f4227dc52362b6e6e436b4376f06c50b68bd57b0320129a", "exact_match": 0.0}
{"doc_id": 1894, "doc": {"question": "where was robin hood prince of thieves made", "answer": ["the United Kingdom"]}, "target": "['the United Kingdom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was robin hood prince of thieves made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" England"]], "filtered_resps": ["England"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "907a9952e32dbcc3d0358ec8e3c0d98399cb21edbac7c120d8122e1ebd666672", "prompt_hash": "412905af439fdd1df637e90a6cb8ba5660102843abb38b572d6266618e61a49d", "target_hash": "1e386a47ae6943870f4d89f4c6392437d916280a97af59e9973331a99c405869", "exact_match": 0.0}
{"doc_id": 1895, "doc": {"question": "who has the most goals in soccer 2018", "answer": ["Ali Daei"]}, "target": "['Ali Daei']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most goals in soccer 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cristiano Ronaldo"]], "filtered_resps": ["Cristiano Ronaldo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "34e86344c1d3627fb28e5c5f2266c6e38c1efacb1970e7ac10ac131ac35cad12", "prompt_hash": "c20057a5e2aeeecc8aae2afe9ae9fe1380f8a4c6c1e2a7950df6a3bae8b0cc25", "target_hash": "b65de07092c1c2e8d869eaff1f3655a9ac85abf32c49a04658f8441ef583c3d6", "exact_match": 0.0}
{"doc_id": 1896, "doc": {"question": "when does it's a wonderful life comes on", "answer": ["Christmas season"]}, "target": "['Christmas season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does it's a wonderful life comes on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christmas"]], "filtered_resps": ["Christmas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "be59eae5b1a66d36eb6472deb06f2daf710370cd22b951a1c215ca255638c93c", "prompt_hash": "ddbb4db9a47bc5fae757a5c1f544e94cbe759b941d18c75e55781c69704dd658", "target_hash": "9fc41f554f2856b4c98849a8bfded184df660a3af39cbdde5b30f2f791623fd9", "exact_match": 0.0}
{"doc_id": 1897, "doc": {"question": "who wrote the original little red riding hood story", "answer": ["Charles Perrault"]}, "target": "['Charles Perrault']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the original little red riding hood story?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Charles Perrault"]], "filtered_resps": ["Charles Perrault"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "665bd5656a2288674523db2dedc2e81d49e4ea6247d6a93583550ea8534f90f5", "prompt_hash": "29994dcb0c0a0729fff1abb9038e380af5e28f6883c893917290ed84f8279fd6", "target_hash": "d295a6ffd63f57b514ed9702c455ca7f058c30795e7804cb743a49c5d66d2504", "exact_match": 1.0}
{"doc_id": 1898, "doc": {"question": "where is the inscription on the statue of liberty", "answer": ["The New Colossus"]}, "target": "['The New Colossus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the inscription on the statue of liberty?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" tablet"]], "filtered_resps": ["tablet"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8c95afda9cca1c863a80f5ae67ae1ae2dccd9206e3ac497477443c99d2e96c24", "prompt_hash": "d353f899d0053211a9600560d890892f749736440ccb403eeb3c1832d881a9bf", "target_hash": "930889c576afa894b278e3cdeff468f765d47b81c6a5b6d076eb7037a3d161a0", "exact_match": 0.0}
{"doc_id": 1899, "doc": {"question": "who was the captain of the mayflower when it took the pilgrims to the new world", "answer": ["Captain Jones", "Master Christopher Jones", "Christopher Jones"]}, "target": "['Captain Jones', 'Master Christopher Jones', 'Christopher Jones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the captain of the mayflower when it took the pilgrims to the new world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christopher Jones"]], "filtered_resps": ["Christopher Jones"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9d66bc493aa46fbbd1c9734c87bcbce568fcc3f60d77cb387ebbdaf60d30e3be", "prompt_hash": "918b1a821258bff825d25909d64008d6278c2a8adc9d03f055e093cf78c396e3", "target_hash": "add202f7fa726d07262ffb746281dab6475a8f0cd131e7274e66f6869a4fd9c3", "exact_match": 1.0}
{"doc_id": 1900, "doc": {"question": "what is the top speed of thrust ssc", "answer": ["1,228 km/h"]}, "target": "['1,228\\xa0km/h']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the top speed of thrust ssc?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "14103fbaafd1c17a887cdccf7abf9e6987d75dacfe9e7be1d542b23587ec3ce7", "prompt_hash": "a2bc7348c34a60dd401bdcf71ba421333cef14a6c8b4494c8445e320ee3c9ada", "target_hash": "2a2af94e4251c2e8df42056fa63bf96a40ae9db1b8ef1cbdd4bc16f27140b600", "exact_match": 0.0}
{"doc_id": 1901, "doc": {"question": "where is the oldest civilization known to man", "answer": ["Mesopotamia"]}, "target": "['Mesopotamia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the oldest civilization known to man?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mesopotamia"]], "filtered_resps": ["Mesopotamia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f57d317b54da648f9b0e60d387369019ba9fe82366a02caeafadd183fdba9e9f", "prompt_hash": "eded8fc09b62b405dde819be46722f4fd38116af1a4941e3834bac0555869378", "target_hash": "04048ab01b4efa9ae5f26ec9e9d76275f4386f5a781fa7e2312dc764b614d692", "exact_match": 1.0}
{"doc_id": 1902, "doc": {"question": "when does scully come back in season 2", "answer": ["One Breath", "8"]}, "target": "['One Breath', '8']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does scully come back in season 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" episode 11"]], "filtered_resps": ["episode 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e057e1866adc835e4add803da6e515c808635345e4bcd961f7fc72a33a6bc666", "prompt_hash": "ef5b4fb8b9edd276781677e1a97e7b6b26ae15197315dca14731dec46196df2c", "target_hash": "8653dbd8a0bf834a6c1f121844b4f0a74351153e5f55bbff60076e29c6d2e0e5", "exact_match": 0.0}
{"doc_id": 1903, "doc": {"question": "how many lines of symmetry are there in a equilateral triangle", "answer": ["3"]}, "target": "['3']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many lines of symmetry are there in a equilateral triangle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3"]], "filtered_resps": ["3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96c0964230661d7242b452a05d40bcb3cea6194febd3b3f0a1af606dbec2765f", "prompt_hash": "74850e378f15465799236246db6b7ea39dcdd145d3859160d1a1fde0524f2b7e", "target_hash": "e8230ace9f0554c09f639f7a596544b0277f9345d9c2a486d6607bf01c25927e", "exact_match": 1.0}
{"doc_id": 1904, "doc": {"question": "has been honoured with the wisden leading cricketer in the world award for 2016", "answer": ["Virat Kohli"]}, "target": "['Virat Kohli']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: has been honoured with the wisden leading cricketer in the world award for 2016?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Virat Kohli"]], "filtered_resps": ["Virat Kohli"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "324f41f67d2a75fac35021ff1189857dcb2a9af915141d426108acb6541f643d", "prompt_hash": "a0cc0766ec8a5dfd68b1a881061e58f65dee354da7875625da8805a86248395e", "target_hash": "38233f9f93a530beb81f3c072d657e532659807ed8ba3bb3d5f7216083fe5b86", "exact_match": 1.0}
{"doc_id": 1905, "doc": {"question": "where was the witch of blackbird pond published", "answer": ["United States", "1958", "Houghton Mifflin"]}, "target": "['United States', '1958', 'Houghton Mifflin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the witch of blackbird pond published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" London"]], "filtered_resps": ["London"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df2c8985fb407aed7fad739c39f9ae30c9eb2e8540a3f8c0bb98f52618fcd29a", "prompt_hash": "7deb918e82be7d8f400d55d0d2089c73c3c6d19f76233338c42308193d7a274c", "target_hash": "15d36b7963cf962611deaa4d7f8122b0b429758446a7ee586a63460a94006789", "exact_match": 0.0}
{"doc_id": 1906, "doc": {"question": "what is the number of total presidential electoral votes", "answer": ["538"]}, "target": "['538']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the number of total presidential electoral votes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 538"]], "filtered_resps": ["538"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "408871ef1cde1f28329239375d75c820cd3be01704866b9cd633055bb53d2a97", "prompt_hash": "2d2de30de457f581fb125fbc7529ad0f0fa8b0917265f08faba9645e82ccf6f6", "target_hash": "3bb9e2a6c1e975a2578b5ffe4cef0e758da05838bdb86249059fa93759333f18", "exact_match": 1.0}
{"doc_id": 1907, "doc": {"question": "what is the breakdown product formed when one phosphate group is removed from atp", "answer": ["adenosine diphosphate (ADP)"]}, "target": "['adenosine diphosphate (ADP)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the breakdown product formed when one phosphate group is removed from atp?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" adenosine diphosphate"]], "filtered_resps": ["adenosine diphosphate"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2d3fad6879c8061d4c7b293dd88d796911575dfd39ce8871ddc3c6455ac5dd11", "prompt_hash": "77024c68ba8ee0cd04a4e651ca6fd9ddd7070246b4c080cc7460e2a351e84946", "target_hash": "266dbf7a1878a74808ccd22314cfb02dd6302ef0a02555f1516f01e6a60d0c9f", "exact_match": 0.0}
{"doc_id": 1908, "doc": {"question": "what tool is available from microsoft as an alternative to the usmt for smaller migrations", "answer": ["Windows Easy Transfer"]}, "target": "['Windows Easy Transfer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what tool is available from microsoft as an alternative to the usmt for smaller migrations?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" robocopy"]], "filtered_resps": ["robocopy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4dfdf40e68b73fda71028a6bb7c78b285c5ae47f241c843ec59a78c1b9c3bacf", "prompt_hash": "af580aead20eea802cb336aa0fd374b573f853d9458c6d4c5f2a4fd03252a3a5", "target_hash": "c87a97638d47102636cc161e4cbbff152bb902940f16b044baff2992efbeb86d", "exact_match": 0.0}
{"doc_id": 1909, "doc": {"question": "when is the season finale of designated survivor", "answer": ["May 16, 2018"]}, "target": "['May\\xa016,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the season finale of designated survivor?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" May 14"]], "filtered_resps": ["May 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "68672ec01ac6c08e80cabd77fb494244da5eef36d209a5ace0aabe763cbeef82", "prompt_hash": "ceea98b6b912535edf7165f8888200fc722abe26352f5b2f96cddee39596b37e", "target_hash": "7550ebb67cfd9b666886ef4362bde97f85528853611797f27072d091043c3113", "exact_match": 0.0}
{"doc_id": 1910, "doc": {"question": "when was the last time an american won the new york marathon", "answer": ["2009", "2017"]}, "target": "['2009', '2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time an american won the new york marathon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c45565775a580819f430ae8c72d096e7c07cb30d6d98eeaf67212caef484d782", "prompt_hash": "291d30534d89bc6f9ecd45d86ff677e951ced9315c4b54fc1804325ffd19f5d2", "target_hash": "b679d5e11d34368b0201bfde95cd4f39371c10b33fb10a07d59a302e6aa12c0e", "exact_match": 0.0}
{"doc_id": 1911, "doc": {"question": "who is the girl that died in eyes wide shut", "answer": ["Mandy"]}, "target": "['Mandy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the girl that died in eyes wide shut?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dominique Dunne"]], "filtered_resps": ["Dominique Dunne"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "72014e0e0017cc2bfb527ca86f9960dbd1bbeaa3a1eea89463720d61a60784cb", "prompt_hash": "80b9ecfcb5604eb69511dd2d9511e0ab1dd5b05317fffbf96fc305fb379e6359", "target_hash": "b1aabeb612d9c8a62c8c3457dd9ad75cbfecf44b1372336a82fa611ed193f2fa", "exact_match": 0.0}
{"doc_id": 1912, "doc": {"question": "who is the prime minister of india full name", "answer": ["Narendra Modi"]}, "target": "['Narendra Modi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the prime minister of india full name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Narendra Modi"]], "filtered_resps": ["Narendra Modi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e73aa4318267cebaac760415512a9c07a22faa843aa64071599c27726add882", "prompt_hash": "163e39ce12a7901d04f6797dc4c9b057ae0e75e47e4452d8343d2ec68bb01c45", "target_hash": "96637b3147abb00f67833e61504296cad30f913659fdac20d65f41b2046a5b05", "exact_match": 1.0}
{"doc_id": 1913, "doc": {"question": "who was the grandfather on the cosby show", "answer": ["Earle Hyman"]}, "target": "['Earle Hyman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the grandfather on the cosby show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Andy Griffith"]], "filtered_resps": ["Andy Griffith"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49944a18ae3ffaab7683f76c9f9204deef8aa257a33903f1328e49f5ffa8c59c", "prompt_hash": "5db441c0945dbc6480465dfc1c3422dbeff0598292021cc60e0a34a10fc77ecc", "target_hash": "6a412cc40a78a70863573a3f734b0233338e7fe7a70113d3784d9f8fe1ef9db5", "exact_match": 0.0}
{"doc_id": 1914, "doc": {"question": "list of books written by abul kalam azad", "answer": ["Ghubar-e-Khatir", "Tarjumanul Quran", "Tazkirah", "India Wins Freedom"]}, "target": "['Ghubar-e-Khatir', 'Tarjumanul Quran', 'Tazkirah', 'India Wins Freedom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: list of books written by abul kalam azad?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" India's Freedom Struggle"]], "filtered_resps": ["India's Freedom Struggle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0640e3260cbae0ad1d3ec3249bfc6f5bc505169eeaa771e873426391ceac8d49", "prompt_hash": "41291710e83a597d7cfa76d8dddc1362df97b50251a43e1e8c969326292975bd", "target_hash": "29d886a9a48e96bcd43cd88f96aca682fd85f5b1378bbd10d7b888cdba2668e6", "exact_match": 0.0}
{"doc_id": 1915, "doc": {"question": "what are the 5 boroughs of new york city", "answer": ["Manhattan", "Queens", "Staten Island", "Brooklyn", "the Bronx"]}, "target": "['Manhattan', 'Queens', 'Staten Island', 'Brooklyn', 'the Bronx']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the 5 boroughs of new york city?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Manhattan"]], "filtered_resps": ["Manhattan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9466e9337cefa94d0ec3648d32c90ee4617f2790aa2baaf2baa4287b2cbfd2ae", "prompt_hash": "cb79d4b90929eb5b08c18ce95b4223080d5518e7f903b444f84cfa120155a8ff", "target_hash": "832e964f77959fba7f0f965e194a557090d9d8ee06ab438b2c70349db20a6621", "exact_match": 1.0}
{"doc_id": 1916, "doc": {"question": "when did ohio state join the big 10", "answer": ["1912"]}, "target": "['1912']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did ohio state join the big 10?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1891"]], "filtered_resps": ["1891"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fd85166036ba95c8e50778c474e28c956236775de57b125d4caa8549e99fbd42", "prompt_hash": "720d9e76fc01c8936c20d1679069d07d6b24e820d94459c9b4714a08a232fc3a", "target_hash": "f96a9f699570cdaad849cbd1db4405807bd9884026f9dd7112631bd71be24f24", "exact_match": 0.0}
{"doc_id": 1917, "doc": {"question": "when did the ouija board first come out", "answer": ["July 1, 1890"]}, "target": "['July 1, 1890']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the ouija board first come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1893"]], "filtered_resps": ["1893"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "36494880a28939940264b0d6ad3b138da867ec44c15551f2302144460b9ca4b8", "prompt_hash": "cd4cc5ac912799ecc976d359f80082045c882cde8dcc6aec89cbcf8d29087ea1", "target_hash": "23c86d708b1dff7fd51299a3251ffccd98eb58a30720e2a2f9112dceb85158d2", "exact_match": 0.0}
{"doc_id": 1918, "doc": {"question": "distinctive characteristics of animals classified as vertebrates include", "answer": ["backbones", "have a vertebral column"]}, "target": "['backbones', 'have a vertebral column']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: distinctive characteristics of animals classified as vertebrates include?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" backbone"]], "filtered_resps": ["backbone"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b92c69c7a0633e09ee73befb12c6a061b648c1f15b807de65b6a01c0727abf2d", "prompt_hash": "f1c399f3dbc47841601f83d69c6d01e877a58f68a57c782f2f13361055b7ef7f", "target_hash": "90bcc579f45448633c5096149278fabcb476fcd4aac9ca7569d1ee3d3cdfe552", "exact_match": 0.0}
{"doc_id": 1919, "doc": {"question": "where was the last world cup held and who won", "answer": ["Germany", "hosted by Brazil", "won by Germany", "Estádio do Maracanã"]}, "target": "['Germany', 'hosted by Brazil', 'won by Germany', 'Estádio do Maracanã']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the last world cup held and who won?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brazil"]], "filtered_resps": ["Brazil"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "828d02bc63f718de5c831e58d21b5ed0e7876a4c02928477f084c1a5b4cdd333", "prompt_hash": "202b4d25902bceaa88953d448a7e83bb955fc84bbdbe18cbdc993732aa86f1dc", "target_hash": "63e58252b4c3633f9686288648a5e83df607c92252ef61ae67842e0b57ab046e", "exact_match": 0.0}
{"doc_id": 1920, "doc": {"question": "how long is a prime minister term in uk", "answer": ["At Her Majesty's pleasure"]}, "target": "[\"At Her Majesty's pleasure\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long is a prime minister term in uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" five years"]], "filtered_resps": ["five years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e4dec91431c8c24e42e7af9fa11c5bd09fa97be8d2b18ffc7ed16c358c32df1", "prompt_hash": "229bf05c0eafde298f9616817f40abf977c2476ed55adbe759dd25a4531406e0", "target_hash": "ca9edc8329dc0e9bd0a75293d679ebf2f7de1ad4ca82fc4bc46ea3c2b0539cc2", "exact_match": 0.0}
{"doc_id": 1921, "doc": {"question": "where does the optic nerve cross the midline ​", "answer": ["optic chiasm"]}, "target": "['optic chiasm']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the optic nerve cross the midline ​?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" optic chiasm"]], "filtered_resps": ["optic chiasm"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "617b6ae16106c7c34478b6301c2d50beab6f7a43f24143f11775adef60cb27f8", "prompt_hash": "e882dda351632ea206e910653f834c5dab1b2541f3b23486add65041a0fc93c2", "target_hash": "e067ec7c1fe881d0f53fc6697a9ea72cf2c51b8025d90fb7ebc1304bfde7959f", "exact_match": 1.0}
{"doc_id": 1922, "doc": {"question": "who was it that described the structure of dna", "answer": ["James Watson and Francis Crick", "James Watson", "Francis Crick"]}, "target": "['James Watson and Francis Crick', 'James Watson', 'Francis Crick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was it that described the structure of dna?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Watson"]], "filtered_resps": ["James Watson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1aa2f8695556fd09a0a958e649b78ca227a7cb6afc8cf9fe1545c93f104938d0", "prompt_hash": "bab970721405464baf436549847dec380edbf7110f47060da7084fe2c9476b13", "target_hash": "35869f6ba937d19503834aabe031855b7656b115d21ddcde374e0d53c4227a5a", "exact_match": 1.0}
{"doc_id": 1923, "doc": {"question": "who sang the theme song from russia with love", "answer": ["Matt Monro"]}, "target": "['Matt Monro']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the theme song from russia with love?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shirley Bassey"]], "filtered_resps": ["Shirley Bassey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d56c6111726b191f7495ca7f60d3849cdc1bc7ed87c7d57a76abd173e3eccb3d", "prompt_hash": "a64939ee519b2d71a30ef3b75451827493c9469d60002a1d65ad36a81a8a4de2", "target_hash": "feee7afa243ad8b164cad687920ea7635a1dad6b375888cc5315a66f7d7bb01c", "exact_match": 0.0}
{"doc_id": 1924, "doc": {"question": "where does the song 10 green bottles come from", "answer": ["United Kingdom"]}, "target": "['United Kingdom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the song 10 green bottles come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" nursery rhyme"]], "filtered_resps": ["nursery rhyme"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "90199956292efc0eb2f1858ab0b7d6d1c49da5794155a87d510adf7a676e9474", "prompt_hash": "ed5ca37c6e59f6aacf15077426b773e48ee4712de8ba762456166cc2c3c4ae5c", "target_hash": "e1f0e94d1cacf16d9316103aebc358ef7dbc9ed7c63ea7550c58c3f5bc3fbb4e", "exact_match": 0.0}
{"doc_id": 1925, "doc": {"question": "what kind of national legislature did the constitution created", "answer": ["bicameral Congress"]}, "target": "['bicameral Congress']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of national legislature did the constitution created?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" bicameral"]], "filtered_resps": ["bicameral"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d69b7b3c37192b0ad763514be69698c6086cfb9aa3320191ecf2c93ec0803f6", "prompt_hash": "4894ec719e877e090b7fc25987ddba4c13bb100719d5bfcb7d59d138e4a3240b", "target_hash": "d9f2a9408b2a0e309a889948f6039f33e2be257b51277733e2cb65b4fc4203dd", "exact_match": 0.0}
{"doc_id": 1926, "doc": {"question": "vikram samvat calender is official in which country", "answer": ["Nepal"]}, "target": "['Nepal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: vikram samvat calender is official in which country?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nepal"]], "filtered_resps": ["Nepal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "942234040283542e5033e52cdd430e46f2f522d68f3f1949ea560ef7ee614308", "prompt_hash": "fdf493f48b364b82f1abc168125a024799e8ff5c04aaf581f32375625ab45385", "target_hash": "5eae663fa5cc1e5ca7453952659a57f448caf997168f538173bed6349a8bb36d", "exact_match": 1.0}
{"doc_id": 1927, "doc": {"question": "when did one child policy end in china", "answer": ["January 1, 2016"]}, "target": "['January 1, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did one child policy end in china?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c3edf3a9ee241f4e811859a74860a287be69d7870b7544678f8ca883b554cf5f", "prompt_hash": "9e001ca2faeec1a41f1ac087f817893568ef92b56511c0514ee7b8da022ddff9", "target_hash": "170160e7d2273b98564ac6b44875cb043836eb72519c9890301b89b944e6fc4e", "exact_match": 0.0}
{"doc_id": 1928, "doc": {"question": "what is the definition of the name thomas", "answer": ["\"twin\"", "twin"]}, "target": "['\"twin\"', 'twin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the definition of the name thomas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" twin"]], "filtered_resps": ["twin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9ee176e5852d0244edd728ec46fb54755337544bb809182dec6ef660fabf026", "prompt_hash": "9b0ef193b4e283c719997a67855b4467df6b53a94f1dd68267f92ace6f449191", "target_hash": "55d1f37167b2717e9b19316de6b487fe74afd7891e52854edc77f7b042da880f", "exact_match": 1.0}
{"doc_id": 1929, "doc": {"question": "what was generally the preferred way for european powers to control colonial regions", "answer": ["Indirect rule"]}, "target": "['Indirect rule']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was generally the preferred way for european powers to control colonial regions?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" direct control"]], "filtered_resps": ["direct control"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b8c4dc8bb81060dbc9e374fdf2fe9cde462ff1f761a4a86d0ee7ee08e79b7a0", "prompt_hash": "52b74fb8e2032757f66c59d0e39213d45159d63eb514f3928ecb570409d2d265", "target_hash": "af50aa27d3483b0247461c4b5f5c4130d8ab26a9b6361f58ea052a209ea6fd83", "exact_match": 0.0}
{"doc_id": 1930, "doc": {"question": "which is the only test playing nation that india has never beat in t20", "answer": ["Scotland"]}, "target": "['Scotland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the only test playing nation that india has never beat in t20?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" West Indies"]], "filtered_resps": ["West Indies"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db99dde3240da6c94372b17171e0c2dd3086706ed0e2d90c5d6ee86fd9c8482f", "prompt_hash": "23c06255ba5c88d5f603e7fa8b78d0dc732e3c526494459ad1ebfe81a2595685", "target_hash": "225c66e5ca83749482352532a44294a7f9c962a8c6ad78a01a5c7ca0e40a9ad1", "exact_match": 0.0}
{"doc_id": 1931, "doc": {"question": "input that tells the computer which task to execute is", "answer": ["source code", "A computer program"]}, "target": "['source code', 'A computer program']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: input that tells the computer which task to execute is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" output"]], "filtered_resps": ["output"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "678bdd52f6785c329b550f3e87abc1285cb7388b2cf0c2025b5b0d0753d0d30e", "prompt_hash": "15e8507541649aa94de0651dc5842cb8b374f5c5dfa9220e6a72603deec0ee34", "target_hash": "9d4c82314af60a5cc195d724ebbe4b248ebccfc90786a6456548c42e9bce7e29", "exact_match": 0.0}
{"doc_id": 1932, "doc": {"question": "who won season 2 of real chance of love", "answer": ["Jackie DaFonseca", "Kamille Leai", "Hot Wings", "Doll"]}, "target": "['Jackie DaFonseca', 'Kamille Leai', 'Hot Wings', 'Doll']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won season 2 of real chance of love?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Batten"]], "filtered_resps": ["Jennifer Batten"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44b17c0f8cc24273d65ea8709d4fc6f2002207f24be595be83e45f40314c1aea", "prompt_hash": "0664f1c98ca6e1b13519f750e28a10fb095c2169dbb2dd0eeb0cd9664888444e", "target_hash": "3534771963642c05fc5b306bae312981c2a9959078ec9eabfe34270553e1cd13", "exact_match": 0.0}
{"doc_id": 1933, "doc": {"question": "who does the voice of nelson on simpsons", "answer": ["Nancy Jean Cartwright"]}, "target": "['Nancy Jean Cartwright']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of nelson on simpsons?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dan Castellaneta"]], "filtered_resps": ["Dan Castellaneta"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d3cf588de2b36d62bd70e79ab07a20a75c6e5844fb79f6dad399409675f6f5e4", "prompt_hash": "b0324e4987951b9fb34738c764758ed4594cf5f1562b14e7a9c1376cd6209f20", "target_hash": "6b39214c7478e2502295102fe5d908aa3b512d92d85979e79f89602d96cab0ad", "exact_match": 0.0}
{"doc_id": 1934, "doc": {"question": "who is the coach of university of kentucky basketball", "answer": ["John Calipari", "John Vincent Calipari"]}, "target": "['John Calipari', 'John Vincent Calipari']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the coach of university of kentucky basketball?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Calipari"]], "filtered_resps": ["John Calipari"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ad5fcc97694fb375ed567e2f057a37bb91632f28fc1285d1c97a0d9aa4bfe1d", "prompt_hash": "db97c49b3db6a459a761afcbe707e1710486fa26f321c5a6fed5f0dc5756f741", "target_hash": "0b4c15a43a299a8e62fa96dbb0ca88095a303cdd35f441a9762c34f0b46b244c", "exact_match": 1.0}
{"doc_id": 1935, "doc": {"question": "who won the gold for the men's figure skating", "answer": ["Yuzuru Hanyu"]}, "target": "['Yuzuru Hanyu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the gold for the men's figure skating?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yuzuru Hanyu"]], "filtered_resps": ["Yuzuru Hanyu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "31694a76a6f1307127857e3599b4911948af3a1ca51c2cf644827a676e136620", "prompt_hash": "0acc2575a02296b57c606306b4c87672bb967812ea837b49bae6a3947d27130c", "target_hash": "f6935a29a19a36c37c3c4f200de042c11f3690a07ff9848ed93d24787842dbfc", "exact_match": 1.0}
{"doc_id": 1936, "doc": {"question": "who sang the song tell me something good", "answer": ["Rufus and Chaka Khan"]}, "target": "['Rufus and Chaka Khan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song tell me something good?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pointer Sisters"]], "filtered_resps": ["The Pointer Sisters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8164457bd4e9817e3d8a600f6e118a36f3344e3a11b484a97d3dae298ca47cb7", "prompt_hash": "15f1b48e61d74c196385c694e457b41a241ee23efbaa8985aca06f404f1cbe58", "target_hash": "c107c9c5ead541036114ce93acdc88f6974a15eb8a79eef7cf30e2417cf8a33f", "exact_match": 0.0}
{"doc_id": 1937, "doc": {"question": "the initial unification of upper and lower egypt took place during which period", "answer": ["3000 BC", "c. 3000 BC"]}, "target": "['3000 BC', 'c. 3000 BC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the initial unification of upper and lower egypt took place during which period?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Early Dynastic Period"]], "filtered_resps": ["Early Dynastic Period"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f1b8776a8f2fded6ddeeeed9cc11ef1f84193a31419ecaba618c0f8d67b1b6b3", "prompt_hash": "474c1a887ae2eedfdcf3c17d0bc68a4744aff56fc96b51f542b3ecf57999bd99", "target_hash": "d05829b0f94582737a05706ee0707dac3fbd5b144572c2eeeb8afa7f7fad2f04", "exact_match": 0.0}
{"doc_id": 1938, "doc": {"question": "who plays voldemort in harry potter and the philosopher's stone", "answer": ["Richard Bremmer", "Ian Hart"]}, "target": "['Richard Bremmer', 'Ian Hart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays voldemort in harry potter and the philosopher's stone?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ralph Fiennes"]], "filtered_resps": ["Ralph Fiennes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e74e5a0aa6ad98730843630842f8dc384347d6e45a804e490d99b4710ce5648", "prompt_hash": "490e3ff0a051b8cafc10f1eb3e9ff1b574a09a6ef650845791fc9fcd49be0c58", "target_hash": "972c97e9247f2017e4dfd9b62f9d92c4f09830e04648c21102a206c4d648e971", "exact_match": 0.0}
{"doc_id": 1939, "doc": {"question": "when does the next star trek episode come out", "answer": ["2018", "February 4, 2018"]}, "target": "['2018', 'February\\xa04,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the next star trek episode come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 15"]], "filtered_resps": ["January 15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb0bfe21733dbccfa9f72c4c236ca9e76907b01b5c4ec0f6f86e06fcca88e1cf", "prompt_hash": "192f1874324bdb84845292727cae9c178a0b999f131ba8cf06c3ecdb4a78b9e2", "target_hash": "ab1c7b236e6f1ac87842f5c7e4125356a938406ab8dd5344987a7991866d67f2", "exact_match": 0.0}
{"doc_id": 1940, "doc": {"question": "who is the present rajya sabha speaker of india", "answer": ["Venkaiah Naidu"]}, "target": "['Venkaiah Naidu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the present rajya sabha speaker of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bharatiya Janata Party"]], "filtered_resps": ["Bharatiya Janata Party"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a1d2905ab05908204a9ce5a810e3732856798eb8ac31021718862064062be541", "prompt_hash": "be2486ead96121199f4b75de19b40dd420a5dfa09dabb91deed02ab2d84186ae", "target_hash": "a23172591c2026392370c8cb25bc142e4f61652d9390f0af0ca98cb585f756ef", "exact_match": 0.0}
{"doc_id": 1941, "doc": {"question": "who is the book of galatians written to", "answer": ["the churches of Galatia"]}, "target": "['the churches of Galatia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the book of galatians written to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Christians in Galatia"]], "filtered_resps": ["the Christians in Galatia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91a5abf6449f11f318d3a96ed594945754040b752ee7583ffc69b010b97ada91", "prompt_hash": "f59006b4ddf571d3402e3449cf409b9a7a7b00f6e8e2fa73862e598be3af023d", "target_hash": "cb3db3408c663b7eb29e5d6f8e37420333455b5e6454101ea2a014fbeec73f23", "exact_match": 0.0}
{"doc_id": 1942, "doc": {"question": "panda is a national animal of which country", "answer": ["China"]}, "target": "['China']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: panda is a national animal of which country?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" China"]], "filtered_resps": ["China"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "59e167ba3e1ab0ea84c2598de4e4392d1beec51e0ab3578bb654c2f3ddd21c77", "prompt_hash": "dc2486b4521d6a53342fb0b5caefcf15df1f6dfb13b2ba48877c6b207a659e16", "target_hash": "11548c21ae9d3fb44407ddaa2e48320ccef615891642b38b1a21470d38e07356", "exact_match": 1.0}
{"doc_id": 1943, "doc": {"question": "when will the flash season 4 episode 17 come out", "answer": ["April 10, 2018", "April 17, 2018"]}, "target": "['April\\xa010,\\xa02018', 'April\\xa017,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will the flash season 4 episode 17 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 14"]], "filtered_resps": ["February 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "512e7206db8cfda771f0942af839c986ac05170f98733bc8ccc9e0eaa9e17a03", "prompt_hash": "9f4b9b8f5df02423782a52f417875ef220f385c6b5967b49460158675c6ea580", "target_hash": "71e07f48e79e32bba4ba9fe3508d755d90a9ad9a5617bcd9aa0f098ef362bafc", "exact_match": 0.0}
{"doc_id": 1944, "doc": {"question": "what do the 3 dots mean in math", "answer": ["therefore sign", "the therefore sign"]}, "target": "['therefore sign', 'the therefore sign']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what do the 3 dots mean in math?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ellipsis"]], "filtered_resps": ["ellipsis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b0f71940dd2e590c33b34371d508b4f4d3f10cb901a206660435a8d958763712", "prompt_hash": "8016d9278992b4ed9ef33a97b716ffcc6cbf2513ae2e5239d042bb6ee248ae09", "target_hash": "859b5e08def0b4f14b0bc1829c1f7a77fb60e9b00cc33d013f0bbc3741053cb8", "exact_match": 0.0}
{"doc_id": 1945, "doc": {"question": "when does star wars battlefront 2 com out", "answer": ["November 17, 2017"]}, "target": "['November 17, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does star wars battlefront 2 com out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" November 17"]], "filtered_resps": ["November 17"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e2163da76fd18932fbf12e52c13f450e086c81eb5660ba79143ebe5a951f89a5", "prompt_hash": "49ae98b27c4b85363a5f5ca138979dddd90e5d276837927839d02dd5925c08e3", "target_hash": "7aa4a3612064375e21eeb81995b6e705e2ca62d159ca98c4c208187cb605b713", "exact_match": 0.0}
{"doc_id": 1946, "doc": {"question": "where did the british army launch a successful invasion in 1776", "answer": ["New York", "Long Island"]}, "target": "['New York', 'Long Island']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the british army launch a successful invasion in 1776?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d5b48ce6ac2747f1252ef21983c826d9e5f808225686c801f04483a291b985aa", "prompt_hash": "3c5b82cba8bb6232abef901c2e7bbbd5057d454b7713e20925445b39c2c81d9c", "target_hash": "3ba0a42fa27361312982285d659ee9b9ad22098141486a9b832ea8e02ccf7c20", "exact_match": 1.0}
{"doc_id": 1947, "doc": {"question": "doric ionic and corinthian orders all refer to types of", "answer": ["architecture"]}, "target": "['architecture']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: doric ionic and corinthian orders all refer to types of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" columns"]], "filtered_resps": ["columns"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "82bb17d391adbc60d73c2769d1a392142abbdea47a4143531d9e440a1e9bbb61", "prompt_hash": "33260f717af41f7ab7bf4ac4d85d4055690cbd7c76320f64ea5ebb39d0dc81f2", "target_hash": "814bf1e9f75afa646c1fa2580998ea9e70df5d69c4df01e2306112d72d658640", "exact_match": 0.0}
{"doc_id": 1948, "doc": {"question": "consist of the sum of the fixed and variable costs for any given level of production", "answer": ["total cost (TC)"]}, "target": "['total cost (TC)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: consist of the sum of the fixed and variable costs for any given level of production?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" total costs"]], "filtered_resps": ["total costs"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6c085605fdb01ea671aaefc462939324c9e50ebae9fc33bb1af88ffd9bd025e6", "prompt_hash": "06afc6d24a3528fc906001aa013cae5a4514e664fd3c9c6b7173dcab172568f8", "target_hash": "9462dc6140512bd424b927d8f8027b1adb63c21c8b0fcec165ae3994b0333c3d", "exact_match": 0.0}
{"doc_id": 1949, "doc": {"question": "how old was sasuke when his clan died", "answer": ["seven"]}, "target": "['seven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how old was sasuke when his clan died?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 7"]], "filtered_resps": ["7"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5c332198edc0faa3550f01874cb3322aac459f48feab6b9b6ba4e57f87defab6", "prompt_hash": "5218622854370d721de75ce961afe9ba458be41bb254f50b2c3761100ce9384e", "target_hash": "0df021bc2ee5f7a96ecc2aed6d1e8a6bbd0fc254176d8606fc5cf30f5137cd86", "exact_match": 0.0}
{"doc_id": 1950, "doc": {"question": "what was the religion in the virginia colony", "answer": ["Anglican church", "The Church of England", "Anglican"]}, "target": "['Anglican church', 'The Church of England', 'Anglican']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the religion in the virginia colony?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Anglicanism"]], "filtered_resps": ["Anglicanism"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0cc0c207829c999716f574a45d64176b54ea3d5eaecb9549fec10583102149ea", "prompt_hash": "55f83dac67c2accaff91fd7ebc3aeeeed0f0e98788c6573c2e64823ed53dd247", "target_hash": "db8063a5245a5162719c9163bf18f6bd11a2b0dbddb7fa131a6be5f63510ce85", "exact_match": 0.0}
{"doc_id": 1951, "doc": {"question": "who wrote how do you do it by gerry and the pacemakers", "answer": ["Mitch Murray"]}, "target": "['Mitch Murray']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote how do you do it by gerry and the pacemakers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gerry Marsden"]], "filtered_resps": ["Gerry Marsden"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "223dea7c62d9892c6c91d2ebad5416be2b878e4c20cfee2d5fb49e55ddef4221", "prompt_hash": "b13ce6f870c93cd61daad5bc70c10424a8458c1913817debeb2ffc83dd634c71", "target_hash": "b060835de16b69b494117f89a1d008517f18f0d97940f48f5c62d9ebfd380211", "exact_match": 0.0}
{"doc_id": 1952, "doc": {"question": "who devised the fingerprint classification still used today in europe and the us", "answer": ["Hem Chandra Bose", "Azizul Haque", "Sir Edward Henry"]}, "target": "['Hem Chandra Bose', 'Azizul Haque', 'Sir Edward Henry']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who devised the fingerprint classification still used today in europe and the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Henry Faulds"]], "filtered_resps": ["Henry Faulds"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea3122e3961fdf6fa9c17a690f3e7d036d1e0835b8f93c6f474a884dfc212155", "prompt_hash": "209f7579b21d7f2d9797f461f27a526f6bf20dfb0091944e0b10601b0121116b", "target_hash": "a1444d8d88ba5e1072a9950963e6e9af0d083bf0ee8af5f039f92aba9e70b8e2", "exact_match": 0.0}
{"doc_id": 1953, "doc": {"question": "the first vice president of india who become the president letter was", "answer": ["Sarvepalli Radhakrishnan"]}, "target": "['Sarvepalli Radhakrishnan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the first vice president of india who become the president letter was?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" V"]], "filtered_resps": ["V"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0c3c30b1c9eb537650f42cc12f2fedb0bcf7d16bf2733aedfde194759d199419", "prompt_hash": "049bb2c31e27f315a5eb1344b10b2d362dec906b27e14e0cd263112086fadd85", "target_hash": "b4468f2a73cd39ffc63e7259297653b43ec86efd8dbbce4708ab7a0ef9628771", "exact_match": 0.0}
{"doc_id": 1954, "doc": {"question": "who led the soldiers in ending the raid on the harper's ferry arsenal", "answer": ["Colonel Robert E. Lee", "First Lieutenant Israel Greene", "Brevet Colonel Robert E. Lee"]}, "target": "['Colonel Robert E. Lee', 'First Lieutenant Israel Greene', 'Brevet Colonel Robert E. Lee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who led the soldiers in ending the raid on the harper's ferry arsenal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Brown"]], "filtered_resps": ["John Brown"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "38f99ca07abce58807f61d39f3c9b9bff2c6a4cecb8fb662302e6f9291e8a00c", "prompt_hash": "d491f188028ccb93c148be54b8ac60eaef7e6b201b736d7ad1f9f0d7ffe572fa", "target_hash": "b572ce7cd3c2719027d57fc443df4bd9026e01f6aca96b7a651a27d4df3fc03e", "exact_match": 0.0}
{"doc_id": 1955, "doc": {"question": "where does the cell spend most of its time in the cell cycle", "answer": ["Interphase"]}, "target": "['Interphase']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the cell spend most of its time in the cell cycle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" G1"]], "filtered_resps": ["G1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32d388199a3bf09f0bdc7c3919544679f6eeede56814dfedcca16b6d768bef11", "prompt_hash": "194ee017c0cdd2a0599def731b80b5be037a1d0b50bcaa9343d01e94847d6229", "target_hash": "5c11eb66d84b956b82fb673ff7e7bfd79c9a607760bbca75cbf39cbc67553fbc", "exact_match": 0.0}
{"doc_id": 1956, "doc": {"question": "who sang a whiter shade of pale first", "answer": ["Procol Harum", "British rock band Procol Harum"]}, "target": "['Procol Harum', 'British rock band Procol Harum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang a whiter shade of pale first?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Procol Harum"]], "filtered_resps": ["Procol Harum"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0faefb8626c55708422535ce779511656bced148c8df4aff94a7841ccc00993c", "prompt_hash": "910596339b75db1ff0158456a0f55efe5c9a9b3035e11248760b923b8bfc23c9", "target_hash": "9557a62dfe41f6c68474b47b5110e1c8590a3d61977901303cfd987a21be7c1d", "exact_match": 1.0}
{"doc_id": 1957, "doc": {"question": "who was first governor general of british india", "answer": ["Louis Mountbatten", "Warren Hastings"]}, "target": "['Louis Mountbatten', 'Warren Hastings']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was first governor general of british india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Warren Hastings"]], "filtered_resps": ["Warren Hastings"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "93ab72b7c821b24fd98dba23ee13914047f94d9593ba11f9a247746ff584c024", "prompt_hash": "b1804e3a499fc37f742c522546beefe437574882ea2b1cc8c497892aea8f85ec", "target_hash": "e3f153424422c6ae4b8ff732764dd0527f89274dc64a5f234df3d0082669859d", "exact_match": 1.0}
{"doc_id": 1958, "doc": {"question": "surface area in many kinds of organs is important because the amount of surface determines", "answer": ["effective gas exchange"]}, "target": "['effective gas exchange']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: surface area in many kinds of organs is important because the amount of surface determines?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the amount of diffusion that can occur"]], "filtered_resps": ["the amount of diffusion that can occur"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "76175b1168364fda479cfb87ddebd5190ef38f5ed67db70fba9b4a36c36f1271", "prompt_hash": "282ed287e8c1438d22ef08d991783bc775de0f504e78527a33f301f2209ed74e", "target_hash": "2416327ee035c92325ff0d11f29397c24545151ff5dbed7c6d79cdfc27eacf0d", "exact_match": 0.0}
{"doc_id": 1959, "doc": {"question": "who has majority in the house and senate", "answer": ["Republican"]}, "target": "['Republican']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has majority in the house and senate?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Republicans"]], "filtered_resps": ["Republicans"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "455c364dac861167ca444cf12a27440a1d1ab5d61ef2dbb4dcf4099aa94ceec8", "prompt_hash": "efb317901e82d80a565ad49b65943814dfc43ddad3cce9e357e425bdd8b75930", "target_hash": "9d31a4350b0cb53202068f77ffe7a9e8d7a837065d1bd474c907f1f0523771f2", "exact_match": 0.0}
{"doc_id": 1960, "doc": {"question": "who is the most losing team in nfl history", "answer": ["Arizona Cardinals", "Tampa Bay Buccaneers"]}, "target": "['Arizona Cardinals', 'Tampa Bay Buccaneers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the most losing team in nfl history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cleveland Browns"]], "filtered_resps": ["Cleveland Browns"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eef45b59a398b53def20140c8f656eb1f5140b2e10057c97fe4e7c50c7e9137d", "prompt_hash": "93e9496e96a049c501ee5ac3bbe76a142cd5ce0041d206a3d08a0dfe5f62d430", "target_hash": "e922cce4a9fc0cd8b5e8d55a4f7daec81bbe639f566e8236e8d36c7c627cbc99", "exact_match": 0.0}
{"doc_id": 1961, "doc": {"question": "what is the highest scoring letter in scrabble", "answer": ["Q"]}, "target": "['Q']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the highest scoring letter in scrabble?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Z"]], "filtered_resps": ["Z"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b33429b343346565c6bb4c98e5eb4e7580c7768848da4fa5846d827bf090328", "prompt_hash": "18377097088901ab0b521d29934dece93a406e8505a76b69c5e1f03a5cd2e9d9", "target_hash": "61cea69273aa12b9aa27d335365a71906ba01bb9f5b9aca3b5cd682898bc574b", "exact_match": 0.0}
{"doc_id": 1962, "doc": {"question": "where did the book small steps take place", "answer": ["Austin, Texas"]}, "target": "['Austin, Texas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the book small steps take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Georgia"]], "filtered_resps": ["Georgia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06169bbdf898a58777b71a6b082d0bd07b6ca9b8f842ff91d6543abaa0a8b572", "prompt_hash": "c74107bb53980617488894faa1554f07dbb6495d5cfcdd8414c2d6d61242dd6a", "target_hash": "46b556033b089d5a1b00fe90f12d0a6133fd72009edf9480c03aa24ca269e6d7", "exact_match": 0.0}
{"doc_id": 1963, "doc": {"question": "when did bihar bifurcate from bengal and some parts of chota nagpur merged into bengal", "answer": ["1912"]}, "target": "['1912']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did bihar bifurcate from bengal and some parts of chota nagpur merged into bengal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1912"]], "filtered_resps": ["1912"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "055746cfbc0fa3043f6b57f7a9c1f998846458b22a1e3f9e794ff61c23a0494e", "prompt_hash": "cfc18efd6a83f77f37adfb0108b7b2de0a7f2f98e113130d2068cfb71efbf6ee", "target_hash": "f96a9f699570cdaad849cbd1db4405807bd9884026f9dd7112631bd71be24f24", "exact_match": 1.0}
{"doc_id": 1964, "doc": {"question": "how many series of diary of a wimpy kid are there", "answer": ["12"]}, "target": "['12']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many series of diary of a wimpy kid are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11"]], "filtered_resps": ["11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91486bc55d7949c3cdf27f7c6da5386af0ece48f7cd1f07a096fef731f50f245", "prompt_hash": "cd9fae973692d9ea589b592874d5ecb2721ddac2a74d526effe12cb3a79421e4", "target_hash": "0485d4130dc78d35a36ed5f5c5bfea741e1e1e0ea37b856d0197e254d85d7d8e", "exact_match": 0.0}
{"doc_id": 1965, "doc": {"question": "who created the very first adhesive postage stamp and what year", "answer": ["Sir Rowland Hill", "1837"]}, "target": "['Sir Rowland Hill', '1837']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who created the very first adhesive postage stamp and what year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1840"]], "filtered_resps": ["1840"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d2c62afbdb0c993e5865d450d007b15652d6e182b98e6a543a6972fd2840c58", "prompt_hash": "83cb8088f878ae552185e43b028e66e0d04db5ce7db565a924b0be6ed64c5aa6", "target_hash": "46be97c17761f020bad4d5828d112aeaf0966659dbeef84053a7258f0570feb0", "exact_match": 0.0}
{"doc_id": 1966, "doc": {"question": "who is the writer of a walk to remember", "answer": ["American writer Nicholas Sparks", "Nicholas Sparks"]}, "target": "['American writer Nicholas Sparks', 'Nicholas Sparks']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the writer of a walk to remember?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nicholas Sparks"]], "filtered_resps": ["Nicholas Sparks"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dd1a71416086e4ef6b659500f810821a3cdf2f23b9af78ec3149f69cf65afd2b", "prompt_hash": "38eaa5f31e1723824db570d2a4069de048c6848883d96270622c849e0c66becb", "target_hash": "7ea337c2aa59d5214ff682ab5273a983691dd9b55e96e1bc3f349eb97ed51c87", "exact_match": 1.0}
{"doc_id": 1967, "doc": {"question": "when did red dead redemption 1 come out", "answer": ["May 18, 2010", "May 2010"]}, "target": "['May 18, 2010', 'May 2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did red dead redemption 1 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2010"]], "filtered_resps": ["2010"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "88448d965421dcd1b9008e202f64aae2b65dca2d35b86c763d0a48a207640bbd", "prompt_hash": "25ec48ca70492c4205f89872d62a6b6b89df1f85d2834071749f7ae11c332e62", "target_hash": "134d731d7b881e13e70d2f33efa011004d75dabfdacca3bd13fbe0959d6cd909", "exact_match": 0.0}
{"doc_id": 1968, "doc": {"question": "who sings theme tune to orange is the new black", "answer": ["Spektor"]}, "target": "['Spektor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings theme tune to orange is the new black?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Natasha Bedingfield"]], "filtered_resps": ["Natasha Bedingfield"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a37d7c9f805ad36cd8798581b1b5997f75f9e5f6445371b75894eb7297ddc27", "prompt_hash": "d091f7b68322f797d0be6db0aeb5ba79cccff2057e96c5c9c82c128b872b4a42", "target_hash": "10fa4d56a33abb9aed5938ab06dbd18a9b494431955a416c1fadf59c688c166d", "exact_match": 0.0}
{"doc_id": 1969, "doc": {"question": "what channel is abc on in phoenix arizona", "answer": ["15"]}, "target": "['15']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what channel is abc on in phoenix arizona?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 7"]], "filtered_resps": ["7"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2e67edc254c8c966590eaf97a2cf259c002260bf9db6647f1769cef22b61bc66", "prompt_hash": "e0e412eaa69c617acaa1af831bf5e05451a30ba1f3805ea85a5addf7b1af71d6", "target_hash": "c5bc95ba9379683d873798779c72d2be31aef493314e068b8f9f46dc030cdcec", "exact_match": 0.0}
{"doc_id": 1970, "doc": {"question": "where does the brazos river start and stop", "answer": ["Gulf of Mexico", "Llano Estacado"]}, "target": "['Gulf of Mexico', 'Llano Estacado']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the brazos river start and stop?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brazos River"]], "filtered_resps": ["Brazos River"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d49b23ef25de6d0c48382c466df39be29210ed9b910e38c0a661f43e0438dd0b", "prompt_hash": "85215026f3bbac44128541b7d1f6fe413fa8e857f6c1645af584d1395200c3fb", "target_hash": "d550fc356173f309b1db3607b3a84db55ab301278b33073d129a4da31dccfe5b", "exact_match": 0.0}
{"doc_id": 1971, "doc": {"question": "what is the purpose of the bromophenol blue dye in the samples", "answer": ["as a pH indicator", "a dye", "a color marker", "a pH indicator"]}, "target": "['as a pH indicator', 'a dye', 'a color marker', 'a pH indicator']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the purpose of the bromophenol blue dye in the samples?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to visualize the bands"]], "filtered_resps": ["to visualize the bands"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e89b9b2b513419fcea984783e970c746a55348d48e0d5bfc46fbd69d030d6305", "prompt_hash": "9bad5e7ad4aae0e4303de3fbfb11b74c9a50218ab922f388519e41aa98536e97", "target_hash": "06ce9cd21e8189be40e95591157378c38b586f27e2b2a6da0d9e39152e864079", "exact_match": 0.0}
{"doc_id": 1972, "doc": {"question": "when does jim propose to pam on the office", "answer": ["In the Season 5 premiere"]}, "target": "['In the Season 5 premiere']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does jim propose to pam on the office?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christmas Day"]], "filtered_resps": ["Christmas Day"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "88654e49e13e8bc7b21a56d22ea950d80a381c3b682671fade073c9f90ae0229", "prompt_hash": "44282889b06033e3f44e32c567aa9fa3565aaf3472527959ac8e8036d44a30d8", "target_hash": "612eef634946488e9056454fb8d908f52c14f7e72812ce5b9c5c375cd95c960f", "exact_match": 0.0}
{"doc_id": 1973, "doc": {"question": "who dies in akame ga kill night raid", "answer": ["Tatsumi"]}, "target": "['Tatsumi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who dies in akame ga kill night raid?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Akame"]], "filtered_resps": ["Akame"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6af3a64d79655443188ca456124c754133948d360de41d385cc222ab4a1d9acc", "prompt_hash": "43d205d4162fa85da21ab4bcd9281e37e3cda2417b8caab7b1bd4fe5f4de44cd", "target_hash": "8c6247527a4f1aa09091194244b305fbfb6135da5e39386b07b4d9d08e254a73", "exact_match": 0.0}
{"doc_id": 1974, "doc": {"question": "who played mary in christmas with the kranks", "answer": ["Felicity Huffman"]}, "target": "['Felicity Huffman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played mary in christmas with the kranks?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kristen Bell"]], "filtered_resps": ["Kristen Bell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db5e518d0b9ab87686092acb1d64adcebb5a360de9d145a4c852a945226bf377", "prompt_hash": "aa728cd0a62c1785d7d2c599f77b43ccec0283cab07264b869f221e9a3621928", "target_hash": "d68b1f65aeedcdcab5c142e022bee844a03fc1ce5a02108a6c73e2da4227728b", "exact_match": 0.0}
{"doc_id": 1975, "doc": {"question": "when does dragon ball super episode 113 start", "answer": ["October 29, 2017"]}, "target": "['October 29, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does dragon ball super episode 113 start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 14"]], "filtered_resps": ["January 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3675440d3290e64dd26cfac110a4f371f28652f7f9f736c8c2dfccfff31cd87", "prompt_hash": "8d767a8bf5db3486bbcf067f635344c5034f6b28ed522b8aa500f6e70e172b43", "target_hash": "83d0fdf19b6ab928c593b5a0704e22a297712a0cde6d7bea4b62118dc8f06c92", "exact_match": 0.0}
{"doc_id": 1976, "doc": {"question": "who does the voice of marceline on adventure time", "answer": ["Cloris Leachman", "Olivia Olson", "Olivia Olson in most appearances", "Ava Acres as a child", "Ava Acres"]}, "target": "['Cloris Leachman', 'Olivia Olson', 'Olivia Olson in most appearances', 'Ava Acres as a child', 'Ava Acres']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of marceline on adventure time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Hale"]], "filtered_resps": ["Jennifer Hale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea4a7d81a6c9b90b555c097018835919eea899942a1fecc45f35a1bf74be656d", "prompt_hash": "fa5a7045a25208d0b7dcd9b0de67cc009ed4920dbdb5e402d46b4c375d6fe038", "target_hash": "6c331167dc7665490675908398a496b2012e543f5d754d1e1e8d854227e91188", "exact_match": 0.0}
{"doc_id": 1977, "doc": {"question": "who has the most green jackets from the masters", "answer": ["Nicklaus", "Jack Nicklaus (6)"]}, "target": "['Nicklaus', 'Jack Nicklaus (6)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most green jackets from the masters?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ben Hogan"]], "filtered_resps": ["Ben Hogan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8896feacadf47d74f9d9c4cc8ba94037ff189c09dac99de49d5acde57dfa1726", "prompt_hash": "3e7213acbefa4e1362f58f2e9d82ae129cf6e598dca29b368501c7eefb4fd11a", "target_hash": "f6f28b9d08d4c497972dd8323dfa99543400194c93001dd1af96408ad4edd376", "exact_match": 0.0}
{"doc_id": 1978, "doc": {"question": "where is the eagle creek fire located in oregon", "answer": ["Cascade Locks", "Columbia River Gorge"]}, "target": "['Cascade Locks', 'Columbia River Gorge']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the eagle creek fire located in oregon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oregon United States"]], "filtered_resps": ["Oregon United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "920864b9af633f300e12760829bf817d6fa5e8418c44df2943ce51d1ef9a307a", "prompt_hash": "4ab02b36bfbdb434f592db2a49244f28853ff618f3e328857b3bf6f7b8d9b6c3", "target_hash": "f46cbd7e26b42ac41d1768c3770c0a3cf7b17350beabd15dff760cbfcbb9c449", "exact_match": 0.0}
{"doc_id": 1979, "doc": {"question": "sending money home to the native country is an example of", "answer": ["international capital flows", "remittance"]}, "target": "['international capital flows', 'remittance']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: sending money home to the native country is an example of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" remittance"]], "filtered_resps": ["remittance"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "98d43b754cc1cc1662d37f4ea2615fa1d3f6e8424d7dc0bbefdca2525b41f0ad", "prompt_hash": "ca3069366ca3c2b88e8ec7afc01652524bf2ae683f2a88089e44868752de6f7c", "target_hash": "f164068f83a8f92b283d66eaa037fd0f3006d932006f6ef2280c2ceb85e0855d", "exact_match": 1.0}
{"doc_id": 1980, "doc": {"question": "how many episodes is ash vs evil dead season 3", "answer": ["10"]}, "target": "['10']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes is ash vs evil dead season 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "88768ff9d42cef773e8aafa003a16d063de69f0628bef6bb0d388bff1d5677d2", "prompt_hash": "b2750a296f62358f4b2a3df67ce54b2dcc1d4180c04b6deb9393b2f8a7334594", "target_hash": "9503346d501aeaec3f521be685eb235cda81c7929ba2f1acdae1be91400b787a", "exact_match": 1.0}
{"doc_id": 1981, "doc": {"question": "what is the song in red dead redemption", "answer": ["Far Away"]}, "target": "['Far Away']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the song in red dead redemption?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Redemption Song"]], "filtered_resps": ["Redemption Song"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9357e3d98b691dedc3ccc5c91ca741850b1bcfa5b3f737d04dd43ab348cd25bc", "prompt_hash": "ed9afa9dbac3916c35b04383a1b117a21e92a209a6996870ad40b8f002c54e6d", "target_hash": "33ac98aa018a68a6af8dfc4bf2c22abdc576ca0859c59312cda1713e64411b8f", "exact_match": 0.0}
{"doc_id": 1982, "doc": {"question": "who is the head of the department of homeland security 2017", "answer": ["Kirstjen Nielsen"]}, "target": "['Kirstjen Nielsen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the head of the department of homeland security 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Kelly"]], "filtered_resps": ["John Kelly"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae4e453297800dabdf96d2e5312c878d9d9407b52f6a088994ca60cf5e25bc32", "prompt_hash": "3f5855fbaf0cab4c16b7aeae714a0045e02374c0b4d97124091bfc7b115999b0", "target_hash": "30a52b2631ead55a114d7b1fb27ab8e011ca989caaf7d85f65de40d843296186", "exact_match": 0.0}
{"doc_id": 1983, "doc": {"question": "who is the ceo of what's up", "answer": ["Jan Koum"]}, "target": "['Jan Koum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the ceo of what's up?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jason Johnson"]], "filtered_resps": ["Jason Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5fffec7769fa244b6abef3e339ef434b57475ae250a5585480ac8115b6ba16c0", "prompt_hash": "ed3f8641ecff0c05df35b3b06304dc254418b7d422fc28882e2b887342af2e66", "target_hash": "849270c709aba320f3f6726c17f683f9324b5e9e48a0e193a579002cf70c86b3", "exact_match": 0.0}
{"doc_id": 1984, "doc": {"question": "what part of dc is howard university in", "answer": ["in northwest Washington", "northwest", "northwest Washington"]}, "target": "['in northwest Washington', 'northwest', 'northwest Washington']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what part of dc is howard university in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Northwest"]], "filtered_resps": ["Northwest"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "21e3fc41959c8eb41e2f47b02544f1bdf6a83fa250ba671de2b5b16a5b3c0da1", "prompt_hash": "3dd5e7afa6293c7c719b5e59cf01d8117f8715e643cc9467545b76a5c7efbd1c", "target_hash": "748001a01f68ac864700bb4fd933310fec32ae7f763348ed7da0b5652dabd52b", "exact_match": 1.0}
{"doc_id": 1985, "doc": {"question": "who has the most olympic medals in figure skating", "answer": ["Scott Moir", "Tessa Virtue"]}, "target": "['Scott Moir', 'Tessa Virtue']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most olympic medals in figure skating?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dorothy Hamill"]], "filtered_resps": ["Dorothy Hamill"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "14f1a97b7e75beb9d3189577238e2d49103715ca96b384b6af03974da0e5242c", "prompt_hash": "9e61e3c9c8f55e294dc12fc5941996f84506ced34599dc09ef55ebd4bbe610f8", "target_hash": "ed693c7af85ed398b606eeaa78e570828ccf6745954f500ebfe1b6cac7c5982e", "exact_match": 0.0}
{"doc_id": 1986, "doc": {"question": "who scored the most points in a single game in the nba", "answer": ["Wilt Chamberlain"]}, "target": "['Wilt Chamberlain']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who scored the most points in a single game in the nba?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wilt Chamberlain"]], "filtered_resps": ["Wilt Chamberlain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f59ff53930081cfb325581d3c0db83ffef60587536fe344107f2e71da649a7c", "prompt_hash": "ad892c1341d3c8148bc93f2167f6de994d783c87551317a32f2550cf0c9665dc", "target_hash": "84924a8be2c9aacc9097f91bbf6a08311409f600a16f07822001cb664a550a11", "exact_match": 1.0}
{"doc_id": 1987, "doc": {"question": "when was the design period known as art deco", "answer": ["1910–1939"]}, "target": "['1910–1939']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the design period known as art deco?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1910 to 1939"]], "filtered_resps": ["1910 to 1939"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0833037a2ae67b5ca9c03b77f02c1344db7605827bf332bcd527827dd8cf5b68", "prompt_hash": "fc044a389bbee3b9e3b2e8079d2accb4ba4da20f4f3c72d80e295c5dea6462a2", "target_hash": "21c69f7d96d6d3badc1802f2a762689ccba64a1733392cbff19af629c9e7fa5f", "exact_match": 0.0}
{"doc_id": 1988, "doc": {"question": "disagreements involving slavery and states' rights were two of the main causes of", "answer": ["the American Civil War"]}, "target": "['the American Civil War']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: disagreements involving slavery and states' rights were two of the main causes of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" American Civil War"]], "filtered_resps": ["American Civil War"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d08e2d67617ee9a8aad2b07d544bbac3be464f6d64e8f6af9326a79d93722f36", "prompt_hash": "f5e1087a343067e7971d3e071081564d73af4b015f78c862bd982197e49ed60d", "target_hash": "53cde504cb5bda3a09d85ee0bc6fb48b0a261fdc5376b5a25ee037c019b1f1e7", "exact_match": 1.0}
{"doc_id": 1989, "doc": {"question": "what was the biggest tank in world war 2", "answer": ["Panzerkampfwagen VIII Maus"]}, "target": "['Panzerkampfwagen VIII Maus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the biggest tank in world war 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Panther"]], "filtered_resps": ["Panther"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed6fbc41e21ba5acda2f483e90335e180694ef0b52544570e013e28077622a8c", "prompt_hash": "37bdc7ef6d72ff9ae73c905cfe01b647db5bb543f1248b27bf344def24d0af96", "target_hash": "3e12567b1420f4d3c6b891bf143dbfedea7658c959f5ae5e5979dbc43a96486a", "exact_match": 0.0}
{"doc_id": 1990, "doc": {"question": "what does the c stand for chrysler 300c", "answer": ["coupe"]}, "target": "['coupe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does the c stand for chrysler 300c?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chrysler 300C"]], "filtered_resps": ["Chrysler 300C"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "09ea914341a1eebf1bc3e5afa85400c821899bb38c2f0f4d9d126e0281454b57", "prompt_hash": "702ec8ac06158386ce0e0d57d0ab4f40109e771f648edc8b0cee5376ba48d1a7", "target_hash": "0103faa0d16774115f1635ee89f04d62ca66199aa12c41634657a117833a0ec5", "exact_match": 0.0}
{"doc_id": 1991, "doc": {"question": "who plays the walking boss in cool hand luke", "answer": ["Morgan Woodward"]}, "target": "['Morgan Woodward']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the walking boss in cool hand luke?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul Newman"]], "filtered_resps": ["Paul Newman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6cd91d07ce3fbdec3e4554da5cebe666608730b3abb6ebf791a40c9a996667e2", "prompt_hash": "a8f4a255a6291f02e09b8b4e3f786cef8dd982a7aa3629e23b3282cd10d1d5a4", "target_hash": "a1e618c41d639a5ad8d593eba115bbfd6d1ba53fe24967e56a68ac8cbdc925f5", "exact_match": 0.0}
{"doc_id": 1992, "doc": {"question": "who was the first lady nominated member of the rajya sabha", "answer": ["Mary Kom"]}, "target": "['Mary Kom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first lady nominated member of the rajya sabha?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vijaya Lakshmi Pandit"]], "filtered_resps": ["Vijaya Lakshmi Pandit"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8af799fb19c32d7561cf4673b9021966d1c82b2578f3ea0d71bd75666ca88a09", "prompt_hash": "d4ad670dbc76ae097e7d17eef44c19edff9ecf1d4f51fde5898713de2836a02e", "target_hash": "211582001d78f0befb19a72f96f072012312617c00480c1fad2f53a9b889a190", "exact_match": 0.0}
{"doc_id": 1993, "doc": {"question": "who has won the canada open women's doubles", "answer": ["Wakana Nagahara", "Mayu Matsumoto"]}, "target": "['Wakana Nagahara', 'Mayu Matsumoto']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the canada open women's doubles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cara Black"]], "filtered_resps": ["Cara Black"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6a99bb9879c6e8a5119cc06c91d5a52eadf963753194dc68b36eb0659a883d40", "prompt_hash": "36f5cd2ff5d0539e9a14b9f3fd740dca412b866773e1eb82651ddc5add4e6cea", "target_hash": "6c6c1f98902c5ffd036fae59429dcc45a7c66caa1c27d04b8c63059472c1fb87", "exact_match": 0.0}
{"doc_id": 1994, "doc": {"question": "who played charles on when calls the heart", "answer": ["Steve Bacic"]}, "target": "['Steve Bacic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played charles on when calls the heart?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew Daddario"]], "filtered_resps": ["Matthew Daddario"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "95274bfcc2fb03bf5252093ea5f5b95d1c4105d3295bdd0ad5933a013772d3f0", "prompt_hash": "d3e3fce935a8a906645b64b1bdf002da18666774a9d05a45b84b151ecf5b1516", "target_hash": "d23e954293127c2be1d0fb2785932ae389a845f918c6e9aa86556ddacbc44dbb", "exact_match": 0.0}
{"doc_id": 1995, "doc": {"question": "how many wins does tiger woods have on the pga tour", "answer": ["79"]}, "target": "['79']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many wins does tiger woods have on the pga tour?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 79"]], "filtered_resps": ["79"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "40357a65bf53001218a54a1869bb2f5a7a375fd27ceb67738d0bbc1a20639289", "prompt_hash": "acf886e187b34c4a10471165f634a3e634fa82461cbc34753a3d0c3bf8a83c61", "target_hash": "3a7defdbbb79b3926013e56da0f3d467a24f7ddebf6f778992423d99efbca7e1", "exact_match": 1.0}
{"doc_id": 1996, "doc": {"question": "who has the most subscribers in faze clan", "answer": ["FaZe Rug"]}, "target": "['FaZe Rug']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most subscribers in faze clan?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Astralis"]], "filtered_resps": ["Astralis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19238190b28695eff8d9a23271acc41797741ec66b3a640b2cb5086eacffb880", "prompt_hash": "dc1c99f9b1dbea0e06c27ff437ec4f461a54f615bbde9c06b6e8cc9911c8eced", "target_hash": "a2f55ffc152252fbfaaf65593bf00f2334ea6313895379557b36fb755c8379c4", "exact_match": 0.0}
{"doc_id": 1997, "doc": {"question": "where was the movie a walk among the tombstones filmed", "answer": ["New York City"]}, "target": "['New York City']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the movie a walk among the tombstones filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "597fb31f6f9867b4fb7a105a3de12146c5bc88f770abd7e10bb613aa3d07d25a", "prompt_hash": "ce4677fc7f53dfc35278e1adee62b2cac40b70c949d6371aa2523a2bbc7c357e", "target_hash": "34b5642e7ac3c37be810afca322277ff3e939f50d7aa576ecb54dd043fa4bbbe", "exact_match": 0.0}
{"doc_id": 1998, "doc": {"question": "where does the paraguay river start and end", "answer": ["Paraná River"]}, "target": "['Paraná River']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the paraguay river start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paraguay"]], "filtered_resps": ["Paraguay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bd74cf72c6ad26e73a15ef31529be3ede82c7da175fa4f0a7f153af3b625850a", "prompt_hash": "9af451978dd03a0c2f72bf87772714fecb42aa19ee7e498dd7f34579ce29292e", "target_hash": "3f3bedd4645d9a4ab25795c4368383bc924f7b75f2c10f90c41ab06106840caf", "exact_match": 0.0}
{"doc_id": 1999, "doc": {"question": "the nurses' health study is an example of what type of study", "answer": ["long-term", "prospective", "prospective studies"]}, "target": "['long-term', 'prospective', 'prospective studies']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the nurses' health study is an example of what type of study?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cohort"]], "filtered_resps": ["cohort"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3fd6d70cc3917a49db7a13e74888b7e7d01e62903b38286523f2971d1a0e84b", "prompt_hash": "92c1c85ff0e78dcfd70df91a9955e766c07bbf03ef9e205ac4f98e9fa9ee9ada", "target_hash": "79758eb434724f4523a8eea9930e58fe4846f675ee84bfcb7a66c80cb2def40e", "exact_match": 0.0}
{"doc_id": 2000, "doc": {"question": "the region that stretches between the black and caspian seas", "answer": ["The Caucasus Mountains", "Caucasus Mountains"]}, "target": "['The Caucasus Mountains', 'Caucasus Mountains']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the region that stretches between the black and caspian seas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Caucasus"]], "filtered_resps": ["Caucasus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9330bd47b2e9afc3a6cd0888ad06d83fdc02281972a7a66b8849689d75cd0610", "prompt_hash": "5c2b5cb8d1ec86dcad357d4edab7881553f8e692364e8d28e3b8477d5ba9c689", "target_hash": "ad41e69dbcb3a9e7af1d72124e720e1634aed816be061c7a16d2602deacc0d32", "exact_match": 0.0}
{"doc_id": 2001, "doc": {"question": "who plays red on orange is new black", "answer": ["\"Kate\" Mulgrew"]}, "target": "['\"Kate\" Mulgrew']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays red on orange is new black?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kate Mulgrew"]], "filtered_resps": ["Kate Mulgrew"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7167e73cdd040a5144eef0895af70247020ebe91012992985d0913f4e352d6fe", "prompt_hash": "6f2043eb07412b74db68e0d5ccb56a31c752ce49be6cd8075970e350f2a254d8", "target_hash": "f86957d2cf35cacf4f90e4ef1470a959a89e510b1bd7e72d1b64e2df4101a541", "exact_match": 1.0}
{"doc_id": 2002, "doc": {"question": "what was the name of the plant on little shop of horrors", "answer": ["Audrey II"]}, "target": "['Audrey II']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the name of the plant on little shop of horrors?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Audrey II"]], "filtered_resps": ["Audrey II"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e50f1c883e62c92629e8d1297474d7857a4ed8b07002213086b56c971cba135f", "prompt_hash": "89997e429fe7784e5606dc509a4cd11d2b9bbc2fcebfbe9e6b9107dfa50fc18b", "target_hash": "51faff7d29fd3e2a32d1b5daf59090fe368898361c5ccad2550235eb92d90e0f", "exact_match": 1.0}
{"doc_id": 2003, "doc": {"question": "what nba player has scored the most 3 pointers", "answer": ["Ray Allen"]}, "target": "['Ray Allen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what nba player has scored the most 3 pointers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ray Allen"]], "filtered_resps": ["Ray Allen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "76cf920d45fa4e72668cf88837076b8e805aa58f75607491427829b463313c12", "prompt_hash": "d9854e58bdb22f9c5861ca0cc17a0e28dff735babe670ff0a1274f55ce0e2e09", "target_hash": "b1e24d3e227f433a561c0d6045417db54f147e25f5ac582137315be230888fc2", "exact_match": 1.0}
{"doc_id": 2004, "doc": {"question": "wal-mart is a prime example of which type organization", "answer": ["multinational retail corporation"]}, "target": "['multinational retail corporation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: wal-mart is a prime example of which type organization?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" retail"]], "filtered_resps": ["retail"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0595aa8d0f66583b3bcdb4f5d263c76908cbd32d7eb7ed96ff7058f65d67a62c", "prompt_hash": "f2160262df30ca904c96aedba3dae76d9b18537adf58d78ad1efba3f9cfc81eb", "target_hash": "8ae3be9ad9c7dbac23c7c49e880e355b52232fc6b0e95e9a5df23385ca734693", "exact_match": 0.0}
{"doc_id": 2005, "doc": {"question": "who won season 8 of america's next top model", "answer": ["Jaslene Gonzalez"]}, "target": "['Jaslene Gonzalez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won season 8 of america's next top model?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Candice Swanepoel"]], "filtered_resps": ["Candice Swanepoel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9551820e3125c1c9795ef39c210253d6a8de5ff6d6905f6301b2075cc689576", "prompt_hash": "cf524e06306b6bb993c759fe146f457ea0b4f991c30ae2d08573613038876c15", "target_hash": "156584f5ee387934fe0c5d76ff63c871e87d7cd51c7c5c8c0285236303608a28", "exact_match": 0.0}
{"doc_id": 2006, "doc": {"question": "when did macbook pro 13 inch come out", "answer": ["October 2008", "June 8, 2009"]}, "target": "['October 2008', 'June 8, 2009']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did macbook pro 13 inch come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2006"]], "filtered_resps": ["2006"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9388a7998dc776d9f4b7294b30facf82a6e499b22ac76fefe672d766aea7923f", "prompt_hash": "4bfc3224bed93df932035e08b408f794d1edced9dde440796102410a3d37e356", "target_hash": "87b339cfed711ae4d4fe6606115d9b20fa8c9b61308efbc559ddad6967bae005", "exact_match": 0.0}
{"doc_id": 2007, "doc": {"question": "when was the mission san antonio de valero built", "answer": ["the 18th century", "1718"]}, "target": "['the 18th century', '1718']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the mission san antonio de valero built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1724"]], "filtered_resps": ["1724"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4865cfdd94e8157f7468991deb428c89241f2c5fc93e20658fe19592c41a994a", "prompt_hash": "3772f0b919ec503b959e7df1fadd41117b7ddd32d92b9c61b683af1a26402d83", "target_hash": "d43b24fd58272e955513da7d919aab4956b29e40d3a77803ee3f8393f22e49f3", "exact_match": 0.0}
{"doc_id": 2008, "doc": {"question": "what is the current rate of interest on ppf", "answer": ["7.6% Per Annum", "7.6%"]}, "target": "['7.6% Per Annum', '7.6%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the current rate of interest on ppf?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 8"]], "filtered_resps": ["8"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1b100879feb65f84c680496312666b0d60dd45dd6d0c70b18ef7cbc9f8d9d2bd", "prompt_hash": "511017d5e8881a9d5de24d90c702d5c9417542fccaa9441b29b77b17f53ac463", "target_hash": "b065015a09933fe91518672f7dadb865717b8189b183bb9b2617cc5bf135d2d4", "exact_match": 0.0}
{"doc_id": 2009, "doc": {"question": "when did the lion king start on broadway", "answer": ["Broadway", "October 15, 1997", "July 8, 1997", "November 13, 1997"]}, "target": "['Broadway', 'October 15, 1997', 'July 8, 1997', 'November 13, 1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the lion king start on broadway?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8d080995b6237375258b27bb6fd226d01f10d4fea80cf79ecb850734bbc0922", "prompt_hash": "326b5152fc944f74c41c7720608dae071591fc9aa8e06967b23353db9845d761", "target_hash": "36b6507b174d138e676fc92a37d88008907e5977037959f3e2287c6e6f780e3c", "exact_match": 0.0}
{"doc_id": 2010, "doc": {"question": "who is john snow on game of thrones", "answer": ["Kit Harington"]}, "target": "['Kit Harington']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is john snow on game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jon Snow"]], "filtered_resps": ["Jon Snow"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f0a34a66f55d2ae35a713ace58ae04dcf8f2e138b558ac0ee34e5762e08ff72f", "prompt_hash": "0f637eb230342a12ce1ae39683c1907f326ec6a579847af0c76d2d6b3f9f43d7", "target_hash": "b9faa9ba997d1c63675e1e4384c671e6e2bede0ea4441fe5778507644c098d54", "exact_match": 0.0}
{"doc_id": 2011, "doc": {"question": "list of strict nature reserve in the philippines", "answer": ["Palawan", "Lake Malimanga", "Olango Island", "Calauit Safari Park", "Lake Buluan", "Calavite and FB Harrison"]}, "target": "['Palawan', 'Lake Malimanga', 'Olango Island', 'Calauit Safari Park', 'Lake Buluan', 'Calavite and FB Harrison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: list of strict nature reserve in the philippines?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mt"]], "filtered_resps": ["Mt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "09d7a15fb03c71cf0a7633a49c6285b9d79c9ef1ded70d9b0892c066e7eaf978", "prompt_hash": "c2c1478875a6c01a268596103bc660f9ac71385431db67b6a0ddeed74cdbe5d0", "target_hash": "11796bc4c0ca8ae81c3afd90d4ef815f424fcf6d1b42ec071506678fb2ffb608", "exact_match": 0.0}
{"doc_id": 2012, "doc": {"question": "when was the last easter that fell on april 1st", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last easter that fell on april 1st?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2008"]], "filtered_resps": ["2008"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6963ca56c44714d4de0de50074bf4fc967b392b7be4517b03682f3fbc7bdedb1", "prompt_hash": "dc5723f4da20480d9ace7650c5a26558cbf889b91c96a82f7c078aeb05d75051", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 2013, "doc": {"question": "what was the city of beijing previously known as", "answer": ["Peking"]}, "target": "['Peking']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the city of beijing previously known as?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Beijing A: Tianjin"]], "filtered_resps": ["Beijing A: Tianjin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae0cbf4f482d88f68854416f31637854ebfaf0caf724540858c0a4f64201f957", "prompt_hash": "69b616216ebae0c2023e27a9991a52c64bb6240e53acaf8ed3c88ead98a3ad23", "target_hash": "f8e566fa01d33e56d21b5e9731f4bf98d8c8d67dff04ffc7af57d61242a2f1ee", "exact_match": 0.0}
{"doc_id": 2014, "doc": {"question": "who is the book of acts written to", "answer": ["Jewish audiences", "to Jewish audiences"]}, "target": "['Jewish audiences', 'to Jewish audiences']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the book of acts written to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Theophilus"]], "filtered_resps": ["Theophilus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ef3bb2a1cd115f0a46e0bc4e1167b04bf6f4fefde5e314ed9e7eb3f3bfc6dd25", "prompt_hash": "9de96f2f77a77d408708f8be41f07ef610438571529ac05c15d71424f60f679a", "target_hash": "ce62b640f860e11bb480a13135f07ca61ce157ec47361281ad141d513a109f99", "exact_match": 0.0}
{"doc_id": 2015, "doc": {"question": "the secret of crickley hall how many episodes", "answer": ["3"]}, "target": "['3']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the secret of crickley hall how many episodes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "602388348d56773458cb4f421d1364aff45f5619ba67ff0b63aa09f70ceb40d6", "prompt_hash": "c25c0c3c1e0799c4aee65d04a90f3b3f2b4eba3457538a6d27be9ecd522d5d6d", "target_hash": "e8230ace9f0554c09f639f7a596544b0277f9345d9c2a486d6607bf01c25927e", "exact_match": 0.0}
{"doc_id": 2016, "doc": {"question": "who does the voice of the gorilla in the movie sing", "answer": ["Taron Egerton"]}, "target": "['Taron Egerton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of the gorilla in the movie sing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew McConaughey"]], "filtered_resps": ["Matthew McConaughey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2063c4194141a3956b452904a30324c08174a1f2fbcbdf6d6e63168041ffff55", "prompt_hash": "7652df67af7232b12450473ac1316f0eef4c500d08717859608fe6f74875668c", "target_hash": "77449c359f9366caf61b67ab09c6925942c6fdf464f3d3b2d2a110503540533e", "exact_match": 0.0}
{"doc_id": 2017, "doc": {"question": "when did billy don't be a hero come out", "answer": ["1974"]}, "target": "['1974']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did billy don't be a hero come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1990"]], "filtered_resps": ["1990"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c347d45e412b09f5ecbdaccd5c191a3d62b04ff4b24082f1314d0b79744881c2", "prompt_hash": "0b0d12db0914bb00e429e5373483112d4d9ab5e1e4d8ccc6e757c177f9d2895b", "target_hash": "2c5699a28bb5131aef9202517992092347e27d0d08ffaaf484ad52573e8443bb", "exact_match": 0.0}
{"doc_id": 2018, "doc": {"question": "when does the regular season start in mlb", "answer": ["March 29, 2018"]}, "target": "['March 29, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the regular season start in mlb?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" March"]], "filtered_resps": ["March"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44c773da93f7e58d8497a56395f63318428e5e11fa837896e31995e6f2f9cedb", "prompt_hash": "eeac3f12a1f2025bfe1b5e315ea43fc30d0afca3f12f5fb54b5d01ae1dbce177", "target_hash": "9f7c7fa67ddefe89a4a5e9fe95a19968cfe0fb969aa6cd0ec8ecef8ac6f39a0b", "exact_match": 0.0}
{"doc_id": 2019, "doc": {"question": "when did the song push it come out", "answer": ["March 8, 1987", "1987"]}, "target": "['March 8, 1987', '1987']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the song push it come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1986"]], "filtered_resps": ["1986"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "adcb34f5121c4723002381c7170d40b919e58060602d6919b0f7a4ffe1e076a7", "prompt_hash": "acb378ff5a87a7be0f5b253cd204a53f4867484e9deabcd38891c8911117bb05", "target_hash": "4dc21da687c14d5f4813365195c3302d890ab266adfee140a11f2b9543ed4b39", "exact_match": 0.0}
{"doc_id": 2020, "doc": {"question": "where does the name loyola university come from", "answer": ["St. Ignatius of Loyola"]}, "target": "['St. Ignatius of Loyola']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the name loyola university come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Loyola University"]], "filtered_resps": ["Loyola University"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "74490ca0c784b7435fe8f1ad4aa6c57afb07ad9e713e9d803ae7d97624e9d959", "prompt_hash": "7e346ef2c2f2bc28f80e2e9dad5dad883368640d81708151cc5156c121d9dab5", "target_hash": "859d11c96637323b20bef485252f617cbca1bbd43a8fe98618cdbc3f20ee0481", "exact_match": 0.0}
{"doc_id": 2021, "doc": {"question": "where did the last name edwards come from", "answer": ["England", "Scotland", "Wales"]}, "target": "['England', 'Scotland', 'Wales']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the last name edwards come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" English"]], "filtered_resps": ["English"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f94590bc6f43b47f48e26a1424abae434f3d394741abd31b47ab8b050db8a69e", "prompt_hash": "6ca0a65a08d855f24c931151d293815a41451227fc33559193d1dc526380c9bc", "target_hash": "ef689e2f763884c1003ffbbf5f07c82fdb8802d78833cc41fa9fa842e9644fed", "exact_match": 0.0}
{"doc_id": 2022, "doc": {"question": "where did the term spirits for alcohol come from", "answer": ["Middle Eastern alchemy"]}, "target": "['Middle Eastern alchemy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the term spirits for alcohol come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ghosts"]], "filtered_resps": ["ghosts"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "680f37d59e0f54111edb51e2a8b7e79760ee269aa7919a129bd4f5b2b298502a", "prompt_hash": "896881e1979c27b1303b421d6e1867b9131ec4869551660dbe280f89d4c38dce", "target_hash": "ec8ac052f674a951504ed5a3991c389500aa08172e282b1d12315b38395739d9", "exact_match": 0.0}
{"doc_id": 2023, "doc": {"question": "when does south by southwest start in austin texas", "answer": ["mid-March"]}, "target": "['mid-March']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does south by southwest start in austin texas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" March 11"]], "filtered_resps": ["March 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "01801ce9b22bdbe097c1013f82c4bc71c30c3e5eac3145f231493ec571478631", "prompt_hash": "a4d8d404488ccce4c04166085b979859cd33ede7c06aeaee080d3af98fc0dbc1", "target_hash": "35bd8b66b5e8f7491b279e2857b7d543f7bb75f869c2ec0354333f7c5a506470", "exact_match": 0.0}
{"doc_id": 2024, "doc": {"question": "the joint between a coxal bone of the pelvis and the sacrum", "answer": ["sacroiliac joint"]}, "target": "['sacroiliac joint']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the joint between a coxal bone of the pelvis and the sacrum?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sacroiliac joint"]], "filtered_resps": ["sacroiliac joint"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "68f1d6995ef4f141d17db887c08a60e05d58e5d97b2f4e2e9f0f260894290b5e", "prompt_hash": "0c872f91fc9d19714bfc34edd55a3a97adefe7175844cc0dd7213c2150faf2df", "target_hash": "c3bebeb349910bcfb8495267e6badd30287453add46fcc1df12437d25dc8c9c9", "exact_match": 1.0}
{"doc_id": 2025, "doc": {"question": "who wrote the phantom of the opera music", "answer": ["Andrew Lloyd Webber"]}, "target": "['Andrew Lloyd Webber']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the phantom of the opera music?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Andrew Lloyd Webber"]], "filtered_resps": ["Andrew Lloyd Webber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eebba5da469f4947cbe4b5971088bc1e8420a7a0372edfd1fbda618493cc910c", "prompt_hash": "3edb468e9535702059089577773cfa7e212bc9f640c5a07b77de08d992ce95c7", "target_hash": "219ffc6e3d042e3362d23b463feba1d180877f8e51518a9582b4d306921734f3", "exact_match": 1.0}
{"doc_id": 2026, "doc": {"question": "when did starbucks become a publicly traded company", "answer": ["June 1992", "1992"]}, "target": "['June 1992', '1992']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did starbucks become a publicly traded company?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1992"]], "filtered_resps": ["1992"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "147ccb3f1d29e62e7fd153f154e18b66fe82bdea6d35185cd78683a65e57cc6e", "prompt_hash": "7f333e78e8b0134eda78cda45627dbac9b06e054e030e796aff210f96f093b58", "target_hash": "182a281c8bace6ae2a40d0d40377998a173889ad7130ee0eaeb20659d2afcbb8", "exact_match": 1.0}
{"doc_id": 2027, "doc": {"question": "when was nepal declared a secular state in bs", "answer": ["January 15, 2007"]}, "target": "['January 15, 2007']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was nepal declared a secular state in bs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2006"]], "filtered_resps": ["2006"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "280510bd4794c62dbe55fee6980ed72a59177c7b29bc1ef0fa5e050e2a760df9", "prompt_hash": "27470f71f744a6fff1012d94ee6aa86b9abd602f8ba504460e39af5c8b5b66ea", "target_hash": "b6a5a9e4c59150d34d99c6c8089ce8207c2a31378dca365023d8b4d16ac5d4d9", "exact_match": 0.0}
{"doc_id": 2028, "doc": {"question": "what type of novel is goodbye mr chips", "answer": ["Psychological fiction", "novella"]}, "target": "['Psychological fiction', 'novella']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of novel is goodbye mr chips?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" comedy"]], "filtered_resps": ["comedy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "76bb9a0a35c4b7a53ceeb5d2ac69758365aeee6e10e2c901496990777acf77a2", "prompt_hash": "25b7ef3439e7fda5d1432327cbceb4b6ec601dada42870f43649bd52892ed739", "target_hash": "e13afe26e93d19d5d34f94aaad30b29cfb9fe683cc6564d77f6cbdebe9f80a5a", "exact_match": 0.0}
{"doc_id": 2029, "doc": {"question": "who is next in line to inherit the british throne", "answer": ["Charles, Prince of Wales"]}, "target": "['Charles, Prince of Wales']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is next in line to inherit the british throne?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Prince Charles"]], "filtered_resps": ["Prince Charles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0fa6b89ec099ccd68a087079ec9d319cbf80d1f12a6eb8fe67619425b9dc60f1", "prompt_hash": "9465a752bc624c83b5cfb10fd045b07fe22e89a66494153851975e9ed1053976", "target_hash": "0c8a6e555eef107c4940c8f23cc8341d0e6a50425add1ba0d8ef4a81f66360df", "exact_match": 0.0}
{"doc_id": 2030, "doc": {"question": "who has climbed mount everest the number of times", "answer": ["Phurba Tashi Sherpa", "Apa Sherpa", "Kami Rita Sherpa"]}, "target": "['Phurba Tashi Sherpa', 'Apa Sherpa', 'Kami Rita Sherpa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has climbed mount everest the number of times?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Edmund Hillary"]], "filtered_resps": ["Edmund Hillary"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e682c53f95825c6b703897da37de5aa63028e2b4f0f9de0c90bbc9ca0a67e7ef", "prompt_hash": "b6e5c87663f1c7bdce65bae4a0c6baa9056017a89d75f0411307c6e8fccc1a47", "target_hash": "c46e052b9b9cf36ddbcd05038657362db05ccba3dd27debd60e8b3eada5a216a", "exact_match": 0.0}
{"doc_id": 2031, "doc": {"question": "when did toyota first come to the united states", "answer": ["1957"]}, "target": "['1957']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did toyota first come to the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1957"]], "filtered_resps": ["1957"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c0825a19a64c2c5fa10c733c14e803c9c8d11036fba9e6921c3a225ab8b437fa", "prompt_hash": "1bc08251ff619251e5da019f93413d0801c05d5d7be4941ce408e0219bc13099", "target_hash": "427d9d0f103b0ad92ea8f31df3217736251147ba87b9f598f2a3629e36798def", "exact_match": 1.0}
{"doc_id": 2032, "doc": {"question": "who plays the main character in hacksaw ridge", "answer": ["Andrew Garfield"]}, "target": "['Andrew Garfield']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the main character in hacksaw ridge?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Andrew Garfield"]], "filtered_resps": ["Andrew Garfield"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4f2a6cfc1275601d06f12d6f7fe33884f5c90ae3bf2acbacec98e6d08b84c2c", "prompt_hash": "8a909c48925eb7cfece6004ebe1587b7c2cdefaa713538c2c3a294b67ba7724e", "target_hash": "d0b34562ffd1e6b6d762e1a886b41797d74a159a2fab4520dee7f1cbf00b7ad4", "exact_match": 1.0}
{"doc_id": 2033, "doc": {"question": "when did gimme gimme gimme start", "answer": ["8 January 1999", "1999"]}, "target": "['8 January 1999', '1999']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did gimme gimme gimme start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1981"]], "filtered_resps": ["1981"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "549e9cdb843f377a79ba20c598917dd7d450796d4f2c1ad6ca6d0dddfa7d8898", "prompt_hash": "35f44fca763047a61ab202ba01b404eadc151e39a7b468aebec73785fe6d3fcd", "target_hash": "357ac7145f93d67d61a876e10d4aca8437e8a873e28e1f718fdc8d1c014d5b37", "exact_match": 0.0}
{"doc_id": 2034, "doc": {"question": "where did you go to drink during prohibition", "answer": ["Speakeasies"]}, "target": "['Speakeasies']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did you go to drink during prohibition?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" speakeasy"]], "filtered_resps": ["speakeasy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ec17a90ee04318bdfa8d0503b9bc86063cda3d859a08f69b904d1f930db35564", "prompt_hash": "b6604ea1a423569157f34ddf48da615fc2393a104a39fd71603bb6e26814453e", "target_hash": "8800f0857df19c9992fdd4bfff64abe4dd43bdeb13d9fb158b237e618b048ebc", "exact_match": 0.0}
{"doc_id": 2035, "doc": {"question": "what was the actual year that the movie regarding the titans took place", "answer": ["1971"]}, "target": "['1971']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the actual year that the movie regarding the titans took place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1983"]], "filtered_resps": ["1983"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fcd223e45b9f8b92c45384f0fce3b2467c09b0a188f611ff13673b95bf1deda1", "prompt_hash": "2a8ff0252b6063a87f9ef198d714acdb8fbe5ac84229797286b3af43f089473c", "target_hash": "413637fc97b9d981395d1412e06e89c038805379ccf2e6a207d242524599ca5d", "exact_match": 0.0}
{"doc_id": 2036, "doc": {"question": "who plays frankie’s sister teddy on grace and frankie", "answer": ["Talia Shire"]}, "target": "['Talia Shire']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays frankie’s sister teddy on grace and frankie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Coolidge"]], "filtered_resps": ["Jennifer Coolidge"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff1bdf12a9baa098fc2980b439eb2038108febeb1a1e7762fd0eb8af381bff2b", "prompt_hash": "6e8bea20cbbf80d974fd9deacaa80c6288a593130113668b566ae22b5325dadd", "target_hash": "6855fe1d64dd7f712ad9fffb08e8f65f7c11975d640c84b4321bdddf16460dda", "exact_match": 0.0}
{"doc_id": 2037, "doc": {"question": "who did dwight howard play for last year", "answer": ["Atlanta Hawks", "Charlotte Hornets"]}, "target": "['Atlanta Hawks', 'Charlotte Hornets']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did dwight howard play for last year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Houston Rockets"]], "filtered_resps": ["Houston Rockets"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "86757bed2b09b408c3689047ba2c38a17f36bfd9e2a321949b94632ab2c01ea8", "prompt_hash": "9f65e7e4d2cb45d2ca7225744bdc4c3e9253a0c5faae5f14e845a8a17016e708", "target_hash": "10cde4633fdc552f01c859c930a4d24d250b99c5c881bd85eadd5773919b365a", "exact_match": 0.0}
{"doc_id": 2038, "doc": {"question": "who made the first spanish landing on the mainland of north america", "answer": ["1535", "Christopher Columbus"]}, "target": "['1535', 'Christopher Columbus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made the first spanish landing on the mainland of north america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Juan Ponce de León"]], "filtered_resps": ["Juan Ponce de León"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d53698d90b125cead9a1cd8fee935e2ce9abe2639ed9ab520ea49e363a5a68f", "prompt_hash": "fbdfdb80d661c1a6cc0c7a7528d250df354793cd78bfbb8639a51d311d4767c4", "target_hash": "7b491c6288ff31f2d03602fc6c4005bbea24d17012ab0fbbca4ef5026fff39df", "exact_match": 0.0}
{"doc_id": 2039, "doc": {"question": "who lives in the blue house in balamory", "answer": ["Edie McCredie"]}, "target": "['Edie McCredie']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who lives in the blue house in balamory?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the blue house is inhabited by the blue family"]], "filtered_resps": ["the blue house is inhabited by the blue family"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "33a32d10aa4a54382bfc1220bb15824c72b68f50aa80a66003a946201c9221d6", "prompt_hash": "c16a2066ad6a1e8e13cf7cff36275d03a2877ddca257f2c3b44ad1fafb8aa6ee", "target_hash": "d79fd5f8fae2b830331600e71d98f7a185d93ab97aff96160cc959b6c13e9603", "exact_match": 0.0}
{"doc_id": 2040, "doc": {"question": "when was the first season of when calls the heart", "answer": ["January 11, 2014", "2014"]}, "target": "['January 11, 2014', '2014']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first season of when calls the heart?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aaba2620c8d6d93e851d97e597eeef09c246928c9c2bec230244e59f0760add9", "prompt_hash": "533ae365598ff759f54b1f5e25bf1ed0f1a3857af23868aa38bfc35566ea91b1", "target_hash": "cbd9b941142b38bc63c2e4b4898d130b743743ea78ab27ecaf918613ae6aa6af", "exact_match": 0.0}
{"doc_id": 2041, "doc": {"question": "who played lead guitar on 25 or 6 to 4", "answer": ["Terry Kath"]}, "target": "['Terry Kath']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played lead guitar on 25 or 6 to 4?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Bowie"]], "filtered_resps": ["David Bowie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8c33d51344e30ec742f68c2bbe30dc9ab4fe2b3f137f8dfbd6c18de5ac0ee079", "prompt_hash": "6e97246349345d09c4d736821569486915a25b602858caf6aab42c41142d782f", "target_hash": "1618212bba5ada1426bbbd87b9d00e411419f40cb58d4e9b2653c55281ba9404", "exact_match": 0.0}
{"doc_id": 2042, "doc": {"question": "who is credited with the discovery of the neutron", "answer": ["James Chadwick"]}, "target": "['James Chadwick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is credited with the discovery of the neutron?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Chadwick"]], "filtered_resps": ["James Chadwick"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "33783b3129eaefbe3b386fb4cfee4d1a702304e7420711bf6df9ce74c54cdb86", "prompt_hash": "3224be054787193ce97f0576a9a522dd2ea65315d68579f93ddafdf777b0e905", "target_hash": "5881ed25d5eb4eac265fe40034860ef39d337d4427994de38b0bb2a1cd791a29", "exact_match": 1.0}
{"doc_id": 2043, "doc": {"question": "when does zay come to girl meets world", "answer": ["May 13, 2015"]}, "target": "['May\\xa013,\\xa02015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does zay come to girl meets world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 2"]], "filtered_resps": ["season 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e3a363f489a6d1c0889eca81d15fbf7f5b8f8b6350d3e89d3f21617d9c4cc19", "prompt_hash": "a86c62ebd1b8c691418d72ddba5eb5d8e2d81ab8f48bdc3fc7a3cc959744a6d7", "target_hash": "a709c471fbd9cbd9a84218ec813fc3dca88f4fa6a46343b2a4b35d5a1deb1654", "exact_match": 0.0}
{"doc_id": 2044, "doc": {"question": "who played the original steve mcgarrett on hawaii five-o", "answer": ["John Joseph Patrick Ryan", "Jack Lord"]}, "target": "['John Joseph Patrick Ryan', 'Jack Lord']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the original steve mcgarrett on hawaii five-o?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alex O'Loughlin"]], "filtered_resps": ["Alex O'Loughlin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "293319d590e4eaa730c577401206b7bad49138fa6a0746da86c3df501ac0abd1", "prompt_hash": "ba1bc9014e97350fd98850e037a6e35e0502408d9d65b1fd8b3f4fff62f89c6d", "target_hash": "eef11a1a3b24d302625ac717d24d8f2bad42f7366252d808e612af6b50b83e59", "exact_match": 0.0}
{"doc_id": 2045, "doc": {"question": "what mlb teams did deion sanders play for", "answer": ["San Francisco Giants", "New York Yankees", "Atlanta Braves", "Cincinnati Reds"]}, "target": "['San Francisco Giants', 'New York Yankees', 'Atlanta Braves', 'Cincinnati Reds']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what mlb teams did deion sanders play for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Atlanta Braves"]], "filtered_resps": ["Atlanta Braves"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0b8afecac73ab76cbb85c915071c763ab18661e15b42c996018f62709d717264", "prompt_hash": "73a570510adce721445a52f4439229601814ffb9ef255cf3d1f299c2ecfea68d", "target_hash": "f3efa8b53920de1c96acf894f6f4b2e2446bc096fae844e10b24ffb0b626af8f", "exact_match": 1.0}
{"doc_id": 2046, "doc": {"question": "when did the phantom of the opera open", "answer": ["October 1986:", "1986", "9 October 1986"]}, "target": "['October 1986:', '1986', '9 October 1986']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the phantom of the opera open?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1988"]], "filtered_resps": ["1988"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c9157f0af054c29a6be9362250bdb86d32957c5b5ad74367642eaedae720b95", "prompt_hash": "62c1f487cae65a9baf72971bcf7dd0f0e2241faa0bdc5a64d9be54650338c98d", "target_hash": "20073ed5e130c82ee8ab1e101ad0a48ab48175f8b62721690252ab5040eefece", "exact_match": 0.0}
{"doc_id": 2047, "doc": {"question": "branch of science that deals with the structure of human body parts", "answer": ["Anatomy", "Human anatomy", "human anatomy"]}, "target": "['Anatomy', 'Human anatomy', 'human anatomy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: branch of science that deals with the structure of human body parts?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" anatomy"]], "filtered_resps": ["anatomy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "894feefafc138a985ab456ec4138b16d8a70d1963a8119754c56a7effb610343", "prompt_hash": "3120c3ed88da3fd564c4270dd0b2400d0c60592e5126c25868c57669c394d12a", "target_hash": "6ac402b8acc8751611d647c81b31f22675c3c759f951df58abc9a9f01b1768c9", "exact_match": 1.0}
{"doc_id": 2048, "doc": {"question": "what's the medal count for canada in the olympics", "answer": ["501", "302"]}, "target": "['501', '302']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the medal count for canada in the olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 20"]], "filtered_resps": ["20"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "de4437502a88f2673d7a7fe90d703687b4a53844a45c9ff7fd65b504498af18d", "prompt_hash": "ac0fde775692de0e0a6ca90236d143ae80c64fe59d80749140e9b1ee810c18f1", "target_hash": "e903dc05dde15eb079185d55c01bb3314d48f910691a9fad6812bf876131af89", "exact_match": 0.0}
{"doc_id": 2049, "doc": {"question": "what is cain and abel software used for", "answer": ["password recovery"]}, "target": "['password recovery']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is cain and abel software used for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" genealogy"]], "filtered_resps": ["genealogy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "808199064a53ea330e6f0d249de405fd4a555ad63f35dd4d456fdf125b1dd21b", "prompt_hash": "de1a371e7b8d91310fa46ed5b98bcfb915da7260b666cafb34dcf9407db7c628", "target_hash": "0664ec61eea0083b0f6e09f495bfa106538a9511d45e1c20f28e01845821b3e0", "exact_match": 0.0}
{"doc_id": 2050, "doc": {"question": "who has given the theory of unbalanced economic growth", "answer": ["Hirschman"]}, "target": "['Hirschman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has given the theory of unbalanced economic growth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gunnar Myrdal"]], "filtered_resps": ["Gunnar Myrdal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "306dd49e19cefe5a3bd924f2177d04e7f635e43311fa09d13287905c52690a70", "prompt_hash": "e93ccf53d2b1889aa3421f8ad108ff01ef10d806b440fa7bb8125f6c42e019f0", "target_hash": "efc6099efddef7c1911746d2c7cdb6bee5f1bf1cd492764bb117554e9b3ff5c6", "exact_match": 0.0}
{"doc_id": 2051, "doc": {"question": "when were the first general elections held in independent india", "answer": ["1951–52"]}, "target": "['1951–52']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when were the first general elections held in independent india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1951"]], "filtered_resps": ["1951"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c9baff7861e51bd8d492b42d2f94087d1afba6350b37f252039084cc36f43109", "prompt_hash": "b81b4ed9ab2cb0204ef150691ab24a6c14a050476b79a7b9056eb4895c0b7bc1", "target_hash": "6fee232f1c1397131a1333bb0d28fa3b3e648d980eb923fdae371680dc3c38cf", "exact_match": 0.0}
{"doc_id": 2052, "doc": {"question": "who plays steve's wife on my three sons", "answer": ["Beverly Garland"]}, "target": "['Beverly Garland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays steve's wife on my three sons?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Love Hewitt"]], "filtered_resps": ["Jennifer Love Hewitt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b2c8e83e6c387b72d2399f86e34f865e29d1aa264d9d5416d44ff8e1f928091", "prompt_hash": "816fdbc2d336ac3b1c059c6eb419d34a1d6f43824d2de3439a92d76601d21e99", "target_hash": "49465e656ae8fa06b2192a7b002dcd40c2fe4000a4f2e3c85967fb8376c9d890", "exact_match": 0.0}
{"doc_id": 2053, "doc": {"question": "who conducted the opening concert at carnegie hall", "answer": ["Pyotr Ilyich Tchaikovsky", "maestro Walter Damrosch", "Walter Damrosch"]}, "target": "['Pyotr Ilyich Tchaikovsky', 'maestro Walter Damrosch', 'Walter Damrosch']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who conducted the opening concert at carnegie hall?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walter Damrosch"]], "filtered_resps": ["Walter Damrosch"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49bc8d59643bd2bc0f5d2f002d7da49b9cb473d13808ecf6199b5fcd863509ca", "prompt_hash": "5c09f384884fa54a4e907afd0b982ecd0adc3518edee74147796b65a08e128b0", "target_hash": "8560b96067043408ae369ed96dc86238779adce58914c3b9685a9b35f49124ad", "exact_match": 1.0}
{"doc_id": 2054, "doc": {"question": "who is the most played artist on spotify", "answer": ["Justin Bieber", "Ed Sheeran"]}, "target": "['Justin Bieber', 'Ed Sheeran']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the most played artist on spotify?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Drake"]], "filtered_resps": ["Drake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c028daa6bf422a466ba3825e8c9d25f2322cfbe523442eea9a4224219f92ce8a", "prompt_hash": "4dab9804445867c547f169bf9bcd288be620e7d87bf3cd58e3a056e919abc91a", "target_hash": "c009328998d69ea67980e3e308c2ae8436db30dc9a01ca9bbb69b5dab7f747f7", "exact_match": 0.0}
{"doc_id": 2055, "doc": {"question": "vine with heart shaped leaves and white flowers", "answer": ["Antigonon leptopus"]}, "target": "['Antigonon leptopus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: vine with heart shaped leaves and white flowers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Clematis (Ancient Greek κλήματις"]], "filtered_resps": ["Clematis (Ancient Greek κλήματις"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b2152da526173d5ac9f5cda36825159ab010cfa2d1907bd4243104dc8c76a7d", "prompt_hash": "f2b6e3829fd4cf026672faf59f2e9ce1d7069c9376495e65fd2917a3c99fb660", "target_hash": "085d94ae92919c33a333f9b3d3455fca4ab81f72645db8f80dca5b344656f8fd", "exact_match": 0.0}
{"doc_id": 2056, "doc": {"question": "who played the first phantom of the opera", "answer": ["Michael Crawford"]}, "target": "['Michael Crawford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the first phantom of the opera?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Claude-Michel Schönberg"]], "filtered_resps": ["Claude-Michel Schönberg"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2555cd382c7cebe4e3bbb1e4f5bd09d61a73538e596c5055a214fbd8eb7ef78", "prompt_hash": "080d80e9dbdcc249f578fd180d6e4d593dd699882f6d5b035ed883327d18bd05", "target_hash": "40738f496aa2a1a4ae4d46876348000a17696efec6cec6992a7918d496212c7b", "exact_match": 0.0}
{"doc_id": 2057, "doc": {"question": "when is the new jurassic world game coming out", "answer": ["June 12, 2018"]}, "target": "['June 12, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the new jurassic world game coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2018"]], "filtered_resps": ["2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aad12503d2fa17a7bfe0e466e363e635358aedd675e7d529b93437561c59a8ce", "prompt_hash": "215320a675aa37bac81efca9ae71c1b7faaa8a7744e084d974bd72d594ebf8a5", "target_hash": "cc289fbd58c4933ace2a854781b36955f3b40eb151e6c07a3d9bd4c679762a9f", "exact_match": 0.0}
{"doc_id": 2058, "doc": {"question": "what part of brain is responsible for complex thinking", "answer": ["the frontal lobe"]}, "target": "['the frontal lobe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what part of brain is responsible for complex thinking?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" frontal lobe"]], "filtered_resps": ["frontal lobe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ec39cc7d10f3485f327e231c511f6bcdc6912e6045244e2ca9c49453f7b1ace5", "prompt_hash": "29a6da3dd0dde7e82150c78ab44d6b1be9d5c73d6457be339a146eba2e6470ec", "target_hash": "464ab7043783db0f147eb809a22da7aa7ca6a56ca48b7fdcd9692dfce868ab51", "exact_match": 1.0}
{"doc_id": 2059, "doc": {"question": "who is the owner of the mandalay bay in vegas", "answer": ["MGM Resorts International"]}, "target": "['MGM Resorts International']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the owner of the mandalay bay in vegas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" MGM Resorts International"]], "filtered_resps": ["MGM Resorts International"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a21d029defd11008708c9fcc7ee8fabc8d128ae31a99f1f5c6502db62b855b00", "prompt_hash": "eaf30fecf7f77a073659c6c906c5d549ac74d2ed5a88a9cbd13bf8521c2931f3", "target_hash": "c88630fa60e326917cb42577bfec1f076a474cab17aeed12f74f2bee3a1f4360", "exact_match": 1.0}
{"doc_id": 2060, "doc": {"question": "who owns spirit of the suwannee music park", "answer": ["The Cornett family"]}, "target": "['The Cornett family']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns spirit of the suwannee music park?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spirit of the Suwannee Music Park"]], "filtered_resps": ["Spirit of the Suwannee Music Park"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aebaa25375bf9caa09f0de6601d7b44248de3b4c27916402ee7f0880d0ee35ee", "prompt_hash": "1b109af1fbef22934ad03de16293f7b5c03711d1b69e096fd8576b33242daacf", "target_hash": "6e43d32b52ab758dfbc6c31147dde9708576c3f5f25c7ffb92acfc7bb4465abb", "exact_match": 0.0}
{"doc_id": 2061, "doc": {"question": "where does the last name baca come from", "answer": ["New Mexico"]}, "target": "['New Mexico']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name baca come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mexican"]], "filtered_resps": ["Mexican"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "70cdf7086f7b70f322681b9ae3c3cbf7c9dd2df9038039d413b973cff9bd8a7e", "prompt_hash": "47d16234160bd359333a4cb914badee060526f84cb567f29a2e7679de0cf392a", "target_hash": "21cd1b1f878b8500f78e9401c671d75c02ebea90b86b2bb402a262967238efa9", "exact_match": 0.0}
{"doc_id": 2062, "doc": {"question": "who voiced simba in the lion king 2", "answer": ["Matthew Broderick"]}, "target": "['Matthew Broderick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who voiced simba in the lion king 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jonathan Taylor Thomas"]], "filtered_resps": ["Jonathan Taylor Thomas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfa4fbcc0c5df0bef4605794d15e6b28aa18b3551f3f151c9f9ce82533f165ec", "prompt_hash": "2fbca1b88bf7b8b38602dfef0e22f195fa970f069daba10fa6e5a02c7ac447ec", "target_hash": "86f91ae077111596a7cf7ff624cf4853eff839042470d78e382c29a65a6eb294", "exact_match": 0.0}
{"doc_id": 2063, "doc": {"question": "who founded amazon where is the headquarters of amazon", "answer": ["Seattle, Washington", "based in Seattle, Washington", "Jeff Bezos", "founded by Jeff Bezos", "Seattle, Washington, U.S."]}, "target": "['Seattle, Washington', 'based in Seattle, Washington', 'Jeff Bezos', 'founded by Jeff Bezos', 'Seattle, Washington, U.S.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who founded amazon where is the headquarters of amazon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Seattle"]], "filtered_resps": ["Seattle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "782603a4e16c4d3fc9382d26fff429780a7d6120091a8d9a2761773f8bc60d71", "prompt_hash": "2513d8322135d802c99db82cf92629b068ec9caf4d83ebef252c52b2ca278a9c", "target_hash": "c63c1fc04db03917825d38e6a728351699cadf708705269092423e9b8e9662d1", "exact_match": 0.0}
{"doc_id": 2064, "doc": {"question": "what is the population of the tri-cities washington", "answer": ["283,846", "over 300,000"]}, "target": "['283,846', 'over 300,000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the population of the tri-cities washington?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 111"]], "filtered_resps": ["111"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bc9c7617197b9437528fd4d88f1c3f737259662a345fcf3f0561ffc784b41638", "prompt_hash": "1f8c0c8603a4ae4b59a134cbb27e2c6a72a8bd3b8901198677e85708adfc6732", "target_hash": "01d70d39a8d45a3cc0ba288e073590f16bd2a653818226b39a20c20eccce9440", "exact_match": 0.0}
{"doc_id": 2065, "doc": {"question": "when does rick and morty play on tv", "answer": ["late-night"]}, "target": "['late-night']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does rick and morty play on tv?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sundays"]], "filtered_resps": ["Sundays"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06bcda3431c063473aebed391ebf1b7068f48314412f296d15fee6fe95795f33", "prompt_hash": "c88e9a6d3c3f64794db715780e3ef694dd30d7467b30fa79293e519ebf161945", "target_hash": "fd607b7a3f019885bb7ed24dca08b4df5cfbc3768c9ae5205719d85abd04eb1e", "exact_match": 0.0}
{"doc_id": 2066, "doc": {"question": "i like the way you move video cast", "answer": ["Big Boi", "Sleepy Brown", "Fonzworth Bentley", "actress Ki Toy Johnson", "Ki Toy Johnson"]}, "target": "['Big Boi', 'Sleepy Brown', 'Fonzworth Bentley', 'actress Ki Toy Johnson', 'Ki Toy Johnson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: i like the way you move video cast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Lopez"]], "filtered_resps": ["Jennifer Lopez"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d85c91b88fb8eebcd8112c49afd2fc3776b0d2dbfb2e7f6a5f061692b2116c7", "prompt_hash": "fb6b56394759b1b553bb85351455490283f31e8be13bc229ddf061208ad5dd63", "target_hash": "221168a0df44aad9c9d8fdd405cb66e204d80a07996275bb1e4aa2509b2fb7dd", "exact_match": 0.0}
{"doc_id": 2067, "doc": {"question": "how many nfl teams has st louis had", "answer": ["four"]}, "target": "['four']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many nfl teams has st louis had?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1448a9af13c8ef855a4aa1391fa625b1fa758f1ab6faa8d873fdcbf4a2c559dc", "prompt_hash": "052e91086e5650d270dee2b621c92f562427cd3402b3ba51e3b7c0af5fed5f23", "target_hash": "c0b12eaa48bad2eab0833b330f1f9a7a1caf1ef07c30f4fc2a1bd97e38e98618", "exact_match": 0.0}
{"doc_id": 2068, "doc": {"question": "when did season 4 of glee come out", "answer": ["September 13, 2012"]}, "target": "['September 13, 2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did season 4 of glee come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 10"]], "filtered_resps": ["January 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d52495514256e016cdf491a61ef18347dcf051417e7ae3d85e8bbc9d7aa820b", "prompt_hash": "5ea916a9c3d1c49abbe69a33503db7a973e9bbb9311af840877ed0b6701fc232", "target_hash": "97a8a036aef63a1477bd6f6e4d3610b4dece94c3c1fb3493c6df062b096e82bf", "exact_match": 0.0}
{"doc_id": 2069, "doc": {"question": "who sings i want to dance with you", "answer": ["George Strait"]}, "target": "['George Strait']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i want to dance with you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pointer Sisters"]], "filtered_resps": ["The Pointer Sisters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f55af511627e1999aa9a4d88e5e12a86a6771250db1c61132380b6f53de4c0b9", "prompt_hash": "ba346e0dda3a5f797cf6dded64d0b008a27f9d901c6d271c5dd234e0e794561f", "target_hash": "910d4fd970445c86fa3296f88f4728cc5484119a1ee2ff778182fd2659c7078f", "exact_match": 0.0}
{"doc_id": 2070, "doc": {"question": "who sang my bucket got a hole in it", "answer": ["Hank Williams"]}, "target": "['Hank Williams']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang my bucket got a hole in it?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dwight Yoakam"]], "filtered_resps": ["Dwight Yoakam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "af74d8722705d5b29e40bc89a16e5c05c1d36d694cc957a347fd34d1709199a2", "prompt_hash": "093988511c76b0699942f66801d1d9aed49cd7a57bcd78bb09095bede2a78c39", "target_hash": "74b39c8b285452eb8cf52f399aa55de966a44177a5aebc797d3a4c417e75dbc8", "exact_match": 0.0}
{"doc_id": 2071, "doc": {"question": "who were the major countries involved in the cold war", "answer": ["the Soviet Union", "the United States"]}, "target": "['the Soviet Union', 'the United States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the major countries involved in the cold war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United States"]], "filtered_resps": ["United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "94e040f37d78f44b6a0ef07c875bffb99588baea8ec440ad35873d91890b48cf", "prompt_hash": "4996c4bf75c3df486e0355077a9cd259d368e8853a808f3ab38b95a5762525d1", "target_hash": "a0a9175f82ec9856f9eac0b8e4875a964e1a5db4a20d2450cf09572a101e63e8", "exact_match": 1.0}
{"doc_id": 2072, "doc": {"question": "when does star trek discovery season 1 end", "answer": ["February 11, 2018"]}, "target": "['February\\xa011,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does star trek discovery season 1 end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" November 4"]], "filtered_resps": ["November 4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a070e21e73d0ff516f14d2d51d06643a9370295dd69acf8ea000099ce98af6ff", "prompt_hash": "52767fd12afa56af6aff3b06e6ccc7dc77832f1c1d4c9df3a82ab14088fb2fee", "target_hash": "88080a954c8db49fd766a9ad6b499298c0b15c8d311e61cad721e929508b2e02", "exact_match": 0.0}
{"doc_id": 2073, "doc": {"question": "who wrote catch 22 (both names)", "answer": ["Joseph Heller.", "American author Joseph Heller", "Joseph Heller"]}, "target": "['Joseph Heller.', 'American author Joseph Heller', 'Joseph Heller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote catch 22 (both names)?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joseph Heller"]], "filtered_resps": ["Joseph Heller"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4759b9cfac31f7983abe573c95a9d490fc78c63b8473e2cba296fc20219bcb49", "prompt_hash": "2a7341e810f5519da5455b1fa62883e7bb55b9195ff78641860590a3139ebd91", "target_hash": "4f564da466a6c1491ec4d57df1405fda294f091930ae7d60693e8f5ecb5b1eec", "exact_match": 1.0}
{"doc_id": 2074, "doc": {"question": "why did kevin can't wait wife leave the show", "answer": ["creative reasons"]}, "target": "['creative reasons']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why did kevin can't wait wife leave the show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pregnancy"]], "filtered_resps": ["pregnancy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "85b801d146ace098da9b7324b796d995b239d1c1f6b45684ae446544d24fc050", "prompt_hash": "4a2876dbac07d1cdabd32d2b4891791a6eaf83679544a2e7d2c5c815287f053e", "target_hash": "78178b601338583fd5fec6cf57433c91f70d133d11f84303eb9409395f3ce51e", "exact_match": 0.0}
{"doc_id": 2075, "doc": {"question": "how many students attend university of texas at austin", "answer": ["51,331 (Fall 2016)", "over 50,000", "51,331"]}, "target": "['51,331 (Fall 2016)', 'over 50,000', '51,331']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many students attend university of texas at austin?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 51"]], "filtered_resps": ["51"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dd9702b82ab7d90e4fc2ff4aaa5ae01e9d717676ce2e04b570c1fb2bcd5dd540", "prompt_hash": "069b07fd34b28e0de251972d66d67611fe58785cdb64aa916394315cb0ae3c59", "target_hash": "92a569f321cda5ddbc77ef4ed8b9f855288a6e6fac61784b56e75911dada930b", "exact_match": 0.0}
{"doc_id": 2076, "doc": {"question": "who plays janie in the search for santa paws", "answer": ["G. Hannelius"]}, "target": "['G. Hannelius']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays janie in the search for santa paws?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Stone"]], "filtered_resps": ["Jennifer Stone"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b73bd1b677cf28a92e599a54f201b21b6c67fe8889808624e030bba928a52c69", "prompt_hash": "5a9830c0c3b238fe6ff775e4048dc3906f52e5edcabbda4fb67bc7afc9536c47", "target_hash": "3df060549fc720242cb26a4ca08b84eeb5436c6c2cf019ff323ae740fdc14f18", "exact_match": 0.0}
{"doc_id": 2077, "doc": {"question": "who were the first non native settlers in the us", "answer": ["European colonization", "West Norse sailors"]}, "target": "['European colonization', 'West Norse sailors']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the first non native settlers in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Spanish"]], "filtered_resps": ["the Spanish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "145ca5e18822ec03208ce636b6ad78ce1f8bfb3fd91d9e6de26fb5bf073d4065", "prompt_hash": "058b85e5084b986d58f7900087297ef6bc3082ed77e17803b11d8e817578066e", "target_hash": "83ef82295ae5b44f3e3cfafc40c00b0678c86a06ace30900613a1a27d79ab13b", "exact_match": 0.0}
{"doc_id": 2078, "doc": {"question": "who played the king in the ten commandments", "answer": ["Yul Brynner", "Woodrow Strode"]}, "target": "['Yul Brynner', 'Woodrow Strode']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the king in the ten commandments?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Charlton Heston"]], "filtered_resps": ["Charlton Heston"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4a1de051078686bfb15c291b184b4be4d78ce94850e080924c966cc3b627f73", "prompt_hash": "0536b42884924d10719caf25a7b3ec3e25c0ce069b3d27856aa492a35070ada0", "target_hash": "2c54bac8300691258e3974e4ab604e3a9250d8d693768f92018de41fb21fc4f1", "exact_match": 0.0}
{"doc_id": 2079, "doc": {"question": "who produced a change is gonna come", "answer": ["Hugo & Luigi"]}, "target": "['Hugo & Luigi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who produced a change is gonna come?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Booker T"]], "filtered_resps": ["Booker T"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7310a59a11190b0184461bdc345d4341c89dd35fe25e52e74a41203b1cdc8c17", "prompt_hash": "c7437cf96df0a120d1c2b30d7ea4967b6d91ab3ed9ccf25cbfdac80a3c87e74e", "target_hash": "ed36df14586705ad82202209e86f4d4972c045363789508e3b6067aab2ae3179", "exact_match": 0.0}
{"doc_id": 2080, "doc": {"question": "who sang theme song for license to kill", "answer": ["Gladys Knight"]}, "target": "['Gladys Knight']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang theme song for license to kill?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shirley Bassey"]], "filtered_resps": ["Shirley Bassey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "572c1ae357c5b790b6c97faa76b3a57bdee9a80ae7841e7e70639cd3cb2c1317", "prompt_hash": "a70afce187bbaa46e892a4b7845959df036795f7f3bec9894c9a96e9938feaab", "target_hash": "68565c88bf6890f125c8022914330c247a5d16f73bb943a991a0a88a4d266a45", "exact_match": 0.0}
{"doc_id": 2081, "doc": {"question": "legends of tomorrow season 3 finale air date", "answer": ["April 9, 2018", "April 9, 2018"]}, "target": "['April 9, 2018', 'April\\xa09, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: legends of tomorrow season 3 finale air date?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" May 15"]], "filtered_resps": ["May 15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f3dc9ac9f8bdedac67a6fbb705b0996a3c02d32e23dcd7622a1bb54ea429363", "prompt_hash": "f5a16a09384a4de7ca69795485e654d272beca0211c52441d931f25ca981c065", "target_hash": "dccfaf176234c7979ac078609bd426fcf7e6d809fff90f58e153e169c97971b7", "exact_match": 0.0}
{"doc_id": 2082, "doc": {"question": "who sings my anaconda don't want none", "answer": ["Sir Mix-a-Lot"]}, "target": "['Sir Mix-a-Lot']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings my anaconda don't want none?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Busta Rhymes"]], "filtered_resps": ["Busta Rhymes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9ac0c6c62529df6a910ab0c07bc97878f32d2e472f8dbc1b1e4e0e31e2d2d174", "prompt_hash": "5f240a95879d08b021b25356e1334fe0214a592992d670c280eaf47cb0c088e6", "target_hash": "6d14326d658673b0bcf5864bda058b209a5c9e4f9a63e9d1568d93296d14944d", "exact_match": 0.0}
{"doc_id": 2083, "doc": {"question": "what was the result of the revolt of 1857", "answer": ["British victory", "Suppression of the revolt"]}, "target": "['British victory', 'Suppression of the revolt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the result of the revolt of 1857?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Indian Rebellion"]], "filtered_resps": ["Indian Rebellion"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f8b8e618ae58ca3fcafe9e0fd5e79bb22548f104e1715cd3c5d79e1e682cac69", "prompt_hash": "550805ea281d2bf2de212a9c0dcf7a06708d77ef5dac8e9502ccc9ed0460c46f", "target_hash": "b2986ab0fe8b73860f6615e1157b338c9a75e1c566193d2f4194ba9b4884cd8e", "exact_match": 0.0}
{"doc_id": 2084, "doc": {"question": "name three large lakes other than the great lakes in the united states", "answer": ["Iliamna Lake", "Great Salt Lake", "Lake of the Woods"]}, "target": "['Iliamna Lake', 'Great Salt Lake', 'Lake of the Woods']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name three large lakes other than the great lakes in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lake Tahoe"]], "filtered_resps": ["Lake Tahoe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "879a61d84e11db724f8c4f84fc72482b3ff5cc9f5d2236f2c67acb5e5dbc62e4", "prompt_hash": "169e50a6f71b71f3653fabe6669ab7619e02328d81c1fe370ea9d336ed817eb6", "target_hash": "887be9c9c6c64f31eb4b6e252c5f56bc16a3b1ab8b323bda26c9f8b60ce7968e", "exact_match": 0.0}
{"doc_id": 2085, "doc": {"question": "who wrote i can feel at home in this world anymore", "answer": ["Macon Blair"]}, "target": "['Macon Blair']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote i can feel at home in this world anymore?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Lennon"]], "filtered_resps": ["John Lennon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c970d32484554fdfe0f5221167157ee61158558bf05f7c52d8051b23fa4b674", "prompt_hash": "f8692eb7b79f6b43d1b2e34cc3e79ba8abad6e4182da0e6ffed53e4940cbb6b8", "target_hash": "360a7a3b7b4813039b9da302ccdd2025609b31735b036821d73a71b20e7d6f90", "exact_match": 0.0}
{"doc_id": 2086, "doc": {"question": "what is the meaning of the word autumn", "answer": ["fall"]}, "target": "['fall']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of the word autumn?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fall"]], "filtered_resps": ["fall"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "25767cf868bb5c038cdbcdbd0abbb17bcfd7a9e83537243d0c7bbf7dcffb728d", "prompt_hash": "4b4d77938bacec9fa46d74da3f632ef8e7cf8dc497db55f45552cd5af1025c7b", "target_hash": "d8739764f33e7ea65effefac4b6d141be20b95a09108a4cadb80594a75f80736", "exact_match": 1.0}
{"doc_id": 2087, "doc": {"question": "who is the speaker in shooting an elephant", "answer": ["Orwell"]}, "target": "['Orwell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the speaker in shooting an elephant?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mr"]], "filtered_resps": ["Mr"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ff665c6f0eb015e1fc8bffd956941e0786f566ee09b758cbecc411fadd1621c3", "prompt_hash": "7578eafc1c444f30ed1e5e2ac7fc3177c44ac4336697a65ccc7270fcac8ce38e", "target_hash": "9e4b041db3ec0750d4e931c14a328db8fbc4f33dd161acba3a0bcc9cfbcda0ff", "exact_match": 0.0}
{"doc_id": 2088, "doc": {"question": "the part of the cytoskeleton made from the protein actin is called", "answer": ["Microfilaments", "microfilament"]}, "target": "['Microfilaments', 'microfilament']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the part of the cytoskeleton made from the protein actin is called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" microfilaments"]], "filtered_resps": ["microfilaments"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49c803a02a28df1dbe6410e41700cd612b464d252344702fe1dc595225a9074d", "prompt_hash": "7be1f139cd3e3dc62df9ddbaf61f412cd7f414594ab49d1f7fb2c000072be0b5", "target_hash": "14647f3023e9ed5e35e9e09cb1e34a862dbbb13abf172d17f6600d4382709f9f", "exact_match": 1.0}
{"doc_id": 2089, "doc": {"question": "when is dancing on ice on the tv", "answer": ["2019"]}, "target": "['2019']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is dancing on ice on the tv?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Saturdays"]], "filtered_resps": ["Saturdays"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9354f5d0ab75bf97434f6416f24dd05a2ac29d36fa167968eb5594d163c6d6a", "prompt_hash": "51beb97138d36b4d2e6a8c0c7b270aae9b510b45edbf12443e02921699b53fb3", "target_hash": "c4f9d7aa0f3855069d4bc2b56119292516d0b4576cc9c882830f638c54d22101", "exact_match": 0.0}
{"doc_id": 2090, "doc": {"question": "who was the first executive president of guyana", "answer": ["Arthur Chung", "Forbes Burnham"]}, "target": "['Arthur Chung', 'Forbes Burnham']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first executive president of guyana?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Donald Ramotar"]], "filtered_resps": ["Donald Ramotar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2db47ba48cc199f657e55a4ab282770ad0611efd1a769271be5fa604c6554363", "prompt_hash": "5dfb0d4b4d65d5fb02aaf4a05c24ac9be8a5541b06302f1b7464295eb9422177", "target_hash": "2792c7175d210aa52a181c216eaa6f146c5685593b6dbdde5caf30db61f73f88", "exact_match": 0.0}
{"doc_id": 2091, "doc": {"question": "when did the us take over wake island", "answer": ["July 4, 1898", "January 17, 1899"]}, "target": "['July 4, 1898', 'January 17, 1899']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the us take over wake island?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1941"]], "filtered_resps": ["1941"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ecdd6e2c6607cb33dadb3b0bcc13f788b8562d82b3e61fa31e8c1cf8a64f89ab", "prompt_hash": "cc688ae37b28fb2bb0a138570181b378a6e39631cfcdc0e58f15afc657fbd6c7", "target_hash": "4c39053230b084da1eecb3797a80324a0d1b5c326be8794954d61659ee6c4375", "exact_match": 0.0}
{"doc_id": 2092, "doc": {"question": "where does the term the west come from", "answer": ["the Roman Empire"]}, "target": "['the Roman Empire']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the term the west come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Atlantic Ocean"]], "filtered_resps": ["Atlantic Ocean"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ebdbfab20dca2f0cb987fd6a265bef5e5ee766a098d996e1394fe5eaae90fc67", "prompt_hash": "a1574af4697af72b06306484232936f01a9867950960fd9103498d5fae62f9b7", "target_hash": "1d47b058baced67af23c4f62bce4dde263f476e5574d6202087ba143f395b50a", "exact_match": 0.0}
{"doc_id": 2093, "doc": {"question": "who played john coffey in the movie the green mile", "answer": ["Michael Clarke Duncan"]}, "target": "['Michael Clarke Duncan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played john coffey in the movie the green mile?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Clarke Duncan"]], "filtered_resps": ["Michael Clarke Duncan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a033a69c6260922882e52e795ccb8e5b00cfeb1eebdb426ba17203f0e356b533", "prompt_hash": "8a0267f71bab8dcde0e5943c1f52bcdb91f8e7f683f2ba1c687e3bd11d47da9e", "target_hash": "a2abe6391036e5b497427de8ff8b62e3ddea73a2fbbdcfef24b402adcdacecc6", "exact_match": 1.0}
{"doc_id": 2094, "doc": {"question": "two main types of research methods used in psychology", "answer": ["quantitative data", "quantitative", "Quantitative psychological research", "qualitative", "Experimental research", "Qualitative psychological research", "qualitative data", "Correlational research", "Descriptive research"]}, "target": "['quantitative data', 'quantitative', 'Quantitative psychological research', 'qualitative', 'Experimental research', 'Qualitative psychological research', 'qualitative data', 'Correlational research', 'Descriptive research']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: two main types of research methods used in psychology?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" observational"]], "filtered_resps": ["observational"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2d05ab00531b22528205faf94ff76f6c96499c76fc804be57f2090f25f1bbbc1", "prompt_hash": "a951b2c7ae9fdeb462b74ad0d16a382970bb795bd29864d55abeb1d5127e098c", "target_hash": "420a0e94b37e47ecd3a00dbacbd58588b037473ca114d1903a82c330182e4caa", "exact_match": 0.0}
{"doc_id": 2095, "doc": {"question": "when do dwight and angela start dating again", "answer": ["In \"A.A.R.M.\"", "Goodbye Toby", "\"Goodbye Toby\""]}, "target": "['In \"A.A.R.M.\"', 'Goodbye Toby', '\"Goodbye Toby\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do dwight and angela start dating again?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 11"]], "filtered_resps": ["season 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "392f3e3df401d5d5b1749d81841fef962dd0151f55e5e73cc72414b3ef078a7f", "prompt_hash": "a80d1423f216602581a75182a4c6ef7dfad77c9e9c2ef3a9c91e33b2a262afea", "target_hash": "2700bd1b1a6e10fd591a9cfe95ed2936ca7eeda368bc2a355f5d5461a70b06b8", "exact_match": 0.0}
{"doc_id": 2096, "doc": {"question": "where's the tv show the crossing filmed", "answer": ["British Columbia, Canada"]}, "target": "['British Columbia, Canada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where's the tv show the crossing filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vancouver"]], "filtered_resps": ["Vancouver"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "95be65307f590143793ce48a14d6fb7ef31e660f954c2c005b97cd310e0ae13a", "prompt_hash": "45122fb93a920be165a2f5ebc653da8cfc5508d5b0e659e5c2249ed3657d22f5", "target_hash": "d1a79b7af2ba1f36e79bee5f26f2e7918c50da2a29a24b03fe1d7a321298c162", "exact_match": 0.0}
{"doc_id": 2097, "doc": {"question": "when does god of war ghost of sparta take place", "answer": ["ancient Greece"]}, "target": "['ancient Greece']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does god of war ghost of sparta take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 400 BC"]], "filtered_resps": ["400 BC"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "034a048e793ef62f02100763bafd7f7e155e797a77c831b01b4351233ddce781", "prompt_hash": "e12e18bc833b4b19cce6a6bcc95568d96e831cf7d4a3d7a7fcbf428c28fd757c", "target_hash": "c09c6c5eb840652057196d0247b1f003849dcd3a80f3da6fbef018ba4a4d7f8c", "exact_match": 0.0}
{"doc_id": 2098, "doc": {"question": "who sings the song i want to go outside in the rain", "answer": ["Milira"]}, "target": "['Milira']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the song i want to go outside in the rain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Black Eyed Peas"]], "filtered_resps": ["The Black Eyed Peas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8b7733f0a18f4b5d083745fe8f35be3b63cd2abf84e8fddb882276dbb264ddd9", "prompt_hash": "c731d9973eb905c28f9e98d553160398ca0024df46cba01272791bc00a19678f", "target_hash": "b07ce1e91e4de95bf52753b0bdbba8db39d4ace7a62a9736730ee675400cbe54", "exact_match": 0.0}
{"doc_id": 2099, "doc": {"question": "who sang one bad apple spoils the bunch", "answer": ["The Osmonds"]}, "target": "['The Osmonds']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang one bad apple spoils the bunch?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pussycats"]], "filtered_resps": ["The Pussycats"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f441839426b7aba63f34b551b2bef830091e4cd8a76dd898534381f404d9efa6", "prompt_hash": "884aa0259928d81173fe4fe1e6acb467c81d87d3e62c3fb9a118c63d9cf05018", "target_hash": "437d93c661739bc2d626040e13122183188759a9ad1ea4ec50fa7dc0ab26190b", "exact_match": 0.0}
{"doc_id": 2100, "doc": {"question": "when does the new season on the 100 come out", "answer": ["April 24, 2018"]}, "target": "['April 24, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new season on the 100 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 10"]], "filtered_resps": ["October 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ddb2d2450b0ca7870704e535de112bc36c15224060769c985885f0a7da732e63", "prompt_hash": "0eed825c86dc52605fb401d95d40dab6c51c3637fff4b6ba2459c5cd3225013a", "target_hash": "31e327e91e9b85068ffcda80cd82756b8f928474eed0445c76bf84ca5771a75e", "exact_match": 0.0}
{"doc_id": 2101, "doc": {"question": "who plays the woodsman in over the garden wall", "answer": ["Christopher Lloyd"]}, "target": "['Christopher Lloyd']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the woodsman in over the garden wall?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jared Harris"]], "filtered_resps": ["Jared Harris"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c004b73459d0e72435202e1d8dbbfcc9315ecd93f449a76e14675cd17a0cd3bf", "prompt_hash": "7b5c2e6cdb5f9b6eb0f267c918e0d6ecc6a2df8f970f52fe291f5450ce94d5b8", "target_hash": "49d9e81a067223ab4308a84cba82821e5ff236ac5cd325474c53797e60682d5f", "exact_match": 0.0}
{"doc_id": 2102, "doc": {"question": "what is the job of justice of the peace", "answer": ["dispense summary justice"]}, "target": "['dispense summary justice']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the job of justice of the peace?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" magistrate"]], "filtered_resps": ["magistrate"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ec6af2ec63700ba50d0d632cec1213a5f39346610c83d9c6b8304d7b7e29df4c", "prompt_hash": "790eb6d279fd26e07fc944bd4644e92cb7e5cc88c73ff0f03142a8788121a640", "target_hash": "198136e7abee9095d9bb93b9f9625e58c0e6e2ea8c4e823d15ecf5b206eb4daa", "exact_match": 0.0}
{"doc_id": 2103, "doc": {"question": "when did hollywood video go out of business", "answer": ["2005.", "May 2010"]}, "target": "['2005.', 'May 2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did hollywood video go out of business?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2009"]], "filtered_resps": ["2009"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eb8d57e1ed1c20ad74fc6eae02cfa95f11befaa1c9bb02bafaa4a5cd086bce1e", "prompt_hash": "28aca20da93494a84d81db793885fd942952ad16e3240821f2f948067c63fcc7", "target_hash": "15ede3dc3d3c0021c6597f2b65a8f3b368363ddf2a57df12a8d5d18241196390", "exact_match": 0.0}
{"doc_id": 2104, "doc": {"question": "what is the largest magnitude earthquake that has occurred in the pacific northwest", "answer": ["The 1700 Cascadia earthquake"]}, "target": "['The 1700 Cascadia earthquake']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the largest magnitude earthquake that has occurred in the pacific northwest?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 9"]], "filtered_resps": ["9"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2e4570591c742e3b4bd56439c978f1a8b208afcf14b83d0591533bd2688ffe85", "prompt_hash": "a0497d0d78fdec4ac1b93c664a653740b61d00a8bb8b2359a187eea4b12cbcce", "target_hash": "ee803ff4f6a0d6a2a31ec42de28255fa50bdbfde02ac293b05649e93f217fb42", "exact_match": 0.0}
{"doc_id": 2105, "doc": {"question": "who did michigan state play in the rose bowl", "answer": ["Stanford Cardinal"]}, "target": "['Stanford Cardinal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did michigan state play in the rose bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oregon"]], "filtered_resps": ["Oregon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f2fc6f00670178ba12e4bd5136954d472c0d3a25041ca89927db29eeb0a12b4c", "prompt_hash": "125b3f1cb033c1ca4a0df264b1b3cb4bf952b516d9045b0b9a078dd575d3decf", "target_hash": "e939f8bccc7f1fb1ec9f23381dd1c42f46a42d199599296bafa5d01241e474e6", "exact_match": 0.0}
{"doc_id": 2106, "doc": {"question": "when did the ncaa tournament became more important than the nit", "answer": ["By the mid-1980s"]}, "target": "['By the mid-1980s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the ncaa tournament became more important than the nit?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1985"]], "filtered_resps": ["1985"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "28a14876a906e67ed6bb8d16cd96b91b7c2442c6686e1a83688bffb5a6a6027e", "prompt_hash": "9418e5114275581c7df00dc4abb07817d4cced5251b633b291f0a6ecc0bd263f", "target_hash": "f565448c49468abff8592b313d83eaa91fd8aa80f6bac2f312e031271aa5ee9e", "exact_match": 0.0}
{"doc_id": 2107, "doc": {"question": "where does junior want to go to find hope", "answer": ["off the rez"]}, "target": "['off the rez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does junior want to go to find hope?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the carnival"]], "filtered_resps": ["the carnival"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cc99da5cff906f49d033b58568e9f28e44ba7fc3c3c8df8597e61a6aa56cff48", "prompt_hash": "eba00ac82abcad46592b6f43b677a9129fe30b22a844375f517125892b827c4d", "target_hash": "07c02ac1b276f5dfaaa64dd5cc89c7bc8b198a59a09126ddd85a5c0f7da39625", "exact_match": 0.0}
{"doc_id": 2108, "doc": {"question": "when was the last time mount ruapehu erupted", "answer": ["25 September 2007"]}, "target": "['25 September 2007']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time mount ruapehu erupted?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1996"]], "filtered_resps": ["1996"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "864908ffdcba399e85b4acd0ec791e64428e57e4c1fe079ad7f1a79c3c34785e", "prompt_hash": "d9111fec8c83adb4a7662a46d8e42985493acd9cffb0c89ff474ef59a2b600f1", "target_hash": "484056955f8def5bbc424a2c7699a6327fb3c3dd174fad82d75347785d3ab06b", "exact_match": 0.0}
{"doc_id": 2109, "doc": {"question": "character in macbeth who is murdered and appears as a ghost", "answer": ["Lord Banquo", "Banquo"]}, "target": "['Lord Banquo', 'Banquo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: character in macbeth who is murdered and appears as a ghost?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Banquo"]], "filtered_resps": ["Banquo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "72fd1092fcfa6c5e97c2450aa6f90e8a10c37148abbe217604b13f7b826e42f6", "prompt_hash": "64829f713779e5873864b1e25cd1fbca75b13045f74602d0904940445d849481", "target_hash": "db29c5e919df3d5c39439ccf9fc2d4ac83a9a29ae655e8337066a6ccf8cb57b1", "exact_match": 1.0}
{"doc_id": 2110, "doc": {"question": "when does season 8 of vampire diaries come out", "answer": ["October 21, 2016", "on October 21, 2016"]}, "target": "['October 21, 2016', 'on October 21, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 8 of vampire diaries come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 10"]], "filtered_resps": ["October 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96c9b7266839cd995444e087a14f6d4287d100dde80c233fa249c4866f3a9275", "prompt_hash": "d3e886225a8955f831fabce4739837fea829c0ed43dc12139a2e3aeb67263c19", "target_hash": "806c7f7930f2f493a9660b74c15f6b4017d2257b8df24c8c8b45a2e97330f630", "exact_match": 0.0}
{"doc_id": 2111, "doc": {"question": "whose new political ideas of natural rights influenced the declaration of independence", "answer": ["Francis Hutcheson", "John Locke"]}, "target": "['Francis Hutcheson', 'John Locke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: whose new political ideas of natural rights influenced the declaration of independence?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Locke"]], "filtered_resps": ["John Locke"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "494c8c342f25832bb4c7c3487bbd2bf33d6cc9edc9fe0c47db5648a176ea2cd1", "prompt_hash": "4a5dceaab73c5e9537b85260e4708c394fd8a24501e65ccf39862ed1f56b08e5", "target_hash": "97aedf547f59e4481ef60e39c9f2849e9e08d889593c2350b498ba928e2eab3c", "exact_match": 1.0}
{"doc_id": 2112, "doc": {"question": "who built the first temple for god in jerusalem", "answer": ["Solomon"]}, "target": "['Solomon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who built the first temple for god in jerusalem?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Solomon"]], "filtered_resps": ["Solomon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "365d3968fb3a28ce0746c58ec0b07512341ef6dc5cde7bcbc498fc69260b5e2d", "prompt_hash": "cccaf985def42ba01eb793b31f36ab010ba1d7c8217097ce78ff571fdd307dcc", "target_hash": "d9bd050c305fa0a59eb99fcd4a14aad56af60ff2ec9d4888a7e5c94fa3296a04", "exact_match": 1.0}
{"doc_id": 2113, "doc": {"question": "who negotiated an agreement with japan concerning the future of korea", "answer": ["the Korean Empire", "Empire of Japan", "Taft", "Korean Empire"]}, "target": "['the Korean Empire', 'Empire of Japan', 'Taft', 'Korean Empire']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who negotiated an agreement with japan concerning the future of korea?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yi Wan-gyeong"]], "filtered_resps": ["Yi Wan-gyeong"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1433e1fd947f68147c589fff134ade580a7b198924a70d5ba30c4274f0b03357", "prompt_hash": "c909b85c04d51ed1202b7e536bfc93d16bb717d505307ef0528ea1952b115014", "target_hash": "a961d11583a110afdeb9d962a7f03d4346724b5c2caa59e9409fab0a31b15e48", "exact_match": 0.0}
{"doc_id": 2114, "doc": {"question": "who does the voice of salem the cat", "answer": ["Nicholas \"Nick\" Bakay"]}, "target": "['Nicholas \"Nick\" Bakay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of salem the cat?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tress MacNeille"]], "filtered_resps": ["Tress MacNeille"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92a0d12f2af4da900a35684bbebd5c5e5477fbe82c73c8d8452042b4baee2efc", "prompt_hash": "76987a05c033319fa5e059e386b64f7209cae252233ce78bbd50cfc517a07a28", "target_hash": "57c17a7dcea7f58418e4a379a574db28455f2c498cc154052d09605fe921fb5a", "exact_match": 0.0}
{"doc_id": 2115, "doc": {"question": "who did the central powers defeat on the eastern front before concentrating back on france", "answer": ["the Russian army"]}, "target": "['the Russian army']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the central powers defeat on the eastern front before concentrating back on france?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Russia"]], "filtered_resps": ["Russia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "364ad0333e6dc96252bcb395004d6d15e2ebbf877ee18e39e7cad96e859ee6b9", "prompt_hash": "e6a6f810bd1306650c30595442908aeddcbd0571673a296c790ae2c29b271a7e", "target_hash": "f5983fb9625e8969cb43f50b999c0ea0bf004af0cd9cc1878fd9da8b8bc6574a", "exact_match": 0.0}
{"doc_id": 2116, "doc": {"question": "how many episodes are there in ordeal by innocence", "answer": ["three part", "three"]}, "target": "['three part', 'three']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes are there in ordeal by innocence?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "707554b10d8ce762ee556b025eda84ebb974430ff1d2b0be897a0b622af17815", "prompt_hash": "fb02c9f9eb01ab15bb1d3aff17bca134ac4ec72bdf81bbd68ff4c1470cef63d9", "target_hash": "b8d4a338cf99edeb46dc8e90b11081c2a212d77c9e8b40b5ee8156bbeb6fc16d", "exact_match": 0.0}
{"doc_id": 2117, "doc": {"question": "what is the enrollment at university of miami", "answer": ["16,801", "16,801 students"]}, "target": "['16,801', '16,801 students']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the enrollment at university of miami?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 15"]], "filtered_resps": ["15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf05345561d26c2f3d5ba3a4c8e078bb3c0806172b110e985e82b0ac409e72af", "prompt_hash": "f99f24a2086fede02ef76b01f8f8c01ef84bb96bd87641799f860b6bbcbb90ae", "target_hash": "ac127db88a1c097276a13f2f5c4d0fe181621d35c6c5b96a789879f26f1f5c62", "exact_match": 0.0}
{"doc_id": 2118, "doc": {"question": "when was the lupus foundation of america founded", "answer": ["1977"]}, "target": "['1977']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the lupus foundation of america founded?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1957"]], "filtered_resps": ["1957"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9bd2b03c370dca7262e05e0e2b185d255a73b4271574a9475558c1331689306", "prompt_hash": "fa39aca8e065a46a36bd77a829ba5295fced821a1a605497e7cb52c03fa17522", "target_hash": "0ebe85c2e0af41b00a87011b5830da89c38a8c39b8fb4042fb18b91fde0eaaf9", "exact_match": 0.0}
{"doc_id": 2119, "doc": {"question": "where did the legend of the easter bunny come from", "answer": ["German Lutherans"]}, "target": "['German Lutherans']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the legend of the easter bunny come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Germany"]], "filtered_resps": ["Germany"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b964555ab1bd694646ba19c599b24a0798df7afe67810f9a8137add04dba46d3", "prompt_hash": "cf280dab2a99d12aa417014104547d4049d69836e22dbfd6a90e686674504611", "target_hash": "76f09d9e9f617126de3793721cb2f2893540309b9da0a09aab6a96928760e159", "exact_match": 0.0}
{"doc_id": 2120, "doc": {"question": "when did the tv show the waltons first air", "answer": ["September 1972"]}, "target": "['September 1972']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the tv show the waltons first air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1971"]], "filtered_resps": ["1971"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "48ac4c7b8b92d6d7973f1a8af4f9ee5f04ad4fa819e45e971fe5dbfb8ee79a05", "prompt_hash": "ebde47bd1dc88542dfcd64014915f91a2cd847d348112c2f0998a1400434a565", "target_hash": "6c1360ba7a02a715f6ede4208cbb3bf39a1c294592492776064adae1f6b90886", "exact_match": 0.0}
{"doc_id": 2121, "doc": {"question": "when does the boy in the striped pajamas take place", "answer": ["World War II", "during World War II"]}, "target": "['World War II', 'during World War II']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the boy in the striped pajamas take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" World War II"]], "filtered_resps": ["World War II"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9e382825db417c3793f35e4f3621a8c02121c54915a95dd19e0600dc47931a9", "prompt_hash": "1d98e85d4f1fdbe9e2653be3bbb5eeb73e4d3d1cfe7caa4d1a106f0159c639fb", "target_hash": "613f1d02115ae173205344a83b785dc44db48afc676d9d48904405d08a62ad2c", "exact_match": 1.0}
{"doc_id": 2122, "doc": {"question": "who sings gimme some lovin in days of thunder", "answer": ["The Spencer Davis Group", "Spencer Davis Group"]}, "target": "['The Spencer Davis Group', 'Spencer Davis Group']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings gimme some lovin in days of thunder?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f4ffbc47306c4cbb1753b7c4d0b5dce8577a00e7b31a596ba7597e7b3d9c43bc", "prompt_hash": "ed1a9d7baa262033fdd5274390f72744e073a6d4e36d9b7955bd46d0c395106e", "target_hash": "d6c5a2ee6cc6c87b8badfd03a418e624bd7746cc8ce0d05a479682c3be1d1997", "exact_match": 0.0}
{"doc_id": 2123, "doc": {"question": "when was the statue of liberty in france built", "answer": ["Paris"]}, "target": "['Paris']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the statue of liberty in france built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1886"]], "filtered_resps": ["1886"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea543190772ecd5ae2a480ea304a485312308aea7d858a334702cc8f1c6e9585", "prompt_hash": "63949744339979de1e625b5dad3809ebbc951daa7041ec076d0db5bc8bbc5562", "target_hash": "5c16081dc8c6c87a151bfa0a53e1e0f4f6af5155559b51f0e6e73b14ccaf13b7", "exact_match": 0.0}
{"doc_id": 2124, "doc": {"question": "what is the largest man made lake in the us", "answer": ["Lake Oahe"]}, "target": "['Lake Oahe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the largest man made lake in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lake Mead"]], "filtered_resps": ["Lake Mead"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bd0f00ad12ea0e1ab873d8559a7d54ba41419dfd513eb7328e6e0812fe7d1a86", "prompt_hash": "888a2cce63f1b0a5520ca5c7e62b25ba408d817d766ab8d200f7c19a46c62657", "target_hash": "935f1846201f4e457b36b0b9c831c8e9da6c5072932cdc7f1e4923c40105e0d4", "exact_match": 0.0}
{"doc_id": 2125, "doc": {"question": "how many points are scored for a touchdown in american football", "answer": ["six", "six points"]}, "target": "['six', 'six points']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many points are scored for a touchdown in american football?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 6"]], "filtered_resps": ["6"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49061099d04a1ca164a5144d248f2deff5f17426333eee5b0e9787926c54f05b", "prompt_hash": "f290dd5b7045dcd2f8fc6be1f8d77ae5f6345fbe9bbcf03e63500295c9dde616", "target_hash": "37bd20cc3257768de6256d29a8021ed9b3bb1829a8f3935b50c177491c9121c9", "exact_match": 0.0}
{"doc_id": 2126, "doc": {"question": "when did the jurassic park movies come out", "answer": ["Jurassic Park (1993)", "2001", "2015", "2018", "1993", "Jurassic World (2015)", "1997"]}, "target": "['Jurassic Park (1993)', '2001', '2015', '2018', '1993', 'Jurassic World (2015)', '1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the jurassic park movies come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1993"]], "filtered_resps": ["1993"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae3ab629dd654150e3aef1757d0945539546b96c999db5c4a496ecb160052185", "prompt_hash": "71d08155e73b9d96070cb4e133371ed641a62e34c81c34b35f45d40247d10929", "target_hash": "9ee771c04ba214873c3c3ca1691234f032c053a6a8dd58bfc2bf4679025fec38", "exact_match": 1.0}
{"doc_id": 2127, "doc": {"question": "when was child benefit paid for the first child", "answer": ["1975"]}, "target": "['1975']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was child benefit paid for the first child?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1911"]], "filtered_resps": ["1911"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dcb9fb7682e41a845580ff27662de32bc086f1ac1489ddfc6ff3936d54500058", "prompt_hash": "f1a0d8d2d29a1cadedb2db947ec522cc8a5974b4155079ebd8ba72d7b39f9277", "target_hash": "30547784fdb6aac7ba2fd06b246ee5c7e1d5fb3c600cdce64155caba0d079427", "exact_match": 0.0}
{"doc_id": 2128, "doc": {"question": "what class of ship is the carnival glory", "answer": ["Conquest", "Conquest-class cruise ship"]}, "target": "['Conquest', 'Conquest-class cruise ship']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what class of ship is the carnival glory?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carnival Cruise Line"]], "filtered_resps": ["Carnival Cruise Line"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b2b6d1509c2cd55783531f0b087579eda96b4dfde656867bd6c89a7749f559e", "prompt_hash": "9d4371aee66f7355369317545667b70d60c87d3d3fe387ca4245288a3a4f0ffa", "target_hash": "d117232d33a0f5b3a8173da82e91a96d085468ebd29220db6a3f1a1d284c5d16", "exact_match": 0.0}
{"doc_id": 2129, "doc": {"question": "when did scotland beat england in six nations", "answer": ["2 April 2000", "25 February 2006", "24 February 2018", "8 March 2008"]}, "target": "['2 April 2000', '25 February 2006', '24 February 2018', '8 March 2008']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did scotland beat england in six nations?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012"]], "filtered_resps": ["2012"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "05674d7671cff3208f5a414343736b79f1f892cca985fd3ee0fa5a71b38fad46", "prompt_hash": "8177728bbc2a88310ffa443a319c12057281b7e1b70592b06843472442f047b7", "target_hash": "059e78ac5d1144330d4f4419c7fcbf91513b79ca5a884c3604798e7af026c2a7", "exact_match": 0.0}
{"doc_id": 2130, "doc": {"question": "who sang the theme song to that 70s show", "answer": ["Big Star", "Todd Griffin", "Cheap Trick"]}, "target": "['Big Star', 'Todd Griffin', 'Cheap Trick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the theme song to that 70s show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tommy James"]], "filtered_resps": ["Tommy James"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dff32b923789c753a7c53fec88d85791afabd1355d096e9a7ecc8daa946a703e", "prompt_hash": "05e49bad6b5c0185a48234da58c4387e22222784f34244434b3b6efcd0149297", "target_hash": "4744780372a73f5629bb7909ddda7236eefefcb515b39de78e3514b212d06d25", "exact_match": 0.0}
{"doc_id": 2131, "doc": {"question": "season 2 this is us number of episodes", "answer": ["15", "18", "13"]}, "target": "['15', '18', '13']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: season 2 this is us number of episodes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 18"]], "filtered_resps": ["18"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "93730dfba7375bc126934308f8b6abe5461b77e0ddd01a16fe1894355e24e1a7", "prompt_hash": "70f45a1ea3d70dcc7d0e80ea326d50216f3d55ba89394a33bcfe695325f06962", "target_hash": "4be52158750a703c8ab90ce470d541c020760db20e87bed2b10953720ba47131", "exact_match": 1.0}
{"doc_id": 2132, "doc": {"question": "who led an attack that allowed for american victory at yorktown", "answer": ["George Washington", "Comte de Rochambeau"]}, "target": "['George Washington', 'Comte de Rochambeau']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who led an attack that allowed for american victory at yorktown?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Washington"]], "filtered_resps": ["George Washington"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c5776208ec90001b6ec96eca2e4d4b475ee61b667a50b332afb4b07104ec32c", "prompt_hash": "2fc3dff19097bb194f571cd98da3c7214283ec4101f7fc0c8f8890ed58620843", "target_hash": "9ee696a350314da1031cb92808d4c3e7ce9044701f22296cc98e1ee545db3430", "exact_match": 1.0}
{"doc_id": 2133, "doc": {"question": "who is the oldest wrestler that is still wrestling", "answer": ["Dory Funk Jr."]}, "target": "['Dory Funk Jr.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the oldest wrestler that is still wrestling?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dean Ambrose"]], "filtered_resps": ["Dean Ambrose"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "063bc27d256106d22212e98658b9b9676dea745839fc989956038efd9de77432", "prompt_hash": "0979598a6e88e3b89a10252c1c6e517cd24b403471418f6952306a4874ff0395", "target_hash": "986ef9575fced29f2af733f53c5e071e9d5eb6bedb6525fd869463f35d426b5a", "exact_match": 0.0}
{"doc_id": 2134, "doc": {"question": "who developed a set of postulates to prove that specific microorganisms cause disease", "answer": ["Robert Koch", "Friedrich Loeffler"]}, "target": "['Robert Koch', 'Friedrich Loeffler']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who developed a set of postulates to prove that specific microorganisms cause disease?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Robert Koch"]], "filtered_resps": ["Robert Koch"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "73dfa9bdfe047834c62787f1769ed496ff571fb32526c8da281b029035a1ed7c", "prompt_hash": "bdc8926d0ae10f5fa46726c4d34418f5583666c5999d63d0e2778f0e61bbe9af", "target_hash": "f3cc1349a66fe9983d757bed8171def8608d2c24ef0595c33920704357a7ab4c", "exact_match": 1.0}
{"doc_id": 2135, "doc": {"question": "what's the name of the last mission impossible movie", "answer": ["Mission: Impossible – Fallout"]}, "target": "['Mission: Impossible – Fallout']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the name of the last mission impossible movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mission: Impossible – Fallout"]], "filtered_resps": ["Mission: Impossible – Fallout"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "200bbfebff5b7360b56e504bbcf3e0efa81b98e062233e79c52a30895fcc3a72", "prompt_hash": "397df2e9083b50ec858422f74fc87433b0182b2594339eadb1b30304b6dc2986", "target_hash": "4ad44dc2fb8020031824892fbec6a97f47a1eda42218657787128dc4dd14bbcf", "exact_match": 1.0}
{"doc_id": 2136, "doc": {"question": "gabbar is back is a remake of which telugu movie", "answer": ["Tagore"]}, "target": "['Tagore']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: gabbar is back is a remake of which telugu movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gabbar Singh"]], "filtered_resps": ["Gabbar Singh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2cb87cbd20b4d19f1889681c3e2903543f2e1949d4103806ba7d47902eab9da5", "prompt_hash": "75dc328bd0035302dc1a7f63170547e6bdd96f24c39bed8188dbea15893d3e72", "target_hash": "d424b8119a132a548af5b573cac0a6f113260b1a99f096f172cc6427fb612aac", "exact_match": 0.0}
{"doc_id": 2137, "doc": {"question": "who sang it going to take a miracle", "answer": ["The Royalettes"]}, "target": "['The Royalettes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang it going to take a miracle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Beach Boys"]], "filtered_resps": ["The Beach Boys"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "66517573a0f01cf54641930dfd9b697d6ece2b25171880936bd11c5dd1019821", "prompt_hash": "1ba6d460ed17f47cffe13b891a9470326f67a5e028f79e4e9681af7d029d4f41", "target_hash": "e63f3ba24c1220b1e3aae76c59aff4f299d0fe7e97bb1296ef31b545a17aad7c", "exact_match": 0.0}
{"doc_id": 2138, "doc": {"question": "winner of the women's royal rumble match", "answer": ["Asuka"]}, "target": "['Asuka']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: winner of the women's royal rumble match?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Becky Lynch"]], "filtered_resps": ["Becky Lynch"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3e0e92b8d4b39a870d4b9e0dd05713ae723f8819de4998a1eb084e9c4a4be6d1", "prompt_hash": "48a4cb01cac47bf1056ca4ed408e5bd5694d7313d35011c8112b27327d0a2f5d", "target_hash": "98cf0a3cadcce0a143f1e20e57d705b803dbd43888086f8720614a8c3fe0c65a", "exact_match": 0.0}
{"doc_id": 2139, "doc": {"question": "who played in the first fifty shades of grey", "answer": ["Marcia Gay Harden", "Jamie Dornan", "Dakota Johnson", "Jennifer Ehle"]}, "target": "['Marcia Gay Harden', 'Jamie Dornan', 'Dakota Johnson', 'Jennifer Ehle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played in the first fifty shades of grey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jamie Dornan"]], "filtered_resps": ["Jamie Dornan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b11106f099be6c39a91c828a4ff5cfaf6125fbdcfb3521fa9ded4e1466a80dfa", "prompt_hash": "94b1dc2f4a1b62cc3a79b7bade2c02dde382bb6e8504f0d6e16c62c4aff6842d", "target_hash": "735532683b8c0076517e7fdcac602bf09f9bea1d83716c44eb501fa58481ddc4", "exact_match": 1.0}
{"doc_id": 2140, "doc": {"question": "what is the most popular religion in sweden", "answer": ["Lutheran", "Lutheran Church of Sweden", "Christianity"]}, "target": "['Lutheran', 'Lutheran Church of Sweden', 'Christianity']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the most popular religion in sweden?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christianity"]], "filtered_resps": ["Christianity"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c96ad7d13576be6b6cfa59d08b7bf21d0197ac92526e262fdad35ad8e0fd8382", "prompt_hash": "5911214e8de1d082d8bd27291fb92e17accaddc16b7e81e36f39b28fb017e50b", "target_hash": "4272e0d5e529d958148ffd1b035df66d31276322b3380f0cdf273602d86ce2d0", "exact_match": 1.0}
{"doc_id": 2141, "doc": {"question": "where was the last scene of the danish girl filmed", "answer": ["the Mount Mannen in Norway"]}, "target": "['the Mount Mannen in Norway']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the last scene of the danish girl filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Copenhagen"]], "filtered_resps": ["Copenhagen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aa734391fa4eab3d312f94d57c0f1d37ca4ed0f9307e8bd30fae12f1b50b8e55", "prompt_hash": "4a7fa8dbe468fd54b91dc13b7c35fb6e459feec0b8db8d54b615de6a0a0c145c", "target_hash": "7e294a2cb2fa7a671f86c947cb06be9bb07a4336c0b145573d20a935fe13320e", "exact_match": 0.0}
{"doc_id": 2142, "doc": {"question": "where was the movie 500 days of summer filmed", "answer": ["Los Angeles"]}, "target": "['Los Angeles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the movie 500 days of summer filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Los Angeles"]], "filtered_resps": ["Los Angeles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f52c799df6f79b47fc46142538a47dd597814b1f3267ed6b497e6af5b4cb1457", "prompt_hash": "288e41cf37007729ac640876c646ca031feb8d1c997dcf6c17a7fd8521956154", "target_hash": "13af6d91f7b7f3967f9d8ec43e593992061fe6b9c3adc1ce5ab870f7f37217cf", "exact_match": 1.0}
{"doc_id": 2143, "doc": {"question": "i want to be with you everywhere song", "answer": ["Everywhere"]}, "target": "['Everywhere']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: i want to be with you everywhere song?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" \"I Want to Be with You Sooner or Later\""]], "filtered_resps": ["\"I Want to Be with You Sooner or Later\""], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ac96db782e31b9f676cbfe89d02cbb99ce9c3dd55b8cb9eab96cd383b98a7270", "prompt_hash": "74fdaf23a5c128a6b3e74e1559478e4644b285e152b91e7eba9e5ca48279617c", "target_hash": "afcfe362b2c36f95ee5776746dd5c8d0f5dd1a41bc8c9ba6a84e92e50439e911", "exact_match": 0.0}
{"doc_id": 2144, "doc": {"question": "who sang the songs on walk the line", "answer": ["Reese Witherspoon", "Tyler Hilton", "Waylon Payne", "Johnathan Rice", "Shooter Jennings", "Joaquin Phoenix"]}, "target": "['Reese Witherspoon', 'Tyler Hilton', 'Waylon Payne', 'Johnathan Rice', 'Shooter Jennings', 'Joaquin Phoenix']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the songs on walk the line?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Johnny Cash"]], "filtered_resps": ["Johnny Cash"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58611b429c574b062c05bf3ae89cea114ed4406bd402c65e3a4198b165858bda", "prompt_hash": "e96c860f75865952d7d196b53fa051788dda2270026bf628193f2b7acbde39cb", "target_hash": "5d70f50a07d17239f9d085a479aeb916cbabcf10148bc4a85cc6314d4020d319", "exact_match": 0.0}
{"doc_id": 2145, "doc": {"question": "who is playing the halftime show for the superbowl", "answer": ["Justin Timberlake"]}, "target": "['Justin Timberlake']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is playing the halftime show for the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "565578e48f0a5b972deb1f9b9b61f52173163f55e3f852c78f94dfe8573dff1a", "prompt_hash": "e84da452e6ced45a16ab3fa4f2a0100e5edbeb1e24c1ae2ba94e85ee0a516275", "target_hash": "5d3c152075c166190023989a2621c5a25323ea4325675ced16b03032cec5a254", "exact_match": 1.0}
{"doc_id": 2146, "doc": {"question": "german word for pleasure from someone else's pain", "answer": ["Schadenfreude"]}, "target": "['Schadenfreude']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: german word for pleasure from someone else's pain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" schadenfreude"]], "filtered_resps": ["schadenfreude"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b2b92a09ed359c584fd88ea6bf77f44453dbb8d3d6f5c1d8f3fa6c86af304d4b", "prompt_hash": "d059924a7aba59cfa741fe489535cd0734a3e6138c5f14521d73a6f76f2b01c3", "target_hash": "9b883673d582aa96f7e6b08ac75ab86ee997416d195861bab04548c1e67b341a", "exact_match": 1.0}
{"doc_id": 2147, "doc": {"question": "actress who plays penelope garcia on criminal minds", "answer": ["Kirsten Simone Vangsness"]}, "target": "['Kirsten Simone Vangsness']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: actress who plays penelope garcia on criminal minds?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kirsten Vangsness"]], "filtered_resps": ["Kirsten Vangsness"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1ad1b7c239993207929ff437bc39a8b52de39f7c15fa71d5d6211d422a68a8bc", "prompt_hash": "52000391a5083649e60e7d6da11aaf3af2299afa3f958543386637f63a182110", "target_hash": "5425293284b4d456720259f17d59106bb935de785aa59a84f376e28c3562fb10", "exact_match": 0.0}
{"doc_id": 2148, "doc": {"question": "when did cricket go to 6 ball overs", "answer": ["1979/80", "Since 1979/80"]}, "target": "['1979/80', 'Since 1979/80']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did cricket go to 6 ball overs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1963"]], "filtered_resps": ["1963"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d297e13ca7b0dde297b07ca82df0bfb0c0fa91b7b5ca406752b03340b75c6ce1", "prompt_hash": "335b72e2f388767106d8f31b8cb838f43130f0524954afae160a6d55832f6d6b", "target_hash": "bcbbe676e62085cbdd5cf3c22769b27c56f71c5d8fe730a0829b04b249acfc92", "exact_match": 0.0}
{"doc_id": 2149, "doc": {"question": "when did the soviet union start and end", "answer": ["in Eurasia"]}, "target": "['in Eurasia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the soviet union start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1917"]], "filtered_resps": ["1917"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f94b769486c3117a0a6591a96e67e35fecbd57cdf6ed0bc8ee80329487ece5dd", "prompt_hash": "8012cbfac15ea67d773ab545b2723e7df5cc2714f770bf6be41e719c28a6d5c2", "target_hash": "0944c865e37f8c103921b7a34e0e125f749fa97ac4639cf4dec6ebb5b65b3136", "exact_match": 0.0}
{"doc_id": 2150, "doc": {"question": "when was the bridge over the hoover dam built", "answer": ["2010"]}, "target": "['2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the bridge over the hoover dam built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1933"]], "filtered_resps": ["1933"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7e03bd125b40389e2cf38081aebb50921f8de819acdd420feab51e8366290340", "prompt_hash": "4a4d338bde1032bc5a4d4dfc33186335d9ded99e99e636f49534337caaf3113e", "target_hash": "2ee1672483fc554adda35e9fdae9ee5db286e7d25dde7bd484842b76ac64c784", "exact_match": 0.0}
{"doc_id": 2151, "doc": {"question": "who sings the theme song for the proud family", "answer": ["Destiny's Child", "Solange Knowles"]}, "target": "[\"Destiny's Child\", 'Solange Knowles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the theme song for the proud family?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ciara"]], "filtered_resps": ["Ciara"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7235893c169e2bf899111fd085f58551cc5c6cf543c761886e20d50e4ed9428a", "prompt_hash": "4da14f425209eacfba9441807f79a6ecaecc93141d4cc9aab213d30fc73e6e7f", "target_hash": "f1f4b5bfadd5c1e330f5ce8eacb050ed6b8a7766e0f4c3a1ce0634e67b06b281", "exact_match": 0.0}
{"doc_id": 2152, "doc": {"question": "who has the most votes for the all star game", "answer": ["fan voting"]}, "target": "['fan voting']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most votes for the all star game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dodgers"]], "filtered_resps": ["Dodgers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b892de900170f2358ae0f5b84bad322f9022ae5cdeba1328ca3700f02dadb1e7", "prompt_hash": "5934f8a7e23c27efcb060f8ae126f9f3f5b6d98c99b14a33b8111bda09476292", "target_hash": "71809c1f115d0f9d5be983dcb54a89617a9f9f8d3013b51993b85c28e6b8b0da", "exact_match": 0.0}
{"doc_id": 2153, "doc": {"question": "when did john hart play the lone ranger", "answer": ["from 1952 until 1953"]}, "target": "['from 1952 until 1953']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did john hart play the lone ranger?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1941"]], "filtered_resps": ["1941"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8e847eba06d5eb0598a17be4ad8497c46b41d6f0dc287fd027a81bcab84ad0f", "prompt_hash": "c92330b5057c4006b5185ec377fc84f7a2ef6811f657fce3139ae72c4aac843b", "target_hash": "c0367827c868ff801e599df11638b008d410497e573bfa2b1f5dcb5101863d70", "exact_match": 0.0}
{"doc_id": 2154, "doc": {"question": "who was assassinated during a visit to sarajevo in bosnia", "answer": ["Archduke Franz Ferdinand of Austria"]}, "target": "['Archduke Franz Ferdinand of Austria']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was assassinated during a visit to sarajevo in bosnia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Archduke Franz Ferdinand"]], "filtered_resps": ["Archduke Franz Ferdinand"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "10d51662e8f785ef977917e3059365592df6547db799c3a149fe290ff1c415d9", "prompt_hash": "309ea97282bf6fd0cf08d708587e22ac2aae4f66dff6b3564426e006c6716ba3", "target_hash": "75f2e9acca4350194b09f5f93f82deebe0aea0f7ff6f6e83c96ef54730bfdd19", "exact_match": 0.0}
{"doc_id": 2155, "doc": {"question": "who won big brother head of household canada", "answer": ["Kaela"]}, "target": "['Kaela']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won big brother head of household canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chris A"]], "filtered_resps": ["Chris A"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "23bdd055f08411929be4f9c39ebe453db4c47c2db467d25ea135dca660ac46bc", "prompt_hash": "feaae4e163313c98449272acb2a954bd428c4d57391cc9d5a0e19bad5b9cd637", "target_hash": "802c367caa40f0964323fce253c5c2f7030b8762dedb6fb5f1c2ade61e401c94", "exact_match": 0.0}
{"doc_id": 2156, "doc": {"question": "who sings the theme song for degrassi the next generation", "answer": ["Jakalope", "The Degrassi Junior Strings"]}, "target": "['Jakalope', 'The Degrassi Junior Strings']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the theme song for degrassi the next generation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Serena Ryder"]], "filtered_resps": ["Serena Ryder"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fe87c9d06f3ef0720e486bf024bf70f7a0ad142d89a8cde9bb0aa4b7cfe63c81", "prompt_hash": "d913765353d0b41fa9db0479cbe7db42c6074d8ae98102dffe96de98683f7e0d", "target_hash": "af3a63151f0bcc7c21d6eaecd6aabc33d0256cf62a1e92c87dc4e050229ab97e", "exact_match": 0.0}
{"doc_id": 2157, "doc": {"question": "who is the authority or governing body of mca", "answer": ["Indian government"]}, "target": "['Indian government']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the authority or governing body of mca?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the minister of communication and information Technology"]], "filtered_resps": ["the minister of communication and information Technology"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "70ccfd95d4560fb99eb00b3bef8477749a05e25e4b419fe7cde6405b3863b1c7", "prompt_hash": "1252a472cb4e8b65e05a880dcc7acce7f77905df94a0f589d094eedd341f9e72", "target_hash": "0afeaae662e681926509c171f14b85fa9c181be7760b84788383bd720d218083", "exact_match": 0.0}
{"doc_id": 2158, "doc": {"question": "who played junior on in the heat of the night", "answer": ["Christian LeBlanc"]}, "target": "['Christian LeBlanc']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played junior on in the heat of the night?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rod Steiger"]], "filtered_resps": ["Rod Steiger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e3bb2549522934db0a93afae2cb1e55bc7d3f7afb6db16aa1e8989934f4cab0", "prompt_hash": "3c4ade7fbad10c9df5fb43b14145553b71e276135ed4405c1250b52a21432ace", "target_hash": "1e6eceae496f525315b0fc7d1782ea6d11aab6803b15097c874f0b8e2d0f4ffc", "exact_match": 0.0}
{"doc_id": 2159, "doc": {"question": "the tenure of the office of the lokayukt of up", "answer": ["eight years", "8 years"]}, "target": "['eight years', '8 years']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the tenure of the office of the lokayukt of up?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3"]], "filtered_resps": ["3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6c73406b998b75bf100534e948a10027a85ee02516652de794261a421bef4d4f", "prompt_hash": "6d1cefdcb13ebd1e579888857a8ad6898775a0598dbc13562a446bed158e5eff", "target_hash": "ce321aa620e186cf58f7e4cecb9931db104202b21f513555a0c049b506a531ef", "exact_match": 0.0}
{"doc_id": 2160, "doc": {"question": "all the gases in the earth's atmosphere", "answer": ["Water vapor", "small amounts of other gases", "nitrogen", "oxygen", "Methane", "Helium", "Oxygen", "Carbon dioxide", "Nitrogen", "Neon", "Argon", "argon", "carbon dioxide"]}, "target": "['Water vapor', 'small amounts of other gases', 'nitrogen', 'oxygen', 'Methane', 'Helium', 'Oxygen', 'Carbon\\xa0dioxide', 'Nitrogen', 'Neon', 'Argon', 'argon', 'carbon dioxide']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: all the gases in the earth's atmosphere?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" nitrogen"]], "filtered_resps": ["nitrogen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44a34b0355dacdbf67c011231f05d021f48147b0e29ac95b1028ff3a8d5b3e04", "prompt_hash": "3f47a3470e6da94d99d55db724764a22d5bfba761b08b13518c2dd13d31e49e7", "target_hash": "ceed774874329bf24ab50fcf2811fedde49dcf6f8f01b088b1f14d1e3a0fec35", "exact_match": 1.0}
{"doc_id": 2161, "doc": {"question": "who's playing in the world series and when is it", "answer": ["Houston Astros", "Los Angeles Dodgers", "October 24 – November 1"]}, "target": "['Houston Astros', 'Los Angeles Dodgers', 'October 24 – November 1']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who's playing in the world series and when is it?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Houston Astros"]], "filtered_resps": ["Houston Astros"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d75585db7498f35c5c053da7269db5ee2d74b489754fa77ec22997ebc2152a4", "prompt_hash": "d8b4d9d67816f93014937a66759efb0091a52ec748f2c352a36364d129bad18f", "target_hash": "a8523bebeb4d2c394b0ac798d245a9cc5c1a4b59fc2f4c02522f7cf7b579be1c", "exact_match": 1.0}
{"doc_id": 2162, "doc": {"question": "which domain of life are humans members of", "answer": ["Eukarya"]}, "target": "['Eukarya']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which domain of life are humans members of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Eukarya"]], "filtered_resps": ["Eukarya"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f84f70638ceee13ffc7bb2cc433fe9c17c09584e55de6e254fd1ad3cd6cbae96", "prompt_hash": "06d646a9993b8e86075df976b0d53b92f9d8674b057833b417a42a2e72e3c849", "target_hash": "44a0d46ad1f1144fb529c4959d69334c609f845c8b81c66d2fb0c8ed29ec61d7", "exact_match": 1.0}
{"doc_id": 2163, "doc": {"question": "who does the voice of the elephant in sing", "answer": ["Tori Kelly"]}, "target": "['Tori Kelly']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of the elephant in sing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael J"]], "filtered_resps": ["Michael J"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9902b1d61441799df6a5fc993b70b424a522e5082f05a9c5c7cfd2992b91b77d", "prompt_hash": "68726c097a50c084704dba8b8db8b7c153be521e9bbd29fbba716abcf132f1ba", "target_hash": "6d1fe6380e8f9bafeaaf7bc27b51081f3171041252937b005ded04f428881603", "exact_match": 0.0}
{"doc_id": 2164, "doc": {"question": "who propounded the idea of basic education in india", "answer": ["Mahatma Gandhi"]}, "target": "['Mahatma Gandhi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who propounded the idea of basic education in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jawaharlal Nehru"]], "filtered_resps": ["Jawaharlal Nehru"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9f034ca19a42da19d6a18dc4f2c28bd8a19d6461dbca55ab76aa4ed24dc12fc8", "prompt_hash": "4598b39f387fc74aa82a79315ddad062098e0efd887369a9810a47e25c7ad933", "target_hash": "372af63225cb4a0444c6b2f6514e66913e8cd939b248560fe0148412ccb4e3ea", "exact_match": 0.0}
{"doc_id": 2165, "doc": {"question": "who is the most followed user on instagram 2017", "answer": ["Instagram's own account"]}, "target": "[\"Instagram's own account\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the most followed user on instagram 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cristiano Ronaldo"]], "filtered_resps": ["Cristiano Ronaldo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f4ddb310295fd4ac8af22cb3ff0bdad15b047f813a1c4d2c745fceb29931e9b9", "prompt_hash": "834d6f4b0e3d00743b7c609836a7512511ebd491f3c24e73add90efa9eaf6b09", "target_hash": "be6d5e766403ccdeb40136b0bb3a412a6e8a08f47f955ff3571df11e48a3d146", "exact_match": 0.0}
{"doc_id": 2166, "doc": {"question": "how many champions league trophies does arsenal have", "answer": ["13"]}, "target": "['13']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many champions league trophies does arsenal have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f89d9c15b7d82e53a3e935877ac82ac57e6f00d2934a52374ff3ce4179a1c5ee", "prompt_hash": "bded1039539355b376d8b2d9442e03c96f85c0724cd32aa7595d512de04968a4", "target_hash": "05e0e0dc2f8231abf8e5c19f60fa8651372432a4f6f7eb1a65428e75d09a953b", "exact_match": 0.0}
{"doc_id": 2167, "doc": {"question": "when is the next deadpool movie being released", "answer": ["May 18, 2018"]}, "target": "['May 18, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the next deadpool movie being released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Deadpool 2"]], "filtered_resps": ["Deadpool 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ab5267618de829a1a72440075b56c8e22baecd0e25204ab1688b19913ab73850", "prompt_hash": "9c4fb3ac9400458c26b21d45050cc73d045b7842034d268d2af7791a77be0673", "target_hash": "1ed2050d35a3ed923af806f6a9515c03274cf4ac7a1e603213f4c095ed4f9fb2", "exact_match": 0.0}
{"doc_id": 2168, "doc": {"question": "who won the academy award for the deer hunter", "answer": ["John Peverall", "Michael Deeley", "Peter Zinner", "William L. McCaughey", "Michael Cimino", "Barry Spikings", "Richard Portman", "Aaron Rochin", "C. Darin Knight", "Christopher Walken"]}, "target": "['John Peverall', 'Michael Deeley', 'Peter Zinner', 'William L. McCaughey', 'Michael Cimino', 'Barry Spikings', 'Richard Portman', 'Aaron Rochin', 'C. Darin Knight', 'Christopher Walken']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the academy award for the deer hunter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christopher M"]], "filtered_resps": ["Christopher M"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf34c0d402ffaf586a016cb4ab874ea491941a4f2eeaaa000b00d649e7706ff2", "prompt_hash": "96a5c7b945bab6d5ed0f0d0517583f06135046676fd9f1d05b5ef5a5f4444f91", "target_hash": "cecff65ed82e52f4d05fbdd409dab0b621a23d1c9bc51e8f3ec747e909abff8d", "exact_match": 0.0}
{"doc_id": 2169, "doc": {"question": "where was part of the classic surfing movie endless summer filmed", "answer": ["Ghana", "Hawaii", "Nigeria", "Tahiti", "Australia", "New Zealand", "Senegal", "South Africa"]}, "target": "['Ghana', 'Hawaii', 'Nigeria', 'Tahiti', 'Australia', 'New Zealand', 'Senegal', 'South Africa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was part of the classic surfing movie endless summer filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Australia"]], "filtered_resps": ["Australia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d4c7500b8646c942889ea67965a6c2a4252b80a9955ecaa77ce56805a6842ba", "prompt_hash": "2718e999a8be52166801ab8e9d72b5030ca764cc6280dc2f57b2babb123d1240", "target_hash": "332f883dcca1958d85971e5fcfff96cffba6a9aaf5a31562e1031abceff15301", "exact_match": 1.0}
{"doc_id": 2170, "doc": {"question": "what percentage of the earth's surface is water", "answer": ["78%", "roughly 78%"]}, "target": "['78%', 'roughly 78%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what percentage of the earth's surface is water?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 71"]], "filtered_resps": ["71"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55e7d98e78ad196621ee15d6debca9ba8295f6a851ed694206ae2fe16de63f33", "prompt_hash": "3e76fd6a9cb7590dc3e24be8b6c741320f6733d0ab8a85d34af356e61304de51", "target_hash": "b140bd3cbcf68bcf778944a6ef8035d4149758884f2a68d9cc9c692849cae61d", "exact_match": 0.0}
{"doc_id": 2171, "doc": {"question": "who killed missy in a bend in the road", "answer": ["Brian", "Sarah's brother, Brian"]}, "target": "['Brian', \"Sarah's brother, Brian\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who killed missy in a bend in the road?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" a truck driver"]], "filtered_resps": ["a truck driver"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32b6f79faf87ad842935d1e434a33cf02886f6f2eaaa733d14e2ece0db220b08", "prompt_hash": "988f9fa0ce410fef66601fdfe45866a4f6cc42f82972cde50225281ffd82c597", "target_hash": "8232669d1da42c4bd7ab0e55d84b3ddbf73e2577882f7d584f98452a6dbb9e7a", "exact_match": 0.0}
{"doc_id": 2172, "doc": {"question": "who became king of erebor after thorin dies", "answer": ["Dáin", "his cousin Dáin"]}, "target": "['Dáin', 'his cousin Dáin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who became king of erebor after thorin dies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dain II Ironfoot"]], "filtered_resps": ["Dain II Ironfoot"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "671ae714dc0f15b29653bbbd2b0412bcefd561081f2ab2415303f0762f8fb2d8", "prompt_hash": "bae565e94c8dbe93dcaf2390fa680f0ba4eb1025c439b25456e8342824cf5bc6", "target_hash": "35fbfd6d23c3dbf6ec0ef4a75dd0992fa3a7e1ec1afc83ff5c114884d3052799", "exact_match": 0.0}
{"doc_id": 2173, "doc": {"question": "what does the ghost of christmas present sprinkle from his torch", "answer": ["warmth", "a little warmth"]}, "target": "['warmth', 'a little warmth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does the ghost of christmas present sprinkle from his torch?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" tears"]], "filtered_resps": ["tears"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "13cee69ba07cb4ad7b0a6b0f26b7bbf60d724c4313e00334c66d897627529265", "prompt_hash": "e80d5a7e0b364329a6f3e0c16c4f9a4b531ffaff2b82e420160b5d888d007bc9", "target_hash": "05b0b18eccc81299a1cf0251c8eb8647487eda46f8d72d635c37ba00cddb712c", "exact_match": 0.0}
{"doc_id": 2174, "doc": {"question": "who wrote the song fly me to the moon", "answer": ["Bart Howard"]}, "target": "['Bart Howard']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song fly me to the moon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Burt Bacharach"]], "filtered_resps": ["Burt Bacharach"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bc4aad4e0c362c3b38d4beed83a6fe429b49223663498084a7e5059da8469314", "prompt_hash": "d9af77bb06975fe5276f51aecba008d18d4da51874cbfeef201d6856d1831bf5", "target_hash": "bc15d5e09440d8272e1d8b42601e7c9f253e4d314629242e888694bc2e377a24", "exact_match": 0.0}
{"doc_id": 2175, "doc": {"question": "who wrote song what a friend we have in jesus", "answer": ["Charles Crozat Converse", "Joseph M. Scriven", "William Bolcom"]}, "target": "['Charles Crozat Converse', 'Joseph M. Scriven', 'William Bolcom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote song what a friend we have in jesus?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Charles H"]], "filtered_resps": ["Charles H"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b030506c524ef788bab6d3e4987c9fa305648e441a289abefb28dfb6021a4161", "prompt_hash": "c9e08d58befb940f854fca50c9d0e932e7ce35a711b6d424fc8605cf921b50c6", "target_hash": "0bf8e820cac9e6e2307d7ad04b39a207ff7fdb3dd197dcd34f9c91a9a791fa34", "exact_match": 0.0}
{"doc_id": 2176, "doc": {"question": "who sang never gonna let you go", "answer": ["Joe Pizzulo and Leeza Miller", "Joe Pizzulo", "Leeza Miller"]}, "target": "['Joe Pizzulo and Leeza Miller', 'Joe Pizzulo', 'Leeza Miller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang never gonna let you go?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rick Astley"]], "filtered_resps": ["Rick Astley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "47ca33fd2918098c96f14fe5dad32103b037252d6829ffc185a74cb53eb03f19", "prompt_hash": "de98ed004bdd77461a9ddd590c6ed803e76e8031ccf224219bd4f8ebb0178427", "target_hash": "32b67485da97b7140595e27dc6c11181fed8569bc66d52a543abd7437495b753", "exact_match": 0.0}
{"doc_id": 2177, "doc": {"question": "when will miraculous season 2 episode 11 be released", "answer": ["30 March 2018", "27 October 2017"]}, "target": "['30\\xa0March\\xa02018', '27\\xa0October\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will miraculous season 2 episode 11 be released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d46b818471c7d6d385fcaa923d24fa1c80c98f607e00130a3db4c56a9cb6446", "prompt_hash": "857491ab4c3a468a1c88a77465f4659b8363209c2ab0c7844170fd2817331b36", "target_hash": "4ff92f0e7c9fe0f06c1ee9e36c31ee6ce320540036c3aa93116fcad84ff9d5b5", "exact_match": 0.0}
{"doc_id": 2178, "doc": {"question": "who destroyed the first temple and exiled the two tribes of judah to babylon", "answer": ["Nebuchadnezzar"]}, "target": "['Nebuchadnezzar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who destroyed the first temple and exiled the two tribes of judah to babylon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nebuchadnezzar"]], "filtered_resps": ["Nebuchadnezzar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7d943a82923313ca6dfb5a918f6cddf7d291ac60d97f0a79bbec67afb112cb54", "prompt_hash": "203c69d4ec36c886dbda5070a9383a13f85617d31e78bd2ea164782d3bc1d8ef", "target_hash": "cf057d0ab9152e4111e327e1791aede401e935c0c316589cbea81a0976e58db6", "exact_match": 1.0}
{"doc_id": 2179, "doc": {"question": "who played forrest gumps mother in the 1994 film", "answer": ["Sally Field"]}, "target": "['Sally Field']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played forrest gumps mother in the 1994 film?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sally Field"]], "filtered_resps": ["Sally Field"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3942d7728a26156b8556d90528809d23fdae1f3be0ce2a2ddc2e2129d6f203ab", "prompt_hash": "7fc569af688e66fb2ddc9e20c0dc0b4b727d9677db3ebc269e83d3b6a30e9d16", "target_hash": "d5a45f163352f82cdac4445537b485d73035d88d3a7063703085ec8683582b13", "exact_match": 1.0}
{"doc_id": 2180, "doc": {"question": "where is iodine found in the periodic table", "answer": ["heaviest of the stable halogens"]}, "target": "['heaviest of the stable halogens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is iodine found in the periodic table?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 15"]], "filtered_resps": ["15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9842dc36e1641641048a9ecd85fee50a82653a55058c52eab2e650e52eecafef", "prompt_hash": "c3a867bb0d9cdca343003c7ddbd9f44697185e1472162fad9fe8b205b8a3e584", "target_hash": "b01c8178966d12974622a0d614621893907baa82b18080a6f00fe505dc7b5a8c", "exact_match": 0.0}
{"doc_id": 2181, "doc": {"question": "what is the name of india pakistan border", "answer": ["International Border", "IB", "International Border (IB)", "India and Pakistan Border", "The India and Pakistan Border"]}, "target": "['International Border', 'IB', 'International Border (IB)', 'India and Pakistan Border', 'The India and Pakistan Border']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of india pakistan border?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Line of Control"]], "filtered_resps": ["Line of Control"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "26c0c63cb4de68e692444bf73972847aae8ad470cf2312fdd81147324b6baa85", "prompt_hash": "6919676fd619543eb38121c264ba16d9e8f9d239d6719c022ff3e99c6ddd37e9", "target_hash": "7da9995e7d35395f1c4b3d15cbdf420268217303d23f93e339125377b0e64a51", "exact_match": 0.0}
{"doc_id": 2182, "doc": {"question": "who made the song we are the world", "answer": ["produced by Quincy Jones"]}, "target": "['produced by Quincy Jones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made the song we are the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Jackson"]], "filtered_resps": ["Michael Jackson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92108a0e80e80a0e888d93821a10d33cf33c00d909a7144f4069caee60ae02b3", "prompt_hash": "17a655956ef36847e6dd7b7782953544ec09aa0a29c83ce0617f3eb700df35eb", "target_hash": "057364c85b972499b6f3850a12c21bec08f238feaffb72606856f9ff19ea5ffe", "exact_match": 0.0}
{"doc_id": 2183, "doc": {"question": "whats the difference between tomato paste and tomato puree", "answer": ["consistency"]}, "target": "['consistency']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: whats the difference between tomato paste and tomato puree?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" tomato paste"]], "filtered_resps": ["tomato paste"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "08d06710cc7bce479741c1ce0ca41329a0a01d302312ec4c82007011ae91bddf", "prompt_hash": "03a1eb253aeb7faed6755cf0e0658d102136e24e4f4dbd73338af77bcda7a0cb", "target_hash": "08b32ba44c71bc3dba07177e1156671fd0951f70c07db74214f17fde35051cdc", "exact_match": 0.0}
{"doc_id": 2184, "doc": {"question": "when did chicano studies become major studies programs on college campuses", "answer": ["By 1975", "1972"]}, "target": "['By 1975', '1972']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did chicano studies become major studies programs on college campuses?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1960s"]], "filtered_resps": ["1960s"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92bedcd0d92a667f26c3a0825eb9d435b32697cc1b19cd5923c4b92dc210583a", "prompt_hash": "da3e4fd4d17ccbec9a636785ac2a2d4190549f3976498f81776e4be53e00cbf7", "target_hash": "22725925d1438a9cd7fd39e118e3f8d455b16a1125a9f6ad770c88307180efd2", "exact_match": 0.0}
{"doc_id": 2185, "doc": {"question": "who is the actress who plays baker on blue bloods", "answer": ["Abigail Hawk"]}, "target": "['Abigail Hawk']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the actress who plays baker on blue bloods?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Esposito"]], "filtered_resps": ["Jennifer Esposito"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3bfa2caadc30b4c08f43fd62863a5ea91dc4852ed89d25800b696335ea6d3fca", "prompt_hash": "3312f923bc1cd203b4958fa7310c82367bd757cb77d8345a54856b5a3ce98b03", "target_hash": "b2cef558e8bd7b2c8d0456939a57c0ae61a095f7895d7b5329ddfe38b2598c99", "exact_match": 0.0}
{"doc_id": 2186, "doc": {"question": "who was the killer in the movie i know what you did last summer", "answer": ["Ben Willis"]}, "target": "['Ben Willis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the killer in the movie i know what you did last summer?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jason A"]], "filtered_resps": ["Jason A"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed8b04f83af2815ed44ea1a1e2e873b52c5ceb91c21da5ca8768f1cbe75bb7a7", "prompt_hash": "9c9d2253e349e686e2568412cc8db8734da1168ebbcd9f106f171983bcee9943", "target_hash": "7391bf22de2a54088ebe3700325f9086541949a5b5a6333c532a227f835b960f", "exact_match": 0.0}
{"doc_id": 2187, "doc": {"question": "how many countries touches the land boundary of india", "answer": ["seven"]}, "target": "['seven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many countries touches the land boundary of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 6"]], "filtered_resps": ["6"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d45943a7d394bf9f9822acc09292821b92b12bbe9a37a9b1194dfbea9ef284b7", "prompt_hash": "95f74a31c7d9705fd94385352425efecc63b73aa55c651ff21bec2b5f96dbd98", "target_hash": "0df021bc2ee5f7a96ecc2aed6d1e8a6bbd0fc254176d8606fc5cf30f5137cd86", "exact_match": 0.0}
{"doc_id": 2188, "doc": {"question": "who played the wicked witch in wicked on broadway", "answer": ["Rachel Kelly Tucker"]}, "target": "['Rachel Kelly Tucker']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the wicked witch in wicked on broadway?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Idina Menzel"]], "filtered_resps": ["Idina Menzel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "540088bbd4926de60f152684ae10f91d4a69a5292c105b62785d451380b74c95", "prompt_hash": "71256b0b45f126b3559f85f2f29f4dadb941b63635b968fa9dc7d26913ca9ddf", "target_hash": "fa226e40f55534832a84fe6d94b3e91321d0a88dfae27233ac7188cf4352414d", "exact_match": 0.0}
{"doc_id": 2189, "doc": {"question": "when was the last easter fell on april 1", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last easter fell on april 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2011"]], "filtered_resps": ["2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c7f359a1092310eff58e79ed6787257ad148f33f12f6420a782b5f0901bea798", "prompt_hash": "4bfaabfa74aab5b0382f392c533b40298a311ecbab61106ec0d7059c40999210", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 2190, "doc": {"question": "how many world series wins does the dodgers have", "answer": ["6", "six"]}, "target": "['6', 'six']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many world series wins does the dodgers have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "30581c9be3986c6650254b90c6d4ae9cc753767ce24a251a9654599633061b8d", "prompt_hash": "42f71dfaf13e419d4af2837e92b9276e820d578cbd775e62be2aa24ba2ba07c6", "target_hash": "25b67b0377e96bcf611a32e31c108b5a330c4b9724a4d4d5b11182a8902f643c", "exact_match": 0.0}
{"doc_id": 2191, "doc": {"question": "when did the second it movie come out", "answer": ["September 6, 2019", "September 8, 2017"]}, "target": "['September 6, 2019', 'September\\xa08,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the second it movie come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "191c17533a269881d4da68f66c50bce863532dbc6951514bc763e6a7cf3b2a91", "prompt_hash": "b1e1a80fbb980de1a53d0eb7a61609d8669ebd7f90ed2a80b128bd3c7928c25b", "target_hash": "ef168344458109f997e859ca48162c960e9671c2943769b993a1ad1ab31a1b2d", "exact_match": 0.0}
{"doc_id": 2192, "doc": {"question": "who wrote the guitar solo in beat it", "answer": ["Eddie Van Halen"]}, "target": "['Eddie Van Halen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the guitar solo in beat it?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Eddie Van Halen"]], "filtered_resps": ["Eddie Van Halen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ece932fc0e408e1a0e2535ab5457a5653db779ed9868f2eaf7297c229ec7da9", "prompt_hash": "be47a8490cb7524fb8c4ee3768766e53865beb3bceb48194211b50f74da4f896", "target_hash": "05b5a0ce2183bea5f7a37f901a8adf9551cb1e5c7a14f31424208cf61ce4af0c", "exact_match": 1.0}
{"doc_id": 2193, "doc": {"question": "when did the anti smacking law come in nz", "answer": ["anti-smacking bill", "21 June 2007"]}, "target": "['anti-smacking bill', '21 June 2007']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the anti smacking law come in nz?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2007"]], "filtered_resps": ["2007"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "31e3485aa65773cbd6dbeb40967b7d611d8745e8ff720f9eb2e5fbfd8f378093", "prompt_hash": "fae8d692adf46881674bf740b0e571e1cd78a586bcf05151292d0ed3a826dadb", "target_hash": "f05341ee166289c17125eb3fc04151cc9b126a50916d7ef99cc0bc2d661ca9c2", "exact_match": 0.0}
{"doc_id": 2194, "doc": {"question": "where was the first sonic drive in located", "answer": ["Woodward, Oklahoma"]}, "target": "['Woodward, Oklahoma']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the first sonic drive in located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" West Palm Beach"]], "filtered_resps": ["West Palm Beach"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c483224a8d14725dab55f1ba2a87cbb14305914b9266e8c51958124ddb02437b", "prompt_hash": "bc372fb8dfe430d8ff5a6e8373c79ee9198a85f131abcd50b8c2c8f978f2cd2b", "target_hash": "434f1c031d62c23b2146908682e659bb77f96365cf8ef9fa30658d19d1e283de", "exact_match": 0.0}
{"doc_id": 2195, "doc": {"question": "what episode does caroline come into the originals", "answer": ["Brave New World"]}, "target": "['Brave New World']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what episode does caroline come into the originals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d91b730b386e01fd0606df1317562991925415a21ea833439a9955ed364e693f", "prompt_hash": "57824c9b9123d4cfe29b384935874765338d673a28a158f0f052ae4f53bcffaf", "target_hash": "0437ce064034e92f20a8074e5988f99fca1e1bd201041148ce8ba56b76b4e943", "exact_match": 0.0}
{"doc_id": 2196, "doc": {"question": "who does the voice of amy on futurama", "answer": ["Lauren Tom"]}, "target": "['Lauren Tom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of amy on futurama?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mae Whitman"]], "filtered_resps": ["Mae Whitman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ce613231b659921cfc381aad8fdfef06d84e1abb7f3ea4ae52c6a6c66672ccb8", "prompt_hash": "00fffcbe23c47274a58556db8c819d61142f6fb8f0d84bdf34a6f752f30058dc", "target_hash": "e298eaa5023b93ac8d2dd307fc2223a9d9933a28a3d545c5bdd28e24000b00fa", "exact_match": 0.0}
{"doc_id": 2197, "doc": {"question": "who is the original singer of just when i needed you most", "answer": ["Randy VanWarmer"]}, "target": "['Randy VanWarmer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the original singer of just when i needed you most?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "71b7e0f086f1d62b7887365e5d0595032aa8aade6faa16626378ae8178359d66", "prompt_hash": "9a98c8c7a98938da74abce7b91da3eb46de32423339f1bdb97f06e16da2e9855", "target_hash": "2721dad8344afa77337212d81003f3977aca5b6e88f47f6a6f72011ccdfda5b5", "exact_match": 0.0}
{"doc_id": 2198, "doc": {"question": "the names of the beaches on d day", "answer": ["Juno Beach", "Omaha Beach", "Utah Beach", "Gold Beach", "Sword Beach"]}, "target": "['Juno Beach', 'Omaha Beach', 'Utah Beach', 'Gold Beach', 'Sword Beach']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the names of the beaches on d day?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Utah"]], "filtered_resps": ["Utah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "22158282dc45069dbf477f716acb46a82866ecb94d5a12cddd565908760a71b0", "prompt_hash": "ea21e7b78e3e7661dec38cb6b9de7faf8b97c9a601709834b00c42447cfadbaa", "target_hash": "9ddc5188913bd52f12fd7bb27b5c6f6d621f878f5e6acfb5063bf86b2f2191f1", "exact_match": 0.0}
{"doc_id": 2199, "doc": {"question": "where was a hologram for the king filmed", "answer": ["Morocco", "Berlin and Düsseldorf in Germany", "Berlin", "Hurghada in Egypt", "Düsseldorf in Germany", "Egypt", "Germany"]}, "target": "['Morocco', 'Berlin and Düsseldorf in Germany', 'Berlin', 'Hurghada in Egypt', 'Düsseldorf in Germany', 'Egypt', 'Germany']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was a hologram for the king filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Atlanta"]], "filtered_resps": ["Atlanta"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c8513fec9a9b16db18aa2a29c4442478efecd1baa5151af7d515fd3b2822b0d7", "prompt_hash": "3372732d8c32e7f6c19c7d9d611cd4a48ee2ee314940fcad9599f6d251761524", "target_hash": "4d318ca228529bb3deaab2f4a312ce2399031d5903cd52cd88d6356ce20e09d0", "exact_match": 0.0}
{"doc_id": 2200, "doc": {"question": "who was the guy who died in glee", "answer": ["Cory Allan Michael Monteith"]}, "target": "['Cory Allan Michael Monteith']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the guy who died in glee?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Will Schuester"]], "filtered_resps": ["Will Schuester"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a64619951a9600357402ecfca5d4d2eb208114db71e392aab301c9bd72150847", "prompt_hash": "8e7d56034cdb79295a2e3efc6c6dfccaae9b3819abc0a626d11422771407c257", "target_hash": "7e8187c9089eeadec7f324f72d14907cc89dd9431bd93a93a70e332297fe72b5", "exact_match": 0.0}
{"doc_id": 2201, "doc": {"question": "when was the first epistle of john written", "answer": ["AD 95–110", "probably written in Ephesus"]}, "target": "['AD 95–110', 'probably written in Ephesus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first epistle of john written?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 96"]], "filtered_resps": ["96"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6798bcadf77308c50c251b692b2d79cd714136f4e64254d4ba09eadf9682e1b2", "prompt_hash": "b1ab71b4c76d69fbdb6ddb0472877125d10d7beaac20c6f78fe673f418d58ee4", "target_hash": "c8702b8705c9449d8aa64dea673765e88462224daf9d227550e6fa0dc3adcdb4", "exact_match": 0.0}
{"doc_id": 2202, "doc": {"question": "who made the movie fifty shades of grey", "answer": ["Universal Pictures and Focus Features", "Sam Taylor-Johnson"]}, "target": "['Universal Pictures and Focus Features', 'Sam Taylor-Johnson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made the movie fifty shades of grey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sam Taylor-Johnson"]], "filtered_resps": ["Sam Taylor-Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbfab945fd9bf0f37ee50f21d83630f8acb84ded896330607476b08b8c5f984d", "prompt_hash": "1d82f022b76ddabab5ae011acd12dc4a0f94ff5aee87c0eb8ea2463a1ae42bee", "target_hash": "44d6944359b031e9d8be14e14d123823dad242af22f3b4f9d7106e0fd81d2123", "exact_match": 1.0}
{"doc_id": 2203, "doc": {"question": "which is the tallest building in the world 2018", "answer": ["Burj Khalifa in Dubai", "Burj Khalifa"]}, "target": "['Burj Khalifa in Dubai', 'Burj Khalifa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the tallest building in the world 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Burj Khalifa"]], "filtered_resps": ["Burj Khalifa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7aeb19e3be7bf3a2d0ae0d097a6e1d59abdf731c6ae7510c14bf316ba265ede", "prompt_hash": "387d121ce9d7bd3581ace60734ef70b1d72efd5f470794b80adc68cf14a36db8", "target_hash": "a0d0e4a6c2becb21e4542f64da3383a4278bb5345cc6a15a9aeddb50cc087813", "exact_match": 1.0}
{"doc_id": 2204, "doc": {"question": "who has won the most superbowls as a player", "answer": ["Charles Haley", "Neal Dahlen", "Bill Belichick", "Tom Brady"]}, "target": "['Charles Haley', 'Neal Dahlen', 'Bill Belichick', 'Tom Brady']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the most superbowls as a player?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joe Montana"]], "filtered_resps": ["Joe Montana"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d5ffd5d51a853f47a040247b035d25b1381df3a1d6b51fb0deb6773e22c1dff", "prompt_hash": "a40ecd8faf51a242f04b6519a83f198f307c894b3c7b4b5f8faac8221b725e11", "target_hash": "d208b7a69a9be03a1fbdd3e29f6d95f4d2891325b984f8e3b22337269b44b03c", "exact_match": 0.0}
{"doc_id": 2205, "doc": {"question": "time of the state of the nation address", "answer": ["after 9pm ET"]}, "target": "['after 9pm\\xa0ET']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: time of the state of the nation address?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 20"]], "filtered_resps": ["January 20"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "76badd3819d526d834815789420bd778c91d4307e4d91888d6e2203d2e5af8b9", "prompt_hash": "09666301d91a5de93d36134b59fb4515fbefbdad7bab2e6ff9af4c27e1192460", "target_hash": "f7cb4b1abe934207bb49313dd6407690a41f5e92720fa62d65cc17a3f3fa10c2", "exact_match": 0.0}
{"doc_id": 2206, "doc": {"question": "who sings the rap in baby by justin bieber", "answer": ["Ludacris"]}, "target": "['Ludacris']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the rap in baby by justin bieber?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" T-Pain"]], "filtered_resps": ["T-Pain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "10f639f65e231a213d464b397e7372afe64b805c8292033a41eab7a32ff92ec1", "prompt_hash": "e56cb3785b32542b89e9bd951162ed50f42545cbfaf046ba5d9378c88fd9b231", "target_hash": "4167c416da1a9c655a846c5727069f269c84570ad95ab4aa73febfc199a3f806", "exact_match": 0.0}
{"doc_id": 2207, "doc": {"question": "when is season 4 of flash coming out", "answer": ["October 10, 2017", "on October 10, 2017"]}, "target": "['October 10, 2017', 'on October 10, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is season 4 of flash coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 10"]], "filtered_resps": ["October 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "22a0bafa172b6edebf9a8da91b1312f6b0d98daed3ae2139311d5f3996159eaf", "prompt_hash": "701b015d3dccdccfec0adbb86af1b45dd43c227ee73cc960fe798364060e87f5", "target_hash": "a8d35dd3ce4227ecce416a1378fd54bd6e1c14f8b184a6195dc08a0da7007493", "exact_match": 0.0}
{"doc_id": 2208, "doc": {"question": "who plays christian in 50 shades of grey", "answer": ["James \"Jamie\" Dornan"]}, "target": "['James \"Jamie\" Dornan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays christian in 50 shades of grey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jamie Dornan"]], "filtered_resps": ["Jamie Dornan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "41dabd7c3dbdb97aae6f1a48e2c467d17eda11f9ac031c233ff4d616b2a1c06b", "prompt_hash": "20d4cae1b32823e813731dd3be3f3aa1fdfa05bb3fe6bc337b622b1bb4f07f51", "target_hash": "2bdcb2f13d9172f92349cee83ee48959e5ee91400b618f4a1af90eedebcda142", "exact_match": 0.0}
{"doc_id": 2209, "doc": {"question": "when did stephen curry won the mvp award", "answer": ["2015,", "2015", "2014–15", "2016"]}, "target": "['2015,', '2015', '2014–15', '2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did stephen curry won the mvp award?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "15c4537af6c5553cc27041dd10358127d841b3de25ecf228f4aaef0b4c85c3f2", "prompt_hash": "f164bdb85023fcb47ae72cb221080103e073639ab830ca163a837fca97cb41ab", "target_hash": "9e27c8c720b4e0aed4ac350b066f64d2261d087e13625b3e89abdb025d9ff874", "exact_match": 1.0}
{"doc_id": 2210, "doc": {"question": "where did the cosbys live in new york", "answer": ["in Brooklyn", "Brooklyn, New York"]}, "target": "['in Brooklyn', 'Brooklyn, New York']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the cosbys live in new york?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Manhattan"]], "filtered_resps": ["Manhattan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9b7ae65b6ea3131bf8a1819832455bf626fb07fd39c715327c1c85b708ff147f", "prompt_hash": "75ac1c482b6e887f9cb102f95f655a634766373331f2da984b220c869e0d5594", "target_hash": "e387d0ec40339b196714055de1e5cee466d7e780f3d0901219d05609960b8587", "exact_match": 0.0}
{"doc_id": 2211, "doc": {"question": "who won the world cup in cricket 2017", "answer": ["Pakistan"]}, "target": "['Pakistan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the world cup in cricket 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" India"]], "filtered_resps": ["India"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "61f3097f7eb7f7817e654d2a1969e463f133620791ce3a815dcc29b4d669ddac", "prompt_hash": "142651ec32513b2da95074d6abee661b803c72436de0202a50a76fc0e8b01ee9", "target_hash": "3d291c4b54e6edc7fdfa6b5ca636d65ecacf962508dda37b5449539ffbb58b72", "exact_match": 0.0}
{"doc_id": 2212, "doc": {"question": "who invented the cornell method of note taking", "answer": ["Walter Pauk"]}, "target": "['Walter Pauk']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who invented the cornell method of note taking?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walter Pauk"]], "filtered_resps": ["Walter Pauk"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7047bca8b266f5d6dc9fc5b0f2aaeb32eba831c24f4fc255e36dd464a33c65a", "prompt_hash": "c0227caa0624affc069646b8237dec407b7088524d8c6fd0603e41c306fd350e", "target_hash": "46f03a3fff5011b833bb99c068e4a46a20624ade515d6ed924b6b92f1315846b", "exact_match": 1.0}
{"doc_id": 2213, "doc": {"question": "where does the water from the california aqueduct come from", "answer": ["Sierra Nevada Mountains", "the Sierra Nevada Mountains"]}, "target": "['Sierra Nevada Mountains', 'the Sierra Nevada Mountains']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the water from the california aqueduct come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Owens Valley"]], "filtered_resps": ["Owens Valley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "565e9f30517128bbd63d7ac38101daafaefcc075b9a83433b1a8e90ee24edf86", "prompt_hash": "ff018e9c663cc2b0562ad17721988441a585a1352306f4050541fb77ebed2af2", "target_hash": "b3105b04a8738e329ccd6437c2811049c5d9d5de160e5d57177d0de9fa7996c7", "exact_match": 0.0}
{"doc_id": 2214, "doc": {"question": "what was the name of the first continent", "answer": ["Pangaea"]}, "target": "['Pangaea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the name of the first continent?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pangaea"]], "filtered_resps": ["Pangaea"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "992b9ff21d9204294b8f4f08f11c044a5c273d22de60917dbb003bed9272e45c", "prompt_hash": "fe2deadb019157e4b0ef75856b609ebc909f634988bfa53b58a8f08b15f6899e", "target_hash": "32d61ef839fec05245a524b84e156a8b6981169254f68825449666295979b0cd", "exact_match": 1.0}
{"doc_id": 2215, "doc": {"question": "phase change from gas to solid is called", "answer": ["Deposition", "desublimation"]}, "target": "['Deposition', 'desublimation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: phase change from gas to solid is called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" deposition"]], "filtered_resps": ["deposition"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "961c8df5582492fd57a6c5dc29130b8e4cdd3960c39a35bcfb9782389e099799", "prompt_hash": "ad91b73f988449f19ec239a7f4f15d137f39b0ab8695b53b81f715d6b7453894", "target_hash": "baebf23b25df7c7acb440da59f0e0d38cb432f07be4bc74a6031763d9fa5fbf3", "exact_match": 1.0}
{"doc_id": 2216, "doc": {"question": "who sang the song should have been a cowboy", "answer": ["Toby Keith"]}, "target": "['Toby Keith']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song should have been a cowboy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dwight Yoakam"]], "filtered_resps": ["Dwight Yoakam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f00686fc05b4c7a332c2dcf20c373de62ac22d386669a6446a4eb0c19bc99a6c", "prompt_hash": "bf94fc6158c087835c3b0bd2f43b29c629f52707467685e886254fb52e28f326", "target_hash": "4bf13d06b4a3d3a94637911487961c0a812ba7a85ef061c770d9f383442b08c8", "exact_match": 0.0}
{"doc_id": 2217, "doc": {"question": "when was the first mad max movie release", "answer": ["12 April 1979", "April 1979", "1979"]}, "target": "['12\\xa0April\\xa01979', 'April 1979', '1979']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first mad max movie release?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1979"]], "filtered_resps": ["1979"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ceae8d917d84232d812aff8bd07066a6f1daeaa8a019608e29a048c2bc33c11", "prompt_hash": "e978a87c9e99b8d4bfa57fe47939ddd9d14288289a7e4596e1662cbc3433a6ce", "target_hash": "08574c6af83346fe74c11e089fd54cc77f6304e0fe83d695e48202b2decb3fb6", "exact_match": 1.0}
{"doc_id": 2218, "doc": {"question": "a political leader during the roman empire was called", "answer": ["emperors"]}, "target": "['emperors']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a political leader during the roman empire was called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" emperor"]], "filtered_resps": ["emperor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dae39710dd9c40905457a1739c9b7ba65a699dfa15b83284debc8a82a9c97ba3", "prompt_hash": "896a66bfb24bc12bb558c97bf77aa8c0d04139630b0de807d34fca1bc240c828", "target_hash": "2513b2e6cd8ee4937ea47c4cb33a65228fd36fe35deffa6605b40cdafeec2575", "exact_match": 0.0}
{"doc_id": 2219, "doc": {"question": "where was uncle tom's cabin first published", "answer": ["The National Era"]}, "target": "['The National Era']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was uncle tom's cabin first published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Atlantic Monthly"]], "filtered_resps": ["The Atlantic Monthly"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1bf011e902950d36e193bf29bd254f61d3661e2364d82ae759b08b2e7381698b", "prompt_hash": "a2d67eba4b03c7cbf308357e8363a4b6f6b40299472e7bb14c85a83dddf5a97e", "target_hash": "5ee8c27524d059aa968f074c9549ee633e92eb6544fba3d079c9bbe5a6ad3aad", "exact_match": 0.0}
{"doc_id": 2220, "doc": {"question": "where does the last name roberts come from", "answer": ["Norman", "Norman origin"]}, "target": "['Norman', 'Norman origin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name roberts come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Welsh"]], "filtered_resps": ["Welsh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf3b47318cefce0e2c732b903ea80dcbb45be0f64fd3f72ed50000c5ffd016be", "prompt_hash": "173da46dad422b911fc6facd412f97fed7cff38f23f06efbc4f546c7a7acf303", "target_hash": "f65180a5c8ce2bd66411718fc6d472aa8a6780e9a11f65e77f0e6e9a3e6d5f14", "exact_match": 0.0}
{"doc_id": 2221, "doc": {"question": "when is the world consumer right day celebrated", "answer": ["15 March every year", "15 March"]}, "target": "['15 March every year', '15 March']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the world consumer right day celebrated?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 15 March"]], "filtered_resps": ["15 March"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "066a1ea6d7df3ce749f279ae2e25122f00b030339d91524e13e625eeb0203e83", "prompt_hash": "69059603e9eb26e40aced9310624a9663d80b0164c8f8336b48d262863fa54e6", "target_hash": "aa052802b6f7a7cee9de96d94ad194c2149444e489c302365f11638b120e8d46", "exact_match": 1.0}
{"doc_id": 2222, "doc": {"question": "royal society for the protection of birds number of members", "answer": ["more than a million"]}, "target": "['more than a million']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: royal society for the protection of birds number of members?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 100"]], "filtered_resps": ["100"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "33f793257cb642d358bb427ae9139c677d50e840f0edeb1520c337135b4cd3ed", "prompt_hash": "5dd6e6d963d92301d88253fa5f31ab6e6f987d637ba2cd7cf24ee58f917bfe87", "target_hash": "acf4b98b6c4aab325e9de84a51f87e56bfc07bce95be1b2663e9138ba6e5f9f2", "exact_match": 0.0}
{"doc_id": 2223, "doc": {"question": "how many wing stops are there in the united states", "answer": ["1,000", "more than 1,000"]}, "target": "['1,000', 'more than 1,000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many wing stops are there in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11"]], "filtered_resps": ["11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0576c5e743eb99d87aa44f5ec3809e7d6547aac1a813b77c57d611ceecfd2406", "prompt_hash": "fcfc7cc288a48eb3fb7beaf226e5ac2a8d7873afbe58df118ce263b4247c0c8a", "target_hash": "205fb0904776d0242ac81fae9a6b9330d3005dda5fe845fd9296d63a87cf1757", "exact_match": 0.0}
{"doc_id": 2224, "doc": {"question": "where do pineapples come from in the world", "answer": ["Costa Rica", "Philippines", "Brazil", "the Philippines", "Thailand", "Indonesia", "India"]}, "target": "['Costa Rica', 'Philippines', 'Brazil', 'the Philippines', 'Thailand', 'Indonesia', 'India']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do pineapples come from in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hawaii"]], "filtered_resps": ["Hawaii"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2441a0e5af43588b878d21ef553febf66441790bdaf7ff66fad5e5c78c4330c3", "prompt_hash": "3f33c5ba1e7a8879fa93485e0f84ff128d9869fbba0119515ef72bf4c71b3113", "target_hash": "12aa3e08284c6eaa19de2544edc96913f6e97a406998ae2a21fefc82546f84ee", "exact_match": 0.0}
{"doc_id": 2225, "doc": {"question": "where does blonde hair green eyes come from", "answer": ["a low concentration in pigmentation"]}, "target": "['a low concentration in pigmentation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does blonde hair green eyes come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Scandinavian"]], "filtered_resps": ["Scandinavian"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55cc280a5a945ea097ccb29576bd2cf3bed8c17000204d1d51059e8d80d47532", "prompt_hash": "c393614e4057970ef2018f016768f39034836b2e409bc65e21cb31cd81307275", "target_hash": "7ae4ea9d76cc80005744ee6a01fd79df168ef1628bbc4b28776b2f9b8f6635dc", "exact_match": 0.0}
{"doc_id": 2226, "doc": {"question": "what is the second book in the alchemyst series", "answer": ["The Magician"]}, "target": "['The Magician']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the second book in the alchemyst series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Immortal Rule"]], "filtered_resps": ["The Immortal Rule"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f59a0feea0f91b085464318d76ef27cdee79a193e67dd4b0fe1baf970027886f", "prompt_hash": "c672e1ed7b66359b02708b303d27c2e53015fa4d055d9ffff895a45bd43645a5", "target_hash": "330ea359f0c778b7e382ec139f83a9abc807864435a4f259c5f67ba1b149ce14", "exact_match": 0.0}
{"doc_id": 2227, "doc": {"question": "skin that covers the palms fingertips and soles of the feet", "answer": ["stratum lucidum"]}, "target": "['stratum lucidum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: skin that covers the palms fingertips and soles of the feet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" epidermis"]], "filtered_resps": ["epidermis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "242180d2fdcc28142a77986b56b708951a4d176c919e930d6503ecd205118f74", "prompt_hash": "e81706b668d8af1d46f2b2b13ba08dc2b28c25996189df581b0e74341a289b6b", "target_hash": "7b8f9347cf926c3ad96e56452b2ec6c52e51ea799a931ee6f06910a6e724cd63", "exact_match": 0.0}
{"doc_id": 2228, "doc": {"question": "what is the latest version of chrome for linux", "answer": ["67.0.3396", "67.0"]}, "target": "['67.0.3396', '67.0']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the latest version of chrome for linux?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 57"]], "filtered_resps": ["57"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a598d37f43021e8c3e607c79197d60faa2ba5e65dd636c0374de9e6dbfd2bf2f", "prompt_hash": "21e40aac986701b4f207f1ca74965b0f7ebcfcb88bebb1fa47041e54e542a713", "target_hash": "8093d51fdd9e2a988b8bebaa3361da6a0bd3bc66a5798c6063f60a95c269cc23", "exact_match": 0.0}
{"doc_id": 2229, "doc": {"question": "what is the largest bill in american money", "answer": ["$100"]}, "target": "['$100']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the largest bill in american money?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $100"]], "filtered_resps": ["$100"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "957b15c178cc4c8a02b16c2afba90f49552bb355386e1d6b12c14923cef90344", "prompt_hash": "612fa964f7759c2a354daeb189184b2ea25f2a46bd6af69287ce7ae2bb2e3aaa", "target_hash": "b377e806796f193a929e49ce48510971b1e0d535664f0d5eaf4068ee15ec4886", "exact_match": 1.0}
{"doc_id": 2230, "doc": {"question": "how long is the bridge between new brunswick and prince edward island", "answer": ["12.9-kilometre"]}, "target": "['12.9-kilometre']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long is the bridge between new brunswick and prince edward island?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d05e0a58e8895436afdcecbf1a76a463abbc5a74667d0067e02697aefb794ba", "prompt_hash": "68de152aaf55ee370edfbe36d9ed2f76fdee3184a2f8490f1f7c9fab595e35ff", "target_hash": "a9db4ed3d8ad89c907f58643e85a937b555aac0d9c4aa2f56a5be15db4238367", "exact_match": 0.0}
{"doc_id": 2231, "doc": {"question": "how many nfl games were played in london", "answer": ["21"]}, "target": "['21']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many nfl games were played in london?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19cda973d997d75ad4007aba3e4d1ec186f9f7cfc8d992427253e9730a5c5b82", "prompt_hash": "5340021e982ddab0b061c13c356486a7e8f49028ff7d2f24f6fb8612b42d01c9", "target_hash": "0659f33a8501f429c33b5ec3c0f0ea517dec38bf3619e68110e9a6513e98507e", "exact_match": 0.0}
{"doc_id": 2232, "doc": {"question": "how many characters are present in icd-10 codes", "answer": ["7"]}, "target": "['7']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many characters are present in icd-10 codes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" three"]], "filtered_resps": ["three"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d33ece8b289710877feb379b5d6691deb762bc684f12c9ffeb9130defa886e4", "prompt_hash": "d52b32eaa3dde8bcc39ed301b7897b2bd3a3eb22ac4cfbc598244d1022b950e0", "target_hash": "daaadef398cfd18ecc086697bc3b2c12ec6c2386996037c659028ab3e97cd18d", "exact_match": 0.0}
{"doc_id": 2233, "doc": {"question": "how many seasons of ray donovan has there been", "answer": ["5"]}, "target": "['5']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of ray donovan has there been?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" six"]], "filtered_resps": ["six"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "54262fffaf87b8597fc90c090034d8730003c0c57b2ad399a79aa9c3a881d599", "prompt_hash": "c8067b0a00cc0620b691d7e7c5ec9decdd1fe59e6384005a0b8ff141247e7bcf", "target_hash": "494a843c2b5d43b794ff50c14c5647bf4ff556a365c58f564af6c235a53602e3", "exact_match": 0.0}
{"doc_id": 2234, "doc": {"question": "where do peaches come from in the us", "answer": ["other states", "South Carolina", "California", "Georgia"]}, "target": "['other states', 'South Carolina', 'California', 'Georgia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do peaches come from in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Georgia"]], "filtered_resps": ["Georgia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e95ae2eeb350f1b47e3487e4e2edaba91646af118fd13ef6b4ded47d2a75a6a5", "prompt_hash": "25a044db0e9f75d3a51d87c2f744783fe7ac1bab2ff479164854edf42ac003d7", "target_hash": "79e2fdb6a884527416d863a5af3f4fb3f0c28ae935fdd0ad0b7e8e312b68b4a8", "exact_match": 1.0}
{"doc_id": 2235, "doc": {"question": "what was the name of atom bomb dropped by usa on hiroshima", "answer": ["Little Boy"]}, "target": "['Little Boy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the name of atom bomb dropped by usa on hiroshima?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Little Boy"]], "filtered_resps": ["Little Boy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1de5adc57790fc7fc17b81ef2a9f70668528f6e103d72e23c7734675a398f9ec", "prompt_hash": "d9c9c3675db74bab2f6f20d3f5dbe008727188e200bafb34634e63e3732f2d3e", "target_hash": "887aabaa2dee93bf102134af95ff9df071ed29e4761871fdf0a24a6511bd3769", "exact_match": 1.0}
{"doc_id": 2236, "doc": {"question": "songs on 4 your eyez only j cole", "answer": ["Immortal", "4 Your Eyez Only", "Ville Mentality", "Deja Vu", "She's Mine Pt. 2", "She's Mine Pt. 1", "Change", "For Whom the Bell Tolls", "Neighbors", "Foldin Clothes"]}, "target": "['Immortal', '4 Your Eyez Only', 'Ville Mentality', 'Deja Vu', \"She's Mine Pt. 2\", \"She's Mine Pt. 1\", 'Change', 'For Whom the Bell Tolls', 'Neighbors', 'Foldin Clothes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: songs on 4 your eyez only j cole?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2014 Forest Hills Drive"]], "filtered_resps": ["2014 Forest Hills Drive"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e1e9a3c9c9528d612d3ec12886c61d4c672636c40c0ceb7a4eaf56ba08d3df7d", "prompt_hash": "ce25b10c4e223d5438a396d10ffe0014250e1116ef537d1415a5f148fd8c041f", "target_hash": "b38cae5c025179e84b5ce1c12cca324863fb047f2e3440d3e2528fe9b456a48e", "exact_match": 0.0}
{"doc_id": 2237, "doc": {"question": "what happened to western rome after it fell", "answer": ["divided into several successor polities"]}, "target": "['divided into several successor polities']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what happened to western rome after it fell?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" It was sacked"]], "filtered_resps": ["It was sacked"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "be064ed526c64fc4694dc4a3c3e90fcf19b3d7cbc8a1a3ff24a4f62933905354", "prompt_hash": "0f027ba73a6923cdfdb3424f4dc0646e78574653402cb836d2ee904cd9ee449a", "target_hash": "3c2eada9f7d159bee987d3ffa697d366b7c7db89ae4ab63449110ae5620fac0f", "exact_match": 0.0}
{"doc_id": 2238, "doc": {"question": "who helped them recapture mycenae once they were old enough to fight", "answer": ["Aegisthus"]}, "target": "['Aegisthus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who helped them recapture mycenae once they were old enough to fight?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Agamemnon"]], "filtered_resps": ["Agamemnon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "94d0d7c020ee6d7e9ce67d2e3d7a866fd86c83db03020434b0e7940c970b2a9a", "prompt_hash": "f285d19020c0a890fa20bda32f3b05f4cd61f8187b8b59bfe7a0e89109ca6be3", "target_hash": "c964540b240b8b0ce5a892d94bba6c06357ec69771ed14b17488e0958e93801a", "exact_match": 0.0}
{"doc_id": 2239, "doc": {"question": "who is the cast of the new tomb raider movie", "answer": ["Alicia Vikander as Lara Croft"]}, "target": "['Alicia Vikander as Lara Croft']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the cast of the new tomb raider movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Angelina Jolie"]], "filtered_resps": ["Angelina Jolie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19145b1bba6f55a6972dcb831e98edc57107fd84c46a33e90127feaed6d42ebe", "prompt_hash": "4c85dc4a94e0dcd3a5093cb95d8f42fb39dab2091abaafed5f928a7f69536803", "target_hash": "8c14948568b7b9d08bf99c327f3ad6896007ffd987188af83c0ec87e463a21b0", "exact_match": 0.0}
{"doc_id": 2240, "doc": {"question": "who came up with the term baby boomer", "answer": ["The Washington Post"]}, "target": "['The Washington Post']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who came up with the term baby boomer?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Whyte"]], "filtered_resps": ["William Whyte"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "308346a9e45f3e1ec0b551b28d0513b79a0141090b252048b4c652eefc62a8fc", "prompt_hash": "d53afa41f31fb930db6392bb7ff4076d860d5fe7a9d9ae3b0b141c8a9e22c64f", "target_hash": "8b76c7a5fa98b107ba9636b6dc8567b0f96e2e4d5eaca9fc1d8f8b482f146b9f", "exact_match": 0.0}
{"doc_id": 2241, "doc": {"question": "where are the cardiac and vasomotor centers found", "answer": ["medulla oblongata", "the medulla oblongata"]}, "target": "['medulla oblongata', 'the medulla oblongata']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the cardiac and vasomotor centers found?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" medulla oblongata"]], "filtered_resps": ["medulla oblongata"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a3f9dc0a6590c10aa8115bee7ea221670aa11beba930b8268068528c3b54072c", "prompt_hash": "0ab6e0701cfe3295b9936291b240344c546ad0ee40d550d53879ce4c84ec3982", "target_hash": "6d6f7cb560c87dc7db22e88099126b594091b9909c5f95ad5ac3008138cfe06a", "exact_match": 1.0}
{"doc_id": 2242, "doc": {"question": "who played anne in anne with an e", "answer": ["Amybeth McNulty"]}, "target": "['Amybeth McNulty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played anne in anne with an e?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Emily Hampshire"]], "filtered_resps": ["Emily Hampshire"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "649240e1df01bc72e97f572063247538b2d38c9a9ea00c53a1a5ab58b7e4ea7a", "prompt_hash": "06120caa0c50ffcce32dd85b7e23d5e87a859f6807faaf4a2af2d8cc4ead36ef", "target_hash": "cf35fbe90e274fe090addeec87b020701a503d224cf1c4c9ef4b99be12278533", "exact_match": 0.0}
{"doc_id": 2243, "doc": {"question": "number of employees in the department of health and human services", "answer": ["79,540 (2015)", "79,540"]}, "target": "['79,540 (2015)', '79,540']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: number of employees in the department of health and human services?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 71"]], "filtered_resps": ["71"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19d7329c6dd8df243ed8b1ee4413ef77a583b40fd8c797826039c1956221a5d6", "prompt_hash": "2ff0fda85c1475eb2670e065e70bec5b8f4826caaff9cb1fc360cf7262cccd66", "target_hash": "c706b76ae67394285ca43f688557429c60b95d755b0b4b6243aeb7d35bc50a8c", "exact_match": 0.0}
{"doc_id": 2244, "doc": {"question": "who played big enos in smokey and the bandit", "answer": ["Pat McCormick"]}, "target": "['Pat McCormick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played big enos in smokey and the bandit?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jerry Reed"]], "filtered_resps": ["Jerry Reed"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7fe6e17a3311041fe0e459789ba949db86588bc1cf403bbfa47f11b5b08d3216", "prompt_hash": "738ffc523b23251a8f1fefc2868036b318d9d19c69a2fbec41bcb2015b662cc5", "target_hash": "41431323ec93d15c9f1e1289096e3c395e715f03c5adc7cf1a4c36315aa9a4c3", "exact_match": 0.0}
{"doc_id": 2245, "doc": {"question": "who plays chris grandy in 13 going on 30", "answer": ["Jim Gaffigan", "Alex Black"]}, "target": "['Jim Gaffigan', 'Alex Black']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays chris grandy in 13 going on 30?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Josh Duhamel"]], "filtered_resps": ["Josh Duhamel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "516af9d3b1259ac6c710454b6e63b16f083838057b3611886f206dbbd76eba46", "prompt_hash": "e18991e34894bfc6fa9b53772b33144dc211cdf059a8c8ed005def7ce68ec2a0", "target_hash": "6c8ba2e390c16a6d10d294502f5fd6df30f89bb5c1b7d21199fc140c6a92122a", "exact_match": 0.0}
{"doc_id": 2246, "doc": {"question": "who has hit a double century in the ranji trophy for jharkhand in 2016", "answer": ["Jaydev Shah"]}, "target": "['Jaydev Shah']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has hit a double century in the ranji trophy for jharkhand in 2016?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Manoj Tiwary"]], "filtered_resps": ["Manoj Tiwary"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c8dfb6bf173edc4cc039b44577f267138498ddca049393d18e158af6e624e945", "prompt_hash": "484fd4036b4ea4d05cb4bd2d8577efe0cc9aa0e1cb2739b9de2bbea6a7aa1985", "target_hash": "c07055a47f03ef4c5445bdbaf28b18c31dbddb5bc3d5ab8edb8ef90d5be87719", "exact_match": 0.0}
{"doc_id": 2247, "doc": {"question": "where is the hero golf challenge being played", "answer": ["The Bahamas", "Albany development in the Bahamas"]}, "target": "['The Bahamas', 'Albany development in the Bahamas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the hero golf challenge being played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" PGA West in La Quinta"]], "filtered_resps": ["PGA West in La Quinta"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cb47456bf46a5d061a42aee42a52239024d0d57f8dc7394bbf85dff87b7f697f", "prompt_hash": "40a36c4d33b929a5d14b317d7ad4f09ac428449fb1eae5994ef1dc355f16248b", "target_hash": "87e6bffd19470ab999eb0dfba3a6c92d17b08663f916edd0ac5ccbf8c418e70e", "exact_match": 0.0}
{"doc_id": 2248, "doc": {"question": "what is the wave length of x rays", "answer": ["from 0.01 to 10 nanometers"]}, "target": "['from 0.01 to 10 nanometers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the wave length of x rays?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 0"]], "filtered_resps": ["0"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea7063282bb39c336a583851662568aa07d2a2a8912a73f8002273e68ca7c614", "prompt_hash": "7667a124fbfbaf2d7a3e3372507d8f64870af40c3bb709ec299b5deb2ca962e5", "target_hash": "73b4bdbbdc9c59ff7786011e33fd3203b8ed8164bc78fc7d69b8c8b1b088281f", "exact_match": 0.0}
{"doc_id": 2249, "doc": {"question": "who won the oscar for best picture in 1976", "answer": ["Rocky"]}, "target": "['Rocky']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the oscar for best picture in 1976?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" All the President's Men"]], "filtered_resps": ["All the President's Men"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58985755958d01ac27d82ac19579ac5a4d464b7cae7a322340ffcd585a638180", "prompt_hash": "c7bf0166bd9159f9191132d407d301c731e64c2f9d7938b956285d97176a7e06", "target_hash": "50c982972678e8491e9577af2d4e4f007e89ad207c7057a40973a19433f1a575", "exact_match": 0.0}
{"doc_id": 2250, "doc": {"question": "who were the two mathematicians that invented calculus", "answer": ["Gottfried Leibniz", "Isaac Newton"]}, "target": "['Gottfried Leibniz', 'Isaac Newton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the two mathematicians that invented calculus?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Isaac Newton"]], "filtered_resps": ["Isaac Newton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8acbb7840bdfd141b5a26abd4044a2608c7c24d6b0bc6452b3e6c4b022fa442", "prompt_hash": "28dce3867ac0a10763d55dc2dc09069ff681c8de18a670914bc5478439a23479", "target_hash": "ce70de8e79d8e9931ffddabf4436148282a02564125b5d45eedc8e342721b57c", "exact_match": 1.0}
{"doc_id": 2251, "doc": {"question": "where does porter fly out of in toronto", "answer": ["Billy Bishop Toronto City Airport"]}, "target": "['Billy Bishop Toronto City Airport']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does porter fly out of in toronto?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pearson International Airport"]], "filtered_resps": ["Pearson International Airport"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f02daa930c7622d54bbc83787896dad8f25e8cef61ca900395fc0a2c143bb4eb", "prompt_hash": "84722697d84d7fd013cfa2de537aec6b96d97c9c17321748d06bad70479b77e4", "target_hash": "32fb4ecbf1633aadf388a4cb1035c30bf623afb02e3b4b99a44918fa7136948e", "exact_match": 0.0}
{"doc_id": 2252, "doc": {"question": "who sang the end of the world as we know it", "answer": ["R.E.M."]}, "target": "['R.E.M.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the end of the world as we know it?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" R"]], "filtered_resps": ["R"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "78fce580a890ee876842690a53e472d93b16730544bca28fac6f4dda51713866", "prompt_hash": "bcdbda912f3e9dbd85da84e49e44ffe4c1270eb5f3f5473d656f9212d62f6045", "target_hash": "719513cd4f65a342507430ae1a6f476663741a1ab85f0f7ebc24c51530b8f9d7", "exact_match": 0.0}
{"doc_id": 2253, "doc": {"question": "who challenged the aristotelian model of a geocentric universe", "answer": ["Copernicus"]}, "target": "['Copernicus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who challenged the aristotelian model of a geocentric universe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nicolaus Copernicus"]], "filtered_resps": ["Nicolaus Copernicus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "30806f6be905d185dae6742d0ac8f5f3caee663a1d3bcb98265e0e4674b6e389", "prompt_hash": "31dec1ac3b2215e08f0ce208b52266da41cd069b49b4cc0fce4db38386e181e1", "target_hash": "b4c1271a74ed802559c3fc9cef404a965a9315d22b5792c10c212e765cfa826f", "exact_match": 0.0}
{"doc_id": 2254, "doc": {"question": "delhi is on the banks of which river", "answer": ["Yamuna"]}, "target": "['Yamuna']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: delhi is on the banks of which river?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yamuna"]], "filtered_resps": ["Yamuna"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9cc8942b9a6ec7e1fdae2255b72f7726ddb352644239f87ed3122f326bc5e68a", "prompt_hash": "3b87d2f3f24130221f845c62ab58dee0ef2e0f94ee75270fdd6ed4e6e48f00a5", "target_hash": "8422ab27cf4c826ec2dc53513cfbe7edea8afe862a6bc493d56e4854f1e4dcff", "exact_match": 1.0}
{"doc_id": 2255, "doc": {"question": "what happened at the 1939 worlds fair in regards to television", "answer": ["Television demonstrations are held"]}, "target": "['Television demonstrations are held']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what happened at the 1939 worlds fair in regards to television?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" RCA demonstrated the first television broadcast to the public"]], "filtered_resps": ["RCA demonstrated the first television broadcast to the public"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f9c7bf1c62bfa27c97e261a16b3bbe1cb0be29345e1d251297c961040be704ac", "prompt_hash": "0faee456d943ca472656475849dfe198ee9f6e9978006dec3323754c5c567071", "target_hash": "63feae8984e97bc2989b04c507a3176f1a540a210f7a82b5c71c8e4d4b7190a5", "exact_match": 0.0}
{"doc_id": 2256, "doc": {"question": "krypton-85 decays by emission of a beta particle. the product of this decay is", "answer": ["rubidium-85"]}, "target": "['rubidium-85']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: krypton-85 decays by emission of a beta particle. the product of this decay is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" rubidium-85"]], "filtered_resps": ["rubidium-85"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c275f2a24c627dc42ef9641180cc1e7cd8802d934853ee5ead2c0960fed1baea", "prompt_hash": "fedc04cc78fb4e7aa3c773269e64909783f6b0d0ef9d9ab87efcbbf298b6cd74", "target_hash": "ff8cf9ca7a5740762b748766e177a2a4005f2b8cfebaa09c9a3c0d02a55717be", "exact_match": 1.0}
{"doc_id": 2257, "doc": {"question": "who starred in the tv show even stevens", "answer": ["Nick Spano", "Donna Pescow", "Christy Carlson Romano", "Shia LaBeouf", "Tom Virtue"]}, "target": "['Nick Spano', 'Donna Pescow', 'Christy Carlson Romano', 'Shia LaBeouf', 'Tom Virtue']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the tv show even stevens?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Hyde Pierce"]], "filtered_resps": ["David Hyde Pierce"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4687d7d74c94a3a50db9c162196ff79c3eb893f335bed002802fc099f523bdbe", "prompt_hash": "1c22c198ab56179ff7c89aff511f69a3a7d229f330f1f57112ab51011df85454", "target_hash": "a599a67a31507a7def02747caae904d62b04c2e2c8a0e008468ae72f4297af13", "exact_match": 0.0}
{"doc_id": 2258, "doc": {"question": "what percent of the us population controls the wealth", "answer": ["1%"]}, "target": "['1%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what percent of the us population controls the wealth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1%"]], "filtered_resps": ["1%"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c1636cd1029b175d9ac56f9d0f99a0162b5ad6238c18fbde601237a9e7ef94b", "prompt_hash": "7c98074dbaf9dc9751b5dd70d8d14cb6c19dd1e9f010e0dccc8be2ddef3ec7ac", "target_hash": "ca24e47c384319acc9a90619c1361b90199a06cb2d32930e1c4a7ec9e5e6d798", "exact_match": 1.0}
{"doc_id": 2259, "doc": {"question": "what is the oasis in ready player one", "answer": ["a virtual reality simulator"]}, "target": "['a virtual reality simulator']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the oasis in ready player one?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Matrix"]], "filtered_resps": ["the Matrix"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2da19b3e47c5bf7d173caf7217fe0196fb1e39adb6d93cc786dc59eb1c5eb650", "prompt_hash": "75774f1a5ade1c8815d25b8f8519abad10a434c79ea6fb60489e799dfa37c493", "target_hash": "4092c5f0c263e90c81288900779ad0e6131eaa580f35f8b1a5f29ea54b84936c", "exact_match": 0.0}
{"doc_id": 2260, "doc": {"question": "who sang the song one of these nights", "answer": ["American rock band Eagles", "the American rock band Eagles", "Eagles"]}, "target": "['American rock band Eagles', 'the American rock band Eagles', 'Eagles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song one of these nights?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kiss"]], "filtered_resps": ["Kiss"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "62c8bd2432b635b9fbf57d892e751059cc03a33f8f8e5a5cd728d7ea9701589a", "prompt_hash": "1b50b0e642dfbac1a1ac51ad4910c2b244fbc32e324b806fd13ef1614c94bb33", "target_hash": "b006b778df3128e7d929f46121b058c340e8d733ba11906ea8f4bb49c1b7bd0a", "exact_match": 0.0}
{"doc_id": 2261, "doc": {"question": "what type of energy do satellites generally use to communicate with earth", "answer": ["radio and microwave frequencies", "electromagnetic waves"]}, "target": "['radio and microwave frequencies', 'electromagnetic waves']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of energy do satellites generally use to communicate with earth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" radio waves"]], "filtered_resps": ["radio waves"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c24c09d9bf1ba941cad41e25a0fe20bf32ad6f9f1838469ce9de85e27fae487", "prompt_hash": "74610bf25484d5f43d492e85570b36d4aed3ce37d085fd466b9e3de446f835f3", "target_hash": "519ce8047a35e2ad2469188331bf74007692c1d200a8369dfeb45fc44f3c34b2", "exact_match": 0.0}
{"doc_id": 2262, "doc": {"question": "who has most followers on instagram in world", "answer": ["Instagram's own account", "Instagram", "Selena Gomez"]}, "target": "[\"Instagram's own account\", 'Instagram', 'Selena Gomez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has most followers on instagram in world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cristiano Ronaldo"]], "filtered_resps": ["Cristiano Ronaldo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f42346949200cc457a4ec22a813cfb7dfa321441a343a67b292dd86dda5788f9", "prompt_hash": "daa70a3da95b54a64360e4295267f62ac88b333859448c03122ea8ca03a91937", "target_hash": "74079dcbe99a0165148719daa0485797503518522f8354466978abb7d6a6a463", "exact_match": 0.0}
{"doc_id": 2263, "doc": {"question": "when did the international space station go into space", "answer": ["1998"]}, "target": "['1998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the international space station go into space?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f381ea4f5a3950345266808028e715142784146c43676f15a61d9673a649ea4f", "prompt_hash": "5f0773b75cef383c28aa814ec77e00d839423357f4c6eaa54236352d4ebc2c01", "target_hash": "af1dd11d685d07f55fbfc760446204424f6189f35a956c9f3b09a3b25cbebbea", "exact_match": 1.0}
{"doc_id": 2264, "doc": {"question": "what was the first capital city of australia", "answer": ["New South Wales", "Melbourne", "Sydney"]}, "target": "['New South Wales', 'Melbourne', 'Sydney']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the first capital city of australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sydney"]], "filtered_resps": ["Sydney"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "102cfb2382ac609d0aa806388da0d8d992199fab9a1ee6d2925b74451ef798ca", "prompt_hash": "282d8ef008567ffaddaf3939a6631a112de59d672c348c5eba957e0ad3304a39", "target_hash": "c880985f53b0d89f07e445325064a7f5768e451b2587165fe043e2924f0053a2", "exact_match": 1.0}
{"doc_id": 2265, "doc": {"question": "what are the toll roads called in mexico", "answer": ["autopistas"]}, "target": "['autopistas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the toll roads called in mexico?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carretera Federal"]], "filtered_resps": ["Carretera Federal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "de5f65b77fa2a5776872b8eb0ce59c7a928e1a3fa521c7e33c21a0964edeb978", "prompt_hash": "b4e68d8c85b96d574ec49fb83805a1cd80427316fa416525ef54b36f103ba3b9", "target_hash": "33fddbd5bb780e251829c557bfe103c3262b6dd297ff8d55000bd520c437c1e6", "exact_match": 0.0}
{"doc_id": 2266, "doc": {"question": "when does the good doctor episode 8 air", "answer": ["November 20, 2017"]}, "target": "['November\\xa020,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the good doctor episode 8 air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 14"]], "filtered_resps": ["February 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ddbe8b962485f6c4b8cde496aaea810f673f8dd82be81f959885338cef8e0ae3", "prompt_hash": "2f35e6674ed38cbef9192cba44fb53c0163e9ea4ecf1df303fc6d4e7329416a9", "target_hash": "3dc167acf7bdc36bf2ae4b2f541b255980b29620fccab89bc8167b676e6373bb", "exact_match": 0.0}
{"doc_id": 2267, "doc": {"question": "what was the name of the horse in steptoe and son", "answer": ["Hercules"]}, "target": "['Hercules']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the name of the horse in steptoe and son?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Regrettably"]], "filtered_resps": ["Regrettably"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eecf700ff4de6f629f4d78e4ff60c2f6954646d999966f0ecf71edee73037580", "prompt_hash": "a150f7c65e1b3ca8683d5a739d12b12f6df0bd6361b5f960344b7b46bdf10fbf", "target_hash": "519a8af17b75305777a5daf909657d1963878f52d2e37ba6b7f1d67fd2fb1ca1", "exact_match": 0.0}
{"doc_id": 2268, "doc": {"question": "what state does rick and morty take place in", "answer": ["Washington", "the U.S. state of Washington"]}, "target": "['Washington', 'the U.S. state of Washington']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what state does rick and morty take place in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Georgia"]], "filtered_resps": ["Georgia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b623caf37de19ee7a71966694df6e2e337e2d42ef694c29449e93df83bdf0f9f", "prompt_hash": "ce1b65551badb7b97507d9dfebd7051b1e971182b7681dbacf51f0d53066cae0", "target_hash": "fc7aba227e59475a7ad8e141cab327027329b0376cd9b84eaeaff91155bf87f2", "exact_match": 0.0}
{"doc_id": 2269, "doc": {"question": "what is an open university grade 3 pass", "answer": ["55–69%"]}, "target": "['55–69%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is an open university grade 3 pass?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 21 credits"]], "filtered_resps": ["21 credits"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "76f43bcfa9a27f0741a13d114e5c9656fa6f14bd842e351c6800310f3926d32d", "prompt_hash": "0e0fd183d31f0116bd3c525ead0dee0fd2f85b1c8184ee1509ff8aae8bf6b2cb", "target_hash": "bd71b720685be64749b113499a09bc48f4575db4ffabaf3d76ab658cf8e0e85d", "exact_match": 0.0}
{"doc_id": 2270, "doc": {"question": "who dies in season 2 of the originals", "answer": ["Kol"]}, "target": "['Kol']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who dies in season 2 of the originals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Klaus  season 2"]], "filtered_resps": ["Klaus  season 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0b1d5223004ab296a1a64869551f9496236cf76e5c8d380721e09f0a1e5a5189", "prompt_hash": "97901028d4c44be5e24c8b8f7bfccbdbdbc3c60c9efd3aab0f619aa5f5f6eda0", "target_hash": "597cfa9fcc89031efa75d513b8d7b77cb18dc0190926eeaa8ec3f2561a5308e3", "exact_match": 0.0}
{"doc_id": 2271, "doc": {"question": "what is the approximate volume of blood in your body", "answer": ["approximately 5 liters"]}, "target": "['approximately 5 liters']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the approximate volume of blood in your body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 5 liters"]], "filtered_resps": ["5 liters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "880923a82ab4bb56e907b3231d05d490e312f554d53f5bf99d019f83204bd809", "prompt_hash": "637af73c6b759c654aa369f6808593fa05711783c5e49eaffe3439843a9f0548", "target_hash": "e764f32ae0bb080cdd29cceace0c0bc65f2e3e9cc6a1d686a5df36572dd1fda6", "exact_match": 0.0}
{"doc_id": 2272, "doc": {"question": "who's winning the election in virginia for governor", "answer": ["Ralph Northam", "Democratic nominee Ralph Northam"]}, "target": "['Ralph Northam', 'Democratic nominee Ralph Northam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who's winning the election in virginia for governor?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ralph Northam"]], "filtered_resps": ["Ralph Northam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1850eabe1091df1a0b3804bc755f8c6cfd0d5afc73ff0a32c87f2ba129287077", "prompt_hash": "2afebce6f87d8c7d3a4857d66b24e23526076f64824e4fea99a982ce5b726c7e", "target_hash": "6808346708f6aa623d9626c0977734f314842284494189bef42a6f857be8aa3d", "exact_match": 1.0}
{"doc_id": 2273, "doc": {"question": "what are three different types of hotel properties", "answer": ["Bed and breakfast", "Hotel barge", "Botel"]}, "target": "['Bed and breakfast', 'Hotel barge', 'Botel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are three different types of hotel properties?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" resort"]], "filtered_resps": ["resort"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a6d735008f1e855f6796578e3987322989023f768c73402d41d60cf01c6369f6", "prompt_hash": "5e2bf041a47c3872a15088c219795fc3392a2f02e4a2e299f4bc6bc9c3075415", "target_hash": "cdf462a1adc8c32a471253e08ce4daf25ca30f01c88669428ea58b169cc70459", "exact_match": 0.0}
{"doc_id": 2274, "doc": {"question": "where does the phrase dressed to the nines", "answer": ["Scots in origin"]}, "target": "['Scots in origin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the phrase dressed to the nines?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" nine "]], "filtered_resps": ["nine "], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d8ebca1392b24aef068e171646810f8d2171c08168aebd197603a224461f7a8b", "prompt_hash": "4107d91cbc7d7f9777afa324cf3f31640153113eaca4d8c47f70072211dc8bf7", "target_hash": "9d2a1ffd29ffadac95254180d9ee20edfbfc692ca6b0c18acc8051148df69c55", "exact_match": 0.0}
{"doc_id": 2275, "doc": {"question": "when did harvard become an ivy league school", "answer": ["1945"]}, "target": "['1945']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did harvard become an ivy league school?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1954"]], "filtered_resps": ["1954"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "04dd34b4f732f890cc28e5514cfd4876203c59d107fe248d38ac5ce357d087fe", "prompt_hash": "4b9305cd6a466e18f590001f6c01ec033089869ebdd3fe5bb5c27422eaea742c", "target_hash": "a4ccf6516c2d80e68c765c49139a5a9f6cd3fb06aded9ea5366468369ddac125", "exact_match": 0.0}
{"doc_id": 2276, "doc": {"question": "who claimed land in south america for portugal", "answer": ["John II of Portugal"]}, "target": "['John II of Portugal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who claimed land in south america for portugal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pedro Álvares Cabral"]], "filtered_resps": ["Pedro Álvares Cabral"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e873b269477877ed260e74788abf7ea6b23ed95fdf4092a187918d31dad73e1f", "prompt_hash": "bc5494c79fcfdaba9259e63ade97d6fb31255357c66b9ec825c2d9051ac5edb7", "target_hash": "e2336326ba4143af027c31b8a60d30b3649299a2fc920d27735b44d658e5301d", "exact_match": 0.0}
{"doc_id": 2277, "doc": {"question": "who proposed that electrons behave like waves and particles", "answer": ["Einstein"]}, "target": "['Einstein']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who proposed that electrons behave like waves and particles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Niels Bohr"]], "filtered_resps": ["Niels Bohr"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "386f5111235d32fcb535d4bcab752ee79a8017f0f7093e45675ebb62fd0df968", "prompt_hash": "c449bd5506037c3d0a462f540e10b7257ddbb1aed7bc10d96006b73501b25918", "target_hash": "dc7c6f64e39001f23c687d40e5a64404dd2c3323b25bc3b8f9bc896813316dbe", "exact_match": 0.0}
{"doc_id": 2278, "doc": {"question": "when did the celebrities enter the big brother house", "answer": ["February 7, 2018", "2017"]}, "target": "['February\\xa07,\\xa02018', '2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the celebrities enter the big brother house?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11 January"]], "filtered_resps": ["11 January"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a5b949518dd07d6b00c41e229804ae26d5a517abc8dc9d4a698af7393d9ada54", "prompt_hash": "6016dc3842faa898924b37d3d334f4dd499e0fcdad5525fb272a4a6435c1ad9a", "target_hash": "e24b72af4d8feca3223067b8cf324daabaa421476a8494e1c37ca6a9ed2a4d27", "exact_match": 0.0}
{"doc_id": 2279, "doc": {"question": "what state courts can order a new trial", "answer": ["appellate court"]}, "target": "['appellate court']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what state courts can order a new trial?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" federal courts"]], "filtered_resps": ["federal courts"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0b0a1585584282d6c4a481923f1c1b8061c577e652c1d4805f6a9f380105624f", "prompt_hash": "988552f941b2e9ae61e45718c877614ccce3fd708412d190c40b8b65fb30ba39", "target_hash": "508062a3b8e277ce56d685984ab63ab05497e4c39fa945284ef9a80bf70b2d91", "exact_match": 0.0}
{"doc_id": 2280, "doc": {"question": "when was the peak to peak gondola built", "answer": ["in 2007 and 2008"]}, "target": "['in 2007 and 2008']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the peak to peak gondola built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2006"]], "filtered_resps": ["2006"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "98d6e4010eda6bb9b7f53f29ef29d258db55b91190ed43715478a4386b7fe125", "prompt_hash": "e063956241a299507d8b16ecccbd860d36f7359e373722004167b0c983796e2f", "target_hash": "d5881f947dc6b3179525a61d3171b98d8cfdc61c3ede3ecca062c35a28d26a15", "exact_match": 0.0}
{"doc_id": 2281, "doc": {"question": "what allows chyme to enter the small intestine", "answer": ["pyloric valve", "the pyloric valve"]}, "target": "['pyloric valve', 'the pyloric valve']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what allows chyme to enter the small intestine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pyloric sphincter"]], "filtered_resps": ["pyloric sphincter"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dbb91dff16b7e236ba024367e58a2868cb2f75db3b6844f0adfec0bfbe880f83", "prompt_hash": "bbbbc9a0cd08603b91c35df8d844910e9e30a681d5e8f9244a05c0ce2b9354e9", "target_hash": "1405c726cde722ccec9552eebb038669cf711f4290e2e3d8a1cde2b2c53daac6", "exact_match": 0.0}
{"doc_id": 2282, "doc": {"question": "where did they film the royal tv series", "answer": ["City of Bradford", "the North Riding of Yorkshire", "Scarborough", "Whitby"]}, "target": "['City of Bradford', 'the North Riding of Yorkshire', 'Scarborough', 'Whitby']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they film the royal tv series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" London"]], "filtered_resps": ["London"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3f30de820c3b6ea23fbd8af69b4ac1ad1bda56813064dad9663561f27bd180e5", "prompt_hash": "0718762672054a0f4631fdc1aa2e758ed2175fb0b3657517c65ac16aa161de94", "target_hash": "e8037ba5624e157aba7da4b6ad1a342356df92e2d86e8578df5964b7be26151c", "exact_match": 0.0}
{"doc_id": 2283, "doc": {"question": "where do they get the hair for a hair transplant", "answer": ["chest", "legs", "shoulders", "back of the head", "torso", "back"]}, "target": "['chest', 'legs', 'shoulders', 'back of the head', 'torso', 'back']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do they get the hair for a hair transplant?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" donor site"]], "filtered_resps": ["donor site"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "868623fbbca49673ca319d6602dcd205ecdbadd172b2b65a1ee72b5cfba94eed", "prompt_hash": "fbffaa9b5bb735314e162085825ed4c4cd4932e1661895805595016da31662c2", "target_hash": "c32801265b7210ac183865f30653a8048adc2d0b6da4c5ae6d8845acbb8a9856", "exact_match": 0.0}
{"doc_id": 2284, "doc": {"question": "how many co2 molecules are produced in aerobic respiration", "answer": ["6"]}, "target": "['6']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many co2 molecules are produced in aerobic respiration?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 6"]], "filtered_resps": ["6"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0288589d4b539f9e6d0e636ce9c385a702bdb4190d3a75a44749073f813873be", "prompt_hash": "1400f50ed1b79ab655b8f97dc9096b1bf6d4864443927cefa2c11a1eca47341c", "target_hash": "b36d04a43c188fa9042ccfe2c13d40316eef06f92e6790f9e9b2395eb813a6eb", "exact_match": 1.0}
{"doc_id": 2285, "doc": {"question": "when did the red guards start and end", "answer": ["1966 and 1967", "in 1966 and 1967"]}, "target": "['1966 and 1967', 'in 1966 and 1967']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the red guards start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1966"]], "filtered_resps": ["1966"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c57c457eb6c2efce379ff21ef1964e65a442a14e7c6c0c9788b7dd6d460fce3", "prompt_hash": "8db0072eab8d44470e4d29b4d17af7072666ef37cd41410d627372cadd685789", "target_hash": "ae598c2d521ca78c3c4e0ae16d006bbe836e619100cc4d9d0d6940bff702fe3e", "exact_match": 0.0}
{"doc_id": 2286, "doc": {"question": "who is the father of genetics and what did he study", "answer": ["the common edible pea", "Mendel", "pea plants", "variation in plants", "Gregor Mendel"]}, "target": "['the common edible pea', 'Mendel', 'pea plants', 'variation in plants', 'Gregor Mendel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the father of genetics and what did he study?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gregor Mendel"]], "filtered_resps": ["Gregor Mendel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06460654f4b2c60c6635fceed15eede8cbc969ca195792ad9af3c761303b2ec0", "prompt_hash": "29ee1bf1b418c7db6ed0c5e79b0d25475bcb0bb3b7471ad5ee76812cfe20b7b9", "target_hash": "4793a79118069e9202f0f88534275f1cb98f965aa04e98cd3514d7de8fb98455", "exact_match": 1.0}
{"doc_id": 2287, "doc": {"question": "what is the meaning of the name mandy", "answer": ["Lovable"]}, "target": "['Lovable']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of the name mandy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" diminutive of Amanda"]], "filtered_resps": ["diminutive of Amanda"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "11529ead82f28b7b584de9e23107afa861157487764b231ec7cb66c26823e3cf", "prompt_hash": "95fad97837861110cc15093446cb06c76eedcb0dcaa84d71c157be0ceee82289", "target_hash": "2a56b836a91d7c7c49424eba3d72c5eef4984f7d0c13819eef6cc5565440f172", "exact_match": 0.0}
{"doc_id": 2288, "doc": {"question": "when do new episodes of mindy project air", "answer": ["November 7, 2017", "September 12, 2017", "December 12, 2017", "November 28, 2017", "December 19, 2017", "November 14, 2017", "November 21, 2017", "December 5, 2017"]}, "target": "['November\\xa07,\\xa02017', 'September\\xa012,\\xa02017', 'December\\xa012,\\xa02017', 'November\\xa028,\\xa02017', 'December\\xa019,\\xa02017', 'November\\xa014,\\xa02017', 'November\\xa021,\\xa02017', 'December\\xa05,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do new episodes of mindy project air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tuesdays"]], "filtered_resps": ["Tuesdays"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9eb4714c90d3c118f92b73a326cac56cd6f9b8f3129f3d74f208a28eca7f2af", "prompt_hash": "127e4989ca48b10fa73d8d0678de41b4c0296644c3a781369f0455638f1b7064", "target_hash": "f04f63796d3b92ab3237919857fa678fa6f3461382b4865a8fc2d5bd1031e44d", "exact_match": 0.0}
{"doc_id": 2289, "doc": {"question": "what material was used to build the roofs of houses in burzahom", "answer": ["birch"]}, "target": "['birch']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what material was used to build the roofs of houses in burzahom?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" stone"]], "filtered_resps": ["stone"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0b3958e5edee3cf5e02f68d7a6ffa9f98d9b89bd70b2957ed3926c162abe39ce", "prompt_hash": "534b08f49dc05ea5a0c6fd029dd17afa253320fefdddd510e405d252ac7ffa71", "target_hash": "9683cb871a5b2447101566907402fa929884ad15f041996ad1473db8e9697d52", "exact_match": 0.0}
{"doc_id": 2290, "doc": {"question": "the inner sanctuary of the parthenon where the statue of athena is located in known as the", "answer": ["cella", "naos"]}, "target": "['cella', 'naos']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the inner sanctuary of the parthenon where the statue of athena is located in known as the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cella"]], "filtered_resps": ["cella"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "730d6d9a969f0f3442ff6961ecb699a7b3141a0f675aced46877b90ce9e0a341", "prompt_hash": "46fe4369c37ffdba8c51252991c90e0765151126dccc114c3ef397199a582bf8", "target_hash": "12c4e4cbee1d8c850d100740f81d68fc4f60a2f04713a75e1f8c54398d790299", "exact_match": 1.0}
{"doc_id": 2291, "doc": {"question": "one piece episode 534 english dubbed release date", "answer": ["February 5, 2012"]}, "target": "['February 5, 2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: one piece episode 534 english dubbed release date?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012-01-21"]], "filtered_resps": ["2012-01-21"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "08658397e6a94923ed49e614cce3c4ba2f221598261d2739f39169d2f10b7fec", "prompt_hash": "bc6479a06b2e9500b943736dac92c71ffe7a0ddcc43a57cdc521519a0a566c18", "target_hash": "fe307c65c505dc58131a7201f4361cc60beb677471c1166ecf92c4c05f76d742", "exact_match": 0.0}
{"doc_id": 2292, "doc": {"question": "who is the original singer of i write sins not tragedies", "answer": ["Panic! at the Disco"]}, "target": "['Panic! at the Disco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the original singer of i write sins not tragedies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nickelback"]], "filtered_resps": ["Nickelback"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "48dd7ecf63f7a25762dfd546f1632daa4ab14e6792c5be619e46daf5701d1523", "prompt_hash": "0612b4e07a3b03187b38514922248f1a4fc8a3e3c00ffb636bb4433d552c6d0a", "target_hash": "2318fff94f2d8deccb89720bbb1fad823e1502ba3e445d028ba21efafdb7802d", "exact_match": 0.0}
{"doc_id": 2293, "doc": {"question": "why did jean valjean take care of cosette", "answer": ["he had promised"]}, "target": "['he had promised']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why did jean valjean take care of cosette?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to repay a debt"]], "filtered_resps": ["to repay a debt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91264bab88ff7afda4f1f78d164baf95f94bbafe49fa511ced480e3ca7bee2d2", "prompt_hash": "77b58af8ac047ce810469edc06352a91d4021cc720663dcbcbb77fd12e22d2f3", "target_hash": "68bcdc7465c5f878c8581bf1addd57f0236d0ca5ca5e921b6070874ea263a341", "exact_match": 0.0}
{"doc_id": 2294, "doc": {"question": "where does the lincoln highway begin and end", "answer": ["Lincoln Park in San Francisco"]}, "target": "['Lincoln Park in San Francisco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the lincoln highway begin and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Atlantic City"]], "filtered_resps": ["Atlantic City"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8eab20c1adae9303486e1bb381f7aaba9c1265a2b1a496f725311abf60187da1", "prompt_hash": "b9d44ef3d5706feec8c4a1e5f05f4f21b661001555fc51e9d8913dc400399cf4", "target_hash": "a20ce79b9ff70c9dda7f9c08ce62482affa70fd667dbcce05f148a23cd9fd275", "exact_match": 0.0}
{"doc_id": 2295, "doc": {"question": "what is the ultimate objective of financial management", "answer": ["Profit maximization"]}, "target": "['Profit maximization']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the ultimate objective of financial management?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Profitability"]], "filtered_resps": ["Profitability"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c405871bb92fee72cc90406b3c49071771d15ba65d7623bb3056f5256a1292c8", "prompt_hash": "2d94d31fea4462e6b417a62ef8cc4c01b0abecdff855014948e4f020158bfbd2", "target_hash": "63cc5e0f04d3f57e1047d0db6842a65a4a73b07c8f749c0445b289181d574a3e", "exact_match": 0.0}
{"doc_id": 2296, "doc": {"question": "what year did the price is right first air", "answer": ["November 26, 1956", "1956"]}, "target": "['November 26, 1956', '1956']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what year did the price is right first air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1972"]], "filtered_resps": ["1972"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "579a41166207d98de26d4947fbe6dc5c22f1662457eb48c64d902c5d86bd1562", "prompt_hash": "b092a1987c3ec83e461272f2c88347b638d69b567fb8b44f26d8bcdfac13733a", "target_hash": "97fd1a212cc5cb30ce3119a1a8887cba277fef02e7e67007fe90a7784fa17644", "exact_match": 0.0}
{"doc_id": 2297, "doc": {"question": "when did under the cork tree come out", "answer": ["May 3, 2005"]}, "target": "['May 3, 2005']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did under the cork tree come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2011"]], "filtered_resps": ["2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "10dd602547bbc114e3986bfb15ee881afdcdb3cca438f35f03f8d15a966eb264", "prompt_hash": "78494d19b897232c9a989a922a287f8b47efbab90088237faec48bf6ecaeec7b", "target_hash": "b04a84da0ffc1bc10b4b6d18a5dc76906e8e023aa2094e6a2306fe2e4235a0e0", "exact_match": 0.0}
{"doc_id": 2298, "doc": {"question": "when did colour tv come out in uk", "answer": ["July 1, 1967", "1967/1969/1976", "1967"]}, "target": "['July 1, 1967', '1967/1969/1976', '1967']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did colour tv come out in uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1967"]], "filtered_resps": ["1967"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a8497feaf7c5453dd024af7db5343c3b679cd90533c378267992c3328e96838", "prompt_hash": "3098b95d52a1594e526c98a6fd5c2f2617a80c43fc2e9fc2626b78469b314385", "target_hash": "470c400f54acf03cbcf437bd646187c459dc1844a038c0c7d917702127dcd542", "exact_match": 1.0}
{"doc_id": 2299, "doc": {"question": "who are the characters in yo gabba gabba", "answer": ["DJ Lance Rock", "Gooble", "Brobee", "Foofa", "Muno", "Plex", "Toodee"]}, "target": "['DJ Lance Rock', 'Gooble', 'Brobee', 'Foofa', 'Muno', 'Plex', 'Toodee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the characters in yo gabba gabba?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" DJ Lance Rock"]], "filtered_resps": ["DJ Lance Rock"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f27ba79ed8648e5ad1548210c3788a572267467a27e8f72ed2c3445bd012371", "prompt_hash": "f066d438302646ef91ff5913029936c127d8291f661d99477680e78d9456d1a1", "target_hash": "9b4262f3cd4df38cb714fe80b5f5ee056205307782ed1da5ff9a2a9202c03917", "exact_match": 1.0}
{"doc_id": 2300, "doc": {"question": "list of local government area in benue state", "answer": ["Guma", "Ogbadibo", "Gwer East", "Gboko", "Gwer West", "Apa", "Ukum", "Okpokwu", "Ohimini", "Agatu", "Kwande", "Makurdi", "Vandeikya", "Otukpo", "Obi", "Oju", "Ado", "Konshisha", "Logo", "Tarka", "Ushongo", "Katsina-Ala", "Buruku"]}, "target": "['Guma', 'Ogbadibo', 'Gwer East', 'Gboko', 'Gwer West', 'Apa', 'Ukum', 'Okpokwu', 'Ohimini', 'Agatu', 'Kwande', 'Makurdi', 'Vandeikya', 'Otukpo', 'Obi', 'Oju', 'Ado', 'Konshisha', 'Logo', 'Tarka', 'Ushongo', 'Katsina-Ala', 'Buruku']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: list of local government area in benue state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Makurdi metropolis"]], "filtered_resps": ["Makurdi metropolis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "455091779dd1b99a5fad97f4d47f1fc76f4c6d1261736d9425fb17610886a3c9", "prompt_hash": "e0b7a794e2e5f538c82088085eb5ccf6ad9b30dc76ccc8aa6adf799fa6eedc72", "target_hash": "28c4d6d6c593b5900072eb75fcef2278cac018ae5bdc36862d58f7a54488578b", "exact_match": 0.0}
{"doc_id": 2301, "doc": {"question": "panic at the disco la devotee video cast", "answer": ["Noah Schnapp"]}, "target": "['Noah Schnapp']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: panic at the disco la devotee video cast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brendon Urie"]], "filtered_resps": ["Brendon Urie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cef7a09de2330498508a3bc7919cc371cb3d790137b3952e7d56168829f962c9", "prompt_hash": "f2905b99d5aa5f280013778e6db986e5fad48b74bd7978aceeaa7369360f47f1", "target_hash": "3b2136b5495b52f25ca6f59fce471bdd9f825f23be1089740f472928b8b4cfa9", "exact_match": 0.0}
{"doc_id": 2302, "doc": {"question": "who played the villain in tiger zinda hai", "answer": ["Sajjad Delafrooz"]}, "target": "['Sajjad Delafrooz']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the villain in tiger zinda hai?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Salman Khan"]], "filtered_resps": ["Salman Khan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7f2130cc162e6cc8cf015952854d34ac8b5951232b5bfecd959c7cd8014b878b", "prompt_hash": "e331eebcedc780bf66fd04ce604e2fdd255086f36ab14bb72dd37a16c704a55f", "target_hash": "33d98adf1319649d6c51864c4297052b9e7c207d539b801504518945991c656d", "exact_match": 0.0}
{"doc_id": 2303, "doc": {"question": "what types of cells go through binary fission", "answer": ["domains of Archaea and Bacteria", "prokaryotic"]}, "target": "['domains of Archaea and Bacteria', 'prokaryotic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what types of cells go through binary fission?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" prokaryotes"]], "filtered_resps": ["prokaryotes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "335b1b3839065ad23b04d81ec6ef1da3b94e82f24f2f1945dc32b00ad8a49f4a", "prompt_hash": "a0cb4dfb91685fe77cd044685c3d4f0c5135c1b5cd90f06351a64bb51fe6dcea", "target_hash": "1d891fdf957c7ff3252dc5f46cf9232aeb032993303c39b97a88c9aa15deb4af", "exact_match": 0.0}
{"doc_id": 2304, "doc": {"question": "who wrote it's gonna take a lot of love", "answer": ["Neil Young"]}, "target": "['Neil Young']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote it's gonna take a lot of love?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Eagles"]], "filtered_resps": ["The Eagles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5eb053ca048577d401e956c82656709c6ee5adb8187134e72adaa79fa20e5d20", "prompt_hash": "9692086d78ee3b4a2034c392318a1477c1965467c382455fd6610dd64ee4085f", "target_hash": "4ac8c0097dc85ada79f85a01d2bd1e83241e090e04378185b4a81a7711105d56", "exact_match": 0.0}
{"doc_id": 2305, "doc": {"question": "who become ceo of wipro company in 2016", "answer": ["Abid Ali Neemuchwala"]}, "target": "['Abid Ali Neemuchwala']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who become ceo of wipro company in 2016?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thierry Delaporte"]], "filtered_resps": ["Thierry Delaporte"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d85e3e20679cb2e7290b8bb987f0e66ab100f9bd1266f86cc5d3a31a69f6cd2", "prompt_hash": "dca619f7590a9dd5c59026e4b368867c0b72c045a2187512054c383325af5555", "target_hash": "678542f432489ecf30c1d8f15c0479dc958242d978d635150f69f1989dec2f23", "exact_match": 0.0}
{"doc_id": 2306, "doc": {"question": "who won the 2018 women's royal rumble match", "answer": ["Asuka"]}, "target": "['Asuka']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the 2018 women's royal rumble match?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shayna Baszler"]], "filtered_resps": ["Shayna Baszler"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "38b84f0a164fa1d17fb10f7af3d3d8a577c2dd0de49d322ee7d5aa26654cfc88", "prompt_hash": "0607ced61a6f73d0a9969f8cc05433c7085c557a4259b8e5b99a08540bedd7bb", "target_hash": "98cf0a3cadcce0a143f1e20e57d705b803dbd43888086f8720614a8c3fe0c65a", "exact_match": 0.0}
{"doc_id": 2307, "doc": {"question": "what channel is big 10 network on fios", "answer": ["585 (HD)", "85 (SD)"]}, "target": "['585 (HD)', '85 (SD)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what channel is big 10 network on fios?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 601"]], "filtered_resps": ["601"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "11184253e106071eac8e005f17b938bb4f3580bf3e95572bdb164e5416271906", "prompt_hash": "54b546f547a3e6b3c8db74e633372d60b367cb39207d5446c66f885f2dd2f931", "target_hash": "865fbb1526257bbd704f73bfe5d207907edee713bf7b9ba41e10c9a7de879693", "exact_match": 0.0}
{"doc_id": 2308, "doc": {"question": "who played hyde in league of extraordinary gentlemen", "answer": ["Jason Flemyng"]}, "target": "['Jason Flemyng']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played hyde in league of extraordinary gentlemen?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rafe Spall"]], "filtered_resps": ["Rafe Spall"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f27406ee5025840cef210005465c43cae89069eb56a8443bbd05118875be1d90", "prompt_hash": "3be8306c7687dc3c46639704885e0a65b6295e734e13016b86a2dd5aa6134d81", "target_hash": "d5bc3094f8f26f02b6778bea96a66527e4d665dcd632e3ac862f3851b5f7f76f", "exact_match": 0.0}
{"doc_id": 2309, "doc": {"question": "when does the 2017 college football season end", "answer": ["December 9, 2017"]}, "target": "['December 9, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the 2017 college football season end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" December 2"]], "filtered_resps": ["December 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d49d833309af7151918b9e54531f67bbded581e888339a8f915447ae1d0d659", "prompt_hash": "bb9b136a76b14a58ed548725f23b94b65b5774dbe672bef43de964a2718ef19a", "target_hash": "586e0f0349b74bba50ab0c24d9ef4cc1268d0963ede6cda6f5ee993cb988307c", "exact_match": 0.0}
{"doc_id": 2310, "doc": {"question": "what is the name of the under butler in downton abbey", "answer": ["Charles Carson", "Mr Carson"]}, "target": "['Charles Carson', 'Mr Carson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the under butler in downton abbey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thomas A: Thomas"]], "filtered_resps": ["Thomas A: Thomas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b2c72efa7859a31fc9b55850f7cf825957e1073a48ff32efea471e2f10f77b5", "prompt_hash": "0eae6956e59dd60a12361a02120f464c5e0d368adf45cf5a6ff34e5207af914c", "target_hash": "9fdb09fbe708a5fd846f47550fbc46279361a105a4e7491153dbf42fb1e6ab77", "exact_match": 0.0}
{"doc_id": 2311, "doc": {"question": "who has hosted the most fifa world cups", "answer": ["Italy", "Germany", "France", "Brazil", "Mexico"]}, "target": "['Italy', 'Germany', 'France', 'Brazil', 'Mexico']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has hosted the most fifa world cups?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brazil"]], "filtered_resps": ["Brazil"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b848f2f3dbab407bfd51e786b2fff85678fa9400f9d5279e922b77af2ea7cfe", "prompt_hash": "e98c13e4b7fabc3d63b310c77a9ee53bfc8a6a2942c382f0738b0ab6971ddb1d", "target_hash": "88c5d39c18249e558f8e0d01b656bb7ac3f5f98422d85d3cb368d9197d54a427", "exact_match": 1.0}
{"doc_id": 2312, "doc": {"question": "who was the top scorer in 2014 world cup", "answer": ["James Rodríguez"]}, "target": "['James Rodríguez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the top scorer in 2014 world cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cristiano Ronaldo"]], "filtered_resps": ["Cristiano Ronaldo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fae77a3fa98f1012f8996f5a868aa8039afc313bef17112b15551270d95d9b90", "prompt_hash": "f2ad7a5987060f595ebafcb3cf1ae49e07f2774d61cf80ac41bf8042fff0bea8", "target_hash": "a57644928553db754ad9c36eff1ae57488e26d8c47d7b70a58e60728cbe866ba", "exact_match": 0.0}
{"doc_id": 2313, "doc": {"question": "when was the last time the carolina hurricanes made the playoffs", "answer": ["2008–09"]}, "target": "['2008–09']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the carolina hurricanes made the playoffs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012"]], "filtered_resps": ["2012"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2befc4435314e83eaece0c46750e74c383e3bbbe6024a1b14fd10a316bc6288f", "prompt_hash": "2e952ebf83f2ce4b45ea130673a34d1911494d772a83cc2f3e5a7a2634862b82", "target_hash": "16ca86e97bd9530b358debef1ceef7f84a5071021436c02763d7a7bb0aab8c2b", "exact_match": 0.0}
{"doc_id": 2314, "doc": {"question": "the first permanent european settlement on future united states soil was located where", "answer": ["Jamestown", "St. Augustine", "Puerto Rico"]}, "target": "['Jamestown', 'St. Augustine', 'Puerto Rico']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the first permanent european settlement on future united states soil was located where?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jamestown"]], "filtered_resps": ["Jamestown"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1b3b8cc33f81516ef6b862f523866ef5e7c6fcf3a125eb2ca2986173470b54e1", "prompt_hash": "6a0de1a5d05e2d5d240fdc7e8b56ff99052dd2d3fc088a4fc0748a05c0915406", "target_hash": "f34a9a4648aa3a616bc5b1f0bd44f503d744105b21e346182f6d91134b2b5510", "exact_match": 1.0}
{"doc_id": 2315, "doc": {"question": "what is the snow on mars made of", "answer": ["water ice", "Frozen carbon dioxide"]}, "target": "['water ice', 'Frozen carbon dioxide']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the snow on mars made of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" water ice"]], "filtered_resps": ["water ice"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "563e6195fa3e5e2737423e8592017996e12f0fcfd9f632dfcb8474f961bb72e4", "prompt_hash": "2ae822f95c3d9258459d9de228de892948102de796a026e2bc3dd9c9b56e97ba", "target_hash": "ce2e918829b1ef807ba6d84d06631af2557dd2f555c3efbb5dcb19db653131be", "exact_match": 1.0}
{"doc_id": 2316, "doc": {"question": "where does the movie mothers day take place", "answer": ["Atlanta", "Atlanta, Georgia"]}, "target": "['Atlanta', 'Atlanta, Georgia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the movie mothers day take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "17729ef273b2370b8649aa2db00935180d36e717807a72ba55867271c31f0394", "prompt_hash": "d2b57473a916f98c7767c511d0512c6ca77b908fad78410afdca1ad83180dfff", "target_hash": "6a3728ea71743af883ff63c2fcfdf4ac0c65fcc7533f31cc0db481e2fab5a552", "exact_match": 0.0}
{"doc_id": 2317, "doc": {"question": "when did the first immigrants come to hawaii", "answer": ["Around 1200"]}, "target": "['Around 1200']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first immigrants come to hawaii?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1778"]], "filtered_resps": ["1778"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a98e092c2a8f8e41f1a2d5a69518e825f5ccad68d863e82bb7a3c5fdfce485d3", "prompt_hash": "7e96881e3cc7cebf400cb4c41d9f8a7e3554905be7c4356c310dc506eb90ed2d", "target_hash": "dce85d3b1fe561deb86f3b211e49f27b1b0f41801b6d7a3f04e004e0543776fa", "exact_match": 0.0}
{"doc_id": 2318, "doc": {"question": "what is the main industry in the canadian shield", "answer": ["mining"]}, "target": "['mining']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the main industry in the canadian shield?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mining"]], "filtered_resps": ["mining"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d173a3f023266c83db6e62f6f2a573a18c16ccd46e72ea41f5b1b97ddb0012cf", "prompt_hash": "568fec804c4ed273e73da2e56070ef810689c75fa8ee290b0d98cc6edebab5fd", "target_hash": "31c1a2022038f1e807193b551df837627f22344593dbf5c4b3dcd4e04e00cb07", "exact_match": 1.0}
{"doc_id": 2319, "doc": {"question": "when does monday night raw come on hulu", "answer": ["the following day"]}, "target": "['the following day']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does monday night raw come on hulu?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 7:00 p"]], "filtered_resps": ["7:00 p"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5a00b0854e9cfb64a8531cbc4735af7ec27a5592a5626e5774b488a411e12942", "prompt_hash": "4712277c833ffb5748ce5da34da15261afe68686eb32103ee69d8142c6e376c8", "target_hash": "7a421479f86e0f9c1266a12c4b840e96b4bf9e547ff92614869d443aa601f122", "exact_match": 0.0}
{"doc_id": 2320, "doc": {"question": "what was the first light beer in the us", "answer": ["Miller Lite"]}, "target": "['Miller Lite']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the first light beer in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michelob"]], "filtered_resps": ["Michelob"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "15fd4953de7b9ff75a7a9dafd1a014eb1a92cc7f9b9db3e3ec71dfd0c9faa9f0", "prompt_hash": "c9d6eaef2d3e6eecff52f440c48d6a2a5ce7e6e03d87fb0e5c989a83dd92c4d7", "target_hash": "1174aa028f402bdf83130c0069934cb78385f804f93eddf4e2b31c672da449ff", "exact_match": 0.0}
{"doc_id": 2321, "doc": {"question": "what was the ancient chinese umbrella used for", "answer": ["a defense against rain"]}, "target": "['a defense against rain']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the ancient chinese umbrella used for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" shelter from rain"]], "filtered_resps": ["shelter from rain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9e9a6e3dbbd45d2be9760e4c5a10cee032529d6f0bd22272b6d0b89f94e3a3ab", "prompt_hash": "ebef158cfbccffc7c8a5a48ccdd9d7ce682cb9007047d1727302de36ede1381f", "target_hash": "2c78d88417801844263bd47d154783df7d4b4043f80dc8330ef36a3c6577fd4a", "exact_match": 0.0}
{"doc_id": 2322, "doc": {"question": "who played guitar on my guitar gently weeps", "answer": ["Eric Clapton", "George Harrison"]}, "target": "['Eric Clapton', 'George Harrison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played guitar on my guitar gently weeps?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Harrison"]], "filtered_resps": ["George Harrison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7837f107b19937ee5f96fcdae76f01bede2b8ba8e6f843ad23cd53c62730e092", "prompt_hash": "ab2ca04fae08dfd25067b83d3553e0405399772dbf73c1c045453eb7094ffb83", "target_hash": "b0b5d57c1a666c319b950d025e20be8ea04bdaf439db0f96f274259ea704b4d9", "exact_match": 1.0}
{"doc_id": 2323, "doc": {"question": "who sings the song i feel it still", "answer": ["Portugal. The Man.", "Portugal. The Man"]}, "target": "['Portugal. The Man.', 'Portugal. The Man']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the song i feel it still?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rihanna"]], "filtered_resps": ["Rihanna"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2352ba91dadd99f87d4cfda7430a83c3f80f79e06a1e2c615957eb492c72f98c", "prompt_hash": "771491553cec86f2390597e39a33e684b02ce673148d262437379ab5503579cb", "target_hash": "72dfb8ff8f1a9cbe0e868e33ca03a5c014cd6f1b8ba0487116f1f57f801f8466", "exact_match": 0.0}
{"doc_id": 2324, "doc": {"question": "what is the name of the protease which is released in the stomach", "answer": ["Pepsin"]}, "target": "['Pepsin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the protease which is released in the stomach?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pepsin"]], "filtered_resps": ["pepsin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e78749cdaa5e9f4fa274aee1263b310cd38f99acacb5b447f95e517d635d836", "prompt_hash": "1857fd68934a1adc20b51bac129fd44beaaae111ed5b973559e2aff75b678031", "target_hash": "a651c66d254013d6bebff1f60be6950985410ed04ce89200d88ad57ac91b9ee1", "exact_match": 1.0}
{"doc_id": 2325, "doc": {"question": "where does fungi go on a food web", "answer": ["the end", "decomposers", "At the end"]}, "target": "['the end', 'decomposers', 'At the end']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does fungi go on a food web?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" decomposer"]], "filtered_resps": ["decomposer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f8eb7f57d3ed46242bdf7bc5d629417db8939a664852b5cdcf74a722467fc559", "prompt_hash": "afcaa847948a0f1fb90cc892eb9a2493fe89cbd39d0be95b9edad03f1584ee69", "target_hash": "c0b5b9956261815a9db1187b4aa3c87dcec739e7ce90846856e60843d74b3c52", "exact_match": 0.0}
{"doc_id": 2326, "doc": {"question": "where is wind power used most in the united states", "answer": ["Texas", "the central plains", "Iowa"]}, "target": "['Texas', 'the central plains', 'Iowa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is wind power used most in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Texas"]], "filtered_resps": ["Texas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e182fcdf7445a9192cefcc5672fd66be0cf2ca2b5149a74a17aee7594080fee", "prompt_hash": "169407cbe416e65db6ee79eb38d51eefee7b09b26f6a7686431a4f6600528379", "target_hash": "86ba01bbd86f24e0e520669c3433262f3ebd872f6bd2ad4311812e86972c4714", "exact_match": 1.0}
{"doc_id": 2327, "doc": {"question": "what are some elements that are similar to silver", "answer": ["Roentgenium (Rg)", "copper (Cu)", "gold (Au)"]}, "target": "['Roentgenium (Rg)', 'copper (Cu)', 'gold (Au)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are some elements that are similar to silver?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" copper"]], "filtered_resps": ["copper"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c9a94ffb3144271a92a6f50890f2aec7288a9e6da52e29460c17ff10ed1b7d81", "prompt_hash": "5bc6ea67519147db166c223c42b0e481ecfc554a10a88e835a0db5315cc1f58b", "target_hash": "bd2b2496dca76e3aac5baa7fb091947e5578d80a331505be90d35e02a8048b78", "exact_match": 0.0}
{"doc_id": 2328, "doc": {"question": "when did michigan last win a national championship", "answer": ["1989"]}, "target": "['1989']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did michigan last win a national championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "92297cbc437907388339be12e8adbcf9c19a927a1b80a5e2432879ca87e6adf0", "prompt_hash": "07a4212d1d8e079a8d875e5d5ce13a3e7a203661f94950a99df0285d47e6eabe", "target_hash": "f29cbf85fafcf80ace6273501b873d82d249a9b85142c12822207ef416c8728f", "exact_match": 0.0}
{"doc_id": 2329, "doc": {"question": "which indian actor has won most national awards", "answer": ["Amitabh Bachchan"]}, "target": "['Amitabh Bachchan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which indian actor has won most national awards?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Amitabh Bachchan"]], "filtered_resps": ["Amitabh Bachchan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "153ed09c5b3a11de179f5a05d2d389a4496d6e88ce855bfc94fcf80771872d64", "prompt_hash": "522f9efff593a4946dc3184208006a683824996ccb8901b64e0a516a4ae4baf0", "target_hash": "03ffbcb222758cf7c008f5e7799ca81a6c8775fffd3743b44e70f1e86c6df284", "exact_match": 1.0}
{"doc_id": 2330, "doc": {"question": "blood alcohol concentration means the parts of alcohol in the blood in relation to what", "answer": ["volume of blood", "ethanol"]}, "target": "['volume of blood', 'ethanol']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: blood alcohol concentration means the parts of alcohol in the blood in relation to what?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" volume"]], "filtered_resps": ["volume"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e448572a0b00088bc89b177762d60503a7096f7ed38e072b8b481035e77aeac", "prompt_hash": "f48ae8f9099823a966ffe37f0e01f9b8f2a3882d82af8e1fe8a104d52d05d077", "target_hash": "aa9e555e1742e5e13e4ac89eb0ea9f592dcdc9194bacc88eef22923f895351b3", "exact_match": 0.0}
{"doc_id": 2331, "doc": {"question": "what year did seven nation army come out", "answer": ["2003", "March 2003"]}, "target": "['2003', 'March 2003']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what year did seven nation army come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1991"]], "filtered_resps": ["1991"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3fd9de307562cd397633e376b7c34c90964f706b5e169650cccb1ad51be58159", "prompt_hash": "f0bf609e6812cc991a1868ca0c6b31bf8794681354815bcd9041a47600981fff", "target_hash": "984e15746422c70851bb877737d471a2e88e63e7dae06b8e78a72c0f75ad05d5", "exact_match": 0.0}
{"doc_id": 2332, "doc": {"question": "role of malonyl coa in fatty acid synthesis", "answer": ["chain elongation"]}, "target": "['chain elongation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: role of malonyl coa in fatty acid synthesis?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" acetyl-CoA carboxylase"]], "filtered_resps": ["acetyl-CoA carboxylase"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f72d4d0a23552ab9807e5cee592fa234b706d801e6e68783fbb340bf4c614de6", "prompt_hash": "b45229ff8b17afeb44f92eb00858cd23edef1c374173772cd8092892db815046", "target_hash": "07640a31a58c65822ce77c1102306dee9d067bca5d5ca8d6dfd663f74faa360d", "exact_match": 0.0}
{"doc_id": 2333, "doc": {"question": "who invented the glasses that give sight to the blind", "answer": ["2006", "Conrad Lewis"]}, "target": "['2006', 'Conrad Lewis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who invented the glasses that give sight to the blind?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George在京"]], "filtered_resps": ["George在京"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "259cde777520c8224e808c3b3c5e81aea8f5ad411e014d193601ca77c988b20a", "prompt_hash": "de78fd2ea256ed34568c3fd7a9cecc2c1415988834f2a44f37222ad9d2bbb770", "target_hash": "55b45d08697fe12f2ab00c5ed9f2eca997f55451ce21f186818722530246a945", "exact_match": 0.0}
{"doc_id": 2334, "doc": {"question": "what is the average time in hospice care", "answer": ["57 days"]}, "target": "['57 days']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the average time in hospice care?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 20 days"]], "filtered_resps": ["20 days"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b9adc41a25a22e46eaec46a0a0ad83d4a97c00d37fc62b220355ae8fe4d3bfb", "prompt_hash": "5bdef8198051dc4718917802617d7dc302bf43ad0e5e6cbf711cc5748fcc6be8", "target_hash": "3c9851de128d368d60bda387beefd817fd56444e0447760ca3de624c7007bf06", "exact_match": 0.0}
{"doc_id": 2335, "doc": {"question": "who plays the little girl in jurassic park", "answer": ["Ariana Clarice Richards"]}, "target": "['Ariana Clarice Richards']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the little girl in jurassic park?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lexie Webb"]], "filtered_resps": ["Lexie Webb"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "303a9c5d4d5cede110f8fe4f392606ddace20ee94861c0baef7e96431c31cafd", "prompt_hash": "eac00d010b645ccbef92294d3591dcd83f0a3e9d31361515063f1936952ed29e", "target_hash": "e3b8d318afb70676236b91e445d6c910d57c52d1930973fb71d404f9b1cc1214", "exact_match": 0.0}
{"doc_id": 2336, "doc": {"question": "who has participated in the most super bowls", "answer": ["New England Patriots"]}, "target": "['New England Patriots']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has participated in the most super bowls?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dallas Cowboys"]], "filtered_resps": ["Dallas Cowboys"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a5c0f889910e66f35800b825a739a79a6f2603581e9f53f2304d7a00538b4b6", "prompt_hash": "021bbf91d7c34e2d58486ba6f6387b1ea88d3ad3158670d3019a769d30c129be", "target_hash": "1b3aea9d6dea18aff5895d0c0931b3473ecb8a2da7c1127348320da079b1e8db", "exact_match": 0.0}
{"doc_id": 2337, "doc": {"question": "when was the last time michigan basketball won the championship", "answer": ["1989"]}, "target": "['1989']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time michigan basketball won the championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1989"]], "filtered_resps": ["1989"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d753db3773146dc2858589dcf63753428f8c01fe00643b000d09b45fcb9225ec", "prompt_hash": "4751463f75277c456f080d19c632a75026285742f2e33ab22745eca2c392c7f4", "target_hash": "f29cbf85fafcf80ace6273501b873d82d249a9b85142c12822207ef416c8728f", "exact_match": 1.0}
{"doc_id": 2338, "doc": {"question": "who plays spock in the new star trek movies", "answer": ["Zachary John Quinto"]}, "target": "['Zachary John Quinto']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays spock in the new star trek movies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Zachary Quinto"]], "filtered_resps": ["Zachary Quinto"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "78008f7089f264df9a000fb73b0ccb233d859864b4c076c6b9f471f9c7ac91ad", "prompt_hash": "47b5f4f81536f4a2f1df5f967c7bdfd7d275b3aa92823bc6b8d32f62778e552e", "target_hash": "2b2050538936264df1c8446c155a30605cf6a50b4e568ca54f8567f164f686b0", "exact_match": 0.0}
{"doc_id": 2339, "doc": {"question": "when did las vegas become a gambling town", "answer": ["1931", "In 1931"]}, "target": "['1931', 'In 1931']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did las vegas become a gambling town?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1931"]], "filtered_resps": ["1931"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c9ebfb056c18fc180a39b1b50ab050ab4d6ee5b2533b697445f65066c069d84", "prompt_hash": "2d4c8762f349b3b607c46c235a8a3ac651332b8ae023fe06322655cedd5cd872", "target_hash": "54831654715723cf024dde12cb7860d22038e3a0f28d906b65f1cac4ae6c3663", "exact_match": 1.0}
{"doc_id": 2340, "doc": {"question": "who wins america's next top model cycle 20", "answer": ["Jourdan Miller"]}, "target": "['Jourdan Miller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wins america's next top model cycle 20?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Candice "]], "filtered_resps": ["Candice "], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5f6ad2425259a334e5283315aaf12c4647c2a539e964569f6be44a82456cdea8", "prompt_hash": "0de80c032f72323db0c1bafeca82ee750968cab8a3c2fb4575f4992ba1d3e5ed", "target_hash": "f9c55fcc5d66d2e21e40786367b28fc8e442ac1f40b5b9438d6bad381c3e2204", "exact_match": 0.0}
{"doc_id": 2341, "doc": {"question": "when was the day that the pubs ran dry in the poem 9-3", "answer": ["31 October 1972"]}, "target": "['31 October 1972']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the day that the pubs ran dry in the poem 9-3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the day that the pubs ran dry"]], "filtered_resps": ["the day that the pubs ran dry"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9b69c36e600b623025b8507d61e6b2d6c06ea8a32af5dcf7fe8759d85cf85a4c", "prompt_hash": "d9f3d1c45334eede0389dec1e6c2609e2986b342b8b5db91e1f6dd5abeda0bb0", "target_hash": "1809d31e2672fbcfa664e887bb737274fa5c29e39f29b04661f0d3de1f0df919", "exact_match": 0.0}
{"doc_id": 2342, "doc": {"question": "who wrote were going on a bear hunt", "answer": ["Michael Rosen"]}, "target": "['Michael Rosen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote were going on a bear hunt?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Rosen"]], "filtered_resps": ["Michael Rosen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "99baff5ac626b13344f964c5e1707ba8cad1d4f36a81300c6555cf1ce2f6af71", "prompt_hash": "456974534537ccf79b6ae600263c1667895b2e90838d73ed856897b7a8603441", "target_hash": "fe991e64930eb97cbc02dec97994a8e1e88d597d9171edad2d7afee81cbdd81e", "exact_match": 1.0}
{"doc_id": 2343, "doc": {"question": "when was son of a preacher man released", "answer": ["late 1968", "November 8, 1968"]}, "target": "['late 1968', 'November 8, 1968']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was son of a preacher man released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1977"]], "filtered_resps": ["1977"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3fb0635fbed0a5ebda5b70f68152c0d4c952cd719f365b997df67fc1ab051226", "prompt_hash": "20e290231be8e739cd71b335c99e3f9e57dc591c682f3cde607bc89cf0e8ad0e", "target_hash": "28b3673774c70581f6433825af851c1fdcb8f7afac0a3fb5c793229ec200034e", "exact_match": 0.0}
{"doc_id": 2344, "doc": {"question": "who has the most restaurants in the world", "answer": ["SUBWAY"]}, "target": "['SUBWAY']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most restaurants in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" McDonald's"]], "filtered_resps": ["McDonald's"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "216c234935b98a8de119d7994a98b870c25b89b8b505fdfba4ff4f1bb0e50e9e", "prompt_hash": "b9fa42a471406ff0d85d6c6506bcad88da12f535c45f6cf85432891938adbbb3", "target_hash": "465e464558033d3272abd09deffeeeb286d0fe42b1f5937bc0c123febb8140e0", "exact_match": 0.0}
{"doc_id": 2345, "doc": {"question": "when did the eagles win the super bowl", "answer": ["2017"]}, "target": "['2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the eagles win the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1981"]], "filtered_resps": ["1981"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d78566414f422643182506dca72f32da36cde8488a3d977d703702a2b93945e", "prompt_hash": "d1fd0ba0b52304a6c985881de9e45e1262cfdacbbc1216f9fe7fead085a3d3e8", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "exact_match": 0.0}
{"doc_id": 2346, "doc": {"question": "when does the eclipse end in the us", "answer": ["2:44 p.m. EDT"]}, "target": "['2:44\\xa0p.m. EDT']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the eclipse end in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" August 21"]], "filtered_resps": ["August 21"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb05fca6aa0fdcb9ae87bd3c8b59960ef53bc8ebc929201d8ab96252e84db85c", "prompt_hash": "9d3daa7a0956c39b84bd17edce1141d4fc106f9907e519fdf445c4f3b4d2f28c", "target_hash": "34237a4c891b7904c0e332830b8daacd9dd66c08f3364a7932b15d6d2f741176", "exact_match": 0.0}
{"doc_id": 2347, "doc": {"question": "hart of dixie season 4 how many episodes", "answer": ["10"]}, "target": "['10']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: hart of dixie season 4 how many episodes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 22"]], "filtered_resps": ["22"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b48537913c9f8ee9edd68a37080f1e79e472aa2c9c61a2499398e0b31f6e0799", "prompt_hash": "c918b68eee43354ea35dfbc11832a238664ab2f1b48adcf71431d1c3957b78e4", "target_hash": "9503346d501aeaec3f521be685eb235cda81c7929ba2f1acdae1be91400b787a", "exact_match": 0.0}
{"doc_id": 2348, "doc": {"question": "green algae is an example of which type of reproduction", "answer": ["alternation of generations", "alternation of generations."]}, "target": "['alternation of generations', 'alternation of generations.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: green algae is an example of which type of reproduction?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sexual"]], "filtered_resps": ["sexual"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6c15a140382524400240409eab090d1e442298401c041663f33737a6a292606e", "prompt_hash": "57ecf2680f95c5cb6aa465494106b664a8f1d4364378df862d121d79eb5f0904", "target_hash": "382b10a05b3e07947ec0f83bb5a3b99d40eb25bb2790475fe1b57923600f3d2f", "exact_match": 0.0}
{"doc_id": 2349, "doc": {"question": "when did avatar the last airbender first air", "answer": ["February 2005", "February 21, 2005", "February 21, 2005"]}, "target": "['February 2005', 'February 21, 2005', 'February\\xa021,\\xa02005']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did avatar the last airbender first air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2005"]], "filtered_resps": ["2005"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "40fa6917a2d18e177e68ef192f3e816a2530bcd049b26359394e6fed81c99c2d", "prompt_hash": "ca76867cdc38efcfeccc9b4c40569b42ee4df3bae99bd72216d53740378cd8f6", "target_hash": "f1577c3b2a7a092bf97f56385a1c97237bbe996770e97a3647fe16c652278886", "exact_match": 0.0}
{"doc_id": 2350, "doc": {"question": "where will the first round of march madness be played", "answer": ["Wichita State University", "Boise State University", "Duquesne University", "Big 12 Conference"]}, "target": "['Wichita State University', 'Boise State University', 'Duquesne University', 'Big 12 Conference']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where will the first round of march madness be played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New Orleans"]], "filtered_resps": ["New Orleans"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "de3f40060156e87660090c4f00e59b6e4ce655b7c04574b566d8e1cd7e7451d3", "prompt_hash": "f95a34f7882bff86db40e324ab52aa3aeef8b8a102ae99d358e27a8fa635a37a", "target_hash": "bb8cdf786170785302d33998cd10a98adb536b54df559aaac62d2e6310cf634f", "exact_match": 0.0}
{"doc_id": 2351, "doc": {"question": "when was the abacus invented in ancient china", "answer": ["2nd century BC"]}, "target": "['2nd century BC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the abacus invented in ancient china?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2500 BC"]], "filtered_resps": ["2500 BC"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c45bfbabeea0f57b0d057e3bebb5cb21bcbede652d3b43e1b33caa38b1fa45c8", "prompt_hash": "32e426d4ea17cd458614b9ac4e7771497fb10ab41b20c0251cf7d936eb988e3a", "target_hash": "da1186c6b16b556fb048c684810ca069b7d80940e76b98864b30bf7c6a7400a6", "exact_match": 0.0}
{"doc_id": 2352, "doc": {"question": "who's playing in the sec championship 2017", "answer": ["Auburn Tigers", "Georgia Bulldogs"]}, "target": "['Auburn Tigers', 'Georgia Bulldogs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who's playing in the sec championship 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Auburn"]], "filtered_resps": ["Auburn"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9fd1ae988bd0cdcb31d7a7788f5e280eced6465433df991f7c0ada852b33c3c", "prompt_hash": "167c1e9310eff1eb14567d7eb4277e7a0b7591613f798c09331d0cba5d6c2f3c", "target_hash": "6136aef1aa9b5f684b1ae5765cbeeb7918c9dcd61278b770b597448ed2474b81", "exact_match": 0.0}
{"doc_id": 2353, "doc": {"question": "who sang the song if loving you is wrong i don't want to be right", "answer": ["The Emotions", "Luther Ingram"]}, "target": "['The Emotions', 'Luther Ingram']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song if loving you is wrong i don't want to be right?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dolly Parton"]], "filtered_resps": ["Dolly Parton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a9381cc7558f13853d86eff8c0adf469042eea3b144ff9c80d915280a7d454b", "prompt_hash": "7d54b8bddc1c45c204f03fc18faadcff03f771c6d3febfe7e22aabe7f8f47eea", "target_hash": "af967e6e7bda056a0308f8a1b8424609cf4f1048e61187710f9ec6f80251925e", "exact_match": 0.0}
{"doc_id": 2354, "doc": {"question": "who wrote the declaration of man and citizen", "answer": ["General Lafayette", "Honoré Mirabeau", "Thomas Jefferson"]}, "target": "['General Lafayette', 'Honoré Mirabeau', 'Thomas Jefferson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the declaration of man and citizen?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Declaration of the Rights of Man and of the Citizen"]], "filtered_resps": ["The Declaration of the Rights of Man and of the Citizen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "795a6b921d3fd6f1801c123044418e90e523e7152c78b9bfdd4d952633f7eb5d", "prompt_hash": "3fce474fc3cf791cb2f738ce67e3139b6baffdd004e3eb27fc04f3b89f2e1e96", "target_hash": "2f33e206de99304d1b59a6d7a28b0ed828ff0e944e2b9699749ae6226c17f9ab", "exact_match": 0.0}
{"doc_id": 2355, "doc": {"question": "what written material is included in the talmud", "answer": ["the Mishnah", "the Gemara"]}, "target": "['the Mishnah', 'the Gemara']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what written material is included in the talmud?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mishnah"]], "filtered_resps": ["Mishnah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b9ceaa031bb3e5086f26cf7bf848bf963b9b916062fbc8f8a4035fba82961dd", "prompt_hash": "3a89fadc94d0f5033fa8cef61b7c94ee090d0a0963dcb2753f0b6030a86f8eff", "target_hash": "8aea4bb1b601dca4a8088fd5dc0251361f6a6a39e6a03eb6a859ace567dfcd6d", "exact_match": 1.0}
{"doc_id": 2356, "doc": {"question": "the boards name on ed edd and eddy", "answer": ["Plank"]}, "target": "['Plank']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the boards name on ed edd and eddy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Board of Directors"]], "filtered_resps": ["The Board of Directors"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c46a4a6c74d4111d2c61a0c6176274fb1d954a94c4c8def74fa42b5cd9b97b07", "prompt_hash": "b8a7f97a27f55bda32babe85bf02dcee38a479ad0cb027d66261661720d8165e", "target_hash": "3a043039b0099b4bbadd1f9dbc4c2fda2298fdccab90d53b259ba4f7b92cc186", "exact_match": 0.0}
{"doc_id": 2357, "doc": {"question": "where do the blood vessels that supply the retinal cells lie", "answer": ["optic nerve", "The optic nerve"]}, "target": "['optic nerve', 'The optic nerve']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the blood vessels that supply the retinal cells lie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" choroid"]], "filtered_resps": ["choroid"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "13af581823b5156b83934ed20e45f4d7f106219854ae2161266fafff133afc56", "prompt_hash": "8164a30b852f8a715ebfb6b80977cc55d087ad09d5d583ad4333e520c86a1cd3", "target_hash": "973edca21948c5b8f628d9aafa8f9daacab17d8ee166a593b28f1cb72222ed4d", "exact_match": 0.0}
{"doc_id": 2358, "doc": {"question": "who is responsible for introducing the principle of uniformitarianism", "answer": ["James Hutton", "geologist James Hutton", "William Whewell"]}, "target": "['James Hutton', 'geologist James Hutton', 'William Whewell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is responsible for introducing the principle of uniformitarianism?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Hutton"]], "filtered_resps": ["James Hutton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19a572b48bde35db4ab530ef86f914d033e0a6554c03dae3f3ace503d14bc047", "prompt_hash": "dfd2d6c8876d25ae36ef98eb670c74f5a6b63fdab93a8ee47dbc8aa1ef1c4ea8", "target_hash": "1aaeb39d394850c7fce7cd1dbc974e80603410f618279f46ce4458901eb91327", "exact_match": 1.0}
{"doc_id": 2359, "doc": {"question": "who was the actor who played grizzly adams", "answer": ["\"Dan\" Haggerty"]}, "target": "['\"Dan\" Haggerty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the actor who played grizzly adams?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Arness"]], "filtered_resps": ["James Arness"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "94e9f014abb291ad1b9768b3fb02af248bb2bec22b1ae35864971fc1f484ff26", "prompt_hash": "f818852ee755d6d4844877c3fe079d94b7022bf73a920a75986829820fe34687", "target_hash": "f5b25380a3dcfb13bb107ca1ffce211fc97d57f79cee5e67bed77659bae2b44b", "exact_match": 0.0}
{"doc_id": 2360, "doc": {"question": "who owns the delano hotel in las vegas", "answer": ["MGM Resorts International"]}, "target": "['MGM Resorts International']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns the delano hotel in las vegas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" MGM Resorts International"]], "filtered_resps": ["MGM Resorts International"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c50a3a9097459323cd95d4ce94fd31264bc7ad53f19c3ed79360ccda2ed89131", "prompt_hash": "e97a5fe0e8506a33dfa296c6bafd9fe99732ab76c9457d61db924c767bbc8f21", "target_hash": "c88630fa60e326917cb42577bfec1f076a474cab17aeed12f74f2bee3a1f4360", "exact_match": 1.0}
{"doc_id": 2361, "doc": {"question": "who is the owner of the crowne plaza", "answer": ["InterContinental Hotels Group"]}, "target": "['InterContinental Hotels Group']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the owner of the crowne plaza?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Marriott International"]], "filtered_resps": ["Marriott International"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf49137849f38cf66e2ad7677532d97f278b9421c43b7d3b15b80f0efa83fcfa", "prompt_hash": "8a75f96e6d2af670c1f55007cfad76ee3a7767bc494bf915eb1307b8f3639ae4", "target_hash": "cd8c82cb3ce8c2486fb2b42dfae745cee4b4d7c8917ff658a4d515df0b8ad02b", "exact_match": 0.0}
{"doc_id": 2362, "doc": {"question": "what is the location at the end of minority report", "answer": ["an undisclosed location"]}, "target": "['an undisclosed location']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the location at the end of minority report?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Washington"]], "filtered_resps": ["Washington"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d3faf42f3610238e55abf2901ca8e19ef3cf6e1c3030c847a80be293e260592", "prompt_hash": "382719022a8fac9634116afe99b04fdb9a7745066c421c743f2b86bd9e0d7a61", "target_hash": "09fe5fbbb5dde3ae57e856f18561fa14d2904e12179453c7f3c9c7fab582c2b8", "exact_match": 0.0}
{"doc_id": 2363, "doc": {"question": "who won the 2017 women's wimbledon final", "answer": ["Garbiñe Muguruza"]}, "target": "['Garbiñe Muguruza']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the 2017 women's wimbledon final?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Karolina Pliskova"]], "filtered_resps": ["Karolina Pliskova"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4dde804ad0c18cefef8e896c9fd20bd0f9c3cd01a632ad6c7907cf783bbace0c", "prompt_hash": "c3ae47f953ccba2106834da09f66c0af48dc45837d14a378d34de8fb31f6adbd", "target_hash": "02540740fee408fece0251816b3fd048445fbe5dc31e6597b89bce9cca95ae1f", "exact_match": 0.0}
{"doc_id": 2364, "doc": {"question": "what part of maryland is close to washington dc", "answer": ["Charles County", "Calvert County", "Montgomery County", "Washington metropolitan area", "Prince George's County", "Frederick County"]}, "target": "['Charles County', 'Calvert County', 'Montgomery County', 'Washington metropolitan area', \"Prince George's County\", 'Frederick County']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what part of maryland is close to washington dc?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Northern"]], "filtered_resps": ["Northern"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e516f52281e05c477916da3b08717d85ca0fb96b15dfd4ab819d32aced56e75", "prompt_hash": "0824356288160db3b3e1201c537860b4cb75dfdb9ee83722b10d0e7c174698b6", "target_hash": "04fb011ffa4fb947a0404cbe6a019f11c1376340e4ab8bc3dd6065f1c8033f83", "exact_match": 0.0}
{"doc_id": 2365, "doc": {"question": "who invented the chip in. debit cards", "answer": ["Helmut Gröttrup", "Jürgen Dethloff"]}, "target": "['Helmut Gröttrup', 'Jürgen Dethloff']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who invented the chip in. debit cards?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Europay"]], "filtered_resps": ["Europay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f131d430c070fe11622edc7c4fd977069bd86a9bbfe89ccc5d6b8757e0af87b", "prompt_hash": "86294ac4be61e5b1e1d6517dccace87415fbf7bda0a9ad3ecfee785fc5511e2e", "target_hash": "c27ed67c1885ff8997eae713251a8eb8fa5486244c54071774f933fbace431a9", "exact_match": 0.0}
{"doc_id": 2366, "doc": {"question": "who proposed evolution in 1859 as the basis of biological development", "answer": ["Alfred Russel Wallace", "Charles Darwin"]}, "target": "['Alfred Russel Wallace', 'Charles Darwin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who proposed evolution in 1859 as the basis of biological development?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Charles Darwin"]], "filtered_resps": ["Charles Darwin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c86ba10da341a766c4e93b7d03b4bbbd2963555e274bae0a96eabee2bafc7872", "prompt_hash": "2bcbcf596aab464bee9baaae8f51e4ee9b4e26bc04ef0b9fceca50c25652235a", "target_hash": "d32bf91f802c8c412cde397fd3ba7bbc661cde9be4e06d72f22209134da2148d", "exact_match": 1.0}
{"doc_id": 2367, "doc": {"question": "who sang raindrops keep fallin on my head", "answer": ["B. J. Thomas"]}, "target": "['B. J. Thomas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang raindrops keep fallin on my head?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bob Dylan"]], "filtered_resps": ["Bob Dylan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db8101953ee70fcff9ff684238620b13a4f705fd7334d2771023d1c8c7c88bc9", "prompt_hash": "252a3883ae3e0d5dd25eadb727893a713071ea56eb43dae7d54c1e4aa47a956a", "target_hash": "01a5420e66874d1d3c6acccadb68ccf44409783ebb93d1084363125a87f512db", "exact_match": 0.0}
{"doc_id": 2368, "doc": {"question": "how many countries participated for the first time in the 2014 olympic winter games in sochi", "answer": ["Brazil", "Uzbekistan", "Turkey"]}, "target": "['Brazil', 'Uzbekistan', 'Turkey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many countries participated for the first time in the 2014 olympic winter games in sochi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 80"]], "filtered_resps": ["80"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0494ab4e1f318b0f2fc2f9aa318f0b93f34f30b21c6dfe109cd5c372971d831e", "prompt_hash": "6534d25c1bb5b3e9cc105412fe25fd87cb1f2befb49e7610e13c72ffd89abdd8", "target_hash": "5af26f36455454cb576e5f528194a7a510b47d28a9810d76df6e424b8d9a1861", "exact_match": 0.0}
{"doc_id": 2369, "doc": {"question": "what is the name of a camel with 2 humps", "answer": ["Bactrian", "the Bactrian"]}, "target": "['Bactrian', 'the Bactrian']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of a camel with 2 humps?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dromedary"]], "filtered_resps": ["dromedary"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6316319539f40314b75cce98b62c71fcf12fd4d024b8033b229f24e0338458aa", "prompt_hash": "b747997209744264fa4ab87c440b62182c2b13ccef3103ef2ed5a32c8abbe593", "target_hash": "b5280310f992e90da4129ebeeb88e31c77573add77f829c963bfe82f4bc9b99e", "exact_match": 0.0}
{"doc_id": 2370, "doc": {"question": "a 58-story skyscraper in san francisco is tilting and sinking", "answer": ["Millennium Tower"]}, "target": "['Millennium Tower']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a 58-story skyscraper in san francisco is tilting and sinking?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the crocker bank building"]], "filtered_resps": ["the crocker bank building"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf80946ce9d957dd872231397d8231d8b79d73d5bbcc9d89490fbf7b9b1e1c7b", "prompt_hash": "3001561a45e3df75434d7342b1cbe4afc347c28c23e5356e8fda7c68baf01868", "target_hash": "49019398c90c60be933da2d558ff8d41ab7c007e2f6ae8b0772483377bd26305", "exact_match": 0.0}
{"doc_id": 2371, "doc": {"question": "when does the men's worlds curling start", "answer": ["March 31"]}, "target": "['March 31']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the men's worlds curling start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 15"]], "filtered_resps": ["February 15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2e96ba62de9543caf98939b15af446f1fa40acbce1563841a3c1090f3fc1c59", "prompt_hash": "a1c3dc8a6cd61741233cffabd7aff276a1afbfa59d5a5791fdb3da8760b4bebb", "target_hash": "3033137ebce571e122c6c250d3873a22b63bf595e4e64c3dd2d9351297efa013", "exact_match": 0.0}
{"doc_id": 2372, "doc": {"question": "who were the three elves who got rings", "answer": ["Círdan", "Gil-galad", "Galadriel"]}, "target": "['Círdan', 'Gil-galad', 'Galadriel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the three elves who got rings?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sauron"]], "filtered_resps": ["Sauron"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cf22d35ac27b5829399bf5a7e61f424f6fcd957c91931435ebd10c6c5e4c9da6", "prompt_hash": "b3e887bc760f1b57743294e3994cb1d699f50d8421d42dde6c5a77820b15a033", "target_hash": "ed60dc9c30f0e63ab43df3db82159a5e43ddcc313e8f4ad2c321ea0ab01f4ca6", "exact_match": 0.0}
{"doc_id": 2373, "doc": {"question": "when did the movie breakfast club come out", "answer": ["February 15, 1985", "February 15, 1985", "1985"]}, "target": "['February 15, 1985', 'February\\xa015,\\xa01985', '1985']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the movie breakfast club come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1985"]], "filtered_resps": ["1985"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "448fe9a81ddbb06921f33560c568d2c4d8f9ad70134b15e81e5a9c8f2beaaa8c", "prompt_hash": "f7755adef2055c250ce9f5f4c3a352247e38479c8307300933f367893a94b306", "target_hash": "00180849d086492a6590e22d945ee81d3d1c3df95171fd235bd026dccbc52ba0", "exact_match": 1.0}
{"doc_id": 2374, "doc": {"question": "who sang the song how far is heaven", "answer": ["Los Lonely Boys"]}, "target": "['Los Lonely Boys']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song how far is heaven?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Carpenters"]], "filtered_resps": ["The Carpenters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea721dad5d7ece745604ce3e2769e7ecfbe9b6ef43d38df9ff3e0146fb78baba", "prompt_hash": "b70a948a2072841cd4d8e402f24240941746a41cb066abbb28b3ab28397b5269", "target_hash": "bbc240bca521a9fd2bfdfa5bd46b546b6f43586b1371e02c403326249096e67e", "exact_match": 0.0}
{"doc_id": 2375, "doc": {"question": "where did zendaya place in dancing with the stars", "answer": ["Runner-up"]}, "target": "['Runner-up']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did zendaya place in dancing with the stars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" semifinals"]], "filtered_resps": ["semifinals"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fd9a94105933b3d016619a45f6ecf7e25ba999b3420082fb14ed2ee8a184ef3f", "prompt_hash": "230389939c3c76ce099f39bf6375af8016f62e61dfff3a3e64b553e16b788733", "target_hash": "59a99e228e9209ef6b7d82a24b2d2aed9a7d60101816544ee8e934f8f7232e12", "exact_match": 0.0}
{"doc_id": 2376, "doc": {"question": "when does sloan come into grey's anatomy", "answer": ["season two"]}, "target": "['season two']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does sloan come into grey's anatomy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 3"]], "filtered_resps": ["season 3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c7af79a770571a22fb164792117df30927cb4fc80f9f1d84425b62bf0bb01c6b", "prompt_hash": "383aa74f7afbfbfc1203be447684dbf7af1c2a4829053379eb119ce28bf6598a", "target_hash": "1ca80514d4fa2a4f3db3dd4a1836eea044d6dc94177cc550634baff3eb0b198e", "exact_match": 0.0}
{"doc_id": 2377, "doc": {"question": "how many countries are a part of opec", "answer": ["14"]}, "target": "['14']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many countries are a part of opec?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dde537c31ce3a5905c7c716a4e4334ccbdf95e0c929c7b6f2c54753013dd67ab", "prompt_hash": "5f82d4195a5ed1e3202dc4426783043667ab99ba68d2df46143a85a85f9a8164", "target_hash": "1c4adeef9d9168604b52ef50b17fab3bebe77f9e001e8dcaa2e38dfcfc074f31", "exact_match": 0.0}
{"doc_id": 2378, "doc": {"question": "when does buffy tell her friends she was in heaven", "answer": ["Something to Sing About"]}, "target": "['Something to Sing About']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does buffy tell her friends she was in heaven?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Halloween"]], "filtered_resps": ["Halloween"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c89ef474efc37083e9cac29931e933d92f29c3fbc2643693207dec21d80237e", "prompt_hash": "61307bf55da9d6cd32bac66cc7148c793d0f63f11dac21ced645def7350a3a0a", "target_hash": "167468860b757488f2aefda8515ff92ba950fad8dd2ff1d8868805c001737120", "exact_match": 0.0}
{"doc_id": 2379, "doc": {"question": "when did the nba start playing zone defense", "answer": ["2002", "the 2001–2002 season"]}, "target": "['2002', 'the 2001–2002 season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the nba start playing zone defense?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49f651835fe8352ecf619859ca62e151051753531dd7766825519643ddca696e", "prompt_hash": "c933bb9b2c98ff61c6731e56161f10ed7d71da3c2f428c241504147eebc80c63", "target_hash": "18d6157b5990a9d10ca07dbd66cd3b65f11d01e0e470ee0776f80f7298217337", "exact_match": 0.0}
{"doc_id": 2380, "doc": {"question": "how many plane surfaces does a cylinder have", "answer": ["two parallel planes", "two"]}, "target": "['two parallel planes', 'two']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many plane surfaces does a cylinder have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b874bac6b5839b09116ef80b636db2ed692253c4bb57563c8798be28bbd72b7c", "prompt_hash": "02efebfa6b45af32263b10b66e7e2025a12b4015e04b3f7f7dbc49c601918d49", "target_hash": "8d112854ded987624428452b5c23bb65227782f3bfeef05c7693679dae66fe3e", "exact_match": 0.0}
{"doc_id": 2381, "doc": {"question": "who plays max voice in a goofy movie", "answer": ["Jason Marsden"]}, "target": "['Jason Marsden']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays max voice in a goofy movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Adam Sandler"]], "filtered_resps": ["Adam Sandler"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96e6f2fae4037e203a88604ca318707a9b8b765a17707efa47f980d8905b49fe", "prompt_hash": "cd1ab3f622ed200e7ccfe6c873ca92c96c1b3a40db9d02390e0475e620c066ec", "target_hash": "c65c1c19dfaf8a8708ba8b87417a1c21e5562e4c16bfb41fdab57fcbdfaffee1", "exact_match": 0.0}
{"doc_id": 2382, "doc": {"question": "when did ford change the f150 body style", "answer": ["the 2009 model year", "1957"]}, "target": "['the 2009 model year', '1957']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did ford change the f150 body style?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1982"]], "filtered_resps": ["1982"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bc156f2195c774539e523bf3b26bb9afad66ca55983dd4158fed2d72e3ba51cb", "prompt_hash": "941eb2176c8659a3affa98572f5e71d9f6bd1599f89a9eb21a71bffed3291c9a", "target_hash": "0547ad072377f74c0d8a4dc490c47733c53e0160efb98e38a0464e35b79e62e2", "exact_match": 0.0}
{"doc_id": 2383, "doc": {"question": "who played scotty baldwins father on general hospital", "answer": ["Ross Elliott", "Peter Hansen"]}, "target": "['Ross Elliott', 'Peter Hansen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played scotty baldwins father on general hospital?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Gregory"]], "filtered_resps": ["James Gregory"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55f686cafea09c3b26e2b438f4f9f9f3992cff7d1a59d24c03c76da94051d6ee", "prompt_hash": "815ab906465fd9a4c669e165e63844158736d52b47fdc2a15bc0dcf99f945619", "target_hash": "8df317734e7d7ab24b300d9127c38a25da4480435ca0fd557044252b7cb24d81", "exact_match": 0.0}
{"doc_id": 2384, "doc": {"question": "when was the miraculous journey of edward tulane published", "answer": ["2006", "March 30, 2006"]}, "target": "['2006', 'March 30, 2006']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the miraculous journey of edward tulane published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2006"]], "filtered_resps": ["2006"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4fb7a6b00ff0898d30c3d79b941722cfc6ad6ac86cde437d4005789505deab80", "prompt_hash": "22184bca92c926d294129ca48f7a6110c09a88610e96df86c5f09ffa28490509", "target_hash": "7ade15ab5e931474565ae62ca93ef57bcd23dfe676fbda162d2c22d249233e9e", "exact_match": 1.0}
{"doc_id": 2385, "doc": {"question": "who is the current defence against the dark arts teacher", "answer": ["Amycus Carrow"]}, "target": "['Amycus Carrow']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the current defence against the dark arts teacher?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rubeus Hagrid"]], "filtered_resps": ["Rubeus Hagrid"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ba4a73360331eff63660ff11c7b75df78fc5699d977dd8dc398e3bbc5712f17f", "prompt_hash": "26187aed3c4d4270fe88dde4f4498ba699d019406c524d49fc10ca3d25d896ff", "target_hash": "99a72db30a90425f49c3d9676a62270c6e7445482f5f8a2dc494f7be95854bed", "exact_match": 0.0}
{"doc_id": 2386, "doc": {"question": "what type of snake is kaa in jungle book", "answer": ["Indian rock python"]}, "target": "['Indian rock python']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of snake is kaa in jungle book?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" python"]], "filtered_resps": ["python"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e25236637664b33ebd4560e4774f1bad0082252c91d4ecf6414bff59f64044d3", "prompt_hash": "9cf26592d09257d3933d33ed2a0ee39a1ea13ef340a0894a6c87fbcedbca60c4", "target_hash": "1eb509d3bbd5edd6f36e3f92315175d9b7c15cf27a6962aec5455c51911e2ebb", "exact_match": 0.0}
{"doc_id": 2387, "doc": {"question": "when does ellen's new game show start", "answer": ["December 18, 2017"]}, "target": "['December 18, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does ellen's new game show start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 13"]], "filtered_resps": ["January 13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d44623deb410252519d2782dc069b575ab95f5dfaf4af2701c6feaaedda6bfd", "prompt_hash": "18fbcd621c858ca56a26373f9c6feafe55a291f5a12d4eea0ac7a611b971c9d2", "target_hash": "041238d044308fa6349c86ba2db94e1d247bf29bfe0a46c4699ebf4a576cec0d", "exact_match": 0.0}
{"doc_id": 2388, "doc": {"question": "who was the original host of jokers wild", "answer": ["Jack Barry"]}, "target": "['Jack Barry']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the original host of jokers wild?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chuck Woolery"]], "filtered_resps": ["Chuck Woolery"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3cd4cb6dd129bbd46511e9836a122ab9f21bd6415a08750964d027b10dc977ed", "prompt_hash": "e4a3a9340f7878220dac5e3e7e9a3c9d5ab197431bec67f9e1ea16a1fa7fc09c", "target_hash": "73672d63edae47087991c801060f76c2b9b3bfd59a10b61c922415fa2622ca55", "exact_match": 0.0}
{"doc_id": 2389, "doc": {"question": "who has more super bowl wins nfc or afc", "answer": ["NFC", "NFC/NFL"]}, "target": "['NFC', 'NFC/NFL']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has more super bowl wins nfc or afc?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" AFC"]], "filtered_resps": ["AFC"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b73ecef1a9f767cb9882c899f9ff49925a5069b58255e3d1a3a2b2bf266968cf", "prompt_hash": "eb1bb4f54dbf40cd2903c6edd89eae68270064a3dbe21cb98d00a193d467a256", "target_hash": "1c7a40115ff7cea21f6e23fa4a275004243c7f2af957efc12a8cecc025be9f37", "exact_match": 0.0}
{"doc_id": 2390, "doc": {"question": "name the tissue type of the inner most layer of cells", "answer": ["Epithelium"]}, "target": "['Epithelium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name the tissue type of the inner most layer of cells?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" endothelium"]], "filtered_resps": ["endothelium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "baeba223689533a8e2d9225223f9669aa819d99d4b25e7cb6eef659b78d268f8", "prompt_hash": "8b531bf902dd9bdbd782cfb66b6cf6970d7659c6d498259c7053f2a9660835c9", "target_hash": "819a59e33bcd594987ac11390739bb0e12e2266f243ba1132960eaff2425d1da", "exact_match": 0.0}
{"doc_id": 2391, "doc": {"question": "what city was zeus the patron god of", "answer": ["Olympia", "Elis"]}, "target": "['Olympia', 'Elis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what city was zeus the patron god of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Athens"]], "filtered_resps": ["Athens"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "63ddd1d4e950813051f9da166eccc12ae432ddd739db38a8550b8c72141e7e0e", "prompt_hash": "dc70a8d7f836cb73367027cd933f2f7a3ff10eee23c24c51f5bffbea69002d55", "target_hash": "c7bcef8474d347f2316fb517c3e60b65b54c46d9620a37c84ad6835573343d1f", "exact_match": 0.0}
{"doc_id": 2392, "doc": {"question": "when does the implantation of the embryo occur", "answer": ["around 9 days after ovulation"]}, "target": "['around 9 days after ovulation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the implantation of the embryo occur?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fertilization"]], "filtered_resps": ["fertilization"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c12718c857c12801e76e018cbe910c327089e154f0b428d655c0efded937238b", "prompt_hash": "a4102700e284dd159479878c8e0df28920fe7405ab89bb0ed71db92ba6ae9de9", "target_hash": "8f618eeb1149bc3ed615294212da1b0cfa301c1eb34d99433a88d0b47e330eb6", "exact_match": 0.0}
{"doc_id": 2393, "doc": {"question": "who is the richest club in the championship", "answer": ["Manchester City", "Aston Villa"]}, "target": "['Manchester City', 'Aston Villa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the richest club in the championship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" West Bromwich Albion"]], "filtered_resps": ["West Bromwich Albion"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "acf080a0951463411879f5ec8ae16965a7c0784f92d038729049240dbac7d1cf", "prompt_hash": "2f749b3a1120378de81a6298e18a6c897dcfe8e563133c65bd8224945511fd9f", "target_hash": "0bb5eda969a5153285cdc91f52c5ed3975d13bdc910e43b8e5ee94aa79f8a1a2", "exact_match": 0.0}
{"doc_id": 2394, "doc": {"question": "when is the opening ceremonies of the olympics 2018", "answer": ["9 February 2018"]}, "target": "['9 February 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the opening ceremonies of the olympics 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 9 February"]], "filtered_resps": ["9 February"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c9f8a8fd0bceb3b73b3bef455700c001af849b3bcaee45c97b6d6d4c49a5ecd", "prompt_hash": "48ec1bc06cb6d7b728d3017dbb627abd1d13d0321a0ffe9e7b6bb0b85d3a9aa2", "target_hash": "ac67c6f841b5e70428b122fb4c6f4481c24863f9e725d3e635bb1541a6fd8f69", "exact_match": 0.0}
{"doc_id": 2395, "doc": {"question": "who has said that caste is a closed class", "answer": ["Frederik Barth"]}, "target": "['Frederik Barth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has said that caste is a closed class?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ambedkar"]], "filtered_resps": ["Ambedkar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5a64923e80e94a2bd2a6e92e1fce1ddf108d93af4abb4cd3610c85b150ecff7d", "prompt_hash": "3241aea028be97a4c8d139040c1ee51419c1e9b9d3011359d3641c183a76c5b4", "target_hash": "e9d47bc4140b4e306c8e732af9c2477028d3f399f9831f3b39bedc4935e76a28", "exact_match": 0.0}
{"doc_id": 2396, "doc": {"question": "all time highest goal scorer in spain national team", "answer": ["David Villa"]}, "target": "['David Villa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: all time highest goal scorer in spain national team?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cristiano Ronaldo"]], "filtered_resps": ["Cristiano Ronaldo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2dfbd33a7a2638475c61c925addb75c180e94dd0d3005307e2dc4d341ac4f8be", "prompt_hash": "fe9dda93d7c40e9b7335c757d08d5de24a1313ae99340db15c4ec90e7547aee3", "target_hash": "c6096bf9a2495dbf754b150912f3501baa99fdf16ae7a7db5ccbe2fdd9ad260e", "exact_match": 0.0}
{"doc_id": 2397, "doc": {"question": "where is the snooker world open currently held", "answer": ["Hainan International Exhibition Center", "Haikou"]}, "target": "['Hainan International Exhibition Center', 'Haikou']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the snooker world open currently held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" UK"]], "filtered_resps": ["UK"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1958069487fc5179e5873cdc69cb36f028e6fb27be734f76304ed0edd0e8e261", "prompt_hash": "fba31ff6f68f5f81967611651a89894f3638e7cad01acf10e8854d2e301edbc3", "target_hash": "1349a8d9e7cc0a29b25421d79621dbae716e9d3cad41cefead732f50a4386af0", "exact_match": 0.0}
{"doc_id": 2398, "doc": {"question": "where does iron ore come from in australia", "answer": ["Western Australia"]}, "target": "['Western Australia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does iron ore come from in australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Western Australia"]], "filtered_resps": ["Western Australia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c9104a83395473a533048e9e59b4b8cacf310979796ffc03b7222cb8f3f394ac", "prompt_hash": "33a1fd77d95cba61ea1b6a9d86cebf5511358dbbe4a35bdfea476d874477a4bd", "target_hash": "16477bf7db9989ad8d96ba8ed23007754a61907cfaab4c58cccfc771f27ec149", "exact_match": 1.0}
{"doc_id": 2399, "doc": {"question": "when did the first pokemon game come out", "answer": ["October 2000", "1996"]}, "target": "['October 2000', '1996']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first pokemon game come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1996"]], "filtered_resps": ["1996"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4bb6fca6d3a5d90a06f048433123a65f24e0a31d1e399886b7ea85e8a8c8d14c", "prompt_hash": "32e202bf46ac1181ddf72526183c7d2fcdc690a2bd7a584314cfe51e15ca939e", "target_hash": "447d7281dc140ab33234898b4a3674d58df31cd4096faff5ea3565a106ec1a9c", "exact_match": 1.0}
{"doc_id": 2400, "doc": {"question": "who plays jill bigelow in line of duty", "answer": ["Polly Walker"]}, "target": "['Polly Walker']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays jill bigelow in line of duty?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vicky McClure"]], "filtered_resps": ["Vicky McClure"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7d5c8580f157333fbeafb4b5d38e09a09b7312836bdd6c6f079c5e02b9f7b638", "prompt_hash": "21c40cddbc9dc1074c1b28fc619c52994697f5849dc168d3d7cc8b507b7eb481", "target_hash": "4f66a18a5d2f1c0716b0cfd07fabc1b65b5a92de839f084b545544840acca7aa", "exact_match": 0.0}
{"doc_id": 2401, "doc": {"question": "what do you say when you win bingo", "answer": ["Yes", "Bingo", "Line", "House"]}, "target": "['Yes', 'Bingo', 'Line', 'House']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what do you say when you win bingo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bingo!"]], "filtered_resps": ["Bingo!"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "724b4ffc4a7e33d35fcc4157aa1ea3d2c0ba4ed9c75c42b468607ea64a21c4e2", "prompt_hash": "cd3ee682c54bd9144289aa89bdb1725db222e6152e75d8abc45baf6f5019105e", "target_hash": "a135c0cf1afe09e5ff31876d355b9d64ae06333b2094d1fe35d6dbb927a5469a", "exact_match": 1.0}
{"doc_id": 2402, "doc": {"question": "who rebuilt the temple after the babylonian captivity", "answer": ["Herod"]}, "target": "['Herod']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who rebuilt the temple after the babylonian captivity?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Zerubbabel"]], "filtered_resps": ["Zerubbabel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb3d0a4931b6c618dc7b4f4dca38c8c5559dfcbd6bb80af7d7846e7c3147455d", "prompt_hash": "aa194811f6cc3c801231e1bb096caed615438e39c1199afe43708c022ea65c60", "target_hash": "dd02aed81210c9daba08f14f4ffe47de38967d2189615ce74e0c4803a13a2c34", "exact_match": 0.0}
{"doc_id": 2403, "doc": {"question": "who won the 10m air pistol gold medal at commonwealth shooting championship in brisbane australia", "answer": ["Shahzar Rizvi"]}, "target": "['Shahzar Rizvi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the 10m air pistol gold medal at commonwealth shooting championship in brisbane australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" India"]], "filtered_resps": ["India"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb214d892ba9234d4ec8869aaf195de7409ca687b4d52f361d7c1152abd5d227", "prompt_hash": "8e2907428c3831de2bf8841c115c0eaa1440010c43e9545638a2c35e17d21775", "target_hash": "0843b3cbc412a8981b9860514581895d7575bc2cd38f761b5600f8f0a3e5672c", "exact_match": 0.0}
{"doc_id": 2404, "doc": {"question": "philadelphia is known as the city of what", "answer": ["City of Brotherly Love", "Brotherly Love"]}, "target": "['City of Brotherly Love', 'Brotherly Love']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: philadelphia is known as the city of what?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" brotherly love"]], "filtered_resps": ["brotherly love"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a689cb548681a3385a9b83178c4df5b795a7230c57c9d11405b5bc49f1550f07", "prompt_hash": "243d2bffe6c30856a7dda103cd2ff956d44b43e233d8b24b6b82932fe9902a46", "target_hash": "8441644251800f0e3f504ca8bd8eae73c615a5ca0aa521036a3a30ea69bdbb2c", "exact_match": 1.0}
{"doc_id": 2405, "doc": {"question": "who is the supreme court judge was a former chief justice of kerala high court", "answer": ["Konakuppakatil Gopinathan Balakrishnan"]}, "target": "['Konakuppakatil Gopinathan Balakrishnan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the supreme court judge was a former chief justice of kerala high court?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" S"]], "filtered_resps": ["S"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c8f25350d0f8594f36f7960c12b2f99a25c1abe47f11ae9fe65e36aa12d43a68", "prompt_hash": "f3fa88d9ed1037a3bebbb74a8598f97bbf8caabb4fba8a8a1cac983ff93bd116", "target_hash": "5683fa9f40d2a5d4b7e4e663b5e88e6325e370fa7165d922e552b6bb7a68e60b", "exact_match": 0.0}
{"doc_id": 2406, "doc": {"question": "who is the young man in hawthorne's my kinsman major molineux", "answer": ["Robin", "Robin, a youth"]}, "target": "['Robin', 'Robin, a youth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the young man in hawthorne's my kinsman major molineux?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ralph Waldo Emerson"]], "filtered_resps": ["Ralph Waldo Emerson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18dc9f58425fb5362328f589cf851e1476c4ce7d63733077c96aa2299c255699", "prompt_hash": "e805c17e061c90eea05c2b7ba08a3ccfd6604e1878c3828cd7c140e3a89c1dc4", "target_hash": "8ada333c7eba437b949db2d4cb063639945622986e322aac90786728899140ee", "exact_match": 0.0}
{"doc_id": 2407, "doc": {"question": "who sings stop listen what's that sound", "answer": ["Buffalo Springfield"]}, "target": "['Buffalo Springfield']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings stop listen what's that sound?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Killers"]], "filtered_resps": ["The Killers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbd4591d0eabdf1b40c5abb5fb480b2b0230cebe744aa9ea3c1b80335412e2eb", "prompt_hash": "160bc12b813a676db9105d394fe339351a3723827c9422d4c36ffdf40baa90cf", "target_hash": "1f6e71481795446818fec1cf4511caf67d90832ca056867457b1756a5cc5dd54", "exact_match": 0.0}
{"doc_id": 2408, "doc": {"question": "where does the red badge of courage take place", "answer": ["the field of battle"]}, "target": "['the field of battle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the red badge of courage take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Civil War"]], "filtered_resps": ["Civil War"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "603e047d7c5f20011fda442df3f3d4d868ce041aa961f3b3cc94a6fc586393c2", "prompt_hash": "b8e414330b3dd6be59776ab4add7f7df5a86443c6d4a6efd5e43dc09a51c162f", "target_hash": "76e4bfd0777811b23db77ca11be2bccdc001f729c0f55513ce2d2f38970c1591", "exact_match": 0.0}
{"doc_id": 2409, "doc": {"question": "what is the first step in the evolution of the eye", "answer": ["photoreceptor proteins that sense light", "eyespots"]}, "target": "['photoreceptor proteins that sense light', 'eyespots']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the first step in the evolution of the eye?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the light-sensitive spot"]], "filtered_resps": ["the light-sensitive spot"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "45d1235313d9b53c374cdef046f295e3e613da9a56542f435cd22fe755335975", "prompt_hash": "26585c875a89ee202480fa876ccef7af171932c9a708ad50330584f1f8b7f68b", "target_hash": "99ffba502d0e871df3d4724d803002141ac1ef2df3dcbce11b20f7a0382ee42a", "exact_match": 0.0}
{"doc_id": 2410, "doc": {"question": "who came up with the first working periodic table", "answer": ["Dmitri Mendeleev"]}, "target": "['Dmitri Mendeleev']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who came up with the first working periodic table?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dmitri Mendeleev"]], "filtered_resps": ["Dmitri Mendeleev"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ebb6cfc35a07465454b4d4b6c14522e489fe28015a30a7a8c7624e2ded6c7dde", "prompt_hash": "d4ebb4438390d2452c24ad74bc11b8aee465ea4e4e53ff1d93fd27e3df66ab47", "target_hash": "854ce18e49abcf329f1d46ac386d400221162fea619c618fcff966975dde2e5c", "exact_match": 1.0}
{"doc_id": 2411, "doc": {"question": "who used morse code in world war 2", "answer": ["naval bases", "warships"]}, "target": "['naval bases', 'warships']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who used morse code in world war 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" German submarines"]], "filtered_resps": ["German submarines"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "707bd5061b857131f853c2b9a5ff53ed2ac90cd65eb48c1c8c05f98393396ec7", "prompt_hash": "723003b38cdad6858095cbb1a8e5be812709fea97b4a24b3279af3b6e03ef705", "target_hash": "69ec6960e7723b0938636d778317949738b2f454685b66d91ebdc5de360306e2", "exact_match": 0.0}
{"doc_id": 2412, "doc": {"question": "where was the first season of slasher filmed", "answer": ["Orangeville, Ontario, Canada", "Sudbury, Parry Sound"]}, "target": "['Orangeville, Ontario, Canada', 'Sudbury, Parry Sound']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the first season of slasher filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "85ee752eb3461287f18de9eb374880d32f1b77bd89e0f3a37f973e7aea191a00", "prompt_hash": "b7212b9a8983465cc9cd86b461f7b2669d3050dcf7127d23bb0c826878d1e910", "target_hash": "872b166cac2dc50aa60e67adfcca7b691c5133c15c7178c6c56f3be92c8fbbfb", "exact_match": 0.0}
{"doc_id": 2413, "doc": {"question": "when did the broncos win the super bowl", "answer": ["1998", "2015", "1997"]}, "target": "['1998', '2015', '1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the broncos win the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1b3aed0c98799a33d8059bd6d78d6760956754385e74264ff437c381ff17866c", "prompt_hash": "8771021ba97a932f26a1375b725c5c2cbdcac521c65f22c73b3ce5a3b3ffe194", "target_hash": "f5e2bea11699927850e35c2febfcbc147299c4a1043b6e5c10c69e19c1eaba91", "exact_match": 1.0}
{"doc_id": 2414, "doc": {"question": "who are you in assassin's creed 4", "answer": ["third-person perspective", "an Abstergo agent"]}, "target": "['third-person perspective', 'an Abstergo agent']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are you in assassin's creed 4?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Edward Kenway"]], "filtered_resps": ["Edward Kenway"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "775e8ff9f3027435bfddefae21e277411d98e9ae4de90af9401e84cef6eef0f4", "prompt_hash": "094de8f4a22cef0421c064719bf821c8acbff06c434429e6a70b1955eedc2ace", "target_hash": "04d7ce00616f3380b4795a4244d436dba9c2913a0d70b77d537c76d39c7dec17", "exact_match": 0.0}
{"doc_id": 2415, "doc": {"question": "where do the events in the story take place geographically", "answer": ["Persia"]}, "target": "['Persia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the events in the story take place geographically?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the United States"]], "filtered_resps": ["the United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f856235f31482b9ee3a9282fcd5216e9442f84b34d4b1dcbb8795cafc39f32ae", "prompt_hash": "3eee47c6737bd2239878abaa23212251173806ece2b82a558b31357c4ff941e2", "target_hash": "ed1d2a86b16ddc7d98419d208532d5a2fa6b3e94c8b2dcd0a2da57d645307185", "exact_match": 0.0}
{"doc_id": 2416, "doc": {"question": "what is a dropped pin on google maps for", "answer": ["marks locations in Google Maps", "marks locations"]}, "target": "['marks locations in Google Maps', 'marks locations']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is a dropped pin on google maps for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to show the location of a place"]], "filtered_resps": ["to show the location of a place"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed7c5859b3397a729d75fbc1134e1ffc361ad4aa51757bb6d58f78318e8698b2", "prompt_hash": "333caa0e0844b1d42633d5edfb76b9f366d02835c54e38261098eb8e4fc0da88", "target_hash": "54079d4cd7cee5de34792a90f03c7b9ecf6401f9b7d29715c81b4ae1ecfab7f6", "exact_match": 0.0}
{"doc_id": 2417, "doc": {"question": "who was the movie citizen kane based on", "answer": ["William Randolph Hearst"]}, "target": "['William Randolph Hearst']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the movie citizen kane based on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Randolph Hearst"]], "filtered_resps": ["William Randolph Hearst"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b6884118311fe13462edd03d858499db34fc8d6e08a33cb2bd706bdc1dde4c6", "prompt_hash": "cd95f716f9cb7702ffa158af3a3ef73c1f6d133e970193ad08053aab698aa8cd", "target_hash": "e60c455484f40fc01749541f6355f8370f0abf9de72032fe8ac6f3cc4ff0669a", "exact_match": 1.0}
{"doc_id": 2418, "doc": {"question": "who was the ottoman governor who led egypt in the years following the napoleonic wars", "answer": ["Husrev Pasha", "Isma'il Pasha and Tewfik Pasha"]}, "target": "['Husrev Pasha', \"Isma'il Pasha and Tewfik Pasha\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the ottoman governor who led egypt in the years following the napoleonic wars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Muhammad Ali"]], "filtered_resps": ["Muhammad Ali"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2353605658c6e780cdda787c0843a6f71864f70ad0df287944d80ce5cab3a2bb", "prompt_hash": "e4b774700c6ae605aeb7ba9a68dcd0cd728ac9b8a8e315736c2ed3d49004d950", "target_hash": "5c06d5589869c6c6be9858455fc34bce39a0583c4f1484b12b40e149db61cb9a", "exact_match": 0.0}
{"doc_id": 2419, "doc": {"question": "which is the first private medical college in kerala", "answer": ["Government Medical College, Thiruvananthapuram", "1998"]}, "target": "['Government Medical College, Thiruvananthapuram', '1998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the first private medical college in kerala?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christian Medical College"]], "filtered_resps": ["Christian Medical College"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8483fa28244040617503bc865571f04c7da14d29e57024c41fb2bb5f981b64d0", "prompt_hash": "51aaf0de5a18d6ca52d884ed5808325b3ea694a01aa3a6dc7a4b482536f67050", "target_hash": "9a0363532d993fb0cdb738c14dedc98b7d17f35660a1a35e47a117ca60a758dd", "exact_match": 0.0}
{"doc_id": 2420, "doc": {"question": "who did the chiefs play in the playoffs", "answer": ["the Tennessee Titans"]}, "target": "['the Tennessee Titans']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the chiefs play in the playoffs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Denver Broncos"]], "filtered_resps": ["Denver Broncos"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b5df1eea0548936a69727f702c4efb330efe94423dd5ab4c9b5d45b622602639", "prompt_hash": "1dac45739bd85490213e3f0e4a35ff81b67833ab2ae5387a6926dd4171a4922a", "target_hash": "57cb71b3b80276557bf1f9bf5edc1470c89c8b3d2c47d02ec82f64c3d8f95577", "exact_match": 0.0}
{"doc_id": 2421, "doc": {"question": "what is the time setting of game of thrones", "answer": ["medieval"]}, "target": "['medieval']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the time setting of game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" present"]], "filtered_resps": ["present"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "28039a596eff4c0d6886d31b3997d02cb6cf3274b4463e6c70a4be5ea5d32b27", "prompt_hash": "08a2815259b313749381d5c9b71081a0723789fe183bd5e262297ae3d1315cbd", "target_hash": "b2d410c126b215078f1ec79921c29101f38e5a7cc01cdcf1e205d77892977b32", "exact_match": 0.0}
{"doc_id": 2422, "doc": {"question": "who has access to the presidential daily briefing", "answer": ["National Security Advisor", "Secretaries of State", "Defense", "the President"]}, "target": "['National Security Advisor', 'Secretaries of State', 'Defense', 'the President']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has access to the presidential daily briefing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the president"]], "filtered_resps": ["the president"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3018984fcd17e7304488c41fc9a04def61258416d756caf726d397f8f14fb76a", "prompt_hash": "bb43c7a0ba309f9a374168896a69001ea52a705aff8888a0522d45f8cc7b5f4f", "target_hash": "71e0f6025be096c891f8bc8ee986700f387bc0f2edffde883b18309a92bc7b48", "exact_match": 1.0}
{"doc_id": 2423, "doc": {"question": "who played buffy the vampire slayer in the movie", "answer": ["Kristy Swanson"]}, "target": "['Kristy Swanson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played buffy the vampire slayer in the movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kristy Swanson"]], "filtered_resps": ["Kristy Swanson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a472686aff0ab980445886d53a6a11f33a14f34162cf641f5b9d4881e1850b6", "prompt_hash": "61084a111dccc50bf3c38cd7a458fd69423bca037de63a165355b69a2c1b5cca", "target_hash": "c0e54b3fbe5fe273bef1e3cb186c3297605149971568041470610e6706e914d1", "exact_match": 1.0}
{"doc_id": 2424, "doc": {"question": "who is ranked 1 in women's tennis", "answer": ["Simona Halep"]}, "target": "['Simona Halep']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is ranked 1 in women's tennis?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Serena Williams"]], "filtered_resps": ["Serena Williams"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bc3811a2a81ff4b7d7b5fbc907bde3f4ecc20f2494460254f22e8301f6bb6cae", "prompt_hash": "893c1c1535c94a4bef61e6b49619f6be48f059c20ad66325bfa743dda87152fb", "target_hash": "a6626f525cd347ce0734be7eb29bbd2c87b9aa5223258a555c44ded4e31cacc7", "exact_match": 0.0}
{"doc_id": 2425, "doc": {"question": "who won the mens curling in the olympics 2018", "answer": ["United States"]}, "target": "['United States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the mens curling in the olympics 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sweden"]], "filtered_resps": ["Sweden"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8c9c6f25ee2f440dd995e4e520246e44bcdc65bb7f4d62939461641736d566d5", "prompt_hash": "f7ad6203d6041f5d2ea186d62b400a95a6353fa4a44dafee260e261c9be1eb55", "target_hash": "248d02cf1d61215522b5495553dd8d7a3b62df655aa003cdc541d7fb5faee586", "exact_match": 0.0}
{"doc_id": 2426, "doc": {"question": "where does what in the sam hill come from", "answer": ["simple bowdlerization"]}, "target": "['simple bowdlerization']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does what in the sam hill come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" American English"]], "filtered_resps": ["American English"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fa3df3be60b1f50fe3f65d8f2911d91730303b132c1f82832ca0328a3b648b6d", "prompt_hash": "81c6853b203f3a07b636ed2daddd6b20ef3b8f9dc6a0cc0e80341f2cd54be500", "target_hash": "b2b88e20213178e29304bf243dbaddd104aff8d512673c72caf167052733f806", "exact_match": 0.0}
{"doc_id": 2427, "doc": {"question": "who defeated the last remaining roman army in europe", "answer": ["Goths"]}, "target": "['Goths']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who defeated the last remaining roman army in europe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alaric"]], "filtered_resps": ["Alaric"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "90195c897869b16c23d0a620e94ea34badff2570e16ad008026ae5ec854b1a1e", "prompt_hash": "f25d146317851f7c774cb7221716f1f390542ee3193348cf14999c0b255e9873", "target_hash": "fcb1fb780f7cce49e3d589c70ab88ebdb10af61dcd6613804a8a9ae65cf9ceae", "exact_match": 0.0}
{"doc_id": 2428, "doc": {"question": "the temperature of gas is proportional to the", "answer": ["volume"]}, "target": "['volume']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the temperature of gas is proportional to the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" average kinetic energy"]], "filtered_resps": ["average kinetic energy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dd101f79fc9957cbda667c8604ada4fda18b727c7cf760d5db1baa9591c39569", "prompt_hash": "32e63f18a1fd6b836dbbccb135d376deea0ad8b11009b5182b790a9b1f828c4a", "target_hash": "da24851eed225a5477e7d7afda08d43a4ad7ffa43a258b2f5f6640827cca9c4e", "exact_match": 0.0}
{"doc_id": 2429, "doc": {"question": "who's flag is red white and green", "answer": ["Hungary"]}, "target": "['Hungary']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who's flag is red white and green?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Iran"]], "filtered_resps": ["Iran"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "386ea5b340bf135fe356f846ef4f7c433fee2c3f13a7ece0b348a2e9d43850d0", "prompt_hash": "b885d0324002cfc06bcf010c44843516f07ad42715481e83511a2bfdbbe809bb", "target_hash": "14e9093ae7ea29bcad5e14f3873e03c72e44a3e99ad8fb15652d2e3b1df5e181", "exact_match": 0.0}
{"doc_id": 2430, "doc": {"question": "country with most olympic gold medals all time", "answer": ["United States"]}, "target": "['United States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: country with most olympic gold medals all time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United States"]], "filtered_resps": ["United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fec76b8941a351c45bbb281b4c1bdb303efc0881c1b375a87eab0db78ad5c1f6", "prompt_hash": "a16b2e2f6ac5d962ac1aea5b05b5a9c1d6a79fd83d8da91fe1c69b1336da2da8", "target_hash": "248d02cf1d61215522b5495553dd8d7a3b62df655aa003cdc541d7fb5faee586", "exact_match": 1.0}
{"doc_id": 2431, "doc": {"question": "who proposed the virginia plan at the constitutional convention", "answer": ["Madison", "James Madison"]}, "target": "['Madison', 'James Madison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who proposed the virginia plan at the constitutional convention?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Edmund Randolph"]], "filtered_resps": ["Edmund Randolph"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "72e0e787dbf076445ad12a07fcff0ccfdf9588cb8ec234b472cf6cc47095910f", "prompt_hash": "965e78d8b1669bbc87f6f01f2b91e8f448f63d07aaf768810f201a81da55ec3f", "target_hash": "811214c06f0756640b825d48d091c2ac91d9acd49a71650abeff793128402dac", "exact_match": 0.0}
{"doc_id": 2432, "doc": {"question": "who plays yoda in revenge of the sith", "answer": ["Frank Oz"]}, "target": "['Frank Oz']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays yoda in revenge of the sith?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hayden Christensen"]], "filtered_resps": ["Hayden Christensen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "66036cd93bc31c5e04114e92f6304858991252a7148d8c1307929d4ea593d8fb", "prompt_hash": "43c237bbc79a5fb2d7ae7c1248959d7f3fae166e968bb9dcc536340bf255dfce", "target_hash": "e3bb975d037515a033c89ff9d6ff1e1a7032999c650ae22fd5b86d12d96ca51d", "exact_match": 0.0}
{"doc_id": 2433, "doc": {"question": "who was the bad guy in silence of the lambs", "answer": ["Buffalo Bill"]}, "target": "['Buffalo Bill']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the bad guy in silence of the lambs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hannibal Lecter"]], "filtered_resps": ["Hannibal Lecter"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ba410c7923584c3c421f427c85338e0cafac6252775b8c58c108dffe03902eb5", "prompt_hash": "46cd2419f74364b010d82f4d52c3bfe0e33fd56af9e69d920dfc7e32faf46765", "target_hash": "a9f37edb9cb69dbd1ac7b11fb24110703dc8afd286aa5516b3bef647589071d3", "exact_match": 0.0}
{"doc_id": 2434, "doc": {"question": "who become the ceo of it wipro company in 2016", "answer": ["Abid Ali Neemuchwala"]}, "target": "['Abid Ali Neemuchwala']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who become the ceo of it wipro company in 2016?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thierry Delaporte"]], "filtered_resps": ["Thierry Delaporte"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f4aa0bf17cbcd2420b78dd792d76d8167c525d956ce2344125b6432918bfd220", "prompt_hash": "d33c195666562d1b625ac74a44efdff62a5b769d07adef647d7baeeeb940f978", "target_hash": "678542f432489ecf30c1d8f15c0479dc958242d978d635150f69f1989dec2f23", "exact_match": 0.0}
{"doc_id": 2435, "doc": {"question": "actress who plays brad pitts wife in war machine", "answer": ["Meg Tilly"]}, "target": "['Meg Tilly']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: actress who plays brad pitts wife in war machine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michelle Monaghan"]], "filtered_resps": ["Michelle Monaghan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad67551d1f1d72fb3160e566bd12cce2def9a7aa9a0a85f4c4e4e8c6f13e41dc", "prompt_hash": "22aef28149c0b1ca4a8e1524146569c381d42b34f87a728ba78ec73fe094e708", "target_hash": "85444e7fe69d5406e462b9bd62a6cd2fffcec09edd7457a6f6cc5c8b5bc98460", "exact_match": 0.0}
{"doc_id": 2436, "doc": {"question": "who comes after the president if he dies", "answer": ["vice president"]}, "target": "['vice president']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who comes after the president if he dies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the vice president"]], "filtered_resps": ["the vice president"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b13ee8ca825abbb8a46501a5d0b66abccf31e7ede66997fb55c5b2cb5ab8fc1b", "prompt_hash": "830aed56e8f6a969823b4d067129308c971b2854c5a165a2edc0b1033bca9f05", "target_hash": "ff8ea9e429e17487059c3dbf28f6f3f2723d4b2a1925b5710f58fd6a5299a1e5", "exact_match": 1.0}
{"doc_id": 2437, "doc": {"question": "who plays the protagonist peter rabbit in the 2018 motion picture release", "answer": ["James Corden"]}, "target": "['James Corden']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the protagonist peter rabbit in the 2018 motion picture release?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Corden"]], "filtered_resps": ["James Corden"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7332110fdba4eec13756e3e19f406cd31d520f8dae3db39aa21f22ea20eec604", "prompt_hash": "d8f28b8471610f0c0843b8b760eecc96f0976690f6e9f8ed8a5ad3186c2af570", "target_hash": "3f06351a5bb6b84841e0be414268fd3ee54d7f1e4d7b76ea755c695b8ed0db30", "exact_match": 1.0}
{"doc_id": 2438, "doc": {"question": "where was the movie i am number 4 filmed", "answer": ["the Pittsburgh metropolitan area"]}, "target": "['the Pittsburgh metropolitan area']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the movie i am number 4 filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Los Angeles"]], "filtered_resps": ["Los Angeles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03ffb3ab0b4148afcac565058ca231d1ea4d77854cef3f41b6f90a0342b0c566", "prompt_hash": "f757c70449eec5e5ddd490e809547b2f85a8e7567407343655dcd7caa2f6ae50", "target_hash": "51758bc8824ab526cd6ac0b2aaa13b6bbeeda5b535a5d2da4382c2d9cbd2813f", "exact_match": 0.0}
{"doc_id": 2439, "doc": {"question": "when did the granite mountain hotshots get certified", "answer": ["2008"]}, "target": "['2008']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the granite mountain hotshots get certified?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2010"]], "filtered_resps": ["2010"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cee8fdb1a821c0334a30f1015e3313e18857f0890502e95c5c60bdb5c8b3db69", "prompt_hash": "18e4f5433d7cbfe3b735b486e4ffca473dc2e897e1ec0f78d1a6acf5640b8a8a", "target_hash": "b1e9e068744cdeb4a51e7d1298fc78edd756e84b1f998e58e6c4e312fb45cbcd", "exact_match": 0.0}
{"doc_id": 2440, "doc": {"question": "when did brent barry won the dunk contest", "answer": ["1996"]}, "target": "['1996']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did brent barry won the dunk contest?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1994"]], "filtered_resps": ["1994"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a3b7eef7d2ca96a7abe096d72088b9cd39493bc4a99dc2738b687ffee2bcbb45", "prompt_hash": "721c723f1a62108eae71fdd61d119e49411fba0ad3592214f507bb6ebdf478f7", "target_hash": "4884bad91c23e17f70a396eecdc381a658567383863ece0c7474966e045c229b", "exact_match": 0.0}
{"doc_id": 2441, "doc": {"question": "what is the purpose of a jake brake", "answer": ["slowing the vehicle"]}, "target": "['slowing the vehicle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the purpose of a jake brake?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to slow the vehicle"]], "filtered_resps": ["to slow the vehicle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "10fc67c91df812b867d17f321c5ab2ab6c166211cc7e0d3b92c515198ff8fe6d", "prompt_hash": "d555644355d95d51363f844051d8380e353bc8b6fe8ae24af8ea073ac12b2ba7", "target_hash": "c2e9fa0cd21120f9409be67329b785a4c5c45e03a1c0cccd935d6c25df7403bc", "exact_match": 0.0}
{"doc_id": 2442, "doc": {"question": "who sings in everybody dies in their nightmares", "answer": ["XXXTentacion"]}, "target": "['XXXTentacion']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings in everybody dies in their nightmares?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Story So Far"]], "filtered_resps": ["The Story So Far"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b0cf0b5e1beeaee7a1bbd0277b66e940dbdacee13fa870315ee00461bf68831", "prompt_hash": "53a59b75c89376dd751faa4ca9eb8279b14ead0bae145925709f268d870dda70", "target_hash": "7db7d23943a9ec63ae7174e77b3352f63a172be2970240c3b31e0294c66e862b", "exact_match": 0.0}
{"doc_id": 2443, "doc": {"question": "who established the idea that microorganisms play a role in disease", "answer": ["Robert Koch", "Agostino Bassi"]}, "target": "['Robert Koch', 'Agostino Bassi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who established the idea that microorganisms play a role in disease?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Louis Pasteur"]], "filtered_resps": ["Louis Pasteur"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1163d8cbfdcb49e48937e7ed42ff4feb8924c91af0cd5c4cc539873e62f9a3fd", "prompt_hash": "28935d6d317304956988f0d04a79aa58c5dc69f1e1044c3c51bcd94fea0488f7", "target_hash": "b404d55f353f61d99f9b63a8e9fe5fb95c5a6dab9119f152e7cdc8523119e24a", "exact_match": 0.0}
{"doc_id": 2444, "doc": {"question": "where was the rise of the planet of the apes filmed", "answer": ["Oahu, Hawaii", "Vancouver, British Columbia", "San Francisco, California"]}, "target": "['Oahu, Hawaii', 'Vancouver, British Columbia', 'San Francisco, California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the rise of the planet of the apes filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Australia"]], "filtered_resps": ["Australia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eebbbcb93c03d1cc0a719c61088b69ef2942fea78d99731a782d7299a7babcd5", "prompt_hash": "386fd4b9fde025ac13caa36b63cd9ac3d17102a8e53460dca8c535c77fe07305", "target_hash": "48183fff2871f3fb0211666f44fba2f6eefa653c90985d1666937a776fb932b5", "exact_match": 0.0}
{"doc_id": 2445, "doc": {"question": "when was the first underwater cable laid beneath the english channel", "answer": ["1850"]}, "target": "['1850']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first underwater cable laid beneath the english channel?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1850"]], "filtered_resps": ["1850"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c8e87f2d0f35086ec49adbbb922a50d39a3b8eac9d413264d62ca0053e14da69", "prompt_hash": "4bd9ce63a14ac27f1cbf17e14d7bd4bdfffd5272edc5cd5ddbd6224a538f2e84", "target_hash": "af59dd60bd49a34706e7a2707e44ff77b163be5250ef324d860f0552cb226a0d", "exact_match": 1.0}
{"doc_id": 2446, "doc": {"question": "who wrote the song balls to the wall", "answer": ["Deaffy", "Udo Dirkschneider", "Herman Frank", "Wolf Hoffmann", "Stefan Kaufmann", "Peter Baltes"]}, "target": "['Deaffy', 'Udo Dirkschneider', 'Herman Frank', 'Wolf Hoffmann', 'Stefan Kaufmann', 'Peter Baltes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song balls to the wall?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Twisted Sister"]], "filtered_resps": ["Twisted Sister"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "597fd9f48c720c9d764839b706a9cca37f56ac991d5d87330a81912fb91035a4", "prompt_hash": "e8d6be001803ccc6385297726f0ae3f163d8325f49031e38934418dc8eae504f", "target_hash": "b32807e49364ea07a1a7219977c515480fe17e8c1f7f82049613e646a2c8e7fa", "exact_match": 0.0}
{"doc_id": 2447, "doc": {"question": "who plays the girl in wreck it ralph", "answer": ["Stefanie Scott", "Sarah Silverman"]}, "target": "['Stefanie Scott', 'Sarah Silverman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the girl in wreck it ralph?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sarah Silverman"]], "filtered_resps": ["Sarah Silverman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4f23b2061659738a06db79525456a84af2370d02bf25c43b2b9c235a4ec23d70", "prompt_hash": "41fce204be49b37dcb91eb81d6f405c51c79c314bc2c190f19331776d0d18183", "target_hash": "7fd0d3961ede750563974b4be57dc04fe895030cb37759ab509dba21b548736b", "exact_match": 1.0}
{"doc_id": 2448, "doc": {"question": "what was the purpose of a revival meeting during the second great awakening", "answer": ["evangelizing"]}, "target": "['evangelizing']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the purpose of a revival meeting during the second great awakening?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to convert people"]], "filtered_resps": ["to convert people"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb72a8e296953c943888b4fdd62b50f00f632f064b6f2786ab2135f8835adedf", "prompt_hash": "054950144388f75810f4bf9216a1b9e634b0042f0331eb30f3a0a2b9b796d6a0", "target_hash": "436ae203f8b6779118f004f34ccd896699cba7a1531e8b02f5574e8d36d904e5", "exact_match": 0.0}
{"doc_id": 2449, "doc": {"question": "when did krakow become the capital of poland", "answer": ["1038"]}, "target": "['1038']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did krakow become the capital of poland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1038"]], "filtered_resps": ["1038"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "02087a3f727f964fd56e8a0bb6506ea96c9f599cbf9422308afb07665afe0426", "prompt_hash": "180a24d67651af29542b6a22b087901a3de0dc6a45c3ec6d89bad187053be280", "target_hash": "a60cf32c4624411917e95e5061bdeda49f126774dcb93ecf1fd1abedcde4f55d", "exact_match": 1.0}
{"doc_id": 2450, "doc": {"question": "where do the neutrons come from in nuclear fission", "answer": ["Startup neutron source"]}, "target": "['Startup neutron source']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the neutrons come from in nuclear fission?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the nucleus"]], "filtered_resps": ["the nucleus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c39677171da64aef6399c61867a905975ad5cbc15efe3b4cd1c3cfe20957805f", "prompt_hash": "80512906f4959d333f5d7d2dc312c94b4f5e89ce1c1728ac0a852b4168750153", "target_hash": "749ca2ebe8a3e6cca945951c9b4195fcd5e8cf397a0134e092cbe579c4d5b32d", "exact_match": 0.0}
{"doc_id": 2451, "doc": {"question": "glycogen and amylopectin are long chains of which simple sugar(s)", "answer": ["glucose"]}, "target": "['glucose']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: glycogen and amylopectin are long chains of which simple sugar(s)?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" glucose"]], "filtered_resps": ["glucose"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03c226c40ce6a786ec2d9aa02829aa16d11ff2f8039b92f5352f8fbe6e49cddf", "prompt_hash": "339f6f5ebcf10a117aa5a065f83885d0202432e9eb08b167ca60177de5819d48", "target_hash": "82587c06123bca09c5f6af3c67401d8f307f86f934bdc4e49c83ae23faf66100", "exact_match": 1.0}
{"doc_id": 2452, "doc": {"question": "who is the actress that plays penny on the big bang theory", "answer": ["Kaley Christine Cuoco"]}, "target": "['Kaley Christine Cuoco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the actress that plays penny on the big bang theory?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kaley Cuoco"]], "filtered_resps": ["Kaley Cuoco"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b6d9c24c51042bcafb799e487d1e31d43d3da2107de687fc2e1d71f22c3c5a4", "prompt_hash": "733932b26718eee3a3b67abf3246d628c059f025114381b385ca922f589f1d8a", "target_hash": "62f13a41579dfb884c54bd73b7e67bd1b06e3f4f2921714ce67a074c9398f9b6", "exact_match": 0.0}
{"doc_id": 2453, "doc": {"question": "what was one reason south carolina gave for its decision to secede from the union", "answer": ["to preserve slavery"]}, "target": "['to preserve slavery']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was one reason south carolina gave for its decision to secede from the union?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" economic problems"]], "filtered_resps": ["economic problems"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b59e5995147713d61bb54943ada2626788b1ba55afcf3c9f7f3ea8c78df80bc9", "prompt_hash": "74655332b271fd59cd385e8fb3d5aad5c87ac67d933395c932623c62dbc3d997", "target_hash": "5a44db9a18d9e9fcb70a6310fe88b8d14e97983243f41eae20f35cd878ffa30e", "exact_match": 0.0}
{"doc_id": 2454, "doc": {"question": "what is the main religion in british columbia", "answer": ["Protestant", "Catholic"]}, "target": "['Protestant', 'Catholic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the main religion in british columbia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christianity"]], "filtered_resps": ["Christianity"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a24c951f7632d4feb5936eb9bc330a1aec549489b7590584321fbdf2a1b228b3", "prompt_hash": "73d6004245226fd92eb07b0b5fc8028b7b27cf56e6170ab95af21fd0a4636dac", "target_hash": "ade0877ec2cbb369502ef006a4dee01da4a5573839c12433de26e9bcb8168cc3", "exact_match": 0.0}
{"doc_id": 2455, "doc": {"question": "where is wake forest located in north carolina", "answer": ["78.51889°W", "in Franklin and Wake counties", "35.97333°N"]}, "target": "['78.51889°W', 'in Franklin and Wake counties', '35.97333°N']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is wake forest located in north carolina?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Winston-Salem"]], "filtered_resps": ["Winston-Salem"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0a498682aadf65fac2e623aa5f5f12f4f81f0baa349be9f0bf94bfe86cb58445", "prompt_hash": "ff70ce6a0c8b673684de87a14f1c2bf1a6d9c6a4db523adecdca0cc3f55297e4", "target_hash": "23beff54d23cbce5c33570e494e2dcb7d19f3718d7b904d332346f83974140d3", "exact_match": 0.0}
{"doc_id": 2456, "doc": {"question": "what is a real world application of an atwood machine", "answer": ["An elevator with a counterbalance"]}, "target": "['An elevator with a counterbalance']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is a real world application of an atwood machine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" elevator"]], "filtered_resps": ["elevator"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "11e64651b4484fa93e626533f627957f23efa69a38d01bf83c4b3bfdc05ba515", "prompt_hash": "f9b4e053e6c3318573d412ad7609ac9263a4fa711d4fe0a09205cf2c18bcb41d", "target_hash": "96716f3a8e270da1ecdd5c53a9f5b1c48e2e1a903c88ef34e0a2e0eba34134f3", "exact_match": 0.0}
{"doc_id": 2457, "doc": {"question": "who started the tradition of coloring easter eggs", "answer": ["early Christians of Mesopotamia"]}, "target": "['early Christians of Mesopotamia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who started the tradition of coloring easter eggs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Egyptians"]], "filtered_resps": ["Egyptians"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed3153422ff5b9860d2911d521249056f472510c249ce91eaa9d9447194ba9f8", "prompt_hash": "0560dfd548ff8721e1ebafe8708569e22b2c578a6e9b7516d7aeb7f38d168b37", "target_hash": "a02c57be215573afb0db232a0f790133c7a5070b0062c8589635fc7033d7625f", "exact_match": 0.0}
{"doc_id": 2458, "doc": {"question": "who is the most selling music artist of all time", "answer": ["The Beatles"]}, "target": "['The Beatles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the most selling music artist of all time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Elvis Presley"]], "filtered_resps": ["Elvis Presley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6db78321fdc0b6a627f2bd2fc8007d734fc1000a3214f9dba6f7c8e5374e81fa", "prompt_hash": "24ab4926aa9f38786966f9690cfabaf69d698800519712af5c2e9b87c122c1bd", "target_hash": "1656edfec581107e0ad591f64a6c0a50abf71367761f6330cd410765c366acaf", "exact_match": 0.0}
{"doc_id": 2459, "doc": {"question": "who plays stacey's mum in gavin and stacey", "answer": ["Melanie Walters"]}, "target": "['Melanie Walters']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays stacey's mum in gavin and stacey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Julie Walters"]], "filtered_resps": ["Julie Walters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "164b7171a0a7c5fa7f1280dc597fb50b924b5388965d63a5e49f2649c76727a8", "prompt_hash": "1fe671bc67cceaaeec7694defada499ea4d323b14deb3c922e61c5d638c124e6", "target_hash": "8f34e43d5afb38cabc62010eb591d206c5846e3978fd2177ff90406d6b32c8c0", "exact_match": 0.0}
{"doc_id": 2460, "doc": {"question": "who sings love me tender in princess diaries 2", "answer": ["Norah Jones", "Adam Levy"]}, "target": "['Norah Jones', 'Adam Levy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings love me tender in princess diaries 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christina Aguilera"]], "filtered_resps": ["Christina Aguilera"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7720c0c439c51f567458041dc5d215b30d2293f432810ecea94ee611c58f98d", "prompt_hash": "0e6282c42da10dcee44c4ee261de3f001ba618e4f916fa86ac658c4d2b5ebffa", "target_hash": "f6103cd14fec07896041b31210345efd6e9ac29ea1ac4646c9270b8adcfc881f", "exact_match": 0.0}
{"doc_id": 2461, "doc": {"question": "what is the term for circular movement around a central point", "answer": ["angular rotation", "Coriolis effect"]}, "target": "['angular rotation', 'Coriolis effect']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the term for circular movement around a central point?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" rotation"]], "filtered_resps": ["rotation"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d581f2df2131b5427155a317dbc300c79482ca594fccad3bfcb8d5e5ce342f2b", "prompt_hash": "ba3ed12e560b17d7ff6977625cdd2f6882a485d22c5a04a0fe607de70ca8428d", "target_hash": "04a93d360fd4ab4f93b2570af4be03c6d491185eff7e26afbee1b99e94dd4a9b", "exact_match": 0.0}
{"doc_id": 2462, "doc": {"question": "who was the first to say i'm going to disney world", "answer": ["Jeana Yeager", "Phil Simms", "Dick Rutan"]}, "target": "['Jeana Yeager', 'Phil Simms', 'Dick Rutan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first to say i'm going to disney world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walt Disney"]], "filtered_resps": ["Walt Disney"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cd8b084f2b3f5c4f6056eed27abd852fd38a9879b39881c4e27ce903ac073bd6", "prompt_hash": "50cda5f3333b38c9946771bea509f1a07249307eed1cbde87a0e44b8799e0fe2", "target_hash": "d777bc2f27dbf14d9faa3346aadb08030e2af70c03092e1f6720bd3cfe2a0416", "exact_match": 0.0}
{"doc_id": 2463, "doc": {"question": "who is considered as architect of india constitution", "answer": ["B. R. Ambedkar"]}, "target": "['B. R. Ambedkar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is considered as architect of india constitution?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" B"]], "filtered_resps": ["B"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "157dd6ebc1e3756938227a57a848f4a06d96b77b18cbfb40504a5c811e96622c", "prompt_hash": "725c6d7d492362497a38c509b0ae51d90fe2419aaa4d11c4c9bcb1915925256b", "target_hash": "395bfae059a2debe9b02257c4ae561a52174cd8ce2eebe5db1f2fa021167dde9", "exact_match": 0.0}
{"doc_id": 2464, "doc": {"question": "who was the girl that played cindy lou who", "answer": ["Taylor Michel Momsen"]}, "target": "['Taylor Michel Momsen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the girl that played cindy lou who?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Taylor Momsen"]], "filtered_resps": ["Taylor Momsen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9f2bc5d024467f625d193ee3fc7a6f92fc6c71455efedbc3450679eb45a2c31f", "prompt_hash": "ae8842b2265b8c528f360f5a61c75b76f77ae2f875be249e80cedea0ec574185", "target_hash": "cea6f7043b22eefeffbb5d2ec25b6e88709a94aafe819dfb3fc252b3078340d3", "exact_match": 0.0}
{"doc_id": 2465, "doc": {"question": "who is the winner of bigg boss kannada season", "answer": ["Chandan Shetty", "rapper Chandan Shetty"]}, "target": "['Chandan Shetty', 'rapper Chandan Shetty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the winner of bigg boss kannada season?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vijayendra Bichu"]], "filtered_resps": ["Vijayendra Bichu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4178d2e55f25ca75d4564371ab9a6156726bd13ca16812dc31ee3b08576db810", "prompt_hash": "59581331e4ee48c1f22a0b5e2b629c1df53e4049039960cf02327a0c27284bf0", "target_hash": "ef230d4d3fb1e818ec82b81712e90a2e211b15308e88d5d8c6cd92d33e59b120", "exact_match": 0.0}
{"doc_id": 2466, "doc": {"question": "where is the new stadium being built in las vegas", "answer": ["Paradise, Nevada", "in Paradise, Nevada"]}, "target": "['Paradise, Nevada', 'in Paradise, Nevada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the new stadium being built in las vegas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paradise"]], "filtered_resps": ["Paradise"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c1de0506f4b3c13621d1003bb7b600976d31cc53196a1c8fa5de9acca3a94d61", "prompt_hash": "dcf2f7a5ddab8394a030bc22beb7e064b7104224933801c540504d2acd00587a", "target_hash": "6d98a9cbb526e8877ad0b122033cc2916e6fc1ba5055976f610d4b8847c120d6", "exact_match": 0.0}
{"doc_id": 2467, "doc": {"question": "where in the bible can i find the lord's prayer", "answer": ["the Gospel of Matthew", "the Gospel of Luke"]}, "target": "['the Gospel of Matthew', 'the Gospel of Luke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where in the bible can i find the lord's prayer?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew"]], "filtered_resps": ["Matthew"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "15067197b6e9c5ef84879673cd420c7f7030ad6ea0cbe3e998347d2d08a03970", "prompt_hash": "a03f13f442f1fcdcf855e32235de43c93b70efab627fcd499aa62932d25ea639", "target_hash": "74ea8924b68cb3e1d42ae8ccfecb7e052153276e8dc6bb23137c7982dbfe52b8", "exact_match": 0.0}
{"doc_id": 2468, "doc": {"question": "where did they live in sex and the city", "answer": ["New York City"]}, "target": "['New York City']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they live in sex and the city?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6bf21c689e0b2d843a521f248d2e9fddb3dbcb2a2da0a4e7410bc6c88274cba3", "prompt_hash": "8c956e4d77c95de7ae5b4d6d85a8bfac40351def6f4888635ab56a590223aa7f", "target_hash": "34b5642e7ac3c37be810afca322277ff3e939f50d7aa576ecb54dd043fa4bbbe", "exact_match": 0.0}
{"doc_id": 2469, "doc": {"question": "who are the artists in the traveling wilburys", "answer": ["Roy Orbison", "Jeff Lynne", "George Harrison", "Bob Dylan", "Tom Petty"]}, "target": "['Roy Orbison', 'Jeff Lynne', 'George Harrison', 'Bob Dylan', 'Tom Petty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the artists in the traveling wilburys?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Harrison"]], "filtered_resps": ["George Harrison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "70467e45c54ebbe26b7a222ae6920bca06d3411d490d2d8e099119d29fe9d48d", "prompt_hash": "581c01ac1fc36a4e490e24cba867b0af19565c640f4024f7fdc0530ca57844f9", "target_hash": "c83f74ad14b080af8f99fb76cc2ce8f613f4bd662b91024d8392454969768d92", "exact_match": 1.0}
{"doc_id": 2470, "doc": {"question": "who played violet in a series of unfortunate events netflix", "answer": ["Malina Weissman"]}, "target": "['Malina Weissman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played violet in a series of unfortunate events netflix?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Malina Weissman"]], "filtered_resps": ["Malina Weissman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8fb3e1f3fbba7ca02ec69b55bc086edfe43346a5d8ae99f883cf95381025c48", "prompt_hash": "8dc9e234b7bfb87ce7ffe3c2408a1cbdcc3daad29185e1ed32c2449256da6cf4", "target_hash": "34d1a86cfb3ed970091a022e1633bf2c9d5e4bd30ce8bc3ab8c0c4b8c8d31aa5", "exact_match": 1.0}
{"doc_id": 2471, "doc": {"question": "who sang the theme song to step by step", "answer": ["Jesse Frederick James Conaway", "Jesse Frederick"]}, "target": "['Jesse Frederick James Conaway', 'Jesse Frederick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the theme song to step by step?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pointer Sisters"]], "filtered_resps": ["The Pointer Sisters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9270d89f074b1cca30031d6550a6079e5e7beb0136b5794f036b1698feda5cc7", "prompt_hash": "d957f48b2f4f9ef7c5973f6eafb026c5410682d483f8f403ba4fd88efae6c660", "target_hash": "0256097fcf46b71ef9ba0845d5858961410096b4b9f1ed7dd70e2772e226068e", "exact_match": 0.0}
{"doc_id": 2472, "doc": {"question": "what was the tower of london originally used for", "answer": ["a royal residence", "as a royal residence"]}, "target": "['a royal residence', 'as a royal residence']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the tower of london originally used for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fortress"]], "filtered_resps": ["fortress"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "898368fc91d8914fe30016d466246f577ec9488a222d9b8ca57ae86b4bd08d37", "prompt_hash": "6e8a3b8969ba17459e5729e6f4a079758824084ac93010c7e7ec5a6e61dd96ab", "target_hash": "9b25bf882707faf8a9de20a87f1caf2ce96ca2bb8b1c3d8ce5d26c803b1700a7", "exact_match": 0.0}
{"doc_id": 2473, "doc": {"question": "who dies in the beginning of deathly hallows part 1", "answer": ["Mad-Eye Moody", "Hedwig"]}, "target": "['Mad-Eye Moody', 'Hedwig']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who dies in the beginning of deathly hallows part 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sirius Black"]], "filtered_resps": ["Sirius Black"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32774d0af0a9729decd32aef81d5e2872580330a3b3d488d49b125f59c3a0637", "prompt_hash": "497ba45f4c6a37f19515533a07f8d568248fb8e0c08fef35cc657f797bad7a30", "target_hash": "b369c26090574b6613bee976b397dd7b3a90a9451e3a5ab7a8853c58fe6ab718", "exact_match": 0.0}
{"doc_id": 2474, "doc": {"question": "how many tornado planes does the uk have", "answer": ["138"]}, "target": "['138']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many tornado planes does the uk have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d202beb5548a0a318c853a68d135c6e5de564808454302c5c8cdf65fccc5e7d0", "prompt_hash": "7bffa4846dd498302d35eedd9dcc6fe5826f188a69a8a18777eac4ef9f0c8ba3", "target_hash": "b7044dcc49ddc7eaba14164b9e360582ba150098440024cddc28b66cfb889f1e", "exact_match": 0.0}
{"doc_id": 2475, "doc": {"question": "who designed the traffic circle in long beach ca", "answer": ["German engineer Werner Ruchti", "Werner Ruchti"]}, "target": "['German engineer Werner Ruchti', 'Werner Ruchti']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who designed the traffic circle in long beach ca?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William J"]], "filtered_resps": ["William J"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c479cb4874c95e40657976b1727d4eb72c06af87f15154faeb2a8b6dbbdefef2", "prompt_hash": "0063ce2da1645f79219b88598d1a9bfea21b8059bf05cfd965f4d16e6c0cf5e5", "target_hash": "99b4805675a04c736c3fcc522bcede7df46709e8237894eb63d580917966dba6", "exact_match": 0.0}
{"doc_id": 2476, "doc": {"question": "who plays carrie's boss on king of queens", "answer": ["Doug Pruzan", "Alex Skuby"]}, "target": "['Doug Pruzan', 'Alex Skuby']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays carrie's boss on king of queens?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Ratzenberger"]], "filtered_resps": ["John Ratzenberger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8239f20a211be13c9b1cce12c26abab89fc1143c3b61785228a8d6105fa1b4bb", "prompt_hash": "5d8049c6845744ca9e5ae15cb33ec41b86118d949d7cd8301f64e135185e67bc", "target_hash": "c38c31c99354c26f55da823aa16773f1aeb50e3b2340ab71fc087221eece147c", "exact_match": 0.0}
{"doc_id": 2477, "doc": {"question": "when did computer become widespread in homes and schools", "answer": ["1980s"]}, "target": "['1980s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did computer become widespread in homes and schools?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c5838d519421aad24dd5c5583b4fee03c510638215b4da81a0688daa36773bd4", "prompt_hash": "addc8e3f72afe0d2ea91fb5114c2f79518906b2ee1ec2413fb571531d3269e0a", "target_hash": "a57d7c881870eea6c580bd441f8611f917d83f663f4165c183cba1a24c92d1c5", "exact_match": 0.0}
{"doc_id": 2478, "doc": {"question": "when did the age of new media began", "answer": ["the 1980s"]}, "target": "['the 1980s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the age of new media began?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1970"]], "filtered_resps": ["1970"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "67d32d8fb469f154a37bd16fe19b06492ba44bd697b4d84ab87b0c3cd6ad866a", "prompt_hash": "5d4b88b2af97d98c207b87de221ac44a2e6fe5662e6301d5536572016aeebf3a", "target_hash": "5977af2947aeedf21efd5cf2eb3d86116a89fd708d9f59d4709ddcf66bc5a517", "exact_match": 0.0}
{"doc_id": 2479, "doc": {"question": "when does agents of shield season five start", "answer": ["December 1, 2017"]}, "target": "['December 1, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does agents of shield season five start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" March 14"]], "filtered_resps": ["March 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "08d23e310024349dd57c08941ca77ae21367d69d5d30e464713cc5782ce2b134", "prompt_hash": "8f92a4f1145081da0b1183173ec846534886e1712dde3b991b0bc7ed640060fd", "target_hash": "db13c6ce2f8a4c02e95e4d924653f5e19686232e03c0eac772e3c6c7b7dda453", "exact_match": 0.0}
{"doc_id": 2480, "doc": {"question": "where does the thames river begin and end", "answer": ["Lighthouse Cove", "Near Tavistock"]}, "target": "['Lighthouse Cove', 'Near Tavistock']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the thames river begin and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oxford"]], "filtered_resps": ["Oxford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b39889384b95e0672ef46aeeba79681e21ddc57a29dc4d89312e665143123f10", "prompt_hash": "e09f8bf322089b76d7e6611ce3de9b3ed99a45cc74aebd2d27c750edcc102485", "target_hash": "0caffbc60d40dd24c88bc0fb9c8960d80dca9b38d8d520c9e1e43097faec87d4", "exact_match": 0.0}
{"doc_id": 2481, "doc": {"question": "where did the term 'robot' come from", "answer": ["Czech"]}, "target": "['Czech']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the term 'robot' come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Czech"]], "filtered_resps": ["Czech"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "516d8075d3176f963af4e448cb7672bb10c0125b677efce35a8d07d130fac6f2", "prompt_hash": "bbd577d966f28abc2f8403ac21225953e665a86fce4c0d4177d90821375ba9fe", "target_hash": "1e70445fc294861215150126ed88b85cd4ac74e3b5f1d463e1488b92a56d7e77", "exact_match": 1.0}
{"doc_id": 2482, "doc": {"question": "who was on the first season of dwts", "answer": ["Joey McIntyre", "Evander Holyfield", "John O'Hurley", "Rachel Hunter", "Trista Sutter", "Kelly Monaco"]}, "target": "['Joey McIntyre', 'Evander Holyfield', \"John O'Hurley\", 'Rachel Hunter', 'Trista Sutter', 'Kelly Monaco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was on the first season of dwts?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Derek Hough"]], "filtered_resps": ["Derek Hough"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a94899b45444ab26c387467f08afcde2bfc0c87593365b6382195e542601d347", "prompt_hash": "7d8ad33963b7a81672601d81ba7565d1a352a6f88f141195e177d8b948dd172d", "target_hash": "1d27b8324c4b040ad264e229ec22acf0d7428f08f70290bf4c86464f4ae33a32", "exact_match": 0.0}
{"doc_id": 2483, "doc": {"question": "who sings i feel love with the blue man group", "answer": ["Annette Strean", "Annette", "Venus Hum"]}, "target": "['Annette Strean', 'Annette', 'Venus Hum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i feel love with the blue man group?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lou Reed"]], "filtered_resps": ["Lou Reed"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d5673700147eaece7854fcfd89b05475d4a625b2fadddf06a976703c12a71cda", "prompt_hash": "1674a943fa7c39d15ce7a1b52b8a2870ee5bf675ca988a90b01dbaba476c6094", "target_hash": "5674b2141955b02c180a3f79743c0c79dbae1e03a2b813b9e0209f981be7c080", "exact_match": 0.0}
{"doc_id": 2484, "doc": {"question": "where is the ucla usc game being played", "answer": ["UCLA", "USC", "Coliseum", "Rose Bowl"]}, "target": "['UCLA', 'USC', 'Coliseum', 'Rose Bowl']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the ucla usc game being played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rose Bowl"]], "filtered_resps": ["Rose Bowl"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "55b120137aba0b6198f1a871cf2d1388de9f359ae209d60e56d10a7ad878db98", "prompt_hash": "8ffe3c518b4e2e788a1123be6cf2e29be4dad295d0c504330a36f43ca3715f91", "target_hash": "1de87eb61d9cd6c436e21c49e70702f05b28736e9e2a6d6eea674460be6d47ea", "exact_match": 1.0}
{"doc_id": 2485, "doc": {"question": "who sings i just want to use your love tonight", "answer": ["English rock band the Outfield"]}, "target": "['English rock band the Outfield']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i just want to use your love tonight?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Commodores"]], "filtered_resps": ["The Commodores"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "33d758ecdce5c6e16a3bc5d7eb5c22607b58deca2f63d090365a8974b120a59f", "prompt_hash": "22f62bcbab501d1b4607d2bf577a041ec86e490da15ae60ba73f183161a64900", "target_hash": "4175808174962605f8ec8b553b3e0eb0b402302f8f02fedc053c5ab64daf245b", "exact_match": 0.0}
{"doc_id": 2486, "doc": {"question": "when does planet of the apes come out 2017", "answer": ["July 14, 2017", "July 14, 2017"]}, "target": "['July 14, 2017', 'July\\xa014,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does planet of the apes come out 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 6"]], "filtered_resps": ["October 6"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b814d7616963c2ed2ccf33df113a9b8ac1b0b15c170eb27cbb92ec6f031fa942", "prompt_hash": "08f0f8141956f29fb8373d38aced0cf16b7b4fbed1aa8bad26016368d62a5014", "target_hash": "c9c7518a771bef823329070b261c024f0f461042ece6c588c97dc6c83ab38cc7", "exact_match": 0.0}
{"doc_id": 2487, "doc": {"question": "where are they playing the superbowl at this year", "answer": ["Minneapolis, Minnesota"]}, "target": "['Minneapolis, Minnesota']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are they playing the superbowl at this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Raymond James Stadium"]], "filtered_resps": ["Raymond James Stadium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4db5a751f454fd94cda251a5dc5ab9703e19ad20d0df09e0d40c5bb1e878c719", "prompt_hash": "cbfd79a972b28ca3007ff83ecb5a2bfa8216fec3bc3a8bd7fd5e23f8a25fec8b", "target_hash": "7ed804c4d1bec102a0b37176fe28104624b4188009318c3864069130df667e86", "exact_match": 0.0}
{"doc_id": 2488, "doc": {"question": "who starred in the movie summer of 42", "answer": ["Oliver Conant", "Christopher Norris", "Gary Grimes", "Katherine Allentuck", "Jennifer O'Neill", "Jerry Houser", "Robert Mulligan", "Maureen Stapleton"]}, "target": "['Oliver Conant', 'Christopher Norris', 'Gary Grimes', 'Katherine Allentuck', \"Jennifer O'Neill\", 'Jerry Houser', 'Robert Mulligan', 'Maureen Stapleton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the movie summer of 42?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chadwick Boseman"]], "filtered_resps": ["Chadwick Boseman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0794962b1734ccb916d2b2ff1f1420f2ca2e130104a718e35ec5a7c153edcbe5", "prompt_hash": "831fc08c94646cded4a17023cc85ca6a5be4d2c3c4c7e85c0ac3acdb9f52cee5", "target_hash": "329e8b352d1f60ee431139c84169efd1316c96ed3e05d289fafbb5c04c54c7fd", "exact_match": 0.0}
{"doc_id": 2489, "doc": {"question": "who plays the voice of johnny in sing", "answer": ["Taron Egerton"]}, "target": "['Taron Egerton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the voice of johnny in sing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew McConaughey"]], "filtered_resps": ["Matthew McConaughey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e0d911b5260d4389957f066eb43bf935b4a9b5be4af51ff88d1fbeec6b44b741", "prompt_hash": "c61b7ec17cd5a2e014766466438a13754b42f68094d9bdb060761d26f4389b4b", "target_hash": "77449c359f9366caf61b67ab09c6925942c6fdf464f3d3b2d2a110503540533e", "exact_match": 0.0}
{"doc_id": 2490, "doc": {"question": "who is the owner of phoenix mall in chennai", "answer": ["Sharyans Resources", "Phoenix Mills Limited"]}, "target": "['Sharyans Resources', 'Phoenix Mills Limited']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the owner of phoenix mall in chennai?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Phoenix Mills Limited"]], "filtered_resps": ["The Phoenix Mills Limited"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a4fd9d6a0c3394ffede7e263aac74f986d9027979361814d68e1633373fea30", "prompt_hash": "332e58b7c462bcad10766b63db06b81e2add74ab9c145430235fb322ae148ae0", "target_hash": "a4598227588a8ed7f7c67867fd50d6895085361b58ee74ab9818940ffef01887", "exact_match": 1.0}
{"doc_id": 2491, "doc": {"question": "who was the greatest pool player of all time", "answer": ["Efren Manalang Reyes"]}, "target": "['Efren Manalang Reyes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the greatest pool player of all time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Steve Davis"]], "filtered_resps": ["Steve Davis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "add1847ad369750cc0d572ef0819c073bfd945f627ec772f73ce3edc24dc20c2", "prompt_hash": "ee24c4e6a2535f5bbf42a4c3c3ba9000b8682053ea9aa75fdea6243c1391737a", "target_hash": "fc53a098426104f778e6dccf86e6af6bb2d957f14dfe747f2aefa64a654989f1", "exact_match": 0.0}
{"doc_id": 2492, "doc": {"question": "what states were most affected by the dust bowl", "answer": ["Iowa", "Oklahoma", "Nebraska", "Arkansas", "New Mexico", "Kansas", "Colorado", "Texas", "Missouri"]}, "target": "['Iowa', 'Oklahoma', 'Nebraska', 'Arkansas', 'New Mexico', 'Kansas', 'Colorado', 'Texas', 'Missouri']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what states were most affected by the dust bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Texas"]], "filtered_resps": ["Texas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cb0341be3796b5d074a2aa31ba6144213af72ba1c0e9bbd9b0e36b6e7d6d7a08", "prompt_hash": "6a3f1636f9765dc42e3cfcde318c92c08c1e9402889da0ad6b1bf995f3923e47", "target_hash": "db4ddfa772cf5f27dfa53cab247ea1351114a409ec7c5ae9cab9455985afa0ce", "exact_match": 1.0}
{"doc_id": 2493, "doc": {"question": "how many episodes are in season 7 of pretty little liars", "answer": ["20"]}, "target": "['20']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes are in season 7 of pretty little liars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 22"]], "filtered_resps": ["22"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44154551337c2a363692228e6c445963d91185a9766af9e595f58929894a61c0", "prompt_hash": "d767b2ea1da25a34dbc2b6e43f1571fecc66c8137dfee71033b1d2b0cd3e3583", "target_hash": "d6ce741f343e4bc662a4b719bcaa42c3883a6915e4764a42b8b24566aa3509f2", "exact_match": 0.0}
{"doc_id": 2494, "doc": {"question": "a type of basic rock popular in the 1980s", "answer": ["alternative rock"]}, "target": "['alternative rock']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a type of basic rock popular in the 1980s?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" granite"]], "filtered_resps": ["granite"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "00d0c414fa656ca0c2a069a1b58ca8e67754deaed117a325c3e7cd9e4579ee2c", "prompt_hash": "9049396b7d22c2befec6d8d66fc858215c7cde2a9f9890ef436c2b6fda06a742", "target_hash": "663ca3d2f2bc1931b4d1c098603b193d1adde911b60afe566d1ce320e2f80e29", "exact_match": 0.0}
{"doc_id": 2495, "doc": {"question": "how are leaders of the two parties in congress chosen", "answer": ["Senate Republican Conference", "Senate Democratic Caucus"]}, "target": "['Senate Republican Conference', 'Senate Democratic Caucus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how are leaders of the two parties in congress chosen?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the party with the most elected members"]], "filtered_resps": ["the party with the most elected members"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a6300d2484ebcb94847794d20d6e6f2e98e963e5ac51e2c88560b8d75a9c56e6", "prompt_hash": "884feed8268cb39bad43e99bf3db63091999815851d969266a3a1a4fc5ea249f", "target_hash": "a300f4c06e085f9d7d683eeb93bbcfdfd843c521b003a77b0f4d3438f93bfd31", "exact_match": 0.0}
{"doc_id": 2496, "doc": {"question": "itai-itai disease was a form of poisoning associated with ingestion of", "answer": ["cadmium", "Cadmium"]}, "target": "['cadmium', 'Cadmium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: itai-itai disease was a form of poisoning associated with ingestion of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cadmium"]], "filtered_resps": ["cadmium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e6b3c82e43e6e608f50648e5be20682ec40bf479833dfc4b64bb764ba18a3ed4", "prompt_hash": "d06ac5dc0d163987d26aae3e6745816e2990c1604d6d6df52c4e1d87546d5cac", "target_hash": "567d586ecd12da4ad11be97e6b9e8505749c89c19fb6303c8b4ed2701f738998", "exact_match": 1.0}
{"doc_id": 2497, "doc": {"question": "what was the code name of the us program to develop an atomic bomb", "answer": ["Manhattan", "Development of Substitute Materials", "The Manhattan Project"]}, "target": "['Manhattan', 'Development of Substitute Materials', 'The Manhattan Project']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the code name of the us program to develop an atomic bomb?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Manhattan Project"]], "filtered_resps": ["Manhattan Project"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "65338acc5130262b56e7735ad660b6a48e06ae4ba23dbea13d82cdae462822ee", "prompt_hash": "d3b13efaa0e1ca380abde68c67765faa1c42e368c3d78c4cbddd63c05184ee99", "target_hash": "12f75526dc68eef6755d3c24da18a12a10b3a2f7cd41187271bf9fedb53b91eb", "exact_match": 1.0}
{"doc_id": 2498, "doc": {"question": "what is the real name of our sun", "answer": ["The Sun"]}, "target": "['The Sun']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the real name of our sun?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sol"]], "filtered_resps": ["Sol"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b087159135874dbcf8df4283259aeea29ed5914758a3969df157ce34bdaa9058", "prompt_hash": "b8f3d3bbcb0cc95bb8ffbf59116723ebdf213b856093d26571d9c9091055f7a6", "target_hash": "8a4361e43694a1c01858b627c49d9a0faa138d351f67538560415d7b3f0d63e4", "exact_match": 0.0}
{"doc_id": 2499, "doc": {"question": "who sang the song oh what a lonely boy", "answer": ["Andrew Gold"]}, "target": "['Andrew Gold']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song oh what a lonely boy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Foundations"]], "filtered_resps": ["The Foundations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1e504d1dc2baf5dbf3e82dcd195251336b2418cdf04bb9051a4b88412c0e9103", "prompt_hash": "a24c1b505c0608af31e7e6242afbb0b16800ea55b77c01ee8dea3270f1229565", "target_hash": "7f30a6f1b28a19dd642ac9bfda5e8392ae0c492677fa805eee354628cd2023c3", "exact_match": 0.0}
{"doc_id": 2500, "doc": {"question": "in the dynastic cycle what is the right to rule called", "answer": ["the Mandate of Heaven", "Mandate of Heaven"]}, "target": "['the Mandate of Heaven', 'Mandate of Heaven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in the dynastic cycle what is the right to rule called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mandate of Heaven"]], "filtered_resps": ["Mandate of Heaven"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "912afdfaa3632e6c2cdbbdebb8af99841b93a10aa1c160e2e9b5ebfd97724e70", "prompt_hash": "ca99238bc97e081e32be1c1000ff0a5d9414d46b96cbadce7d33dc2e65ff3cd1", "target_hash": "95987f7f1e8ae89b3547238b564f0342d6878b9ca2b5a4eaf91dc3099f2a6b8e", "exact_match": 1.0}
{"doc_id": 2501, "doc": {"question": "when was the problem of cfc's discovered", "answer": ["late 1970s", "1974", "1973", "1970s"]}, "target": "['late 1970s', '1974', '1973', '1970s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the problem of cfc's discovered?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1974"]], "filtered_resps": ["1974"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7bbaff1cc03fbe6cae72749a3f275daa9afff4387f28ea052a1e3d3fbaa5fc99", "prompt_hash": "4d621b3655e74e9cffd472f2555d1ff4320132f662593b704ef2d1a19325118d", "target_hash": "164e4a86927c9c0d0500cec7dc1e6d6f071b2b2f3d507f9ff029697eac5f9f05", "exact_match": 1.0}
{"doc_id": 2502, "doc": {"question": "which president of the united states was a boy scout", "answer": ["Gerald Ford", "President Gerald Ford"]}, "target": "['Gerald Ford', 'President Gerald Ford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which president of the united states was a boy scout?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dwight D"]], "filtered_resps": ["Dwight D"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f9c581869e98e6857088cebacd450a4dceb710be407663288dca6c4a033ee241", "prompt_hash": "4f16b9b5a1c1eb3630090c3c04500557142286c8607a710023019f6dbc3cc707", "target_hash": "23988ccbf8f104a3e2479d0afb4ccb8c3d4ff6cfc462903fe635e10f08389df1", "exact_match": 0.0}
{"doc_id": 2503, "doc": {"question": "who won the 2017 sports personality of the year", "answer": ["Mo Farah"]}, "target": "['Mo Farah']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the 2017 sports personality of the year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mo Farah"]], "filtered_resps": ["Mo Farah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1486c554f4c769097777e07b0c888e341893ed81c7c6f55d637a6141e3a0f761", "prompt_hash": "285e15d2111ef5556bc50406c223bd472f7ea0ed1a7b7af54e3bef55f40c0eb3", "target_hash": "a8dbda421ce3b21c9e5ed7cdffbeb3bc19e14fc43e8d703b0628474a434e8d3d", "exact_match": 1.0}
{"doc_id": 2504, "doc": {"question": "the first significant restriction on free immigration in u.s. history was the", "answer": ["Naturalization Act of 1790", "Chinese Exclusion Act in 1882"]}, "target": "['Naturalization Act of 1790', 'Chinese Exclusion Act in 1882']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the first significant restriction on free immigration in u.s. history was the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chinese Exclusion Act"]], "filtered_resps": ["Chinese Exclusion Act"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "749fab8d7be24a79c3efd4b60153d15bb62cf7d98cf90c4c4d8fc3bf850846d2", "prompt_hash": "74030be245d13ad88c4c146ed94f1d65808332df569b01627f89d58aecd023e6", "target_hash": "3e507d402735061ee01867dbd4ef366964a3870d655441d82ee637904541ce8a", "exact_match": 0.0}
{"doc_id": 2505, "doc": {"question": "who plays young voldemort in the half blood prince", "answer": ["Hero Beauregard Fiennes-Tiffin"]}, "target": "['Hero Beauregard Fiennes-Tiffin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays young voldemort in the half blood prince?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Daniel Radcliffe"]], "filtered_resps": ["Daniel Radcliffe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a6cfdf287aadfeea8c93b774333e4581d8b53e913d9ef7451a5becfef726cb7", "prompt_hash": "24d05eacaf1d6728faaab7e4f827d914f29257d5547daf10076315f4e8ca493e", "target_hash": "0ef3ee477c0925fad6bb894ebfd45c1499ca320459511cb0735154639425ad3d", "exact_match": 0.0}
{"doc_id": 2506, "doc": {"question": "when was the last time mount etna exploded", "answer": ["July–October 2012", "16 March 2017"]}, "target": "['July–October 2012', '16 March 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time mount etna exploded?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bd04537adc4fc10a1775807ec0b6bacad3de96682ccd42192072b685b1ee0f06", "prompt_hash": "8995c636eeedb46ebc00b4b03bb8fa7e67a9d1344fac4edef828375f49d11e8d", "target_hash": "e631c58c368df8e2c755e6bd976c59f33280fbe4cf05313270d13a1da814f3c9", "exact_match": 0.0}
{"doc_id": 2507, "doc": {"question": "where does some like it hot take place", "answer": ["Chicago", "Miami"]}, "target": "['Chicago', 'Miami']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does some like it hot take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Florida"]], "filtered_resps": ["Florida"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e1d596a5348d0e5168fa523045d3777e60accc8e160c2be77a4b853cb2bcb123", "prompt_hash": "cb51d35a5e9735ea89bc5d21ca0168aad1eae00a5dcf343961fe72f2db1a1bc1", "target_hash": "ac0dd15b8c21f0d7f923fc6bc5e7508ec9b43639b5f9676cad79cc930529f0b2", "exact_match": 0.0}
{"doc_id": 2508, "doc": {"question": "when did dragon ball z air in australia", "answer": ["Fall 1998"]}, "target": "['Fall 1998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did dragon ball z air in australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1990"]], "filtered_resps": ["1990"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d634ccb61eed308e6695fb58b4b7b37e6cb0996858746eb38f2d50c000f77045", "prompt_hash": "6e2107995786675022a85575a43c9bbc698025212ba8a52b95ff4769a7b7fa88", "target_hash": "e59cc932f3726c3fb6bf5524413634804ba3d7e1c07dcc69f5878df653024e5d", "exact_match": 0.0}
{"doc_id": 2509, "doc": {"question": "describe the various uses of forests to human beings", "answer": ["a source of lumber", "acting as a carbon sink", "aiding in regulating climate,", "recreational areas", "purifying water", "serving as a genetic reserve"]}, "target": "['a source of lumber', 'acting as a carbon sink', 'aiding in regulating climate,', 'recreational areas', 'purifying water', 'serving as a genetic reserve']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: describe the various uses of forests to human beings?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lumber"]], "filtered_resps": ["lumber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "15bba0fb5a9c1a9b3b7145f815f7778873e1f01639adfe5f6f7807f7b7c899bb", "prompt_hash": "6e5f094a0e0e026f7a27615a5e8df7515a26cf3e7fa5ee627b9a1fc92e1fb6f0", "target_hash": "a0c35fca0630f0979ef0b014cf4d888e1c9087a33f5217d48ad1e5ef5e63025a", "exact_match": 0.0}
{"doc_id": 2510, "doc": {"question": "how many episodes in my name is earl season 4", "answer": ["27"]}, "target": "['27']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes in my name is earl season 4?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 13"]], "filtered_resps": ["13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e92d888feaabff2371017d16a94b669ee4116e3ec19639209b50cd227c0e96d6", "prompt_hash": "fa711baf3c8c8bd7b74fff7c08e59fbcb4a237f484fb94096baf203d1da7962c", "target_hash": "a884fa75540dd3a7a21a4ea103a1fd31f0474da4934ee4983c64ca164c529704", "exact_match": 0.0}
{"doc_id": 2511, "doc": {"question": "where did maria work in west side story", "answer": ["the dress shop", "dress shop"]}, "target": "['the dress shop', 'dress shop']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did maria work in west side story?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the bakery"]], "filtered_resps": ["the bakery"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9e794139fd765f31a31030e1a519ff7846ac179b564850b507e7abf41372560b", "prompt_hash": "d404f2d863cb594552c8f6e4c5b0b268a28cea5f046455ac5f72605204de190d", "target_hash": "09403712a0a7beeb63c9285fb12c754ae4bf42a3cc6cd6b8865131b094cb7a4f", "exact_match": 0.0}
{"doc_id": 2512, "doc": {"question": "an influential religion to enter china along the silk route during the han dynasty was", "answer": ["Buddhism"]}, "target": "['Buddhism']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: an influential religion to enter china along the silk route during the han dynasty was?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Buddhism"]], "filtered_resps": ["Buddhism"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f807bea5257bfd7b2494d3969994c5e0fe55e48419327dff7c0da5d9da743b98", "prompt_hash": "eec298e816007745962c0d897c8a0b5d456b0421d7871b8aff53461b6a5b6048", "target_hash": "ec9e19bda8bfce78abb29dee3228fa696681b07f759d836770989b6c5e2b5375", "exact_match": 1.0}
{"doc_id": 2513, "doc": {"question": "where does the last name andersen originate from", "answer": ["Danish", "Norwegian"]}, "target": "['Danish', 'Norwegian']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name andersen originate from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Danish"]], "filtered_resps": ["Danish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "50d04d9dd23c8ad2d0186d639c808e8a3324ba8592b4815aea95792d7c1111cc", "prompt_hash": "be82aaa62bede0aa40a4d95fca28c2f281ccb0c7b3e2fbc450b0eb7f40d97cbb", "target_hash": "3c32cdbbed3249ed2914c0187de4b784adeaab6e9e29c61890bb390bb8385e24", "exact_match": 1.0}
{"doc_id": 2514, "doc": {"question": "where will the next summer and winter olympics be held", "answer": ["Beijing", "Tokyo"]}, "target": "['Beijing', 'Tokyo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where will the next summer and winter olympics be held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rio de Janeiro"]], "filtered_resps": ["Rio de Janeiro"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44d86ed25a86f52ac1122e6e4dd5e5d40a3851ebfb218f351df7a0e5a1e1aaca", "prompt_hash": "8f60ec9f3630a76dc145d6ef3c84651ccb3a5e810e60f23ac220839094082b17", "target_hash": "0a4acde9cdc617185ad44307c5c950bf51224dc019ee715e34ed94f4ee560a8c", "exact_match": 0.0}
{"doc_id": 2515, "doc": {"question": "who played mr. kincaid on the partridge family", "answer": ["David Joseph Madden"]}, "target": "['David Joseph Madden']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played mr. kincaid on the partridge family?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Cassidy"]], "filtered_resps": ["David Cassidy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "50157b180ed1dd59184fa3f788e9eb337eccc5ee7a759db4e11a80814905b8f9", "prompt_hash": "a1e67e83bb8fc89fc66f8b71d436a7d90de8b5de6d432f4492a9a10a9100c727", "target_hash": "0b3044138129dab0d3c06bef00e026a139dac210a3926daaa8873defb6c867e9", "exact_match": 0.0}
{"doc_id": 2516, "doc": {"question": "when did they stop making pull tabs on beer cans", "answer": ["1990", "1970s"]}, "target": "['1990', '1970s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they stop making pull tabs on beer cans?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9de88c4d39fc463d43a725ddc287e5869ba92e1a5b3d954466bdec42f3b8909", "prompt_hash": "44d8732acce98b8cb1f57c5a7e2a760816a0558954fd000fe9d237499b7efe69", "target_hash": "c1c5e9a17348346140740f8a695551df3288323f4104174b6c82594141edd3b2", "exact_match": 0.0}
{"doc_id": 2517, "doc": {"question": "who sings i wanna rock'n'roll all night", "answer": ["Kiss"]}, "target": "['Kiss']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i wanna rock'n'roll all night?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Def Leppard"]], "filtered_resps": ["Def Leppard"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a47e075e13a64840dc87115bea555a0ebf7e45e2e2bacd74b49fdb774d8949c8", "prompt_hash": "b48017a5d14a0a1deeefdda1411e27e1e4eb83c7defad193d733fe30ac9ecbb6", "target_hash": "ffe45c08df7efc77e4df836885c8dc81228d83491549c51461c7fbf00c3acfab", "exact_match": 0.0}
{"doc_id": 2518, "doc": {"question": "who scored fastest 10000 runs in test cricket", "answer": ["Alastair Cook", "Kumar Sangakkara", "Brian Lara", "Sachin Tendulkar"]}, "target": "['Alastair Cook', 'Kumar Sangakkara', 'Brian Lara', 'Sachin Tendulkar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who scored fastest 10000 runs in test cricket?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sachin Tendulkar"]], "filtered_resps": ["Sachin Tendulkar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "41221f913809a044bb83f6d4a19785f4b98ca2aee5c37cd3b527e4aa7e5db5c3", "prompt_hash": "dabd754eb915d9211f174e7d8b0c99479793a0e31490f7c475e22774838ca61e", "target_hash": "89cc3697f5c38e620def0d751a739ea1816f4104653fa36d6446f75644beaec7", "exact_match": 1.0}
{"doc_id": 2519, "doc": {"question": "who presides over the joint sessions of parliament", "answer": ["President"]}, "target": "['President']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who presides over the joint sessions of parliament?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Speaker"]], "filtered_resps": ["Speaker"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "14b7c2cb2cae084f0eb051a5a44dafc8c98ff41cbc4ea4a4a5c20a9e92b5a310", "prompt_hash": "8e336fc0ae859c698efb635514cb7f1a9e410deb11bb84b96e4da5b4d634e4fb", "target_hash": "b27a8f3cb1769606ff32b3a58c38f1e71ffbc5081691a2a3932ca7bb9aca003a", "exact_match": 0.0}
{"doc_id": 2520, "doc": {"question": "cast of the have and have nots play", "answer": ["Tony Hightower as Frank", "Palmer Williams Jr. as Floyd", "Maurice Lauchner as Lewis", "Jeffery Lewis as Wallie", "Alexis Jones as Diane", "Kislyck Halsey as Rose", "Patrice Lovely as Hattie"]}, "target": "['Tony Hightower as Frank', 'Palmer Williams Jr. as Floyd', 'Maurice Lauchner as Lewis', 'Jeffery Lewis as Wallie', 'Alexis Jones as Diane', 'Kislyck Halsey as Rose', 'Patrice Lovely as Hattie']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: cast of the have and have nots play?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Esposito"]], "filtered_resps": ["Jennifer Esposito"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c112f4436436ced59e02116126b6c95090295c9c5d43b1285ecbc184dbb6041", "prompt_hash": "911a40f91ac55c46e66de5aeef4dcc7b3eba15bde4b8e67f95e5801284904f58", "target_hash": "03ea0010c880f458b57940dd16f6954de1c50961cd6a8f622e594098764eed95", "exact_match": 0.0}
{"doc_id": 2521, "doc": {"question": "who starred in the movie on golden pond", "answer": ["William Lanteau", "Christopher Rydell", "Dabney Coleman", "Henry Fonda", "Jane Fonda", "Katharine Hepburn", "Doug McKeon"]}, "target": "['William Lanteau', 'Christopher Rydell', 'Dabney Coleman', 'Henry Fonda', 'Jane Fonda', 'Katharine Hepburn', 'Doug McKeon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the movie on golden pond?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Katharine Hepburn"]], "filtered_resps": ["Katharine Hepburn"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "66453e8735995f0d35d39cb749eb5384770ba63e1c40a4270accbe36ae2a3583", "prompt_hash": "661fd74d747d65cc9ef1c1da977cf6301255ed3e1f19d8311759341f575a92dd", "target_hash": "e7b84b864efa9cdd7339aa0b642b86370ac89c7d19bfed99c313a061748dbb6f", "exact_match": 1.0}
{"doc_id": 2522, "doc": {"question": "what is the salary of us secretary of state", "answer": ["$205,700", "$205,700 annually"]}, "target": "['$205,700', '$205,700 annually']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the salary of us secretary of state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $199"]], "filtered_resps": ["$199"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c7de8938faaa832f346cb5eb3ff98f4bfcd2ae0fd4f2fa984c546c5753b4309b", "prompt_hash": "fc1d09ce9a24ae5cb2bec4d8b9fd3e419bae7a7524048faa7b138e29540cd9b9", "target_hash": "5519a96693b0d4cdbafb43899a72d4ca5e8e32661e4c768b6594da1972b8aa13", "exact_match": 0.0}
{"doc_id": 2523, "doc": {"question": "latest series of keeping up with the kardashians", "answer": ["14"]}, "target": "['14']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: latest series of keeping up with the kardashians?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 19"]], "filtered_resps": ["19"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2e6eb6c69b92511d6aa8b3867e8a27578b6016a56237b342c9a05b3598b4353f", "prompt_hash": "289a2638c5991118225f0d9edacc1e556d9e3e1f9b236d5df20a5a14094e17aa", "target_hash": "1c4adeef9d9168604b52ef50b17fab3bebe77f9e001e8dcaa2e38dfcfc074f31", "exact_match": 0.0}
{"doc_id": 2524, "doc": {"question": "avatar the last airbender season 3 episode 12", "answer": ["The Western Air Temple"]}, "target": "['The Western Air Temple']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: avatar the last airbender season 3 episode 12?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Northern Air Temple"]], "filtered_resps": ["The Northern Air Temple"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49943ad2e394714558993cc5bee8905af64137bf6409288e291547b7b77ad578", "prompt_hash": "76317e6d8c9855129b0ef75e69a3bcb467bca1b3b7fa77443b97052a1a450dc2", "target_hash": "e614ac99680887cf3f6b4fb95be8d27da5acb2c02691cedce8ec8b9fdfc7c7e8", "exact_match": 0.0}
{"doc_id": 2525, "doc": {"question": "the mughal garden of rashtrapati bhavan is modelled on which garden", "answer": ["the Persian gardens", "charbagh"]}, "target": "['the Persian gardens', 'charbagh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the mughal garden of rashtrapati bhavan is modelled on which garden?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Taj Mahal"]], "filtered_resps": ["Taj Mahal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5937d285f77fdda3676f06a08c9ea8424ba5ec80ba8514a63eef8eabaf59a2a0", "prompt_hash": "7ea53246d0c2809f080968f3606c6420a237f33ab7870d5fb1a44434ef243059", "target_hash": "2719ecb53a44a1f17f221b30a92872163a639c57edd730431cc2a1057a6d6d65", "exact_match": 0.0}
{"doc_id": 2526, "doc": {"question": "when was to god be the glory written", "answer": ["around 1872"]}, "target": "['around 1872']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was to god be the glory written?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1949"]], "filtered_resps": ["1949"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b267ea09defebeb3cdab727952f58dbb6aec164fba3a542e5ac69daaa7dfc1ef", "prompt_hash": "5664350a9826edea50358355dc977878a7eb6232e5457fdf093c9eb7914c04ab", "target_hash": "fa93df2ab36ecdd6a28fa31c1202da278cd9437e0484814f267cce409b53f2c8", "exact_match": 0.0}
{"doc_id": 2527, "doc": {"question": "when was the cat and mouse act introduced", "answer": ["1913"]}, "target": "['1913']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the cat and mouse act introduced?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1914"]], "filtered_resps": ["1914"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "164dee89490026c3f1b49207b41b5f6b16805448a5aefc248f890d1d4fb7a7ad", "prompt_hash": "2fa3876a923c6b34bf9d68dda9897185b50586d12157b0283c179ebdfdb35a74", "target_hash": "7e3671e8bff86a92c246f3f55d695717a2fcc80c1a08ecaca1fc80219a13856c", "exact_match": 0.0}
{"doc_id": 2528, "doc": {"question": "who sang i dig rock and roll music", "answer": ["Peter, Paul and Mary"]}, "target": "['Peter, Paul and Mary']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang i dig rock and roll music?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Cars"]], "filtered_resps": ["The Cars"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a6de1687d2af695f181589b8fedf31595695327d1c2204835fff595b41e542b3", "prompt_hash": "21df1017f3d53bbe7ff42ed95117792b754cc2a1d78fd7ea36ed515a71e91c92", "target_hash": "09aa3dc38fbd57d16084260702c874c5dd2983f604110e1511f04ce04e703235", "exact_match": 0.0}
{"doc_id": 2529, "doc": {"question": "who played skeletor in the movie masters of the universe", "answer": ["Frank Langella"]}, "target": "['Frank Langella']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played skeletor in the movie masters of the universe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dolph Lundgren"]], "filtered_resps": ["Dolph Lundgren"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1de0c60beba3b20db677b4d0bf9bd5a759f03d4d54faea7b69476eea6d6faa4a", "prompt_hash": "4dc875df25d441780b197778e7ec263979389c841de53c217b0d05a3da2770fc", "target_hash": "27605c732b4c9735afd24b98e996f436bbbd8e0a69b978501c7b98c0421df88f", "exact_match": 0.0}
{"doc_id": 2530, "doc": {"question": "who has been ranked no. 1 in the latest football rankings announced by fifa", "answer": ["Germany"]}, "target": "['Germany']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has been ranked no. 1 in the latest football rankings announced by fifa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Belgium"]], "filtered_resps": ["Belgium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3764d4aa86fcc128c2da69229c7b72964abe9d6e080c4aa6848b836d4fd3056f", "prompt_hash": "ae461f436b2396f6c56de1a3d458877c723cdb8a50faaaef5b8569f789d55a79", "target_hash": "2a95ebbaf7ff0a79617e9ffbb054bdd0ed5c21d0291d63ffa971746e102c8c22", "exact_match": 0.0}
{"doc_id": 2531, "doc": {"question": "explorer who led an early voyage to the coast of newfoundland", "answer": ["Jacques Cartier"]}, "target": "['Jacques Cartier']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: explorer who led an early voyage to the coast of newfoundland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Cabot"]], "filtered_resps": ["John Cabot"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "747254acb6f300182d387d2e516bc2e55aa93a472b24964c6f41d194db8547ab", "prompt_hash": "0e978a9b749fad8eb38e64204beb86104611dcbb4039d74ac87b82e99944fc58", "target_hash": "54a61c930afff5cd6884e8d7d61c171c74b4ccb46dd1511d09db3b1b7f3cff3f", "exact_match": 0.0}
{"doc_id": 2532, "doc": {"question": "how many walker texas ranger seasons are there", "answer": ["Eight", "8"]}, "target": "['Eight', '8']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many walker texas ranger seasons are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b6e88150d3f1132a5d4ed2d04c0a3f35d83504933e7640b73c125062a25dfbfd", "prompt_hash": "89d2d197e7454bb5060cdc44ba4f230641e945c9a59592b964a98ebc8a64fe70", "target_hash": "3838f7bd3b6b14f252c9d547b0c9706ab36836c65dbd83061af734ca122aed5d", "exact_match": 0.0}
{"doc_id": 2533, "doc": {"question": "where is the light between two oceans filmed", "answer": ["Australia", "New Zealand"]}, "target": "['Australia', 'New Zealand']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the light between two oceans filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32e1ef5501752c412d7b21e121b69b062c39af314c8ce49a6b019d7c46b7194c", "prompt_hash": "f051fd31523c878f87ea17fc6267e9b49c6887aeb10658a7c4684832c33c6411", "target_hash": "f7d72485fae667e1eee7b997388ed1d1c77faefce5c3a7f0ae43bfc435c13523", "exact_match": 0.0}
{"doc_id": 2534, "doc": {"question": "who played bubba in the heat of the night", "answer": ["Carlos Alan Autry Jr."]}, "target": "['Carlos Alan Autry Jr.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played bubba in the heat of the night?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rod Steiger"]], "filtered_resps": ["Rod Steiger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2faa6533a6f51efe12663292bd2263aedecef42927415b67ab807df79cb98cf", "prompt_hash": "67324c9edfb60c10bf83143883fa7fa8b9b3e2cff55573529c6220b52d0113f4", "target_hash": "d53b934ba2338feb8a8e25b8b1bcb3029a87ad8836c3f9ba4e36ccee13e3ba7e", "exact_match": 0.0}
{"doc_id": 2535, "doc": {"question": "who played sarah hedley in when the boat comes in", "answer": ["Rosalind Bailey"]}, "target": "['Rosalind Bailey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played sarah hedley in when the boat comes in?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sarah Greene"]], "filtered_resps": ["Sarah Greene"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fc8c69abb70037548838f669d22eca68895e7d425a4cfe872a26bbd9b7537b0b", "prompt_hash": "3a8b75ca517bee74ade01a3dde37f69b8e2dfad8c6e35b1345a6d378174ef0fd", "target_hash": "fb87df601612e9a36d6f3d9729c16704777d42734e83903fe74f354d2b793853", "exact_match": 0.0}
{"doc_id": 2536, "doc": {"question": "who played the beast in the movie beauty and the beast", "answer": ["Dan Stevens", "Adam Mitchell"]}, "target": "['Dan Stevens', 'Adam Mitchell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the beast in the movie beauty and the beast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dan Stevens"]], "filtered_resps": ["Dan Stevens"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9b83e4c9c627cb47cdd3e72dcf5b01efdedea2dd0def9f2d3d523efd3dd4bc00", "prompt_hash": "b129d5f66edaf4689bc5e3a76c06028faa26b7bdd6c5af1fc3fd41447598d540", "target_hash": "3b4b03e92b37e78f882c1621d6f00bd8c6641f3f3a113fb927901d5f9ddf86a9", "exact_match": 1.0}
{"doc_id": 2537, "doc": {"question": "when did the us cut off trade with cuba", "answer": ["1961", "1962"]}, "target": "['1961', '1962']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the us cut off trade with cuba?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1960"]], "filtered_resps": ["1960"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a28c7edbf0071be7164d6c13f4a9063f99e008ce97240ef98e4e7c9e811ed17a", "prompt_hash": "741cdd09f27b514b5924852f46163041b79c252416cf9a33068352a8d41ca7e0", "target_hash": "1e7e1ab065a9388132603b3d236fa0a198431394efb9193ccab96998795158cf", "exact_match": 0.0}
{"doc_id": 2538, "doc": {"question": "who developed the first printing press in 1430s", "answer": ["Johannes Gutenberg"]}, "target": "['Johannes Gutenberg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who developed the first printing press in 1430s?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Johannes Gutenberg"]], "filtered_resps": ["Johannes Gutenberg"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c42f84d4eebd9f5ad474f3ebf1657b37209dd4ee0300f894e90ca6de1079890", "prompt_hash": "61fdcbaa3bf41a054c2701bf53d713e99afcc0d6f37a7a5e5b87e406d40b65a5", "target_hash": "8c2ecbac7c9f3bb7a9e2c539e1db65dcf21cf923484ebc6d62814f6b0f975554", "exact_match": 1.0}
{"doc_id": 2539, "doc": {"question": "where does stevia in the raw come from", "answer": ["South America", "the plant species Stevia rebaudiana"]}, "target": "['South America', 'the plant species Stevia rebaudiana']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does stevia in the raw come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" stevia"]], "filtered_resps": ["stevia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4f2352c2cf56b9de3c8129aba84a9f02e59aa1d5fc3c4c034a7e55f2e8a2531e", "prompt_hash": "170c1c83bdb7d4983b1ee52d0c738518aab4367c619bef9337f9587c5b494469", "target_hash": "e5b24c844c62312d69ff45c22564efc79e75587f2885cb7bb24acd482d2ad89a", "exact_match": 0.0}
{"doc_id": 2540, "doc": {"question": "how many jimmy johns are there in the us", "answer": ["more than 2,500", "almost 3,000 stores", "2,630", "2,630 (December 2016)", "more than 2,500 locations"]}, "target": "['more than 2,500', 'almost 3,000 stores', '2,630', '2,630 (December 2016)', 'more than 2,500 locations']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many jimmy johns are there in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3000"]], "filtered_resps": ["3000"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3f2505844fcd41d747f9ba0dbb776862208c0a7b8f52981e61f465a25c2926ee", "prompt_hash": "8c44c654f80f3991bc3dc7036f8aea0a75a20719abbe2a64e1835dce6dee6fdd", "target_hash": "b86444296a82f566c1f17f6debd86ba248c6af8b55b0a62867e2cbd149cfe938", "exact_match": 0.0}
{"doc_id": 2541, "doc": {"question": "who played the music producer in pitch perfect 2", "answer": ["Keegan-Michael Key"]}, "target": "['Keegan-Michael Key']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the music producer in pitch perfect 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Keith Pennickin"]], "filtered_resps": ["Keith Pennickin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "473184bb9f0eb9bf9bc3c5043235d8d93b008f04cf132991d22521eb7aed9a55", "prompt_hash": "dd67c51b049339a343cc83a0b3264dbb615bfd5bb316e152bb39f6452c6d380e", "target_hash": "792dceb89d679d96d376bd6d314f6743a6f5aef0c044e89f214f66dddd6df61a", "exact_match": 0.0}
{"doc_id": 2542, "doc": {"question": "which layer of the meninges is connected to the brain", "answer": ["pia mater", "The pia mater"]}, "target": "['pia mater', 'The pia mater']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which layer of the meninges is connected to the brain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dura mater"]], "filtered_resps": ["dura mater"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "238728e7a02cdf0a3d286da93a6644507d93626bf83368aa0a765f1ed5cc2e1b", "prompt_hash": "5d877b1a3bbccf4df0593aa3c9126af74dde9b14872ae137ef67120cdda48679", "target_hash": "a780b58057fd7abef660ee6dfd55c6675f30a5e58e1ef3093370462531ad3a9e", "exact_match": 0.0}
{"doc_id": 2543, "doc": {"question": "who has the most ncaa basketball tournament appearances", "answer": ["Kentucky"]}, "target": "['Kentucky']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most ncaa basketball tournament appearances?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Duke"]], "filtered_resps": ["Duke"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f0e86f60acd193bad362ab80bd98fcd8207b70fade7d91da22b90d37eb94c8b2", "prompt_hash": "51fda622888950c169ff817f6349cdf1cb9093d4260c32fa7fc3aa828914abae", "target_hash": "d71f40f36bc302d6912d766767857e509b05abb0566cc8f61c96f0adc2e65df2", "exact_match": 0.0}
{"doc_id": 2544, "doc": {"question": "when will the flash season 4 episode 14 coming out", "answer": ["February 27, 2018"]}, "target": "['February\\xa027,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will the flash season 4 episode 14 coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 14"]], "filtered_resps": ["February 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d38482e76b025b956116090102ca0c8bc8732ede724f625bab3b38ad2749d4b", "prompt_hash": "bd9efab1f24789f4916f0e1dc955844e7e8267d99fbc06361c1e0d4de8542e0d", "target_hash": "9350009ca89e6117c085c4dd050881bfd8a20c57fde62702bb16d8dc57e3c3a5", "exact_match": 0.0}
{"doc_id": 2545, "doc": {"question": "mainland greece is a body of land with water on three sides called", "answer": ["peninsula", "peninsular"]}, "target": "['peninsula', 'peninsular']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: mainland greece is a body of land with water on three sides called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" peninsula"]], "filtered_resps": ["peninsula"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae6d7b88409a053b02a3afc6be095eb7af801beabd6a9d71bb3b51c629004891", "prompt_hash": "060c8fadb219054ef56b0b317c964a4402a7ebc44ff15a25c78ea8678f511eba", "target_hash": "715db6769f576275e5dc2699d4d66830610d1f4fa52c4a342de3967ad6ff9d40", "exact_match": 1.0}
{"doc_id": 2546, "doc": {"question": "what type of legal system is used in the uk", "answer": ["Scots law", "Northern Ireland law", "English law"]}, "target": "['Scots law', 'Northern Ireland law', 'English law']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of legal system is used in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" common law"]], "filtered_resps": ["common law"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "88bbc7f98ce5503d7656530505a55908cf7bf69e8a9ae88d0961451ec6c5ba47", "prompt_hash": "e990864d0970dd1ec6779678930d4e3752632b629db15decd049c86c3130a916", "target_hash": "a103b2719baa333c6715fe3cd91f155d5da936bd14997c5da8c895c0a35eebce", "exact_match": 0.0}
{"doc_id": 2547, "doc": {"question": "who painted the world famous painting the last supper", "answer": ["Leonardo da Vinci"]}, "target": "['Leonardo da Vinci']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who painted the world famous painting the last supper?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Leonardo da Vinci"]], "filtered_resps": ["Leonardo da Vinci"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c5d9e9dc087e8c445bf2f3c8a870630f6bef421514bd381030c8c31e1b0acdab", "prompt_hash": "d8de1f9c585ba4640b22b0fac12c51d1baf199df138d4bcc8f4ece5183cb03eb", "target_hash": "7de925a672a00c53e70550e09280fba6881e1ed562cfa1f05c9836c7ce1a56f8", "exact_match": 1.0}
{"doc_id": 2548, "doc": {"question": "what was the original purpose of the kremlin", "answer": ["the tsar's Moscow residence"]}, "target": "[\"the tsar's Moscow residence\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the original purpose of the kremlin?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fortress"]], "filtered_resps": ["fortress"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "39719fee1bae2bbf5ea34a4fc357628d7d181f20a0433656711cd959b7b1ed82", "prompt_hash": "72bb75ae97481830027a24943a6864e52a3083f91f475b0c211d03aa4e978ae4", "target_hash": "75fa87253ba2b8b63078182c55ae89cb25b4a13e0ce53cbda65cc2511367d930", "exact_match": 0.0}
{"doc_id": 2549, "doc": {"question": "what is the third season of total drama", "answer": ["World Tour", "Total Drama World Tour"]}, "target": "['World Tour', 'Total Drama World Tour']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the third season of total drama?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Total Drama Island"]], "filtered_resps": ["Total Drama Island"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3dec7a71398cc8beb88177dbded32a686b0341160c91ec117f3da8502b21bcef", "prompt_hash": "372155e453d4b0ca1bac4e7f98ba5aef33993504b78bb48a76317451ce83eaca", "target_hash": "897beff29473862c98552f08c2fb3db3e5a6de4da3c3c32c593ebe13e0dabc73", "exact_match": 0.0}
{"doc_id": 2550, "doc": {"question": "the probability of making a type i error when retaining ho at the .05 level of significance is", "answer": ["5%"]}, "target": "['5%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the probability of making a type i error when retaining ho at the .05 level of significance is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 0"]], "filtered_resps": ["0"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9d4ee276e7544a5e6f1f523b34c7eafdd0162e2f56bb2c75d1cb0ac42c7e2632", "prompt_hash": "9fcff0eca23e8de019b1a61fcdc23080cb444a5aaebe469c957db46d43f90448", "target_hash": "e3296fd10ba6a294022b260ac07d4aca527866b912825ae8bce58d610735066f", "exact_match": 0.0}
{"doc_id": 2551, "doc": {"question": "what nfl coach has the most wins ever", "answer": ["Shula, Don", "Don Shula"]}, "target": "['Shula, Don', 'Don Shula']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what nfl coach has the most wins ever?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Don Shula"]], "filtered_resps": ["Don Shula"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ba8f78d3912281fc4636bc28cae21386f04dbb7828cf4c4c30862138564135e2", "prompt_hash": "53ea259451b1691082d1d965abc5599c9462fb5e1dd3cae606c53c0eb19d2389", "target_hash": "8b26fb60241ac8e973bd022664d40afef8960d763a48dd86d69e1b748b08b8ba", "exact_match": 1.0}
{"doc_id": 2552, "doc": {"question": "the rule of the three rightly guided caliphs was called", "answer": ["Rashidun", "the Rashidun", "The Rashidun Caliphs"]}, "target": "['Rashidun', 'the Rashidun', 'The Rashidun Caliphs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the rule of the three rightly guided caliphs was called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rashidun"]], "filtered_resps": ["Rashidun"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "abb2e1c6b4b526f7a4aaf4265f8531337fee6317425e82882067d84e7ca5cb8f", "prompt_hash": "610b6934d6c6dcc07e9a1d799e7c644226e8dc63a0abfe4bb41fc62dc1123d41", "target_hash": "90031d3f748888b5bcaae3e77233a2907c6e2cea96f764793c8aa8fb6a141e0a", "exact_match": 1.0}
{"doc_id": 2553, "doc": {"question": "ray charles hit the road jack album name", "answer": ["Ray Charles Greatest Hits"]}, "target": "['Ray Charles Greatest Hits']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: ray charles hit the road jack album name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Modern Sounds in Country and Western Music"]], "filtered_resps": ["Modern Sounds in Country and Western Music"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "99cd4c6926c4eb0e8bb9ad984603312e685538a9c4053c7d1fe27da0f5a54a22", "prompt_hash": "19e95807a433828b8ae3878d64e220531410425360465f64897e94baf6e01cd9", "target_hash": "4c8d612f87f51641e5eae7a2a261d7dbf67a16c6754114c87bbe4099e2aafa43", "exact_match": 0.0}
{"doc_id": 2554, "doc": {"question": "who sings the wizards of waverly place theme song", "answer": ["Selena Gomez"]}, "target": "['Selena Gomez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the wizards of waverly place theme song?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Bieber"]], "filtered_resps": ["Justin Bieber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "81607a4fc964fa0ee1c0c11f4e4260cc90ab2eb4ab72d83fb6176937e5492ff3", "prompt_hash": "6a79fea36cc28aa7c3fcc29df148d25aaafb99439155d78ca8b712fa8ded5d82", "target_hash": "7f8a9f3d9bf93a44e539c6488e27396ba9666db586f3fd033360d8a104729a7b", "exact_match": 0.0}
{"doc_id": 2555, "doc": {"question": "how many times has the saints won the super bowl", "answer": ["1"]}, "target": "['1']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many times has the saints won the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" four"]], "filtered_resps": ["four"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9179d6ee6688aaf699480668ccbe6e60b003eb63dae417f2cb2fddc0631503a", "prompt_hash": "9092009a9415ac49fa1f0bf2cedcdf76558e281ee6f361628b946770d3d6d559", "target_hash": "31ddcada1d7c1fc1613304acb1fa93dd118bc955ad0081db064236cddab6fb64", "exact_match": 0.0}
{"doc_id": 2556, "doc": {"question": "where was the statue of liberty originally built", "answer": ["Paris", "France"]}, "target": "['Paris', 'France']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the statue of liberty originally built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" France"]], "filtered_resps": ["France"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d127153e9fcdd6ac8e6910196efcd11870064a9568006dd79d3045cad07e34fa", "prompt_hash": "87c91798ca4d769d6565db928934161a500b8a8853ff189d9b0d0f368e2004fe", "target_hash": "c2450bf39a12df049bd6494d6f2e376f10e6405d58236c69813e7d3f489ea7bb", "exact_match": 1.0}
{"doc_id": 2557, "doc": {"question": "the first political society started under british rule in india was", "answer": ["The British Indian Association"]}, "target": "['The British Indian Association']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the first political society started under british rule in india was?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bengal  A: Bengal"]], "filtered_resps": ["Bengal  A: Bengal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "862039dba2518dbe04622cdc29dc4462b20296e38db633788c43f6398e326914", "prompt_hash": "4064211b36ff7f0f843dc1e6b695d51ca67693d0dcb2a726e1f9038fcb1446f7", "target_hash": "afd87686742596a2ac45c12ff8f430e208d52ac47773f252a1a5c1c8903e707e", "exact_match": 0.0}
{"doc_id": 2558, "doc": {"question": "where does the bob and tom show broadcast from", "answer": ["WFBQ in Indianapolis, Indiana", "Indianapolis, Indiana"]}, "target": "['WFBQ in Indianapolis, Indiana', 'Indianapolis, Indiana']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the bob and tom show broadcast from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Los Angeles"]], "filtered_resps": ["Los Angeles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d4733b3295fcc1b0882af74c1175b86b4acfe179f20eee58c59bd5e45a4f3c0e", "prompt_hash": "6103d4534a951b33ffa824c9c25da4854de3bccbd8cd2983d29277ba14e89772", "target_hash": "b8b5de73097ce53d2e9347e11ab1d411c7eac40bf6a86f5d3784c0c110759777", "exact_match": 0.0}
{"doc_id": 2559, "doc": {"question": "what age do you have to be to buy cigarettes in california", "answer": ["21"]}, "target": "['21']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what age do you have to be to buy cigarettes in california?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 18"]], "filtered_resps": ["18"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "16830a01090a0fe222808ff2ca2c5d2b401e7353d93cb75da748897f8c01cdef", "prompt_hash": "43d5b2c690e67e2d2b0235a89df6ef640ba2c6bf5c202a13bf77cd33af4b0ade", "target_hash": "0659f33a8501f429c33b5ec3c0f0ea517dec38bf3619e68110e9a6513e98507e", "exact_match": 0.0}
{"doc_id": 2560, "doc": {"question": "who did the whistling in the muppet movie", "answer": ["Andrew Bird", "musician Andrew Bird"]}, "target": "['Andrew Bird', 'musician Andrew Bird']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the whistling in the muppet movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gonzo"]], "filtered_resps": ["Gonzo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbee8715430b95941241145a55ceb418cd00ef317c7adefcc63266ac29a0f6a8", "prompt_hash": "04dfc3441d31ba7ee994951cea904022eb2ff65b71a816da291eeffebc5cd2d2", "target_hash": "82d60acbd78489fd6e7f8ed8779c0ccf966eca09d639a362c173cad4839f8c7c", "exact_match": 0.0}
{"doc_id": 2561, "doc": {"question": "where would a subcutaneous injection be made in the skin", "answer": ["the subcutis"]}, "target": "['the subcutis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where would a subcutaneous injection be made in the skin?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" abdomen"]], "filtered_resps": ["abdomen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "979f813f72bffe5b46f94f9de6f2929a836c93e9e0b41222e1d5861991da54f3", "prompt_hash": "0812f06f8aa04e5dc98df993dbe57d8eb0deb05d1c15a1e0e2424eca7f43f934", "target_hash": "581fd91bfa2e7e6feae861a0cf1315bac04081e30e30c4e8e139f36ab738b054", "exact_match": 0.0}
{"doc_id": 2562, "doc": {"question": "who sang the song with reba does he love you", "answer": ["Linda Davis"]}, "target": "['Linda Davis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song with reba does he love you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Trisha Yearwood"]], "filtered_resps": ["Trisha Yearwood"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8494d79a6a65311cbb4b6169f7d71d9e7cd80ebc16c8b4c59e3af331eaff0388", "prompt_hash": "8808b07641f0227c510cc2f4303a687ddfcc7151167f88d29763147051196643", "target_hash": "6bece365698b064db20a42aa34862d9ba3b093ab7632d9ad230e62e3b68da1e1", "exact_match": 0.0}
{"doc_id": 2563, "doc": {"question": "who wrote you must have been a beautiful baby", "answer": ["Johnny Mercer", "lyrics by Johnny Mercer", "music by Harry Warren", "Harry Warren"]}, "target": "['Johnny Mercer', 'lyrics by Johnny Mercer', 'music by Harry Warren', 'Harry Warren']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote you must have been a beautiful baby?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Neil Young"]], "filtered_resps": ["Neil Young"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d5268ae281925ce0546d52ff62a1c1c3a5d4089af140af9cc5151a564f764041", "prompt_hash": "58f17bbface5fdc3f5db3f3a755fb7c98d149b2201681c6d83f35277e6ed89a8", "target_hash": "49f3dcf67fbdf48ec7c854d5951b165170fff1aeb28b3d4da2684a8641596d25", "exact_match": 0.0}
{"doc_id": 2564, "doc": {"question": "what was the immediate catalyst to the civil war", "answer": ["slavery"]}, "target": "['slavery']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the immediate catalyst to the civil war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the election of Abraham Lincoln"]], "filtered_resps": ["the election of Abraham Lincoln"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b6a100faebda62a320deb6b3579527f1e858f245f466eac2ac9f79d614a44f58", "prompt_hash": "c62b7383e93a85f8ed92775571596d0bdf4214e3c5ab32c49d803009eea360b3", "target_hash": "78885f118ceb49c8a1eea62d7f18a5b67981fd9bd20a0f0983da0f83b6559791", "exact_match": 0.0}
{"doc_id": 2565, "doc": {"question": "which player won the pga championship by the most shots", "answer": ["Rory McIlroy"]}, "target": "['Rory McIlroy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which player won the pga championship by the most shots?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tiger Woods"]], "filtered_resps": ["Tiger Woods"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7eb7db21ad8a57cb870b4c0ea86c14e2b093edd0426e78cb3889c88b9a1da886", "prompt_hash": "eac2498687d5da8dd89df7a251b030e027d7702ce852f82828bbc81b36397ac5", "target_hash": "b428eaf6e575d02e85ce0da7648bb946e4356937b0964ac5be52b226618615d3", "exact_match": 0.0}
{"doc_id": 2566, "doc": {"question": "when are you considered under the poverty line", "answer": ["US$11,770"]}, "target": "['US$11,770']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when are you considered under the poverty line?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $24"]], "filtered_resps": ["$24"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03ea7222ee62c230fd6a5036a0d51a1d536e9f1de8a443e952d0b56d2791f029", "prompt_hash": "359fc6c3682c10e9d1c9e735e4332e6c1cc5008a1bfe5214395cc0ba613b2723", "target_hash": "59990a22f5f3c3c7cc52e86e0146808dc8221c4443c293d9f2244ad0ffc962b8", "exact_match": 0.0}
{"doc_id": 2567, "doc": {"question": "who was the author of the art of war", "answer": ["Sun Tzu"]}, "target": "['Sun Tzu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the author of the art of war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sun Tzu"]], "filtered_resps": ["Sun Tzu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "70f54e5b3ded3fe46edd8f166084fe7bc98d8ddbeb54b9d0ad60e22f8ff14cba", "prompt_hash": "2ad36d7cb225c013f42c5c999ae25815b885755528aa2fde06bbb24c0e14db9f", "target_hash": "8e1cb69043bd02cf77128f179f2a6806e1b216d27ebc38f944bc0b39b4e58b28", "exact_match": 1.0}
{"doc_id": 2568, "doc": {"question": "what has been the origin of most classical dances of india", "answer": ["the Sanskrit text Natya Shastra", "Natya Shastra", "The Natya Shastra"]}, "target": "['the Sanskrit text Natya Shastra', 'Natya Shastra', 'The Natya Shastra']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what has been the origin of most classical dances of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hindu mythology"]], "filtered_resps": ["Hindu mythology"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ffd5c4441c38a10330d8c5a8667b0369dbaa73fd9d94686dc49d44da122c31fd", "prompt_hash": "2e968ffc23e5654b25fe7c5923c1290c8b8207920a8a334dcea629bc4fac0435", "target_hash": "46dc9c3dd769e3c14414d89879b05df941b0bcb802c396fd5d92fb832fa1425e", "exact_match": 0.0}
{"doc_id": 2569, "doc": {"question": "when was the last time kentucky won ncaa", "answer": ["2012"]}, "target": "['2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time kentucky won ncaa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1986"]], "filtered_resps": ["1986"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "17c533979165330f37b74249fc4c76cffc2cd6cc04a5cbde48b84e048b00e803", "prompt_hash": "9340c14f4d8bc36bed48c75e8bc63d2822bce74b9534d04e34d3d51a0c5180ee", "target_hash": "04b09e138d9d7538745af7c282a2efd5708aae0ad9c106ae836329c5f0a7ce4a", "exact_match": 0.0}
{"doc_id": 2570, "doc": {"question": "what olympic athlete has won the most medals", "answer": ["American swimmer Michael Phelps", "Marit Bjørgen", "Michael Phelps"]}, "target": "['American swimmer Michael Phelps', 'Marit Bjørgen', 'Michael Phelps']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what olympic athlete has won the most medals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Phelps"]], "filtered_resps": ["Michael Phelps"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e031bce54cd9cb43cb3022342df105089b3dccfbbd0b8711bdc73f36079f4c01", "prompt_hash": "cb50dc90e6b878051b45959b5ab1adc0f0a391c2b3ce700935f0b57f96022368", "target_hash": "3c4efa023e54839405e6f38230a711bb3eeb2de65ca9af0e263168d8f1fed8f9", "exact_match": 1.0}
{"doc_id": 2571, "doc": {"question": "when did the movie peter pan come out", "answer": ["2003", "25 December 2003"]}, "target": "['2003', '25\\xa0December\\xa02003']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the movie peter pan come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1953"]], "filtered_resps": ["1953"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5326cabcca5eaf121411554e08d66efbf23bd357ab122a28c4419501b5467b9d", "prompt_hash": "55587cd4eaf154170e2eebc0d21d0d851662c145352695e76d5dfe726a259830", "target_hash": "71091854de7a7ea8d540f59440e8957e0ad6ac8ffcfe7d34d8434a3731937cfb", "exact_match": 0.0}
{"doc_id": 2572, "doc": {"question": "what is the transfer speed for sata 1", "answer": ["1.5 Gbit/s"]}, "target": "['1.5\\xa0Gbit/s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the transfer speed for sata 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 150 MB/s"]], "filtered_resps": ["150 MB/s"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "16086c48e3e808ddc9ad69d17550b3e130835507ee410a8ed58a589213e751e5", "prompt_hash": "f2aae340ba03de51d39ed1ef4669be3bbb2707d9627dce3fe36af8f9c1fc8506", "target_hash": "ab7d5f513618ef73ad309ec7ec1789f41664de50c04b800d2cb8426921689952", "exact_match": 0.0}
{"doc_id": 2573, "doc": {"question": "where does new crust come from in sea floor spreading", "answer": ["basaltic magma", "volcanic activity"]}, "target": "['basaltic magma', 'volcanic activity']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does new crust come from in sea floor spreading?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mid-ocean ridge "]], "filtered_resps": ["mid-ocean ridge "], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2cdd41be263bc3f3ef4e094530cda7ea616789daf71515723509a65b9331e87", "prompt_hash": "cb390414662f01ae494ff0aa45f1b2963ce23875caf70990cb29c850baad1c5b", "target_hash": "f82461994efcc908fabff5b42000faf2d1c2eae2c9758980ff5e6aec5708fda0", "exact_match": 0.0}
{"doc_id": 2574, "doc": {"question": "who cracked the enigma code in world war 2", "answer": ["Turing"]}, "target": "['Turing']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who cracked the enigma code in world war 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alan Turing"]], "filtered_resps": ["Alan Turing"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18170074152d30adc301139aaba3a67bb9ba2c2ffd75930599207dbeb4e87b98", "prompt_hash": "2aa3b24aa9c943a29a220c8cf38625f2b10c059efc5a94037fb3c3904d394dd2", "target_hash": "7221edff3f0dea8c5a4d2062add1d9d7215f36931bfd19aaab34af00e01b3027", "exact_match": 0.0}
{"doc_id": 2575, "doc": {"question": "what were the two causes of the dust bowl", "answer": ["severe drought"]}, "target": "['severe drought']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what were the two causes of the dust bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" drought"]], "filtered_resps": ["drought"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c6acfc473f9670515ecb432acf09259b241ec08bd2d843782797232abfbcce3", "prompt_hash": "8f6f0fbc6254dfba122da7f5388e38492060aae6d61c3bdb216a776f65cf5cf1", "target_hash": "539fc32875fde4118a0c990efc41a61e29fd0c5d5b2dafd950969e5c64c0e049", "exact_match": 0.0}
{"doc_id": 2576, "doc": {"question": "what type of reaction occurs to form a dipeptide", "answer": ["peptide bond"]}, "target": "['peptide bond']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of reaction occurs to form a dipeptide?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" condensation"]], "filtered_resps": ["condensation"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a14560084d54d3da985f799a884de3c27e8dba7b4a0aa884c23fdc8179c79635", "prompt_hash": "7ad6d08fd4a47f9502dffbf7ac165a12fee89f85114a68cdd9c2f001fd1aa8a2", "target_hash": "836e10a28a614312e1653aa61627d75011a60fe2ba73a501f567adfcb02864ba", "exact_match": 0.0}
{"doc_id": 2577, "doc": {"question": "where does the un security council get troops for military actions", "answer": ["member states of the UN", "member states"]}, "target": "['member states of the UN', 'member states']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the un security council get troops for military actions?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" member states"]], "filtered_resps": ["member states"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e34493ecb57e26674b3ce2cd644f2d62d2e5601442e7cb70c854d7e661a49476", "prompt_hash": "2ddfde652df1d2b409e74d4fa9d95ef90bd1dc7887cfa9d5412cf9c893dc2c60", "target_hash": "52b69a477b091b6561e9cce8cddd7cab26c3bffdf3b68e472e3dcf50e32a35fc", "exact_match": 1.0}
{"doc_id": 2578, "doc": {"question": "who voices hiccup in how to train your dragon 2", "answer": ["Jay Baruchel"]}, "target": "['Jay Baruchel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who voices hiccup in how to train your dragon 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jay Baruchel"]], "filtered_resps": ["Jay Baruchel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a859fecab759e0971eec464ff112e4947e0fbe0443093ab398e02e0dc6735dad", "prompt_hash": "17c200b156af55169a5e59600d46e5d1f5179bb94b5532c444ba89b2106f8dee", "target_hash": "a9e33e3315bed87569ecf7cbf4f3f8ab3c0921f90ccfd35d663dcdb0b86fd326", "exact_match": 1.0}
{"doc_id": 2579, "doc": {"question": "who does luke skywalker fall in love with", "answer": ["Mara Jade"]}, "target": "['Mara Jade']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does luke skywalker fall in love with?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Leia"]], "filtered_resps": ["Leia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0a471dbf40316024708038d2375684d7d75a173b2593baacfb120dd85d38505c", "prompt_hash": "30ddb2249ca16bec622b0310342a4bf839bec6561849a5521473523ddc6e11a1", "target_hash": "5178efb9ed24ed6b1d0791a0f55592d693c48956ee088cefc9e27394291e4c24", "exact_match": 0.0}
{"doc_id": 2580, "doc": {"question": "if there is a line that lies parallel to the horizon it is said to be", "answer": ["A vanishing point"]}, "target": "['A vanishing point']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: if there is a line that lies parallel to the horizon it is said to be?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" horizontal"]], "filtered_resps": ["horizontal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8410c9beb209a4ee9e9fde02d053df6a6d231e283b6504113ff134e616565e49", "prompt_hash": "c358e6af3d3fb301a8971624ad8da9c105918ec855f4d99575aa95ee591de113", "target_hash": "6356c47e252fd2c791ac0a706074510d991d41422445975b0870a7168dca58f1", "exact_match": 0.0}
{"doc_id": 2581, "doc": {"question": "when was the last year the raiders won the superbowl", "answer": ["1983"]}, "target": "['1983']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last year the raiders won the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1983"]], "filtered_resps": ["1983"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df518db269a55248c4e46a2efc5d3589ea438400ee06b223301fbbb90e8d1735", "prompt_hash": "b1d0f520b62e4d4f68ae29939834adb50fa98c45d6b5c6f8e7b2d487628b6414", "target_hash": "0e762ec36fa410e205903e938523d534249d38027c286e0b2efd2df9399a61ca", "exact_match": 1.0}
{"doc_id": 2582, "doc": {"question": "how much energy does coal produce in the us", "answer": ["16.5 quadrillion BTUs", "30.4%"]}, "target": "['16.5 quadrillion BTUs', '30.4%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much energy does coal produce in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2011"]], "filtered_resps": ["2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5dcaeb34734e48db1c519c4e97761964f8d738bfdb077869bb23235b164e5546", "prompt_hash": "7091907e28129baff9961b80ed1060947169ac1f41b7b678bb0690edd306bcbd", "target_hash": "9ead3ec98180c1522e587790762b38a32176a952d6d876e0263482f7d9596c16", "exact_match": 0.0}
{"doc_id": 2583, "doc": {"question": "what is the definition of the word hosanna", "answer": ["save, rescue, savior", "rescue", "savior", "save"]}, "target": "['save, rescue, savior', 'rescue', 'savior', 'save']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the definition of the word hosanna?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" praise"]], "filtered_resps": ["praise"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d3f50c3a4178b174f54143305f4a7d96ea7f63b8a68b676fa0b782e61716c3f2", "prompt_hash": "e6cd63abe0f88281cc1f494e55385d29ffc3eb1e7298553ffccf446242083e2c", "target_hash": "4c149d5dcda3eb0f013b825aba206485892170f5e8026b92aa81b88fcb42af02", "exact_match": 0.0}
{"doc_id": 2584, "doc": {"question": "where does the sun hit the us first", "answer": ["the summit of Cadillac Mountain"]}, "target": "['the summit of Cadillac Mountain']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the sun hit the us first?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" East Coast"]], "filtered_resps": ["East Coast"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d55ce1b941e88625be4a224ac0830688ab24968635b2a3afc6fc5fea13175782", "prompt_hash": "1436ed2ac4bd08a4179a1ca6a03704b5c8d090e9e88786dea7e2c21f74e4c4b2", "target_hash": "167759e5ddbf3e39236ea50e177eca7864a0e647a331e1567a6c38700ed9a9a5", "exact_match": 0.0}
{"doc_id": 2585, "doc": {"question": "who are the only 2 families that have had 3 generations of oscar winners", "answer": ["The Hustons", "the Coppolas", "Hustons", "Farrow/Previn/Allens", "Coppolas"]}, "target": "['The Hustons', 'the Coppolas', 'Hustons', 'Farrow/Previn/Allens', 'Coppolas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the only 2 families that have had 3 generations of oscar winners?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the willises"]], "filtered_resps": ["the willises"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1e15eaaa59834c7e863f86675883f17d66d0012faab6494f67960a1d83396e93", "prompt_hash": "ae3dc13574392074410686dcefe1ef5b468c2d50637dcaf29ccd2c8823612f8e", "target_hash": "83cee8a00bbc6872ed1185c7e71c85c87336b6ca91a2269d5fbf75f006680aaf", "exact_match": 0.0}
{"doc_id": 2586, "doc": {"question": "what is the name of the chief justice of ghana", "answer": ["Sophia Akuffo"]}, "target": "['Sophia Akuffo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the chief justice of ghana?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kwasi Anin (NPP)"]], "filtered_resps": ["Kwasi Anin (NPP)"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3d45628115ae6169cd0c596bb25c48d7ee25f510d4d4bf18871733960ea2d09", "prompt_hash": "3ac8b4e503af758f4c842faaaad1f1bf0b4faf5b18495a729388ee3929660ed1", "target_hash": "faeecd300e2dc21702ca0bdd0fcd2d74e849219d8592500a27415c4a0aa897a3", "exact_match": 0.0}
{"doc_id": 2587, "doc": {"question": "who is considered the father of modern cosmology", "answer": ["Edwin Hubble"]}, "target": "['Edwin Hubble']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is considered the father of modern cosmology?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Georges Lemaitre"]], "filtered_resps": ["Georges Lemaitre"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5078045b32000cee7053680ae24e26c939536f348cff5739e64b84ed0bd89d74", "prompt_hash": "7a09c1acc8550a4067f3d255efe9463be405248c7b8bc2839c431511a01b0245", "target_hash": "846219833bc56aa01bd0e4f0e880aa92e65972054b52b9736b331b336304a7be", "exact_match": 0.0}
{"doc_id": 2588, "doc": {"question": "who discovered one of the first taxonomic classification schemes", "answer": ["Swedish botanist Carl Linnaeus"]}, "target": "['Swedish botanist Carl Linnaeus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who discovered one of the first taxonomic classification schemes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carl Linnaeus"]], "filtered_resps": ["Carl Linnaeus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4dd5bf964f98b175f40d37f88a4bc0dc5e8315ecfa3c18d8efb0d198f542f383", "prompt_hash": "bce6592f5cb7af3cde068b26b6864acbe2335bcdcb9bc0e24f248ce2614e6b4f", "target_hash": "36d04072289fa7cce492c3e01b1cc8faa766139f9c980c38e1ef6ecf70e0a973", "exact_match": 0.0}
{"doc_id": 2589, "doc": {"question": "who plays on the tv show the resident", "answer": ["Merrin Dungey", "Emily VanCamp", "Bruce Greenwood", "Melina Kanakaredes", "Manish Dayal", "Matt Czuchry", "Moran Atias", "Shaunette Renée Wilson"]}, "target": "['Merrin Dungey', 'Emily VanCamp', 'Bruce Greenwood', 'Melina Kanakaredes', 'Manish Dayal', 'Matt Czuchry', 'Moran Atias', 'Shaunette Renée Wilson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays on the tv show the resident?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matt Czuchry"]], "filtered_resps": ["Matt Czuchry"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c69875e5d508a1b1437d36503bb8880b77a453f97732006990e80658659548a", "prompt_hash": "736dc2553918b88868c0c8d62f104e24670a3523e60ac26dfddc65f4ea6dcd27", "target_hash": "3ead0d5b8dd279adf456d926e1d567e5c19d2e9a167eeea911352275f80c0376", "exact_match": 1.0}
{"doc_id": 2590, "doc": {"question": "when did are you smarter than a 5th grader first air", "answer": ["February 27, 2007"]}, "target": "['February 27, 2007']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did are you smarter than a 5th grader first air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2006"]], "filtered_resps": ["2006"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "87f25a13fa4862c3adbf635d8f15989257981c1882e7ad7333446fe261871a4b", "prompt_hash": "ef441f17de8dc668fdb9dd2b0408d602ac927b8eb124ba0cef8fd6e9b9836a82", "target_hash": "74e3dd3bf1803dd20c43e6a1db42711822f437cf01e777713f7b128443010aff", "exact_match": 0.0}
{"doc_id": 2591, "doc": {"question": "who played the detective in little boy blue", "answer": ["Stephen Graham"]}, "target": "['Stephen Graham']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the detective in little boy blue?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Caruso"]], "filtered_resps": ["David Caruso"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "532a8b480ce45106655715bd4065c7f97097f94abf0971f3b53f83d06399f86a", "prompt_hash": "a72cc26c968c398297a8772131442466d68c652f840598d7c876f26fabb7a406", "target_hash": "b42671ef894610163c17ad0fee69f12b07aa3a280c7e3a907d49a08889051fe6", "exact_match": 0.0}
{"doc_id": 2592, "doc": {"question": "what parts make up the peripheral nervous system", "answer": ["autonomic nervous system", "somatic nervous system"]}, "target": "['autonomic nervous system', 'somatic nervous system']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what parts make up the peripheral nervous system?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sensory neurons"]], "filtered_resps": ["sensory neurons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "38302e63288201e999c8a353322f098918c1bbf15ec78fc4ecf37f2f0ea1954e", "prompt_hash": "e5257ad809d4cb48a765753da1a90b8a6436af5e0ab128825f498b7ed55e2bd6", "target_hash": "b97a0310c3612ce8e118ead9b4d01ac82c7241456fa6e623a07dfcad4d74c63f", "exact_match": 0.0}
{"doc_id": 2593, "doc": {"question": "when is star vs the forces of evil coming back 2018", "answer": ["March 3, 2018"]}, "target": "['March\\xa03,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is star vs the forces of evil coming back 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 13"]], "filtered_resps": ["January 13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "98809bd58122e05492f9d0a277067198c406800b4b4bf50205fc7849f63a9bec", "prompt_hash": "2549af7b6b74f1e148eabcbc0e981b68f13a4074dc470fa3f559543a1ba6e9bc", "target_hash": "33a361f1c9d481846b95bcef3458c8818f4f31da3e8d9448c1e8a7a92fe0b706", "exact_match": 0.0}
{"doc_id": 2594, "doc": {"question": "who gets to race in the daytona clash", "answer": ["Daytona Pole Award winners", "former Clash race winners"]}, "target": "['Daytona Pole Award winners', 'former Clash race winners']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who gets to race in the daytona clash?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" NASCAR Cup Series"]], "filtered_resps": ["NASCAR Cup Series"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d25e7437b83339a5d030e54657392b5f2c844acaf091c128c0d005dab79b49d", "prompt_hash": "4c8c6146e48b3e8731e5fc8b3fd0691ec0cbfb4ceb11d578fdd800c07f776d7e", "target_hash": "0da774add56376a4876c1ce86dc7d55ac51e72edd741a4d5fcc092a48e318510", "exact_match": 0.0}
{"doc_id": 2595, "doc": {"question": "in which sea pearl is found in india", "answer": ["the Indian Ocean"]}, "target": "['the Indian Ocean']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in which sea pearl is found in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arabian Sea"]], "filtered_resps": ["Arabian Sea"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e5a140d8daf69866536dbc5c6b165e4a64dbfc735d69ea14c236df00749d6b85", "prompt_hash": "10622ea5ca55c05cc34a5cf9c08f1b3c28afaaa2c4a580b9033e0363b98f6049", "target_hash": "65334f7ef69cacdd275424ced9e3b206d9163d218c8c5991c78eaf6ac655081b", "exact_match": 0.0}
{"doc_id": 2596, "doc": {"question": "when did they stop making the nissan xterra", "answer": ["2015", "after the 2015 model year"]}, "target": "['2015', 'after the 2015 model year']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they stop making the nissan xterra?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2014"]], "filtered_resps": ["2014"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "16b107760848b9aa00401bbdc30e49cca4841055eb18c94e1ea60871e1a14657", "prompt_hash": "2c445331f87b95c1f827493918122fcee7de8ea33abed84a6f6d0fce38c2a489", "target_hash": "0915a31900c5bee100774d594fa85ba7b94e660e1aadfb50d33159725ec89006", "exact_match": 0.0}
{"doc_id": 2597, "doc": {"question": "what does ctrl+shift+t do", "answer": ["New tab", "Open a previously closed tab"]}, "target": "['New tab', 'Open a previously closed tab']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does ctrl+shift+t do?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Opens the last closed tab"]], "filtered_resps": ["Opens the last closed tab"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a5ac3b6a0cfeb833c0110bef7eaa9ae6c86ff35e3c0eeff77b799d0dde2067db", "prompt_hash": "d249a1a7b66fea4308c68f34d49ff2e8c354b900f22ca307c16bab145dadc8a1", "target_hash": "86dd2b308e4a3ac88f9a14e8e025b82c37f55502f12a48fe864a2b6fa05d42a8", "exact_match": 0.0}
{"doc_id": 2598, "doc": {"question": "who sang heard it thru the grapevine first", "answer": ["Gladys Knight & the Pips", "the Miracles", "The Miracles"]}, "target": "['Gladys Knight & the Pips', 'the Miracles', 'The Miracles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang heard it thru the grapevine first?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Marvin Gaye"]], "filtered_resps": ["Marvin Gaye"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "25f6b4e7ee16111af349e5259d665bc5c98445385b979d77106a6f1733662a8c", "prompt_hash": "3a2c9891e6349391100d414b623699ad9a4971c44751817f40fbd627c3084bb4", "target_hash": "78e39d2c0f281eb3adfc33d97486a80c5cf4d103f208af77c3a770160d5e4eb2", "exact_match": 0.0}
{"doc_id": 2599, "doc": {"question": "who sang theme song for dukes of hazard", "answer": ["Waylon Jennings"]}, "target": "['Waylon Jennings']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang theme song for dukes of hazard?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Waylon Jennings"]], "filtered_resps": ["Waylon Jennings"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ab26250c9c8dc28acfa68ac648b591d6c1a7eb58a676e95db52d4379898893b", "prompt_hash": "34f26f818d890997c497fa0d53ce8d104206cf00404a82636a29b8497a22b27a", "target_hash": "5d64af8bcf2b4f1be00a30c56884ac3f80d1d83854d1255d008b73b81325939e", "exact_match": 1.0}
{"doc_id": 2600, "doc": {"question": "where does the coral sea meet the pacific ocean", "answer": ["in the east", "South Pacific"]}, "target": "['in the east', 'South Pacific']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the coral sea meet the pacific ocean?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Coral Sea"]], "filtered_resps": ["Coral Sea"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4842be6137c6321f07003d36c09349c34240e4134d4b63a79e11746537b67a27", "prompt_hash": "83b77b151285ed854502665ce52dc122f9d23f3f4bc3761dadbaee99e57191f4", "target_hash": "64335c1e85ff7fc15302141085b702227c56fbe1b851e1e76e26a8dec00d9fe5", "exact_match": 0.0}
{"doc_id": 2601, "doc": {"question": "when does the new season of are you the one come on", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new season of are you the one come on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 11"]], "filtered_resps": ["January 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4cf28d1b5d8a96b8d44f4db323f93b820d7d0560f4e9428408d7097fa0822f3e", "prompt_hash": "76235a153d40e99c1f62da09cb6f62b577eaddc4ce868265d7ef9085eb2a940f", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 2602, "doc": {"question": "where did the beer hall putsch take place", "answer": ["Munich, Bavaria"]}, "target": "['Munich, Bavaria']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the beer hall putsch take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Munich"]], "filtered_resps": ["Munich"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "37cda5fd80b77d753ae46dbacd59660b6292f099f25e73d612dfea36a56e9ed2", "prompt_hash": "476d791c8632d6040f3f19051b6db5a24d1392d352e26a86e697c8b54ac98caf", "target_hash": "53a2d8a810fcd3a61edf2239d3eb8ec9cd388579b71cd17388a3bbb6d67e1f68", "exact_match": 0.0}
{"doc_id": 2603, "doc": {"question": "where did the northernmost land action of the american civil war occur", "answer": ["St. Albans, Vermont"]}, "target": "['St. Albans, Vermont']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the northernmost land action of the american civil war occur?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Petersburg"]], "filtered_resps": ["Petersburg"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2057ea75cb41c6e5a49216c4974979cefdc90269851d6978fc59e7c69d5722a3", "prompt_hash": "814099df213ae2c16a070d3bc0e5cdf5ffd0e86347355fd74ef8658d5d287b60", "target_hash": "6720e184409edc91c0af751e51108e918a8369deede76766001614287fb5abbb", "exact_match": 0.0}
{"doc_id": 2604, "doc": {"question": "what is the meaning of molech in the bible", "answer": ["king"]}, "target": "['king']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of molech in the bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Moloch"]], "filtered_resps": ["Moloch"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "17d53296e04b2612233ff3c6877be41a55ce78596c2f63764786aeea8b674fc5", "prompt_hash": "a6738beffc109014f028155663edd519c299c70bab085c366057e30d129faaf1", "target_hash": "7b80adbcfdafacc224f8c1b9495475745eda2a1500180963d48d7a3471b87f7a", "exact_match": 0.0}
{"doc_id": 2605, "doc": {"question": "what areas of africa were involved in colonization", "answer": ["90 percent of the continent"]}, "target": "['90 percent of the continent']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what areas of africa were involved in colonization?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Europe"]], "filtered_resps": ["Europe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "892cb8cb509e6a3126dcbc8096dd75fcf3e2f9966427dd32975c79784ae8bfe7", "prompt_hash": "a2983c9fa8b2a6cdf7fcc7c712c24ddd933bb9dceec922f4742b8b21624ddca6", "target_hash": "0d4f7b0ef079a75d80f67c9cb8c8add6514656a7bdc8a6df03b38c936eb57d73", "exact_match": 0.0}
{"doc_id": 2606, "doc": {"question": "who took the first steps on the moon in 1969", "answer": ["Neil Armstrong"]}, "target": "['Neil Armstrong']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who took the first steps on the moon in 1969?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Neil Armstrong"]], "filtered_resps": ["Neil Armstrong"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6e3bb4a4fe68f1d1e54da7828f0a71085bf90e768d9099af681adbe698186843", "prompt_hash": "866f2a9c999358c6105bbc0e635a53eb0a208aff6c6d0b498bb788aa91efe33b", "target_hash": "8882b1660b85e2123b8c76af565cfde37eaafb3a277979eddc6cdaa775dad0cb", "exact_match": 1.0}
{"doc_id": 2607, "doc": {"question": "who played john connor in the original terminator", "answer": ["Michael Edwards", "Edward Furlong"]}, "target": "['Michael Edwards', 'Edward Furlong']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played john connor in the original terminator?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Biehn"]], "filtered_resps": ["Michael Biehn"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ab750b49425c00182893817bb751b8084ff05175fb33563f0966974148d08f3", "prompt_hash": "203e9cb70d782015caeb5b715d640095bb4fdf5e828f7e13c5aa0d42a7bb62cd", "target_hash": "d9d035a19de0e3ac825e9b56b6306e17c469171ce9a5ba221b3629d26649d404", "exact_match": 0.0}
{"doc_id": 2608, "doc": {"question": "what type of planet is neptune known as", "answer": ["giant", "ice giants"]}, "target": "['giant', 'ice giants']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of planet is neptune known as?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" gas giant"]], "filtered_resps": ["gas giant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "13dfc33dc77fab7147ee0f4bba73793d6b624ac09dd10a0eddbcd54eee87e663", "prompt_hash": "3d23f68df3f0ba9c49c21db81dc1014551ee7af8430862f2e57f6ec875c9d971", "target_hash": "f6fc82f484938b30f503ce6be45f5fd14d096ba5c542cf8223aef59b5ffb74ad", "exact_match": 0.0}
{"doc_id": 2609, "doc": {"question": "who was the original host of what's my line", "answer": ["John Daly"]}, "target": "['John Daly']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the original host of what's my line?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Houseman"]], "filtered_resps": ["John Houseman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9917eeb11d6db9d560c5ef28b48b33e66249d4cfa64910c300ee0f92cb5ba132", "prompt_hash": "d756b52524c82d0ed41e283d4f8e4ab16d821565830039f32fbf1aa25301a339", "target_hash": "c1f0354d9e79f353e48ab93cb876adbcbde282e37f10a1cfde013336a3f37857", "exact_match": 0.0}
{"doc_id": 2610, "doc": {"question": "who was the drummer with the travelling wilburys", "answer": ["Jim Keltner"]}, "target": "['Jim Keltner']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the drummer with the travelling wilburys?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jeff Lynne"]], "filtered_resps": ["Jeff Lynne"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "091c4fa7a66b4a8055e4b1a826648a13fe7c609931b1581c3578eadcb606fedb", "prompt_hash": "93f623dd2fa16ee47ff8db37cc749531e00abedecfc30ac92e0eeb6a57e2bc8b", "target_hash": "6ce76934e5d4fac27cd958196edbf3a0ada33733f4b1caaf4fb41205c83f6dcf", "exact_match": 0.0}
{"doc_id": 2611, "doc": {"question": "what was the name of darth vader star destroyer", "answer": ["Devastator", "the Devastator"]}, "target": "['Devastator', 'the Devastator']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the name of darth vader star destroyer?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Executor"]], "filtered_resps": ["Executor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "565ded1e1af5e07409af1941bfc861be6a26d83f8a486fbc88c95284222f52fc", "prompt_hash": "6af5c033d2d8cf7ec5be7c386d88a55b87809f5159a03a0c5d3790e2135664ae", "target_hash": "b318634f276c276ab6843195f550ac9758f9beff39a1dce5d5e07a8f83802424", "exact_match": 0.0}
{"doc_id": 2612, "doc": {"question": "by 1914 which of the following african countries were still independent", "answer": ["Liberia", "Ethiopia"]}, "target": "['Liberia', 'Ethiopia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: by 1914 which of the following african countries were still independent?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ethiopia"]], "filtered_resps": ["Ethiopia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d1eceb55d3cd8bdbd5950b0b43887e87847df240bd1fd2f99868a3aa28d6d630", "prompt_hash": "d1701161c718fc8710e71c1eac62d50d64177285f771443c84651e414c14ae55", "target_hash": "a497b2fe993303486c8fa586018ebfb0a728134c6c41535f1f8169cf822fd345", "exact_match": 1.0}
{"doc_id": 2613, "doc": {"question": "what is best thing i never had song about", "answer": ["revenge and karma", "karma", "revenge"]}, "target": "['revenge and karma', 'karma', 'revenge']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is best thing i never had song about?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" a girlfriend"]], "filtered_resps": ["a girlfriend"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35f9e37422718821c1b7d0d917823174cedc88a913c7ab4ab236bf811c35f8c2", "prompt_hash": "acc51480388dacb9acc9ff1a38a0d3ea086d81749e1606aa5b8cf161217c20bd", "target_hash": "6c11ab447b3d5ab92f880a32e61481296ea5197c2a5ac278e93268c2ed639864", "exact_match": 0.0}
{"doc_id": 2614, "doc": {"question": "who did university of miami play last week", "answer": ["FIU"]}, "target": "['FIU']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did university of miami play last week?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Florida Atlantic"]], "filtered_resps": ["Florida Atlantic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b110858e255df2750cf97f505841c254483a7cb864ab444bc256ec7c9010354", "prompt_hash": "6d6078b57bd049c727dc0d77e32b380cddb069ba59870d5957f27c41f696e563", "target_hash": "55809955c5eb6e8b331e4014d558f33365b7ecde986128542392ccd21eea56fd", "exact_match": 0.0}
{"doc_id": 2615, "doc": {"question": "who appoints the members of the board of governors of the federal reserve", "answer": ["the President"]}, "target": "['the President']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who appoints the members of the board of governors of the federal reserve?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the president"]], "filtered_resps": ["the president"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d5415372e786f5da7e169ffd886ed5c65592b5f5faf9895b9618b465560e3ab", "prompt_hash": "76a72e160781ce5e06a0aac0190c70cd78d90ddf2f7a971150b18848fa0721be", "target_hash": "8411dd33a8292ec426cb7ef9a3a92c9a0ed1343f8827d3bfbdbb8e4db90bdf82", "exact_match": 1.0}
{"doc_id": 2616, "doc": {"question": "when was hitchhiker's guide to the galaxy made", "answer": ["2005", "1978"]}, "target": "['2005', '1978']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was hitchhiker's guide to the galaxy made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1981"]], "filtered_resps": ["1981"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a03e03367824f7fff95ae93c0b58f43276268a465ca439442b3eef449d8f4a3f", "prompt_hash": "351ed3b9402b43ea30fabcbe4b1d2141bdd9b9fa30de9c2394152b0c9d829972", "target_hash": "1dfe9c0bc73b99e294238235481d6fe56656b6fba4c6d4507b8d30630aa65a8e", "exact_match": 0.0}
{"doc_id": 2617, "doc": {"question": "how many episodes are there in modern family", "answer": ["201", "232"]}, "target": "['201', '232']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes are there in modern family?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 113"]], "filtered_resps": ["113"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dc6b346ff042e34b15f86b20c584f9ebefb98f90d72900e5b3e799233d535f1e", "prompt_hash": "9626fd25407f7db6723d19b29ccc62d0953a7a95c4f5098261ec425f45157591", "target_hash": "06c7d07972f5215e262765733d0b5b89ecfaa929165f39d24be2fd62411f84d4", "exact_match": 0.0}
{"doc_id": 2618, "doc": {"question": "where does the eurostar train arrives in london", "answer": ["St Pancras International"]}, "target": "['St Pancras International']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the eurostar train arrives in london?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" St Pancras International"]], "filtered_resps": ["St Pancras International"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a4811873b95ba82a2535112fdebc251b55c03e036e904b3f0c489c5712693bc4", "prompt_hash": "0a79cba6734e54f24ab3a28e3b63566450ae6bdfb9d4519ef87de736e695fc8d", "target_hash": "7b5cd11e02e10db20053a7747d9d041fbca9bbc08a0b8653627ee03dc3d870d2", "exact_match": 1.0}
{"doc_id": 2619, "doc": {"question": "where does the karate kid 2010 take place", "answer": ["China", "Beijing", "Beijing, China"]}, "target": "['China', 'Beijing', 'Beijing, China']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the karate kid 2010 take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Japan"]], "filtered_resps": ["Japan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "25a998891b8c7fb271509e08e920a9599b1dff7c019bdebc3a48eb159e978358", "prompt_hash": "3cebd58b38e2f39ef60a06b70b7d633ae2b6d66d25991ed35285a2c73f303f60", "target_hash": "ee369dbe3b4f2c70a653b351b102bd94307d0b7013ea587c3f5a3920f32fdedc", "exact_match": 0.0}
{"doc_id": 2620, "doc": {"question": "who played adaline in the age of adaline", "answer": ["Adaline Bowman", "Blake Lively"]}, "target": "['Adaline Bowman', 'Blake Lively']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played adaline in the age of adaline?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Reese Witherspoon"]], "filtered_resps": ["Reese Witherspoon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5f147251ba9be66880e581876fd5eaeaa33550d1a9140181058fc98d47a0d6d6", "prompt_hash": "8a04eb35a39ca0dffc69e84989501c147c5f670944832bae7bdb9566bde8373f", "target_hash": "5fa0552f7c02b9094ac8c9c3068a496fe1a28cbf6558d2e6add3ae446c724e7e", "exact_match": 0.0}
{"doc_id": 2621, "doc": {"question": "what is the general structure of an url", "answer": ["a protocol", "a hostname", "a file name"]}, "target": "['a protocol', 'a hostname', 'a file name']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the general structure of an url?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Uniform Resource Locator"]], "filtered_resps": ["Uniform Resource Locator"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1e6de0189a34696a2659569465b0126c33f24b0d1882987babb93fb761e0a4bc", "prompt_hash": "c45f25537787266fbdde1e249a4d1ecb50db56ada35ff92b524e3e56f9bf49d1", "target_hash": "2a45069b111aeabc41c66ac6ac9690bb2b147eb22e77a6bf7793af2a7fb4bb58", "exact_match": 0.0}
{"doc_id": 2622, "doc": {"question": "who said the poison is in the dose", "answer": ["credited to Paracelsus", "Paracelsus"]}, "target": "['credited to Paracelsus', 'Paracelsus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said the poison is in the dose?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paracelsus"]], "filtered_resps": ["Paracelsus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a8c5e48db0537d9b677afb77cb8ac500383a2ba2eeb98de79ce72f39761ba917", "prompt_hash": "3ea2d6ef6f8df41112d964a8ff61d057f635a3c4571b4581fddf60b9d4c2e156", "target_hash": "172453c01a6247a5f7774a61de3e33310085d5075fae3fbd1cdc714a3fee0145", "exact_match": 1.0}
{"doc_id": 2623, "doc": {"question": "who played young clark kent in 1978 superman", "answer": ["Jeff East"]}, "target": "['Jeff East']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played young clark kent in 1978 superman?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christopher Reeve"]], "filtered_resps": ["Christopher Reeve"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "300113ffb1007365363b65f0babd42f8c6dd586c3e1d7cfb24a597eecd2e3063", "prompt_hash": "a87a74826a423eb3555e3d811c18dd7417a65accd080ed370183056118ea59d3", "target_hash": "464380e59861b040d140bc6c17585a0192e34aee46a3efa3eb18073df298c858", "exact_match": 0.0}
{"doc_id": 2624, "doc": {"question": "when did equus first appear in fossil record", "answer": ["1820s", "~3.5 million years old", "about 3.5 mya"]}, "target": "['1820s', '~3.5 million years old', 'about 3.5 mya']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did equus first appear in fossil record?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 55 million years ago"]], "filtered_resps": ["55 million years ago"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4cba1606b03471af2ca0c6d59b0e2d76ced8265468568fabcab1d279e8522d0", "prompt_hash": "f34c7e3cb50486c38958924952d55f76a970dd323967237099feeddfc9b7f890", "target_hash": "6a34a03403a31fef92ba5eeb5818bfe3f4def1f33a884a87052c143cf05aaf41", "exact_match": 0.0}
{"doc_id": 2625, "doc": {"question": "who plays poppy in the beat goes on", "answer": ["Amanda Leighton"]}, "target": "['Amanda Leighton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays poppy in the beat goes on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kristen Wiig"]], "filtered_resps": ["Kristen Wiig"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c6c72cb1924ca112a2b7fd9e44ae055611a88dc4701ad685370b510119ecf5cf", "prompt_hash": "59b783fe5db9859fb24156412591cc97ce4eb2bbb61edaaeef9795c01b30abbb", "target_hash": "d797d1b26ef1deff4b872cc06c0515f372ad1eb6798966e7d588e9553900362a", "exact_match": 0.0}
{"doc_id": 2626, "doc": {"question": "when are the opening ceremonies for the 2018 olympic games", "answer": ["9 February 2018", "9 February 2018"]}, "target": "['9\\xa0February\\xa02018', '9 February 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when are the opening ceremonies for the 2018 olympic games?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 9"]], "filtered_resps": ["February 9"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "98078083442a0aa0ec621356a1222688b322456c5c55491edb6266c602d25ef0", "prompt_hash": "81f23228f4eb4fc482585a3ea73f32ce9138a413a18421a9a3a0e2a4c96edfeb", "target_hash": "53ca26dbacc3884d1d27a318578439d7dc3d5d7243ae83e17259e5c1bd249037", "exact_match": 0.0}
{"doc_id": 2627, "doc": {"question": "how long do nhl players stay on waivers", "answer": ["24 hours"]}, "target": "['24 hours']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long do nhl players stay on waivers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 24 hours"]], "filtered_resps": ["24 hours"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e0f5aecba1afbbe46b641f60fbfe64183ee6ecbe40832b396eabff27f5e8a072", "prompt_hash": "7faffe10b8afa8ffdc389a9631af2dd863b0e883cc93e7fea34159ec45dc2814", "target_hash": "6b551ada0ddc42505567e532cd4251dcc1c6f520ffecb75a14aa70adf2d2464f", "exact_match": 1.0}
{"doc_id": 2628, "doc": {"question": "the removal of temperature in fire fighting method is known as", "answer": ["cooling", "penciling"]}, "target": "['cooling', 'penciling']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the removal of temperature in fire fighting method is known as?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cooling"]], "filtered_resps": ["cooling"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "93beb1fc7f6696850f687b4fd612b6adad7144764d7f28a762b249576f1e48fc", "prompt_hash": "cadf7f4400e6a5eed43f33c43f5aa0b547c9fb9f192a8c0bbccd3a50d7f153d1", "target_hash": "f02e94d4887f9790a45edbd4bed57c31d0c9c2697a2301e2f268ba1bfb4e1830", "exact_match": 1.0}
{"doc_id": 2629, "doc": {"question": "lilo and stitch the series season 1 episode 1", "answer": ["Richter"]}, "target": "['Richter']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: lilo and stitch the series season 1 episode 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stitch's Big Lecheche Adventure"]], "filtered_resps": ["Stitch's Big Lecheche Adventure"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7a170b7eda43fec6796f8dacacf820ea0a484e3eb05701b7c1d71a560e8dd31", "prompt_hash": "30073dc21cc9c422814d40377d9aade9fdb4b813dc7ade558a049340e9179cf9", "target_hash": "a121b6407b32967ac65af65145cc9b3ff959d11ec83f542ce4cffd5c5e83646f", "exact_match": 0.0}
{"doc_id": 2630, "doc": {"question": "what are the main functions of the stem", "answer": ["Production of new living tissue", "Storage of nutrients"]}, "target": "['Production of new living tissue', 'Storage of nutrients']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the main functions of the stem?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" transporting water"]], "filtered_resps": ["transporting water"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d848e92a025e69c5e183e7e855729a518c66f98b1896127d386623819e3381ff", "prompt_hash": "dd37aefb50573d4495270292aff5bee672dc68df62766caaa5cf938e617f69fc", "target_hash": "2403172e82652f5f1e649ccdea27d5a46052a9a9a009e4de6e75f06b7de986e1", "exact_match": 0.0}
{"doc_id": 2631, "doc": {"question": "who was the baby in ghost busters 2", "answer": ["Hank J. Deutschendorf II", "William T. Deutschendorf"]}, "target": "['Hank J. Deutschendorf II', 'William T. Deutschendorf']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the baby in ghost busters 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bill Murray"]], "filtered_resps": ["Bill Murray"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b7cdf7b57329b8849c4b5241c029e047b507067a83db1457add2c489a41ef93", "prompt_hash": "67b01dd89ac96bf1ba5874e2091e84cbeba119f3e99ed0f381d7f90b0b0a7d7d", "target_hash": "67da8aa843a0b19d904222bf5a3a99ce99d76faaecb6e3b58f4b8c6cbf42ed04", "exact_match": 0.0}
{"doc_id": 2632, "doc": {"question": "who sang from russia with love james bond", "answer": ["Matt Monro"]}, "target": "['Matt Monro']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang from russia with love james bond?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shirley Bassey"]], "filtered_resps": ["Shirley Bassey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6daebffb89232a0c5f064a4d22f9858c2209b0a9f486679949e839d000387fda", "prompt_hash": "dcca63ec1cefab103e3464e0a0fc0646c6e1f16816c2305435ad897d8c9604bd", "target_hash": "feee7afa243ad8b164cad687920ea7635a1dad6b375888cc5315a66f7d7bb01c", "exact_match": 0.0}
{"doc_id": 2633, "doc": {"question": "what football player is called the honey badger", "answer": ["Tyrann Devine Mathieu"]}, "target": "['Tyrann Devine Mathieu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what football player is called the honey badger?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Wilson"]], "filtered_resps": ["David Wilson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cf9823565d9d55604bc6f059869565cd2b815ee20b04d88bc64235ac594fd21d", "prompt_hash": "d7b779619ff16769a7ee1b0a792720260d09cb91ea16671dd1dcbae158116f3b", "target_hash": "84c15372f93ed7d380da6c32353df8aa87a80078a2e678b129d93c1e296ffe62", "exact_match": 0.0}
{"doc_id": 2634, "doc": {"question": "which material is the heaviest in term of density", "answer": ["Osmium"]}, "target": "['Osmium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which material is the heaviest in term of density?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" platinum"]], "filtered_resps": ["platinum"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19a1f73e438f979cc978c9e848e88567679f56c23d8b0bcb486d4cf850dfaf62", "prompt_hash": "b40cd11472e46247ef782064a0d5059221fdb5436676dd0b086c3369c3a5cc56", "target_hash": "b70563b31658a208c97bebf505663beb9a834a870995a50f11a06cca2cd61116", "exact_match": 0.0}
{"doc_id": 2635, "doc": {"question": "who sang for lee bowman in my dream is yours", "answer": ["Hal Derwin"]}, "target": "['Hal Derwin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang for lee bowman in my dream is yours?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Bowie"]], "filtered_resps": ["David Bowie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c6f9850dd11a4617716b1e7c55b695f2d3c3d4656b0ed3487452ecac736f5efa", "prompt_hash": "f3a4703bb60e077bfb33c986c3f0ee9e7581f985303f8933c2ab23a19d681674", "target_hash": "ccc3be570ab6321cab89da3f9076e605a2f7cd7cadc5f566b1994b8a13135943", "exact_match": 0.0}
{"doc_id": 2636, "doc": {"question": "who has the most conference championships in college basketball", "answer": ["Kansas"]}, "target": "['Kansas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most conference championships in college basketball?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kansas"]], "filtered_resps": ["Kansas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "391c4e83850d9d5f3fac86a60643cc164b5909d6160825a4a03e95c4e805977f", "prompt_hash": "e1cdaeeb5dcf553b474bdfbe1126eedb56335ab475e312956424e040257bcada", "target_hash": "df498ba2e112ffd089570d3647f7eeedc4a8a6b1f1a7c9175bf0175ef6838f1e", "exact_match": 1.0}
{"doc_id": 2637, "doc": {"question": "who played sandy's jock boyfriend in grease", "answer": ["John Travolta", "Lorenzo Lamas"]}, "target": "['John Travolta', 'Lorenzo Lamas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played sandy's jock boyfriend in grease?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jeff Darnell"]], "filtered_resps": ["Jeff Darnell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "647fb05bc656443303c775456d7c2a00e3dd30fac377237e81e655d3742f576c", "prompt_hash": "91c309b6018de48c29b13c7002dc7c4a43ddd9d86540169136503a632e40d44f", "target_hash": "9a82ed65ecd07a74aaed1720cf82a6d6c9ab1c936f065fb0f8c81eaca233f653", "exact_match": 0.0}
{"doc_id": 2638, "doc": {"question": "who plays the doc in back to the future", "answer": ["Christopher Allen Lloyd"]}, "target": "['Christopher Allen Lloyd']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the doc in back to the future?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christopher Lloyd"]], "filtered_resps": ["Christopher Lloyd"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03d6648a9a2f986b6a4757db994463901589a2e5de582208ed9cd6717146a220", "prompt_hash": "6d7960e06a649a723f5ee9700e9f00d48721e432ea976de7c8ace56c44905e52", "target_hash": "0add858bfda613c199fa21b4b6dff26793953251dbe67e9574b0556d956a7860", "exact_match": 0.0}
{"doc_id": 2639, "doc": {"question": "when did come fly with me come out", "answer": ["25 December 2010"]}, "target": "['25 December 2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did come fly with me come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1958"]], "filtered_resps": ["1958"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "409660cde34bbb5cfebefd76bd5c3c0c7293b909b112e014b9735f39a477911b", "prompt_hash": "bd2ad8830b03d31a550b695d4e86565d79114009a280cb1b8ec7aa461611e148", "target_hash": "1ca1aeb6f8cc092e025f2585b1937edc9581b12b6ad8a9d1fc7d8ac6e0126bcb", "exact_match": 0.0}
{"doc_id": 2640, "doc": {"question": "what type of bridge is the charles bridge", "answer": ["a bow bridge", "bow bridge", "Stone"]}, "target": "['a bow bridge', 'bow bridge', 'Stone']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of bridge is the charles bridge?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" stone A: stone"]], "filtered_resps": ["stone A: stone"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c4f8295013db7c673cc30318c0a4dc4e4e480bcea4883f550f31c3cf4af5cb71", "prompt_hash": "9625b737ff17b23bc03daa041e949d1ccdc8b4ca7f1971196d602f175bb0c6f3", "target_hash": "866745c5b2df90ad6d43eb21da674ebacd49da7d67384b152a52589a9c0d4663", "exact_match": 0.0}
{"doc_id": 2641, "doc": {"question": "who is pictured on the 5000 dollar bill", "answer": ["James Madison"]}, "target": "['James Madison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is pictured on the 5000 dollar bill?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Salmon P"]], "filtered_resps": ["Salmon P"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "daef88cd201f0c0c030ccbe855b6596cfdea265699e840e48a0a415c5bc30129", "prompt_hash": "b8f2b066d8f0f4e08aae81117b0ab42042b89544d4e99032578a0a0d3a180463", "target_hash": "2898c48058f57a070ab737cdb85e06dd3f03a16417cb6469e6d737aba91838d2", "exact_match": 0.0}
{"doc_id": 2642, "doc": {"question": "where is the word leviathan mentioned in the bible", "answer": ["Book of Job", "Psalms", "Isaiah"]}, "target": "['Book of Job', 'Psalms', 'Isaiah']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the word leviathan mentioned in the bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Psalms"]], "filtered_resps": ["Psalms"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f6a1cd0ad065c41803fc2fb7d141ae79d2e3ea6ecebca3a5916b02c35e5b9457", "prompt_hash": "1161def86859f0f01e8aeae37513d3850d72abf460eea4626a2a6b83824eca0d", "target_hash": "72bbf13cabf92638632dfe8b59459562cd7c31fe458e027f29f19df529f0be2c", "exact_match": 1.0}
{"doc_id": 2643, "doc": {"question": "who won the american league east in 2017", "answer": ["The Yankees", "Houston Astros"]}, "target": "['The Yankees', 'Houston Astros']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the american league east in 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Houston Astros"]], "filtered_resps": ["Houston Astros"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aa2b5747e6e48951ccf66cf3a32ddeaa397962e9010b2b290838b7889e13f66d", "prompt_hash": "abed82f753c150a8e81eb39df667bce650b93ce66fdcdd5fdef667926556dc9d", "target_hash": "d00656055635badd60306245710e199a4194bc4e8ab965e284e643c723398143", "exact_match": 1.0}
{"doc_id": 2644, "doc": {"question": "a system of fortifications along france's border", "answer": ["Maginot Line", "The Maginot Line"]}, "target": "['Maginot Line', 'The Maginot Line']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a system of fortifications along france's border?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Maginot Line"]], "filtered_resps": ["the Maginot Line"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "be3149c7657f436f8226588147d467879647abb0feee5e0e22c35e7e6424c814", "prompt_hash": "6d3cbd02499b41df580a5e45f20dd344a497052dc92faeffaf46afa79cba3874", "target_hash": "3a94f4475301fd57d6a3c97e9192418ddc0ae88e8417d7fda01bafc1d3dd02f5", "exact_match": 1.0}
{"doc_id": 2645, "doc": {"question": "who was hope brady's mother on days of our lives", "answer": ["Ciara Brady", "Addie Horton"]}, "target": "['Ciara Brady', 'Addie Horton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was hope brady's mother on days of our lives?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Erica Kane"]], "filtered_resps": ["Erica Kane"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6fa003ebd8136a2f48aedb3e3d6c69f4f478a9e8b5efee4cfedc3167fb2e4df9", "prompt_hash": "fa47b321c5d0b4d23dff223adb348d5941cfd5fd47124d9392808df1f1ef6e9c", "target_hash": "747189dfceb26942015e67a64474e0f80ac3acc02d7d6081274f5dd80010f17f", "exact_match": 0.0}
{"doc_id": 2646, "doc": {"question": "how much does a new gulfstream g650 cost", "answer": ["US$66.61 million", "$66.5 million"]}, "target": "['US$66.61 million', '$66.5 million']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much does a new gulfstream g650 cost?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $60 million"]], "filtered_resps": ["$60 million"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "af1f3e187aae8984d0da2db081f39ec25e2a2941106ee5a1b7d4c32ebebecddd", "prompt_hash": "becb32fadf8ea230d5cbf4d9d07c97a69aed8f58e3a2a564f5e851b58468e9cc", "target_hash": "dc82b750fcc835d174bbd428e4e65d1bc9e226f2f80c5c84460bd0240079e144", "exact_match": 0.0}
{"doc_id": 2647, "doc": {"question": "which us ship was captured off the coast of massachusetts by the british", "answer": ["USS Chesapeake"]}, "target": "['USS Chesapeake']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which us ship was captured off the coast of massachusetts by the british?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" USS Chesapeake"]], "filtered_resps": ["USS Chesapeake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2353462da4459d5e571a6f805015ddbf935067befbee498892f39ac4899235a", "prompt_hash": "0185aa7b2e99ea7cd0411d9f4dbdcbe1b0bb9b3e9fde7a592dbf7f7f7f3294f4", "target_hash": "18c19add5c46732747a6a5897decf3afd23a72a2bedb02017d0868e856f1d343", "exact_match": 1.0}
{"doc_id": 2648, "doc": {"question": "when did congress passes legislation to fund the national cancer institute", "answer": ["August 5, 1937"]}, "target": "['August 5, 1937']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did congress passes legislation to fund the national cancer institute?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1937"]], "filtered_resps": ["1937"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "167c2947647fef3fabbf39c17c6df0856fe62213e999e9d5253a0a08a49466bb", "prompt_hash": "e88b4ed16e7f85087e0f5ad52a3a0ae505d610b9af0c4a328e928baf77445d21", "target_hash": "9c2e110e8bc92558c7c45d2eb2a9ded4abdcf686d53b0939b3c9635583c75927", "exact_match": 0.0}
{"doc_id": 2649, "doc": {"question": "where was the film coming home for christmas filmed", "answer": ["British Columbia"]}, "target": "['British Columbia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the film coming home for christmas filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Atlanta"]], "filtered_resps": ["Atlanta"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6a452fa2d23624597d3bc363a477f13367d8cd09d06c776008b7290ac24838df", "prompt_hash": "127adc612c3cecac648bc424951b1f74a69c573a0f65734a0c89281bfc27d0dc", "target_hash": "89c56dc3a52e8139a5328b0561ea1f03770ee45ab241946ab86472a6d7a41d58", "exact_match": 0.0}
{"doc_id": 2650, "doc": {"question": "who won the 7 man elimination chamber match", "answer": ["Roman Reigns"]}, "target": "['Roman Reigns']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the 7 man elimination chamber match?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Randy Orton"]], "filtered_resps": ["Randy Orton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "201a473221003e61a849fe42fb2e26721e88e7a95ef6cf155b9de914a80ab285", "prompt_hash": "66bf44ec34e05f94c34e2ccaccd2e68e66b9a67fa18f10161923d5476f9c4955", "target_hash": "d7b85a0e0820b987c60d7d13bcf56381e88e1f232f225266fb0128c7bdb10a30", "exact_match": 0.0}
{"doc_id": 2651, "doc": {"question": "who wins the final fight in real steel", "answer": ["Zeus"]}, "target": "['Zeus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wins the final fight in real steel?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Max A"]], "filtered_resps": ["Max A"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d441c81d8dd171a043e57a98d9d2865f86858188cd34564659f5cda626af9b8", "prompt_hash": "fd62c0f572aeb7187f6c08941c2679aed4f3603759e8394e39c12faefcef4997", "target_hash": "c657bf444d18710fe4e4265054eea581defefda8aa81c34ce69301853a4e325b", "exact_match": 0.0}
{"doc_id": 2652, "doc": {"question": "published a treatise on perspective della pitture (or on painting) in 1435", "answer": ["Leon Battista Alberti"]}, "target": "['Leon Battista Alberti']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: published a treatise on perspective della pitture (or on painting) in 1435?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Leon Battista Alberti"]], "filtered_resps": ["Leon Battista Alberti"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77cac48f1a21cce5fe20bd8e9ecd876fe27d10c2f26f17cb74867afd663b0310", "prompt_hash": "0fd20770cd8497f1b94c4e68f35c425ea069480c5724a2197a92702a6c5a01d2", "target_hash": "971c644bf2e91f4080628e77c49a0883c25c38e174845e942b8a82262d19dd7b", "exact_match": 1.0}
{"doc_id": 2653, "doc": {"question": "who is the first indian who got oscar for lifetime achievement", "answer": ["Mehboob Khan"]}, "target": "['Mehboob Khan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the first indian who got oscar for lifetime achievement?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Satyajit Ray"]], "filtered_resps": ["Satyajit Ray"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bc4aedf7e6c953289e16701351b149afb701f6759bdf01c73286553c4a08c794", "prompt_hash": "56e40312e845d9fc024e034488b875a9ca308a9c51e8976c8cfa4f920385ab99", "target_hash": "0594de34b7a3b9da889939bce9a6d872254d8dbc0579f8ff80b292ff8ba72b88", "exact_match": 0.0}
{"doc_id": 2654, "doc": {"question": "what network is showing the monday night football game", "answer": ["ESPN"]}, "target": "['ESPN']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what network is showing the monday night football game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ESPN"]], "filtered_resps": ["ESPN"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b537d4a22b0f1a85bad197d0dd0074aefe23fe31e0b6ad5b0201b8d8e5303a8b", "prompt_hash": "3d9f8e0d02323d2c0d69e76f5b5b2f6e1996fa64bd1ae7e2d60aa7527852c2e1", "target_hash": "b5deda8d8bfce767eccd073e32e3153f2418a9815d7f2eef1060153ad245c555", "exact_match": 1.0}
{"doc_id": 2655, "doc": {"question": "how did leo dalton die in silent witness", "answer": ["in an explosion"]}, "target": "['in an explosion']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how did leo dalton die in silent witness?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" gunshot"]], "filtered_resps": ["gunshot"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c5d450e0eae7d6ec08f40acaef6aea3f819865939fc50d2ef70d250a84be0c6d", "prompt_hash": "9e8a45aa27df25bd1e9365d150572230eca0d2904541218788551726fe12f0b1", "target_hash": "ef2bb0444c5be6a61bea6d85dda0e1a160d9aca9e0c21b064034b3a0e0ee18a1", "exact_match": 0.0}
{"doc_id": 2656, "doc": {"question": "who holds the record for most platinum albums", "answer": ["The Beatles", "Elvis Presley", "AC/DC"]}, "target": "['The Beatles', 'Elvis Presley', 'AC/DC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who holds the record for most platinum albums?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Jackson"]], "filtered_resps": ["Michael Jackson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03641b9222f0a3015f12d0d7f6fb9f4626053ca2a82416cd13017ee2da8093b7", "prompt_hash": "b4732063982e2f51917490ac0482f000e2473c58b8b3a623a675cce780fde513", "target_hash": "5972e228385bd389a434c93f9b49b4d1e9184b9280ff86bda2c85a16a6105664", "exact_match": 0.0}
{"doc_id": 2657, "doc": {"question": "which battle ended britain's support for the south", "answer": ["Battle of Antietam", "the Battle of Antietam"]}, "target": "['Battle of Antietam', 'the Battle of Antietam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which battle ended britain's support for the south?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Battle of Antietam"]], "filtered_resps": ["Battle of Antietam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "04c84df9e1214fdb6d0546262e9810216b49013b88287c56643e5261583ea450", "prompt_hash": "706a36209401e1d8485c625255d06a67b63023f01e70b27f6d370bfd2b1296d3", "target_hash": "6d4ab22e37df4cdfee1e2dfccb9c281a954a008945ce8bb8de1074e49642819b", "exact_match": 1.0}
{"doc_id": 2658, "doc": {"question": "when does panic at the disco album come out", "answer": ["December 15, 2017"]}, "target": "['December 15, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does panic at the disco album come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" May 14"]], "filtered_resps": ["May 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9e592c14e4d817373b7be2dc148596075ba128efc6c8ed4d4e1b48d42e2149d", "prompt_hash": "d12c8ef736e85017d070caf3477fb9fd0acc43882d8dd7c5a56f6eb7435f9bd2", "target_hash": "4b29c9766a995177228b7c837965c750acaf717ce04fd020f670bb94d2b9673e", "exact_match": 0.0}
{"doc_id": 2659, "doc": {"question": "when does the flash season 4 episode 3 air", "answer": ["October 24, 2017"]}, "target": "['October\\xa024,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the flash season 4 episode 3 air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 11"]], "filtered_resps": ["October 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3420e64c4c5a6a58cb58500b2255ac6c713550407f0bd7d239e334cb102f2144", "prompt_hash": "00e99d9d40f003e64bd2a0d7b76fa051f74627a1cbdb5a2d48780cce78e7297f", "target_hash": "da84d5ae8a66a1dc00c1844a9bd6a543daffc90fdfab3f8bd9a80dcfefd67775", "exact_match": 0.0}
{"doc_id": 2660, "doc": {"question": "who directed the iconic animated short educational film ek anek aur ekta", "answer": ["Vijaya Mulay"]}, "target": "['Vijaya Mulay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who directed the iconic animated short educational film ek anek aur ekta?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rajkumar Santoshi"]], "filtered_resps": ["Rajkumar Santoshi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4eb26bd230f7cb9ba2b99966648002c9722159bb13d4db7e7aa914ea9807354", "prompt_hash": "bf199c483b3f30a5a5bb7e3560b2e37d08961801b1fe42a021f0a2c46149ff37", "target_hash": "0ab49547a80060d6e55d7142ce633837b6c6dc10d86ebb1584e03f180049f912", "exact_match": 0.0}
{"doc_id": 2661, "doc": {"question": "tad the lost explorer and the secret of king midas english cast", "answer": ["Ariel Winter as Sara Lavrof", "Cheech Marin as Freddy", "Bruce Mackinnon as the mummy", "Lewis MacLeod", "Liza Ross as Grandma", "Fiona Glascott", "Adam Jones as Max Morden"]}, "target": "['Ariel Winter as Sara Lavrof', 'Cheech Marin as Freddy', 'Bruce Mackinnon as the mummy', 'Lewis MacLeod', 'Liza Ross as Grandma', 'Fiona Glascott', 'Adam Jones as Max Morden']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: tad the lost explorer and the secret of king midas english cast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Tennant"]], "filtered_resps": ["David Tennant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db32c0e7497741a04eb007af11c0306805597c646bb3e595fcca70c25608e697", "prompt_hash": "45d9988a7d3234f67487a9329fa14d32cf05c41f7ea431b6eabe0b58ba774e25", "target_hash": "972c5e75ba312329083e46db925755e0d5127fc3c19e53026e6e5b18e45de722", "exact_match": 0.0}
{"doc_id": 2662, "doc": {"question": "who is the actor that plays saul on grace and frankie", "answer": ["Sam Waterston"]}, "target": "['Sam Waterston']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the actor that plays saul on grace and frankie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael J"]], "filtered_resps": ["Michael J"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4c121f982a78eae1b5709604a8fd8a6966dc4c2b81192d38d2da4a71c68cc706", "prompt_hash": "ae67a1785a9476d4f1e47f8f6e628d7ef4431ffdfa24b9644932bd1891053ca3", "target_hash": "21f89dfcc6618c17e294d6ac02165acd92c753b3ad454d1c9f93bd83f29e95d7", "exact_match": 0.0}
{"doc_id": 2663, "doc": {"question": "who holds the most home runs in mlb history", "answer": ["Barry Bonds"]}, "target": "['Barry Bonds']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who holds the most home runs in mlb history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Barry Bonds"]], "filtered_resps": ["Barry Bonds"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a0034fae7da034b23173ca8d3650e0f1e3420030150869b69279b235fb3ceae", "prompt_hash": "1d0f15ebb8a0c418f755cd133891fc19e754ff443ef9059ab14550f0c9cad3aa", "target_hash": "dbea28ddb2a564f000cc38e7af82e403a0800d5a774e1c71bd4e550d38792e5b", "exact_match": 1.0}
{"doc_id": 2664, "doc": {"question": "who cooks for the president of the united states", "answer": ["Cristeta Comerford", "White House Executive Chef", "The White House Executive Chef"]}, "target": "['Cristeta Comerford', 'White House Executive Chef', 'The White House Executive Chef']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who cooks for the president of the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the White House chefs"]], "filtered_resps": ["the White House chefs"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e93bc6869ac3ec4c88a355f22b5230f48724da4eb68d52edf6de2fff665e2f7d", "prompt_hash": "1e8ccaf8ff61df6d1319c004c693976fc599ff5eb91f0963d80b515db2ab1570", "target_hash": "4e7e4ce0da34be6111fd979614371369153df407d6dd739a9e1e6230e8339e34", "exact_match": 0.0}
{"doc_id": 2665, "doc": {"question": "who hosted and won the inagural world cup", "answer": ["Uruguay"]}, "target": "['Uruguay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who hosted and won the inagural world cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Beckham"]], "filtered_resps": ["David Beckham"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "398027fc60e4fc5c343d2c23fe05d887367c1ec4ea5b09b5d97147ce51f8c769", "prompt_hash": "ce951fbe25f4382117925bfe526a887fb1a6182499bc8cc7767de62d1d8541e8", "target_hash": "41838295626455a4cd0a91780465d1f2ffd9a93c5a21b7382ee6a08c96eddc86", "exact_match": 0.0}
{"doc_id": 2666, "doc": {"question": "how many cracker barrels in the united states", "answer": ["639", "more than 600"]}, "target": "['639', 'more than 600']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many cracker barrels in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 675"]], "filtered_resps": ["675"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1fe109b99f3c00a917d772930b3af83bec58fc14412ee74003716fcd552e2b61", "prompt_hash": "10f3628f56e856857e8047e382969806c405faa6aa4827fc2123d0afb2805ce0", "target_hash": "11d458ad074c0e1bf7ae5ca8c5716abc0c426c05065ae9cb96fa63d10fb74d68", "exact_match": 0.0}
{"doc_id": 2667, "doc": {"question": "striking the funny bone is actually compression of the", "answer": ["The ulnar nerve", "ulnar nerve"]}, "target": "['The ulnar nerve', 'ulnar nerve']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: striking the funny bone is actually compression of the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ulnar nerve"]], "filtered_resps": ["ulnar nerve"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cdf3e8194d2db170e39bd4d7890431f9a1728d9e2371334fadb369601c119461", "prompt_hash": "38a24c4dea2b21739fd9989ec998a27fe519b5f5053d119dc0c4c25893417871", "target_hash": "7ec025cf1111992182bc722668cb9792d3fa0ea6729df15237f562280e51d7fa", "exact_match": 1.0}
{"doc_id": 2668, "doc": {"question": "who won the wwe championship elimination chamber 2018", "answer": ["Roman Reigns"]}, "target": "['Roman Reigns']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the wwe championship elimination chamber 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Braun Strowman"]], "filtered_resps": ["Braun Strowman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1368e08ec0d8556021558769ae6f797ac6d682163ffccd00bc92f61a23ef2214", "prompt_hash": "5f73ff2584a0a7bb17eccb82d25de52f2523b8bd4beeb92b578d1d468b7a54c2", "target_hash": "d7b85a0e0820b987c60d7d13bcf56381e88e1f232f225266fb0128c7bdb10a30", "exact_match": 0.0}
{"doc_id": 2669, "doc": {"question": "who won the most stanley cups in history", "answer": ["Montreal Canadiens"]}, "target": "['Montreal Canadiens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the most stanley cups in history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Montreal Canadiens"]], "filtered_resps": ["Montreal Canadiens"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "376893cf39e6c57c4d8891ccaa6797f7038175cb19edc6f4ac26d1cf968d0029", "prompt_hash": "63537c89f481b366a14cff87fd78f78ef229b7a26685b5098e2c702f8e57655c", "target_hash": "3675ceb75705ebf8f9dbd3eff5b537b40acd109b63727a9cfeb713ee7b0f992d", "exact_match": 1.0}
{"doc_id": 2670, "doc": {"question": "who starred in the movie the day the earth stood still", "answer": ["Jennifer Connelly", "Jaden Smith", "Kyle Chandler", "Keanu Reeves", "Jon Hamm", "Kathy Bates", "John Cleese"]}, "target": "['Jennifer Connelly', 'Jaden Smith', 'Kyle Chandler', 'Keanu Reeves', 'Jon Hamm', 'Kathy Bates', 'John Cleese']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the movie the day the earth stood still?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Rennie"]], "filtered_resps": ["Michael Rennie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c55e9b90e23fec51ed112c57a50923b728f2d20ec54cbf9e92301354bc0dcd98", "prompt_hash": "f6f3eda82ac8cd87a0aa2956fa5aff6bd15db04d8b3df9e1308d17848c624eca", "target_hash": "c55ff80ac8b74c8c086e4ae311a5953f24846d1d7da2781bf745c6d6cbc9af5d", "exact_match": 0.0}
{"doc_id": 2671, "doc": {"question": "who won the battle of britain in ww2", "answer": ["Britain", "the United Kingdom", "British"]}, "target": "['Britain', 'the United Kingdom', 'British']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the battle of britain in ww2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Luftwaffe"]], "filtered_resps": ["Luftwaffe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eeccb98f4ad9d43732b46d75d3e052472c76a42150063b23be3b55445883f761", "prompt_hash": "7c2d29aba3d12acc9f69e00dcebc13025b18fbb1099300958f3b425f74208a3b", "target_hash": "458763df8160bd0b67dd76370ee19b1904e7ee0722c7f497fea00baa180ffc40", "exact_match": 0.0}
{"doc_id": 2672, "doc": {"question": "when does shadowhunters season 2 episode 17 come out", "answer": ["February 13, 2017", "January 23, 2017", "July 24, 2017"]}, "target": "['February\\xa013,\\xa02017', 'January\\xa023,\\xa02017', 'July\\xa024,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does shadowhunters season 2 episode 17 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" March 10"]], "filtered_resps": ["March 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cff0ec4bc749f75ebd610653d1ca6cdeb275b0617ca76d261f590c27637f1661", "prompt_hash": "7375ff32f6aa88263643b7b55fc0c73c15429b9abed3510f9e85e9bfdf8f6a8a", "target_hash": "1cd45ddea882d09911ad483e2d776ebc7891630cee422e1c26cc290e13e6cdd5", "exact_match": 0.0}
{"doc_id": 2673, "doc": {"question": "who played kelly taylor's mom on 90210", "answer": ["Ann Gillespie"]}, "target": "['Ann Gillespie']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played kelly taylor's mom on 90210?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brooke Elliott"]], "filtered_resps": ["Brooke Elliott"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "842a867015172d95cfe9871bd3b6bddcf29240caf387644808ca94ceb81458d1", "prompt_hash": "3af6279909d59c2f579a350c8e83e30e8f57c3e9bf59a0a527015da452d28216", "target_hash": "88b600030d0de6e1aee6b8b04155646742a18d31db2d9e4585230d5d4470be72", "exact_match": 0.0}
{"doc_id": 2674, "doc": {"question": "the concept of micro financing was developed by who in 1976", "answer": ["Muhammad Yunus"]}, "target": "['Muhammad Yunus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the concept of micro financing was developed by who in 1976?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Muhammad Yunus"]], "filtered_resps": ["Muhammad Yunus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a20940b2797f83b66a26085c3866105716d2741ebdcc94d78db656d5e8e2d0c", "prompt_hash": "d169309ed3a676482b87fc5a2a6aea76293900054491c71d9f24da2ac63cb1a6", "target_hash": "6b199dbb41a3cb7d69415ed3050202fbddf9c6e65e4ff57769e35934aca4f7fe", "exact_match": 1.0}
{"doc_id": 2675, "doc": {"question": "who is the actor that plays ducky on ncis", "answer": ["David McCallum"]}, "target": "['David McCallum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the actor that plays ducky on ncis?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David McCallum"]], "filtered_resps": ["David McCallum"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "94c6862fd9ddda0a7f00edde6c1a35f02a655945c75b77429a70c798571f0eae", "prompt_hash": "eb044bce91d7be83f8a518f4a4203b447d27ca371a4aa0c53009a78b520b205f", "target_hash": "7d6e2ef3ca44dec119e41fba3ae9d8b762f96b3a382b6212c72a175bc37b7ad3", "exact_match": 1.0}
{"doc_id": 2676, "doc": {"question": "where was the first cell phone call made", "answer": ["April 3, 1973"]}, "target": "['April 3, 1973']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the first cell phone call made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York City"]], "filtered_resps": ["New York City"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "60ee6c81255ec8e4c4998cb3f9cc8a016de4f37c1ebbffa59689ee9e0620955b", "prompt_hash": "b2295e57b68d6d40fc4f6376b109c207c76ebf5f4e25c70222f80df43e29a870", "target_hash": "0665e6e2ab50ad678504b330b9b5fe9ededf8c7bea60a1a58467652a6d2e7267", "exact_match": 0.0}
{"doc_id": 2677, "doc": {"question": "what breed of cat has spots and stripes", "answer": ["tabby"]}, "target": "['tabby']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what breed of cat has spots and stripes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bengal"]], "filtered_resps": ["Bengal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b4b5a8a6980100ea66c5e69f50f2729876132e85d241d0a4bcafb9a31c54585", "prompt_hash": "515f19ba39f00d41a53ddee702554087081cd353bebe753672d4da464e14af37", "target_hash": "83854c66a817ee901d8b965a5da3fdd4b96346cd932e00e2c40d5e3268a20b7b", "exact_match": 0.0}
{"doc_id": 2678, "doc": {"question": "what are the monomer building blocks of dna and rna", "answer": ["Nucleotides"]}, "target": "['Nucleotides']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the monomer building blocks of dna and rna?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" nucleotides"]], "filtered_resps": ["nucleotides"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b20d3f9ed6c1757c4c565149582fc0ebae94a891146198d15557592c12ec7983", "prompt_hash": "18928b0afff965ffb7ee8c29edd34bba88bf7bb1ba2d881cc5f153e82991c961", "target_hash": "036f680ecca8e88dfecd99aa8bb33a27ed408b67ecc582ead105762ad0031c9a", "exact_match": 1.0}
{"doc_id": 2679, "doc": {"question": "what is the 3rd largest state in usa", "answer": ["Alaska", "California"]}, "target": "['Alaska', 'California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the 3rd largest state in usa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Texas"]], "filtered_resps": ["Texas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea4f7faa57f7cd423dcb1c57ebdeaa38aceb42d57b6fee79997b91a54b931420", "prompt_hash": "332f2a64d624995e00018ee096abf63914bed05cbbdf7c617d28e25dfc028738", "target_hash": "6a43e0d7f9d4f16725f8047835961a8ab542b5e7e003c87f6ab96726d77b05cc", "exact_match": 0.0}
{"doc_id": 2680, "doc": {"question": "which mode is used for short wave broadcast service", "answer": ["Olivia", "MFSK"]}, "target": "['Olivia', 'MFSK']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which mode is used for short wave broadcast service?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" HF"]], "filtered_resps": ["HF"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "341ab4ef38efbb3f3467ebc4c45cbc7da136ce53f96355550f95bb2400464836", "prompt_hash": "9daac32e43d72d0872ed2aa0905daaa79ef3bea5f6e08191a07300606f9ad589", "target_hash": "7f8f9f6c3b16427c0662b845cd87e0b816940572515f67f7e275a3ff82bc5a68", "exact_match": 0.0}
{"doc_id": 2681, "doc": {"question": "where does cerebrospinal fluid flow to when it exits the cerebral aqueduct", "answer": ["the fourth ventricle", "fourth ventricle"]}, "target": "['the fourth ventricle', 'fourth ventricle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does cerebrospinal fluid flow to when it exits the cerebral aqueduct?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fourth ventricle"]], "filtered_resps": ["fourth ventricle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "47851d7d6fa2c9a34ae88c4ce5d8a4cdfea587e787fccbd5596e9098503923f4", "prompt_hash": "7c023f0314cc668e1c50ef4af6cbaf9ecdea90d2ae79096b5da999bc1256f327", "target_hash": "92545ab2307062e2c91c503522dba2548c7cacae20638888ab37d6ad75840dab", "exact_match": 1.0}
{"doc_id": 2682, "doc": {"question": "how old is baby in dirty dancing movie", "answer": ["17-year-old", "17"]}, "target": "['17-year-old', '17']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how old is baby in dirty dancing movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" seventeen"]], "filtered_resps": ["seventeen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "80619821ee6e298fa0824352ef5c278b49458a2481070b84d6019197646ab339", "prompt_hash": "3e3b68700c6d73369a8e1739ac5ee02e24ff87574889f3cd1ee05e0ae0328ec2", "target_hash": "a5d463c3c9bb2616e178173991a40e994c095c1dd78f0a12185aec975d8fb7df", "exact_match": 0.0}
{"doc_id": 2683, "doc": {"question": "when did lord howe island became a world heritage site", "answer": ["in 1982", "1982"]}, "target": "['in 1982', '1982']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did lord howe island became a world heritage site?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2000"]], "filtered_resps": ["2000"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c8d87e00272f1d6722f55023c336e8c09bb9d88ebedb05c343bee2dfc5aff96", "prompt_hash": "cd5282e6d951879abd89c9d6fae5c1344f743c8d342828fb128085607745c61d", "target_hash": "45bae8af11c921b3e631b73e9e0fe3cc2b060704bb720a075d9ec0894ede5110", "exact_match": 0.0}
{"doc_id": 2684, "doc": {"question": "which foreign currency option is the​ right but not the​ obligation to buy foreign​ currency", "answer": ["foreign exchange option"]}, "target": "['foreign exchange option']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which foreign currency option is the​ right but not the​ obligation to buy foreign​ currency?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" put"]], "filtered_resps": ["put"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7f012e77058e3e2977a4841b6c6cb3b7256530ce373f4528a0864cfb7b4289b", "prompt_hash": "a24b0753e799278c42892af29e6f6d07ccfd22b1cd69cdd16f614bf2182db7f0", "target_hash": "4d325839542a18823dae26068364265d92d02dc4eb36d4a895085e1fac1accd5", "exact_match": 0.0}
{"doc_id": 2685, "doc": {"question": "how many episodes in great british bake off 2017", "answer": ["10"]}, "target": "['10']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes in great british bake off 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b1bdced10cca646a73869308d460bb35bd37196e7ff3dca55417802eb1ace743", "prompt_hash": "71666552156a2f8ae1f09ee36da1fde1ca4711a09ec80b494f2e0992b41bac49", "target_hash": "9503346d501aeaec3f521be685eb235cda81c7929ba2f1acdae1be91400b787a", "exact_match": 0.0}
{"doc_id": 2686, "doc": {"question": "who was the first baseball player to be featured on a postage stamp", "answer": ["Jackie Robinson"]}, "target": "['Jackie Robinson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first baseball player to be featured on a postage stamp?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Honus Wagner"]], "filtered_resps": ["Honus Wagner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad01462e2367e53934e613a3728fe81aa21f9ff3dc5e0f4c38c7465c40b342d8", "prompt_hash": "726dc58f1d377cbc23c69da1063b9a79736998d1164b9fb1e227dc4666358260", "target_hash": "c0280d632a50be8f6cc958174247307fabc7378bee0b499b74f5104f6c1a7397", "exact_match": 0.0}
{"doc_id": 2687, "doc": {"question": "what type of car is a g wagon", "answer": ["Mercedes-Benz"]}, "target": "['Mercedes-Benz']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of car is a g wagon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mercedes-Benz A-Class"]], "filtered_resps": ["Mercedes-Benz A-Class"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "53fa4b0a26529ff704a57bfe354c4192b5cd3fedf389544ae54be52e66de4ee5", "prompt_hash": "c28bf97a7b8942230237364e20798a77bc04c0940c1639b4f6c3c7be97276422", "target_hash": "89c1345af92833c02082ef8f68011a9ea3b0b2a8017f777854659ff906b02f1e", "exact_match": 0.0}
{"doc_id": 2688, "doc": {"question": "when is the last time congress passed a complete budget", "answer": ["FY2010", "March 23, 2013"]}, "target": "['FY2010', 'March 23, 2013']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the last time congress passed a complete budget?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "99a1ad04da4e175c432a166b9cb1ae090c45a2e38e6b29d6a24add8e24eb6aae", "prompt_hash": "10185a908c027278702ebd70c8cb03af3fe8ee3f6b3a3c0442d39bf8df56075d", "target_hash": "9471020c5eccefefab3178a46d555ab765cfcb22b88c3d41e238ca156a317cf0", "exact_match": 0.0}
{"doc_id": 2689, "doc": {"question": "when's the last time scotland won the calcutta cup", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when's the last time scotland won the calcutta cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012"]], "filtered_resps": ["2012"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "087213cc2068e73bf606a77e613fce8fb4131df170d88ce1f1337385465d26ac", "prompt_hash": "2e6899b7ead1116ba2e0d8b7ca33a8535f1a66a2df7b87136abd8a1a8e3c8c2c", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 2690, "doc": {"question": "when is dragon ball super episode 131 releasing", "answer": ["TBA", "March 25, 2018"]}, "target": "['TBA', 'March 25, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is dragon ball super episode 131 releasing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 14"]], "filtered_resps": ["January 14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5af8c086b9e7dae5a90294711036cd53aa9d64b92cbe64786e1e22e49168bb54", "prompt_hash": "db25529ca9bffb04f3652f98203d0502fae5f2b14dac0ea79d60ed4e7237b516", "target_hash": "1c4d7bd8b3382735bc144072a8d71709374f3d7f23de8f715f040e1f9ed9e86c", "exact_match": 0.0}
{"doc_id": 2691, "doc": {"question": "who sang i knew the bride when she used to rock and roll", "answer": ["Dave Edmunds"]}, "target": "['Dave Edmunds']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang i knew the bride when she used to rock and roll?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pretenders"]], "filtered_resps": ["The Pretenders"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1f3340d9d738cbf4fc6f268e7ea324e31d816205b5ec3c04a6ac5e1fd7db61b9", "prompt_hash": "80bfcdae3669e40854eec716a947fe0c3717c8dc8cf894d6bf8670ca09fc0c4b", "target_hash": "b86c67b8306efc01e869e0c060a116e155c4302975ae808e1108cc173ded2956", "exact_match": 0.0}
{"doc_id": 2692, "doc": {"question": "where did the term liberal arts come from", "answer": ["the Roman Empire"]}, "target": "['the Roman Empire']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the term liberal arts come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" medieval Europe"]], "filtered_resps": ["medieval Europe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3ca355c3edd13cc2f43df0da4e131a0bbb43f355e2914218d293502db8e687b7", "prompt_hash": "1b1de45df00338c9ed5bffe76f71bcc5666e5edc266c0c63380354451069fe2a", "target_hash": "1d47b058baced67af23c4f62bce4dde263f476e5574d6202087ba143f395b50a", "exact_match": 0.0}
{"doc_id": 2693, "doc": {"question": "who wrote when a man loves a woman", "answer": ["Calvin Lewis and Andrew Wright", "Calvin Lewis", "Percy Sledge", "Andrew Wright"]}, "target": "['Calvin Lewis and Andrew Wright', 'Calvin Lewis', 'Percy Sledge', 'Andrew Wright']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote when a man loves a woman?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nicholas Sparks"]], "filtered_resps": ["Nicholas Sparks"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f20d7d7ba65f99275de631c855065d98a5e4842758ae3d0db573a781c770364e", "prompt_hash": "cdc5e86d5b66983e46d390411dc391212e83bff605f0fae91fe91075c5dbca19", "target_hash": "41def723fd0c0710feb624ca6bbd707b1cae0a71e03aae14d2b24baca59f1c7a", "exact_match": 0.0}
{"doc_id": 2694, "doc": {"question": "the site of secretion of h+ and cl-", "answer": ["Parietal cells"]}, "target": "['Parietal cells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the site of secretion of h+ and cl-?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pancreas"]], "filtered_resps": ["pancreas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b5e38e7fb61746dba0754367d179ce9ea35264e442a1d45b849de2b1d15313ba", "prompt_hash": "ebb011ab320d4f552b2f4877fd6f279d5ecfff253c5e63a9d435eeb1c85be954", "target_hash": "ab7ffd415f338f5c34c0a24f17fbf6d024914a3771263864352cfa4a6f6e3867", "exact_match": 0.0}
{"doc_id": 2695, "doc": {"question": "who has the talismans in sailor moon s", "answer": ["Haruka and Michiru", "Haruka", "Michiru", "Sailor Pluto"]}, "target": "['Haruka and Michiru', 'Haruka', 'Michiru', 'Sailor Pluto']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the talismans in sailor moon s?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sailor Mercury"]], "filtered_resps": ["Sailor Mercury"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f2720f08386309469f32b5da99bdb3055679b7489f738a821c9212662b5f11e0", "prompt_hash": "7c52e0ef4b40fb874b1c1076859b2029530b9337e8c82d4d1205b17a4efd20f5", "target_hash": "3e0316262cac9c029380c570f0454f0e08ee46b4de1407454b55924c47460e76", "exact_match": 0.0}
{"doc_id": 2696, "doc": {"question": "tv show theme song would you like to swing on a star", "answer": ["Out of This World"]}, "target": "['Out of This World']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: tv show theme song would you like to swing on a star?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Andy Griffith Show"]], "filtered_resps": ["The Andy Griffith Show"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "19bd765b5003680fd70d9a7559c89e549b33294ec1146ec79076eebaabbaeedb", "prompt_hash": "27d9fd98c640d32a68ef88e0ae322792452a9660bc427249394cf8215f5f6e21", "target_hash": "beca4b6484582e479cac0a94726d1958f1ca3fd0623c5758fb6a4915ac6f917f", "exact_match": 0.0}
{"doc_id": 2697, "doc": {"question": "where are haploid cells found in the human body", "answer": ["sperm", "sperm or egg cells", "ova"]}, "target": "['sperm', 'sperm or egg cells', 'ova']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are haploid cells found in the human body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sperm"]], "filtered_resps": ["sperm"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "637488b7e73d264c486e605c44bf8415ef191109e745ce2193877b1cc3ae53d9", "prompt_hash": "65173161486510b3e7d6665442f79eece771c484ed775e0df456aef83b5c9839", "target_hash": "b93eebf7948fc1ddc672680261ff5b32f58ca37ea4964ff4e4bc6a58aad5780b", "exact_match": 1.0}
{"doc_id": 2698, "doc": {"question": "when did the three little pigs come out", "answer": ["first published in 1890", "1933", "c.1886", "1886"]}, "target": "['first published in 1890', '1933', 'c.1886', '1886']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the three little pigs come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1933"]], "filtered_resps": ["1933"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "05a829bd388597d5d37b3b52b5be781476363494675b4c241f510dcec9a695d1", "prompt_hash": "74a0b982556cbe714d23ab14ffe910ebd4bab067a744ac2e4f11b68215a1e9da", "target_hash": "16fd19627da8d1641222b04b497a77e92a8f5cb762b10beb289cd9509f48c6fc", "exact_match": 1.0}
{"doc_id": 2699, "doc": {"question": "where did the california gold rush take place", "answer": ["Northern California", "Sierra Nevada"]}, "target": "['Northern California', 'Sierra Nevada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the california gold rush take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" San Francisco"]], "filtered_resps": ["San Francisco"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0598473571b01df0527214e45091f7ab4d7a52593ba630b2ccab0032460579da", "prompt_hash": "d2d9366c5e32df69e1705bfa5cee83f506442f9d7091acb73858e305a45cd0e7", "target_hash": "485285f6507d622c3a62253130f3d9cd64f052fb7fb8cef47b74bbcf8664651e", "exact_match": 0.0}
{"doc_id": 2700, "doc": {"question": "how many strong verbs are there in german", "answer": ["more than 200", "more than 200 strong"]}, "target": "['more than 200', 'more than 200 strong']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many strong verbs are there in german?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1000"]], "filtered_resps": ["1000"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e051635c6fa0b5a2d65233e32836723f1839715ca968cbadd65236c70cdcdd08", "prompt_hash": "78e08920663ddf511af85471521bd9fd6783d6640e21b192186084678c4a2bbe", "target_hash": "488b6e0855446e2b868865ca68106d8d8b636d2b81101eb449e9a5d667b94d5a", "exact_match": 0.0}
{"doc_id": 2701, "doc": {"question": "who invented the submarine during the civil war", "answer": ["Horace Lawson Hunley"]}, "target": "['Horace Lawson Hunley']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who invented the submarine during the civil war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Bushnell"]], "filtered_resps": ["David Bushnell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3037e5399c2d45e3781598e0e8a107122051c244056536030d1317cfe6545ad", "prompt_hash": "972a04fcf2c1a83128bfe30ba31e53f12bfe3893d7d614c0e420cdfc86f4371f", "target_hash": "7b854960370425e0a58a519bef18d55e9b534d505fb8c939bf8a42a2d7d24d9b", "exact_match": 0.0}
{"doc_id": 2702, "doc": {"question": "who has won more grand slam titles in tennis", "answer": ["Roger Federer"]}, "target": "['Roger Federer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won more grand slam titles in tennis?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Roger Federer"]], "filtered_resps": ["Roger Federer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96e23384dd144050fd7bcd485a6f049f2a31cff1ccdc56a154b4c065cb3800a2", "prompt_hash": "287746e2e33cd8b2dafebdd45ee9d86e3b89023c6ce1f2388a796a599b42a6f9", "target_hash": "6858b4fe714464a9751819ee4b8ef3f3c03ff673126aaaf25b8a748dfc549c38", "exact_match": 1.0}
{"doc_id": 2703, "doc": {"question": "chandra and bhaga river meets at the place", "answer": ["Tandi, in Lahaul", "at Tandi, in Lahaul"]}, "target": "['Tandi, in Lahaul', 'at Tandi, in Lahaul']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: chandra and bhaga river meets at the place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Prayagraj"]], "filtered_resps": ["Prayagraj"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ecf52fd4ade043f21469dca4ec1c29c5768d5735acf6a4438bd0f5bd47c8829c", "prompt_hash": "aa7c8aed42056c98bd6e294862f28b9edd58ffc8a719de11cfd423acfef578fd", "target_hash": "8eafb0f4d12239e04d414a7f95f5d3f2810176e8daa6619e1c6582be34f3be48", "exact_match": 0.0}
{"doc_id": 2704, "doc": {"question": "the recipient of first jnanpith award was an author which language", "answer": ["Malayalam"]}, "target": "['Malayalam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the recipient of first jnanpith award was an author which language?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bengali"]], "filtered_resps": ["Bengali"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a1acf174d84b053df662250eb8dd5d441665e32ef52d328248f28ea3203c4040", "prompt_hash": "e38872138a0f6b138fd1b0caf3b1854e943af68689f049cc11cecc0b16013e38", "target_hash": "38226e0b39a7a324771d83a4873bca5dfbfbe7469264cb5e5e9c48259f142c31", "exact_match": 0.0}
{"doc_id": 2705, "doc": {"question": "who is the cast of despicable me 3", "answer": ["Steve Carell", "Nev Scharrel", "Miranda Cosgrove", "Pierre Coffin", "Kristen Wiig", "Trey Parker", "Andy Nyman", "Julie Andrews", "Steve Coogan", "Jenny Slate", "John Cygan", "Dana Gaier", "Adrian Ciscato"]}, "target": "['Steve Carell', 'Nev Scharrel', 'Miranda Cosgrove', 'Pierre Coffin', 'Kristen Wiig', 'Trey Parker', 'Andy Nyman', 'Julie Andrews', 'Steve Coogan', 'Jenny Slate', 'John Cygan', 'Dana Gaier', 'Adrian Ciscato']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the cast of despicable me 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Steve Carell"]], "filtered_resps": ["Steve Carell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea44a6bffc8c4f3ad736a265dab2d28e202b047fe51bb4725e35c4eb7d33bfeb", "prompt_hash": "7efb19f74a161bd7909b308959b3dca83cc0fd1953cfa76232e44f6c1afa6c99", "target_hash": "e9cf173593be396776f8ab5a2786515560f319460472ce6162a5c6fb3432209c", "exact_match": 1.0}
{"doc_id": 2706, "doc": {"question": "what is the tigers name in life of pi", "answer": ["Richard Parker"]}, "target": "['Richard Parker']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the tigers name in life of pi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Richard Parker"]], "filtered_resps": ["Richard Parker"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "28fa50de313f294933508ec0cfad67d78512c4f696b89b9411e810812fafbbba", "prompt_hash": "63d7e90a92b230f6e827a99714f0f1e0f36d9cf8b140dccdbce2b62edb75013b", "target_hash": "1f91b6edc7f65fb799be8afcba9435994191b3153f0b4a9ac92458d17854df98", "exact_match": 1.0}
{"doc_id": 2707, "doc": {"question": "who is the presiding officer of legislative council in india", "answer": ["Chairman"]}, "target": "['Chairman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the presiding officer of legislative council in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Speaker"]], "filtered_resps": ["Speaker"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c66be9f4cc657528c7fd540b9a8e81f05ac1d2336a398d5aa225a78049b3df38", "prompt_hash": "a71cf0a4699c1d2d077e16a65d7a40ff951e3113f43cd7b1cad037b46af23561", "target_hash": "4b32281a8ac0f177f36eed7ba4fec20d075d5031b7c6e82abc8aebe1f7891a69", "exact_match": 0.0}
{"doc_id": 2708, "doc": {"question": "who will get ticket to finale in rising star 2", "answer": ["Hemant Brijwasi"]}, "target": "['Hemant Brijwasi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who will get ticket to finale in rising star 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ritika"]], "filtered_resps": ["Ritika"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2277873b0bbac667a275b46ae8ff8b3a2b7ef73d99b7c7d1f8894fa2f8b618ad", "prompt_hash": "ce158eb1dedba2a088187709265397492d2572403d02248481b6072e8035795a", "target_hash": "01919d157429a880d0b858457863e1a3c0032268e45e53e4a24c4aa636a8a02f", "exact_match": 0.0}
{"doc_id": 2709, "doc": {"question": "referring to the organs of reproduction is called", "answer": ["Reproductive system"]}, "target": "['Reproductive system']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: referring to the organs of reproduction is called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" gonads"]], "filtered_resps": ["gonads"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a34e644b0d5de7332a9c57d6b6c8150f2cbbb406449c85eb8fd8918a05b5eca", "prompt_hash": "314116c86548dbec16937b204f88d1c5567172de0c4f618cccbba31ebae1d14a", "target_hash": "e61fe174991aaf43e756c0a8838aa353d19e16163cb210513f1230df4a8b2368", "exact_match": 0.0}
{"doc_id": 2710, "doc": {"question": "in which country were the two battles of el alamein fought", "answer": ["Egypt"]}, "target": "['Egypt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in which country were the two battles of el alamein fought?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Egypt"]], "filtered_resps": ["Egypt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ee2e77b8f4bf81b199efa5aabdc266609039fc32d8788e112b6340ac850d4ec5", "prompt_hash": "2d864025e247b52ec01b5cabf03d687042f9d33988b3c5feaf728205f59df9fd", "target_hash": "3805f88755fac2aedf4b6e3e90c22ac89dd56ddc838f719efbeb1dbbe090f2d4", "exact_match": 1.0}
{"doc_id": 2711, "doc": {"question": "who invented the printing press and in what year", "answer": ["the German Johannes Gutenberg", "Johannes Gutenberg around 1440", "1440", "Johannes Gutenberg", "around 1440"]}, "target": "['the German Johannes Gutenberg', 'Johannes Gutenberg around 1440', '1440', 'Johannes Gutenberg', 'around 1440']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who invented the printing press and in what year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1440"]], "filtered_resps": ["1440"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a901cb9f74829ca0a18a5d2b92deca0eb89f87ab39caa875c531741cb07f40c2", "prompt_hash": "7d636cdc2c4f6b01d1cdb196f402d0cbf780de5ba7a89628a6dd5b12237912c1", "target_hash": "b681e4aff373e38e53581264805b80dca1fa58a53253206546d8b451a74557b0", "exact_match": 1.0}
{"doc_id": 2712, "doc": {"question": "who wrote the song always be humble and kind", "answer": ["Lori McKenna"]}, "target": "['Lori McKenna']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song always be humble and kind?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Taylor Swift"]], "filtered_resps": ["Taylor Swift"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f7af763b7cc55063fa7cc26377e6d7c70714325468c63696f48ddfafc40fe28e", "prompt_hash": "0301e438e5696ea018780b011f88acceb1e4b18760aad38872461ffc031151e1", "target_hash": "34d532a679c8b80c02bca6c8bc3abd1fcdd17d38a55130cb2aea34147adbb8ce", "exact_match": 0.0}
{"doc_id": 2713, "doc": {"question": "a drug that might be used in surgery for its amnesic properties is", "answer": ["benzodiazepines", "Benzodiazepines"]}, "target": "['benzodiazepines', 'Benzodiazepines']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a drug that might be used in surgery for its amnesic properties is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ketamine"]], "filtered_resps": ["Ketamine"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "371a6a1329140f19995ff7a9812248fc77e605a591c7786c9fbc125ea02040a9", "prompt_hash": "38d67e9b6916acd887eaf1d187ac4c977bd2423789045f8f53235626fea853f1", "target_hash": "6fe683b0cce1fcf50642ddf3978e22b626ba35c938f8618dc9db1294464ad7f8", "exact_match": 0.0}
{"doc_id": 2714, "doc": {"question": "when was the last time the womens hockey team won gold", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the womens hockey team won gold?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fead91502f4886e54dee9bc813e6025eeaa0b75a09bffa6e113513f92c4a318c", "prompt_hash": "1ceea64801773e20814ef41b5eec899853f278a6bd09ddd355c4b2324e0c6596", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 2715, "doc": {"question": "region of antarctica where march of the penguins filmed", "answer": ["Dumont d'Urville in Adélie Land", "Dumont d'Urville Station", "Adélie Land"]}, "target": "[\"Dumont d'Urville in Adélie Land\", \"Dumont d'Urville Station\", 'Adélie Land']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: region of antarctica where march of the penguins filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" South Georgia"]], "filtered_resps": ["South Georgia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c086d95d33f8307c81ec49b4233bc30426642da26d39895d4e4fa933d6a7f367", "prompt_hash": "84e9ef6ae84b53c245cfdc85e7a2f89b5022f442c7e7ccda8395ad7112a7863c", "target_hash": "e9bdd61258c29ba9dc840113f701a446d8e475b21234fc00e1310bb77fc904b8", "exact_match": 0.0}
{"doc_id": 2716, "doc": {"question": "which film won the oscar for best animated feature in 2007", "answer": ["Ratatouille"]}, "target": "['Ratatouille']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which film won the oscar for best animated feature in 2007?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ratatouille"]], "filtered_resps": ["Ratatouille"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9482205ff1229557eb44f268aa9d4fac3fcdac202ac6f0ae1c21df9b7a267bd1", "prompt_hash": "c0d35791192c33f87d88b20a11121c8cda8d46017f90681c7f972a962db9245f", "target_hash": "75e4409052f09d334684ebea2a3ceff86fd4e538f108170a648603f4e396d766", "exact_match": 1.0}
{"doc_id": 2717, "doc": {"question": "which episode does gideon die in criminal minds", "answer": ["\"Nelson's Sparrow\"", "Nelson's Sparrow"]}, "target": "['\"Nelson\\'s Sparrow\"', \"Nelson's Sparrow\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which episode does gideon die in criminal minds?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 110"]], "filtered_resps": ["110"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e4f8542b41e8030752c4bfb97979fc83132d1b8ea48ff9a9c6a02caa3616ab03", "prompt_hash": "23c42ceb10387dc49474bb50016977bd97637fda0084f911323e0627a456f19d", "target_hash": "87c85d266134054f36b039f53aef29ddd584de9facea2e980178a531d2834e65", "exact_match": 0.0}
{"doc_id": 2718, "doc": {"question": "what is the first line of an http request to the server", "answer": ["A request line", "the status line"]}, "target": "['A request line', 'the status line']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the first line of an http request to the server?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" GET"]], "filtered_resps": ["GET"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5c220c6efe01350002e269bb215cdaffb35983ab0b6aaeff2581b8edfc7163a6", "prompt_hash": "326cba96b4f6d670a5120390d1e6bd6c8987ce0e31aa6b370680e53f7723b842", "target_hash": "1113743876714a50d152cb98a8f8709fa736058ac511ba687fce88896488428b", "exact_match": 0.0}
{"doc_id": 2719, "doc": {"question": "when are the next commonwealth games going to be held", "answer": ["2022"]}, "target": "['2022']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when are the next commonwealth games going to be held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2018"]], "filtered_resps": ["2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9eada0ad71b7500c19513d2c24cce0d2e98e021c270e84dc4dabd4f9a5e19551", "prompt_hash": "10d2f55c8eec02b4cc6877fcaaa7a21f4880fa2a77830a31f2364e93851374dd", "target_hash": "a8c384900355da6977fb40ce84370b10358f57cb8221c2b1472089a261272cfd", "exact_match": 0.0}
{"doc_id": 2720, "doc": {"question": "what is the multiplication sign on the computer", "answer": ["a rotationally symmetric saltire", "the symbol ×", "*"]}, "target": "['a rotationally symmetric saltire', 'the symbol ×', '*']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the multiplication sign on the computer?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" asterisk"]], "filtered_resps": ["asterisk"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "728a7283a3cb7afc6a0bf6f224fdd531ed9c9857eddb34c9f039b647506db3c7", "prompt_hash": "815b306c2b23bdc0118f1cb660a96f1d72b07d8fea8b24426162b86eb98833c9", "target_hash": "47680a7f576a7146f439df74d16de8ba69ec14642bb87b213309c700ad6a2f7e", "exact_match": 0.0}
{"doc_id": 2721, "doc": {"question": "which site of an enzyme is called allosteric site", "answer": ["regulatory site"]}, "target": "['regulatory site']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which site of an enzyme is called allosteric site?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" active site"]], "filtered_resps": ["active site"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7207f15d54862f0b352e8d02faab04562d1f70e44444d52b65a8596b694c0ae3", "prompt_hash": "98112049fb91f755435ab9dd1f43b030026945c09714298c838fbcda23c5dd55", "target_hash": "d8194606803a3f846a85988dc55e28d7e3aec50879f5516e832899a0eec0f69c", "exact_match": 0.0}
{"doc_id": 2722, "doc": {"question": "who has won the most world series in baseball", "answer": ["the New York Yankees", "New York Yankees"]}, "target": "['the New York Yankees', 'New York Yankees']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the most world series in baseball?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Boston Red Sox"]], "filtered_resps": ["Boston Red Sox"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c11c4442a975cfc245da69a2834a10567b6c63c186852d328cf65d6170b9e78", "prompt_hash": "cf4cdf52c2746e89018909c47167f817e42fcdecb885bb5287164b243ba9fc69", "target_hash": "ad42e1afb0972a701f8d40d546492032631f8022d8b63d89c32cc529dddb6c35", "exact_match": 0.0}
{"doc_id": 2723, "doc": {"question": "towns on the border of texas and oklahoma", "answer": ["Texhoma"]}, "target": "['Texhoma']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: towns on the border of texas and oklahoma?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ardmore"]], "filtered_resps": ["Ardmore"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e8863c1eba61f05bff15b7489d89dcbf7101df371e0a953e45750b3f4c778824", "prompt_hash": "9f8bcce3f6a7b32890b80facb72dbd70396a5adc31ed9ca650d49746444d81bd", "target_hash": "a56d5b3a3f56971e120e4a5ec844f0b85a5875ccfb3a989a694fa8e772bd6f3e", "exact_match": 0.0}
{"doc_id": 2724, "doc": {"question": "who has the most rushing yards in a super bowl", "answer": ["Timmy Smith"]}, "target": "['Timmy Smith']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most rushing yards in a super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Emmitt Smith"]], "filtered_resps": ["Emmitt Smith"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1bfb10cfac79bea10b86dc8774ccc54cf9a428de38c267f334efd94ea8930f2e", "prompt_hash": "565271c6cd204118a55866b07cf8f7bc6c5c09687a8e3823c986af3bcf24942f", "target_hash": "60fc83a9eed0b341350f318b845148087ac70a84fb70c8ada306a919cfaa5702", "exact_match": 0.0}
{"doc_id": 2725, "doc": {"question": "in photosynthesis the carbon in co2 is initially fixed to what molecule", "answer": ["3-phosphoglycerate"]}, "target": "['3-phosphoglycerate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in photosynthesis the carbon in co2 is initially fixed to what molecule?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ribulose-1"]], "filtered_resps": ["Ribulose-1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "132fb637996d591482fc414e3f7b0cf869deaef3c83d7022e83f0f25ba1b86f7", "prompt_hash": "5e0f5ac4ee77c690cd33d1b685a9d5dca28cddbec260828613cd2aea82ab92ae", "target_hash": "8acb3a1fb87f1119ccb97dbfdb858f51cc816a56d2662cc1c456eddf21af3fff", "exact_match": 0.0}
{"doc_id": 2726, "doc": {"question": "who said that religion is the opiate of the masses", "answer": ["Karl Marx.", "Karl Marx"]}, "target": "['Karl Marx.', 'Karl Marx']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said that religion is the opiate of the masses?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Karl Marx"]], "filtered_resps": ["Karl Marx"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "59328248cc609cda7a38467f738e522888b65b1b79644a7f67fa55e0d67e1942", "prompt_hash": "5199f317a94ae60b4b1b05f1a27a7625944af991125f5fdc74ed4917a1b4d768", "target_hash": "b3f4b27796d6e783801240e58f16ae785c8cf6562e94ba19e9a297f2a8227992", "exact_match": 1.0}
{"doc_id": 2727, "doc": {"question": "which was the first village of karnataka which declared independence", "answer": ["Hyderabad"]}, "target": "['Hyderabad']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which was the first village of karnataka which declared independence?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Somapura"]], "filtered_resps": ["Somapura"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "89922f401710f9ec898cc66f5e8b6e890c5058d298abd8087747182b16a8858d", "prompt_hash": "4594c8bd405981ec5d9d3eaf976885f3650b7ea1d40a6dc6a28c538b5ef71e99", "target_hash": "4612f286360ba926a4e571c1f661c893aa7f0d6c2814b9634117fe7f10f69457", "exact_match": 0.0}
{"doc_id": 2728, "doc": {"question": "who sang once upon a dream at the end of maleficent", "answer": ["Lana Del Rey"]}, "target": "['Lana Del Rey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang once upon a dream at the end of maleficent?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kelly Clarkson"]], "filtered_resps": ["Kelly Clarkson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "373b154625441ff353298238331800ce568545e31b1deef77eb33a477c657e5e", "prompt_hash": "b440d25bce2c4cedcc949bdfa231ccb4bbed98224aa092d08a8cee2970e9e950", "target_hash": "522b1614bc8cbacbd9b0c59f33dd085063d28738274f1cd1172f853284254e59", "exact_match": 0.0}
{"doc_id": 2729, "doc": {"question": "who was last person to be executed in us", "answer": ["Ruben Cardenas Ramirez"]}, "target": "['Ruben Cardenas Ramirez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was last person to be executed in us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gary Gilmore"]], "filtered_resps": ["Gary Gilmore"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a798c4d910ac6122cfc92424b2198d08ec501751547280fcde9873293c0573f", "prompt_hash": "83f588bbc13443964b9ae8ac10a015df096bf0de44defe7cb6bee346328d8bc8", "target_hash": "066d9e040756b243f23a821c11f516ee7ff9b62033d0a94022072183719f6a37", "exact_match": 0.0}
{"doc_id": 2730, "doc": {"question": "where does the story the great gatsby take place", "answer": ["Long Island of 1922"]}, "target": "['Long Island of 1922']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the story the great gatsby take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "800a011b409e0906989d640589e1b22b6e8dece8c902ff868439120d38fc0751", "prompt_hash": "2ff1158476c30f7f9025b15375947ae6e5b34a153fd8613e37e9a58016cb1a6a", "target_hash": "8d44630d8660393b011cb6126d911b374052aae8296b6849070948aad484bc94", "exact_match": 0.0}
{"doc_id": 2731, "doc": {"question": "how many scholarships do division 2 football teams get", "answer": ["36.0"]}, "target": "['36.0']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many scholarships do division 2 football teams get?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 24"]], "filtered_resps": ["24"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "97aa07124f21bd59828100afafdab6503761d6e4d1db9408bef47f398752f769", "prompt_hash": "ca1b4b7936f57b86b24562f7c0953a791ec96cc6d6b36cf71aec85ca1f4fb280", "target_hash": "850989fd3ac670e3b51aeef78cc097819f68bebf6b1007f4ba127ac74fd4842e", "exact_match": 0.0}
{"doc_id": 2732, "doc": {"question": "who scored the most goals in premier league season", "answer": ["Alan Shearer", "Andy Cole", "Chelsea"]}, "target": "['Alan Shearer', 'Andy Cole', 'Chelsea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who scored the most goals in premier league season?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alan Shearer"]], "filtered_resps": ["Alan Shearer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9b6f87b70cf9a6d098284da2ed5669c6c026856c0999695f820e39824447e8e", "prompt_hash": "695ae23512afd0fe207633c5b961fd6408509efdb262514d7ed6cb6dc57b7496", "target_hash": "88f4b016a24e029ec0ceace6e4885a0a4a13f8c8ff6d70816c14b8acaa35925b", "exact_match": 1.0}
{"doc_id": 2733, "doc": {"question": "who sings in cry baby for johnny depp", "answer": ["James Intveld"]}, "target": "['James Intveld']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings in cry baby for johnny depp?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joan Jett"]], "filtered_resps": ["Joan Jett"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0a538287f720927deedb86bcd3f1bfce99333131feaf2b84b911ece9fdfac307", "prompt_hash": "1af91abeda8a0cb91938809b68399025c82189215755e384d65e0720ec122b54", "target_hash": "85d9704c04d9d1860da1401bb42595dd077714b3420fde8b7c6063aa64d66214", "exact_match": 0.0}
{"doc_id": 2734, "doc": {"question": "who lived in jamestown before the arrival of the english", "answer": ["uninhabited"]}, "target": "['uninhabited']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who lived in jamestown before the arrival of the english?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Powhatan"]], "filtered_resps": ["Powhatan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "70b59cf7f880e3a1ab739b7fcf3cebe24caec37ba6927552307988fd37efe102", "prompt_hash": "676894c05cc639aef473c22ce9729089fbcf7dfc89d74f24db84e27e20ba553f", "target_hash": "944c129d5b1922d84c520ea81a3d850a53a3014f049c90c29cd4f09cd924d465", "exact_match": 0.0}
{"doc_id": 2735, "doc": {"question": "what is the corporate tax rate in great britain", "answer": ["20%"]}, "target": "['20%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the corporate tax rate in great britain?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 20"]], "filtered_resps": ["20"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9ea7050e3dd3e81a27af0eede10906a26bde12560de5c61d4b0cd829b45034b", "prompt_hash": "21377e25922c785ebef038052190d0732120a89b3c7e8ac56d44132ceda57154", "target_hash": "6cc14bc2e3fd1ebdffa06ca74b3886c581fa6dba078e2b484fd087b9e4305a50", "exact_match": 1.0}
{"doc_id": 2736, "doc": {"question": "who sings i want to rock and roll", "answer": ["Kiss"]}, "target": "['Kiss']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i want to rock and roll?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joan Jett"]], "filtered_resps": ["Joan Jett"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dee2fdc1f853b10b05d1f942acdfdd3a408dfba5a89a00c754ad6a1ad81ec7d6", "prompt_hash": "5d65d58618ab31b4012a3c1748332424250c54d2e6e281ffcd355aa7c9f0f793", "target_hash": "ffe45c08df7efc77e4df836885c8dc81228d83491549c51461c7fbf00c3acfab", "exact_match": 0.0}
{"doc_id": 2737, "doc": {"question": "how many seasons of the smurfs are there", "answer": ["9"]}, "target": "['9']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of the smurfs are there?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dec6d920fcda771631e64794b1c6dad7f012753dd7b9adaf6b91e4ff1c008d43", "prompt_hash": "076a497a1d07fd37391552c408201f6d4dc0f2a5bc588c105192f25d42b8b8a4", "target_hash": "b5b29fd42eafcda6b1173ba04a7ee6e19b09063668df563afe1265cf897eeac5", "exact_match": 0.0}
{"doc_id": 2738, "doc": {"question": "who wrote the song city of new orleans", "answer": ["Steve Goodman"]}, "target": "['Steve Goodman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song city of new orleans?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bob Dylan"]], "filtered_resps": ["Bob Dylan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f0dd9f21d65bef55f9a19fc98eb844afc95720c98558d1b8032d1ac9b346e7f", "prompt_hash": "91afa5b803e486947bcf02abf0544efc3621d82bb632fd5a2e3ad379dfcdb243", "target_hash": "3aa164d79413f49258ea8dda335135713137e438a0f8441c318f9408d7dce799", "exact_match": 0.0}
{"doc_id": 2739, "doc": {"question": "when did i'm like a bird come out", "answer": ["October 24, 2000"]}, "target": "['October\\xa024,\\xa02000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did i'm like a bird come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2011"]], "filtered_resps": ["2011"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae2b60494e3dec89a0ee3df85c10d927b8a6fcaf138a7ea9df2f71cdedf7d300", "prompt_hash": "a6d364456910f871d3e924b443a5d44f4b50f8eeb19448bde53c363dbe2bb5f9", "target_hash": "52a24b4aba5445b8da8179b13d88aa3700538ca5b34db96c7e864163421f57ed", "exact_match": 0.0}
{"doc_id": 2740, "doc": {"question": "who sings god is great beer is good", "answer": ["Currington"]}, "target": "['Currington']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings god is great beer is good?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" TobyMac"]], "filtered_resps": ["TobyMac"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb2cb96c9de55369d695f2f5c9c9f101efb1d2746d89ff3b82d04e74e65b52cf", "prompt_hash": "5aa64e949ab1fb9268abd7be2006ca25abea3298b115ac51d8b35736bf4198f4", "target_hash": "45f57f3fc7f735fb59e2ab2d295caa3bf41448c4b0d35a4516436449b38e3d02", "exact_match": 0.0}
{"doc_id": 2741, "doc": {"question": "when was night at the museum 3 filmed", "answer": ["British Columbia, Canada", "January to May 2014", "2014", "London, England", "from January to May 2014"]}, "target": "['British Columbia, Canada', 'January to May 2014', '2014', 'London, England', 'from January to May 2014']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was night at the museum 3 filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2014"]], "filtered_resps": ["2014"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2e00fcefd63eda1618ed6c94c9fbed857575c5082198b1d77c485679eb5ed471", "prompt_hash": "c15097400a66a94a3c62c0b5d7fb902497dcb15fe6cc61bebc9187b63b098685", "target_hash": "98b78365ef13edcb35ebc07c6f1a988a5ef600644ca09c5627b4f1951cd44c83", "exact_match": 1.0}
{"doc_id": 2742, "doc": {"question": "who added a press room to the white house", "answer": ["Richard Nixon", "President Richard Nixon"]}, "target": "['Richard Nixon', 'President Richard Nixon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who added a press room to the white house?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Theodore Roosevelt"]], "filtered_resps": ["Theodore Roosevelt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e6d32cc22562ea1a98b019ed90cd78f0b1ee193c1f1f851cbbf14c8614ddbd67", "prompt_hash": "8a3e230bc595cfb776ef0035068f2a45ae799f534e838449105fb7923190f9e4", "target_hash": "1450901b16326429eeeebbca508508d679fe5418b685f3a965330852667e45c0", "exact_match": 0.0}
{"doc_id": 2743, "doc": {"question": "when was the first documented case of tool mark identification", "answer": ["1835"]}, "target": "['1835']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first documented case of tool mark identification?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1908"]], "filtered_resps": ["1908"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "647f4c6e573d4cb1001333c522bcb6ab44369b5bceb808b824e7b56b01446ed3", "prompt_hash": "66a6bba2a8f4ec2aa42d18b5028be27fed5346957d521bf275732b908ff33814", "target_hash": "98017d4abcf3e74ebe3aae6817e655e3f735368f057fda300d184541e3b127e9", "exact_match": 0.0}
{"doc_id": 2744, "doc": {"question": "who did the 49ers beat in the super bowls", "answer": ["San Diego Chargers", "Denver Broncos", "Cincinnati Bengals", "Miami Dolphins"]}, "target": "['San Diego Chargers', 'Denver Broncos', 'Cincinnati Bengals', 'Miami Dolphins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the 49ers beat in the super bowls?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dallas Cowboys"]], "filtered_resps": ["Dallas Cowboys"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "077dc35d484ea0f0eea53078667d880d26aee8f940009071082a875f526e15a5", "prompt_hash": "f03fe4553dfcf1fc93216896d7518a08a0e44a28dfc1f40b71cc34fbb0489afb", "target_hash": "5dcac2bee4d7287cddf282a4f5f06c9670f41cebb39e37b23496cadc3473e35b", "exact_match": 0.0}
{"doc_id": 2745, "doc": {"question": "what kind of dog is charkie from curious george", "answer": ["black cocker spaniel", "cocker spaniel"]}, "target": "['black cocker spaniel', 'cocker spaniel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of dog is charkie from curious george?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chihuahua"]], "filtered_resps": ["Chihuahua"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6c6bd94edcef2124157e0fd6f457c83486e17f305c55f4e1db7d539e17906504", "prompt_hash": "26d40c67c547a1089c2cffae21659681136461bc05e8b2cd245d573a57b6b497", "target_hash": "31a21e523005fb78a8a56501d722a29309a0f52d8ff51eb5a018f2011af0562e", "exact_match": 0.0}
{"doc_id": 2746, "doc": {"question": "where are mucosal associated lymphoid tissues present in the human body and why", "answer": ["oral passage", "salivary glands", "gastrointestinal tract", "breast", "skin", "thyroid", "lung", "nasopharyngeal tract", "eye"]}, "target": "['oral passage', 'salivary glands', 'gastrointestinal tract', 'breast', 'skin', 'thyroid', 'lung', 'nasopharyngeal tract', 'eye']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are mucosal associated lymphoid tissues present in the human body and why?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" gut"]], "filtered_resps": ["gut"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "feaa50c51ddd4904415d922068744b2a5ab9c8dfa97e3fa440877083b3819357", "prompt_hash": "fdda66601025916993106e863fb4b04b60f12526c1c7680cf9676bbbeab3b5dc", "target_hash": "cb6b340031c6fb5d932a7bc80fac826143619b0e9454a615ca08a4aa4a9e7027", "exact_match": 0.0}
{"doc_id": 2747, "doc": {"question": "what are the roles of international monetary fund", "answer": ["foster global monetary cooperation", "secure financial stability", "to foster global monetary cooperation", "reduce poverty around the world", "facilitate international trade"]}, "target": "['foster global monetary cooperation', 'secure financial stability', 'to foster global monetary cooperation', 'reduce poverty around the world', 'facilitate international trade']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the roles of international monetary fund?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to foster international monetary cooperation"]], "filtered_resps": ["to foster international monetary cooperation"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2779733eb59023d933f292fedbf3214d887ba0258293a198ce849d4bab18d86d", "prompt_hash": "18ad81984ce6f9a6a0dc212a876a24fb8ef86586cb03e39d4d06b0dcec062424", "target_hash": "af5d8bece8828423f094f22f28337a582764a312ba8ebe39e9e31bfc78cb0320", "exact_match": 0.0}
{"doc_id": 2748, "doc": {"question": "when was 1 john 5 7 added to the bible", "answer": ["the 9th century"]}, "target": "['the 9th century']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was 1 john 5 7 added to the bible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1526"]], "filtered_resps": ["1526"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6a2c081d6d28fcba2ee121b5f9bdf73bfc0140e5e97e89bf124b443068b65f4d", "prompt_hash": "b323723960e1b951dbe45330b1583bef3da14a475781008c07a1a28e76be5e35", "target_hash": "19b9af59a6858dd421aee754d32b7fbbdd9b3184e2d29f62b4d886a9bbb4de34", "exact_match": 0.0}
{"doc_id": 2749, "doc": {"question": "where does the saskatchewan river start and end", "answer": ["central Saskatchewan", "the Hudson Bay", "Saskatchewan River Forks, Saskatchewan", "Saskatchewan Glacier, Alberta"]}, "target": "['central Saskatchewan', 'the Hudson Bay', 'Saskatchewan River Forks, Saskatchewan', 'Saskatchewan Glacier, Alberta']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the saskatchewan river start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lake Diefenbaker"]], "filtered_resps": ["Lake Diefenbaker"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58fad5235a1e77968c665d7a44cad1db05121a3c340668c74f261071919313a0", "prompt_hash": "d3a7afe7b85a310b37365afdd522346172c74e82b1af929d2d2102a53910e88b", "target_hash": "d928facc07948e18898b24ff0aefd8cfb0af9566074778550d5f67d211b7727b", "exact_match": 0.0}
{"doc_id": 2750, "doc": {"question": "calpurnia son name in to kill a mockingbird", "answer": ["Zeebo", "James Zeebo"]}, "target": "['Zeebo', 'James Zeebo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: calpurnia son name in to kill a mockingbird?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jem"]], "filtered_resps": ["Jem"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d7b6711a5651757b72bccad2730be054756e4e63e7b4e80bdfeb1e1942781c8", "prompt_hash": "a890f1a334046484bcb0dce3fc9636e5ab2db60cab680d9583f00bfbb1a9adc8", "target_hash": "b88f618aa8ab751a6146d2a5585881ab4bb8b519da8c95e806f04464eec09a6c", "exact_match": 0.0}
{"doc_id": 2751, "doc": {"question": "what are the first and second major religions in france", "answer": ["Islam", "Catholicism", "Not religious", "Protestantism", "Christianity"]}, "target": "['Islam', 'Catholicism', 'Not religious', 'Protestantism', 'Christianity']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the first and second major religions in france?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Catholicism"]], "filtered_resps": ["Catholicism"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "124e5e2f5b77d0b5c977f4d18ab70c70febb6e56d941526c0335d925126ff289", "prompt_hash": "f03a9f87203452d5f62ab0588d56105da42903d5e922cab0fb48cf2cfdfea780", "target_hash": "e9304035dfa4bf52ca6fe8f9deeddfd3c09c452e475f4705fba90738731670e0", "exact_match": 1.0}
{"doc_id": 2752, "doc": {"question": "the plane of earth's orbit is called the", "answer": ["ecliptic"]}, "target": "['ecliptic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the plane of earth's orbit is called the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ecliptic"]], "filtered_resps": ["ecliptic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea4a86ca2999f3fbd0a2374041cfb91fa8fc1a7eb885a8ff74f0cd2ab8506e81", "prompt_hash": "4192e29ea8fe6d55d6d34f8c9e8f1ea1bc62b23eda644ff67a8bfb29d593482e", "target_hash": "2e83cd13277a29f3f6e5c5e398f4dcc0f4eb6a96564c0b868964b31948f478b9", "exact_match": 1.0}
{"doc_id": 2753, "doc": {"question": "what is a another name for the water cycle", "answer": ["the hydrological cycle", "the hydrologic cycle", "hydrologic cycle", "hydrological cycle"]}, "target": "['the hydrological cycle', 'the hydrologic cycle', 'hydrologic cycle', 'hydrological cycle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is a another name for the water cycle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" hydrologic cycle"]], "filtered_resps": ["hydrologic cycle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a4bde73d489bdbe4e87e748757b69a2f3ce291fcf68365268bd70830f768f2b", "prompt_hash": "926dd72e86c5de4467cb39edd1b550a8b4bd86606b6175bf1b9e2dd97a6f4d91", "target_hash": "9a6e7cf6e340e0f092f814e2fa0704f3a5afcc51541b4ecaa64e21800411471e", "exact_match": 1.0}
{"doc_id": 2754, "doc": {"question": "when does the champions league round of 16 start", "answer": ["13 February"]}, "target": "['13 February']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the champions league round of 16 start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12 February"]], "filtered_resps": ["12 February"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8918945adb025158462eb1b084f7fbd39475acf5df52076c35d94a4300c931fc", "prompt_hash": "0167c732eff2f929af386a7e8891dbc0f265ab3bd45cdffd9ce2ccb8a4d83f5d", "target_hash": "b4022049add98753d00650bd9335301a92c598f1ea98777a884a3ed900eaf622", "exact_match": 0.0}
{"doc_id": 2755, "doc": {"question": "with a land area of 54 314 square miles where does wisconsin rank among the 50 states", "answer": ["25"]}, "target": "['25']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: with a land area of 54 314 square miles where does wisconsin rank among the 50 states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 23"]], "filtered_resps": ["23"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "447ce732e12b46f648d09d096dd7835d78efa5793b1b61f783ca870768eda857", "prompt_hash": "ec81a8c0331bac53ac33652cb58b6fd0620806cd76c792984735a4c6d59e729c", "target_hash": "dbfd7873a376a93103433e75ee5985f03acc4da29b7e84566430ff97a2d456a6", "exact_match": 0.0}
{"doc_id": 2756, "doc": {"question": "when did a wrinkle in time start filming", "answer": ["November 2, 2016"]}, "target": "['November 2, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did a wrinkle in time start filming?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b0befa1e3a239f807342c6429b10e0e00f49f94d4143f0c3bc1871cdbf873ee3", "prompt_hash": "b69943817814ca23e55a68d772b017085cbcccdce05d39d9a1ca8acda35d5367", "target_hash": "9a39fcdf751e01d7a676c2e955cf97e84a6ac0f0e2b94afa4e19e2e17c96b9fc", "exact_match": 0.0}
{"doc_id": 2757, "doc": {"question": "who played mr. willoughby in sense and sensibility", "answer": ["Matthew Gregory Wise"]}, "target": "['Matthew Gregory Wise']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played mr. willoughby in sense and sensibility?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hugh Grant"]], "filtered_resps": ["Hugh Grant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cfdebe894ca531d03e8792881dfe9a5d7289710d9e38739a21d90ad31b3391f8", "prompt_hash": "9bd11fba7cb3b1fe4496afb982333fe3cd3bcbf0670bafe4023ab64722f982e3", "target_hash": "d6b0d58162ea7f0b66baa2a5857eebd627f3e518fb271079bf8ca9580cf59249", "exact_match": 0.0}
{"doc_id": 2758, "doc": {"question": "who won the men's single title of australia open on 1 february 2015", "answer": ["Djokovic", "Novak Djokovic"]}, "target": "['Djokovic', 'Novak Djokovic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the men's single title of australia open on 1 february 2015?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Novak Djokovic"]], "filtered_resps": ["Novak Djokovic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0546aaa679066ff9d5cbddd8441ec05f0ddeb44b4c0b7d39b9fd6000dc5d6d43", "prompt_hash": "beafbced80396fe669b800faa5eb216f1e7f6b7a3dddc808c88f83a54542ffb5", "target_hash": "601f9de9ec2516bd3a028a9de45492bc4b79bceae2e6ab2012d7b26f4f1d5ab5", "exact_match": 1.0}
{"doc_id": 2759, "doc": {"question": "when does nathan get in a car accident", "answer": ["The Show Must Go On"]}, "target": "['The Show Must Go On']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does nathan get in a car accident?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" after the prom"]], "filtered_resps": ["after the prom"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c889c69bb4315103418b54789d57dbb82a389fe2c544ceadf2e75ea2edb963c4", "prompt_hash": "69ba9e7191e0fa72161586bea28b3986e4e0c6b9804b69637f7ace83fd7d4378", "target_hash": "5bbe48e87ec76c0b0f088d20bcf9daf772fc40c25e125b72f092452cab0c921a", "exact_match": 0.0}
{"doc_id": 2760, "doc": {"question": "what is the white stork a symbol for in western culture", "answer": ["bringing babies to new parents"]}, "target": "['bringing babies to new parents']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the white stork a symbol for in western culture?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" death"]], "filtered_resps": ["death"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8634d3e2cc40efb6cd2bed49f96107a03a51e87a3f2a9eddd60ed7affca6f1c", "prompt_hash": "5bfbbd6a30ed1d6a6706fbdea867bae556117c101b81545b611b2af21c1f0d3e", "target_hash": "36839b4f29ddc61f31ef0277f2c4069dd3ce87dcf2319930bff84fd51d64e3b7", "exact_match": 0.0}
{"doc_id": 2761, "doc": {"question": "who got the first arjun award in athletics", "answer": ["Gurbachan Singh Randhawa"]}, "target": "['Gurbachan Singh Randhawa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who got the first arjun award in athletics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" P"]], "filtered_resps": ["P"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e43295ab1290d4f9010426a136f1d49b57e3c9b0baf450c31fefc6c96a7f2920", "prompt_hash": "fb0ea85ab5e422f3de040a1a655028954dd48055393a36529299d8a5455eb138", "target_hash": "478d04e2d646e1f9ce246022fdab6f2b168f4de5c43db81a13e32fcc4a6c81b6", "exact_match": 0.0}
{"doc_id": 2762, "doc": {"question": "who sings good night on the beatles white album", "answer": ["Ringo Starr"]}, "target": "['Ringo Starr']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings good night on the beatles white album?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul McCartney"]], "filtered_resps": ["Paul McCartney"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7c1b325a72f78fd1ea64c9a9a1dfcecd8aeb647c4c90570203ae026ac1f70d36", "prompt_hash": "238b812bfb07a4eaca4125ae1d053d456df0dda97e787351c661c5239eed76bf", "target_hash": "1b348a8008ff26e97cf4a82a010bcda5f35c9868c9fb16651fe13bfd8c4430eb", "exact_match": 0.0}
{"doc_id": 2763, "doc": {"question": "when does season 5 of ruby come out", "answer": ["October 14, 2017", "October 14, 2017"]}, "target": "['October 14, 2017', 'October\\xa014,\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 5 of ruby come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0db3d2349b32181bce11b574d6df6a1b9b77432c043340fba8657d07c8f5f8c7", "prompt_hash": "2f91bf27e1ec9fab40677d67efe34e0c8915e0080acd15ceacb12b582b152218", "target_hash": "1da6c565f8e01b17ac65e418993ba4edcfcdf5567945badb350035a8a39d471c", "exact_match": 0.0}
{"doc_id": 2764, "doc": {"question": "where are the winter olympic games being held this year", "answer": ["Pyeongchang County, South Korea"]}, "target": "['Pyeongchang County, South Korea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the winter olympic games being held this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pyeongchang"]], "filtered_resps": ["Pyeongchang"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d4b0fa19f083f3aaf886cd68e19352ca45f3df719ced67bfc6a7d5d2f8fd831a", "prompt_hash": "4bffaac495453c4e134a4c642a00e7202f4a93981e8ab04b7ad55de370a07f8e", "target_hash": "8ad537eaa485bdb2bf6ec892cd063468a32d95758cd4e32f07a319fe9914ecbd", "exact_match": 0.0}
{"doc_id": 2765, "doc": {"question": "nitty gritty dirt band fishin in the dark album", "answer": ["Hold On"]}, "target": "['Hold On']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: nitty gritty dirt band fishin in the dark album?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Fishin' in the Dark"]], "filtered_resps": ["Fishin' in the Dark"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "be6b5c1f51f428d1cfb09bd0cb175bb53ba75ced22fafe3f584420fcea564e2b", "prompt_hash": "b87defe3db179d5424c8996282fa2ec1e3226e101dd678263de5b83388af1104", "target_hash": "7d0da406078df2f8d70be6431ba2657870d6b486db3632fcc725de699ee938c6", "exact_match": 0.0}
{"doc_id": 2766, "doc": {"question": "nickname given to railroad executives due to shady practices of their businesses", "answer": ["Robber baron", "Robber Barons"]}, "target": "['Robber baron', 'Robber Barons']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: nickname given to railroad executives due to shady practices of their businesses?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" robber barons"]], "filtered_resps": ["robber barons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9428499979918daaf2d07b4922c053234dfd5ab7fc168813f69ae3e905de48ab", "prompt_hash": "03510772d6a39d20a7f4ae0033c07df4d6211a818cd59ae366434639b90403d7", "target_hash": "d05ead9fc9be9c07c1ec9d425512ff2a9e7f6a6fe5747921cb5ee35f503dac40", "exact_match": 1.0}
{"doc_id": 2767, "doc": {"question": "what was the first star trek enterprise ship", "answer": ["NCC-1701"]}, "target": "['NCC-1701']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the first star trek enterprise ship?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" USS Enterprise (NCC-1701)"]], "filtered_resps": ["USS Enterprise (NCC-1701)"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d2ae7f2562cf1698bb8a02a8bc389af2c7318a2c676ac2cc76dd3c77d8574a33", "prompt_hash": "3fb0c60845e2ebb700481fe705b968d63d12287b47fdca5126e7de395f4e4289", "target_hash": "1c2ef4ef4cd727f7258db5b624ac1256bdeca8ea391717b9a2f5444996f50183", "exact_match": 0.0}
{"doc_id": 2768, "doc": {"question": "what is the full scientific name for the e. coli bacteria", "answer": ["Escherichia coli"]}, "target": "['Escherichia coli']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the full scientific name for the e. coli bacteria?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Escherichia coli"]], "filtered_resps": ["Escherichia coli"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e2fd5e4bc82588907efad442c62626ba0c2f6bb960580a3ceb79ae353b37e1d6", "prompt_hash": "e7bea2be470f4480c4f0bc09b30c7c88f33afa6a99d5d0ce4ca469fd3ddc8326", "target_hash": "4360054e1fb5f52f5180f59718d9ce2bd8052b8606b80c4dd56680b9a3bed091", "exact_match": 1.0}
{"doc_id": 2769, "doc": {"question": "rain sleet or snow that contains a high concentration of acids is called", "answer": ["Acid rain", "\"Acid rain\""]}, "target": "['Acid rain', '\"Acid rain\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: rain sleet or snow that contains a high concentration of acids is called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" acid rain"]], "filtered_resps": ["acid rain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d397c6df24ad4a69ce36bf9a363735c4bc8b0534c106aabb58b9077fd1f9d45a", "prompt_hash": "7a5e12542b10b841ab01937840a8dbc27c3d9688301907afa16505c3c41c8000", "target_hash": "d9a0b899791f077863a3696848851a3277c9309495e6c2feb36e7a56882f5dd3", "exact_match": 1.0}
{"doc_id": 2770, "doc": {"question": "who had won the first grammy award for the best new artist", "answer": ["Chance the Rapper", "Bobby Darin"]}, "target": "['Chance the Rapper', 'Bobby Darin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who had won the first grammy award for the best new artist?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Janis Joplin"]], "filtered_resps": ["Janis Joplin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0fd77bc3288217cbb86711daee1c13b067fb8c3e6880b2f2907899d3bb7b5f61", "prompt_hash": "f11989b24049b52ab24574bc975f443ddfa984c8e4a4539c66c112581fef46a2", "target_hash": "0d13b3961a7c1a1529d1793f039109c49ef268b6b29e74d4c00ec18a74a62d50", "exact_match": 0.0}
{"doc_id": 2771, "doc": {"question": "where did the christmas tree at rockefeller center come from", "answer": ["State College, PA"]}, "target": "['State College, PA']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the christmas tree at rockefeller center come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New Jersey"]], "filtered_resps": ["New Jersey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "efdf04b44a37b235d21da615a76a7e7407fd3024a2d205bcc52ae7cad4ba7264", "prompt_hash": "dc42e3ab150aa94597e58407841e64f5f376d3967a76ffef6a3d88ec6699a78f", "target_hash": "86775487e908fa2190ebaa74e33dcfef65a887600d8ec5e83c65815f1b18989c", "exact_match": 0.0}
{"doc_id": 2772, "doc": {"question": "when does the new season of lost in space come out", "answer": ["April 13, 2018"]}, "target": "['April 13, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new season of lost in space come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2018"]], "filtered_resps": ["2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3e51ef77f527c288f811d4892479fb16bbac8b478f9f06503801aee6672d2115", "prompt_hash": "27714cf8ea2e69d5ade37988049b6f3b912850c361f4ac93c5b6adeb01a15a0a", "target_hash": "4d4b143703a16c949f7f1756aac18633f6e238879a24f19b017201255e52b00e", "exact_match": 0.0}
{"doc_id": 2773, "doc": {"question": "where are the organ mountains in new mexico", "answer": ["southern New Mexico"]}, "target": "['southern New Mexico']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the organ mountains in new mexico?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Organ Mountains"]], "filtered_resps": ["Organ Mountains"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "90260e55bd95549aa55ee434e47ccb008724769cc8be003764120127fe76a432", "prompt_hash": "56321ac9dd4619605033adb708b6c343ee0dc84c28938ddd8ccb1962441ea706", "target_hash": "bab08517398e7cd97957e5a291e2d22a1d6773b9cc2b81f5daacb373f65a97f1", "exact_match": 0.0}
{"doc_id": 2774, "doc": {"question": "creating appropriation bills falls under which power of congress", "answer": ["raising revenue"]}, "target": "['raising revenue']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: creating appropriation bills falls under which power of congress?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the power of the purse"]], "filtered_resps": ["the power of the purse"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9e7ea262a39178a209865fb97b72c51a0ffe3e0b6de47d57093a0a23fd53acc3", "prompt_hash": "d2c15f4d7e6e6af6195dfe78f22bac089e41e3b6471ae6a5ff81d93abe41dde2", "target_hash": "de27391bdf82b5a31e054f734ea4d6ef50b6cab3e1f931fd2eae4faa647d9035", "exact_match": 0.0}
{"doc_id": 2775, "doc": {"question": "who played the hobbits in the lord of the rings", "answer": ["Dominic Monaghan", "Ian Holm", "Sean Astin", "Elijah Wood as Frodo Baggins", "Billy Boyd", "Elijah Wood"]}, "target": "['Dominic Monaghan', 'Ian Holm', 'Sean Astin', 'Elijah Wood as Frodo Baggins', 'Billy Boyd', 'Elijah Wood']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the hobbits in the lord of the rings?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Elijah Wood"]], "filtered_resps": ["Elijah Wood"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7cf381dea945eddd0f6e1a6d7d35862f81e6920d02f3de988d1553491c547d9", "prompt_hash": "6e9c7672f330d1e43ddb80d5b4f79e7ce04bfcb4cbaeff3555212e2d9cef711f", "target_hash": "730dd9f0325336af8f5143a3e8c9966347d657960e81e20e568389f35854e0be", "exact_match": 1.0}
{"doc_id": 2776, "doc": {"question": "when did the usa join world war one", "answer": ["April 1917", "April 6, 1917", "in April 1917"]}, "target": "['April 1917', 'April 6, 1917', 'in April 1917']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the usa join world war one?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1917"]], "filtered_resps": ["1917"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1feaa9fe1a3c81baaf119ab5f68840814bfcf357fc788b4d049b4f9d722ab456", "prompt_hash": "379b4ab45f90248a4682ebc072b9d87547262bd54fe74483e53eb2796a7c4b5a", "target_hash": "fc251ffa209b1f849e8fefd4001407db9a18100a3c16a1f9e3c7ecbe61301aa9", "exact_match": 0.0}
{"doc_id": 2777, "doc": {"question": "when did they start 3 pointers in basketball", "answer": ["1961", "1945"]}, "target": "['1961', '1945']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they start 3 pointers in basketball?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1979"]], "filtered_resps": ["1979"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "09648560f9a1f10bcd28652241f7ef18026941f304292c9c796494e6668dc93a", "prompt_hash": "50ada39ce5dee96ee7eecf5c7dea419ad5f7b8440e547da12ed9f88bb2046071", "target_hash": "27b2bc973d5f99cfc715e5508667b3291da668dcde748a08e4d4e7df77b2ebe0", "exact_match": 0.0}
{"doc_id": 2778, "doc": {"question": "through what process has the colorado river created the grand canyon in the usa", "answer": ["erosion"]}, "target": "['erosion']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: through what process has the colorado river created the grand canyon in the usa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" erosion"]], "filtered_resps": ["erosion"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f7ea26d5ab9aeaf8b190d34a1bec44ca6287ab77e17870aab44bd7b33c53443a", "prompt_hash": "b86348c456e5519ee708084c066252956a39095eeeb6e6e1e1d5a7d74e695dbb", "target_hash": "eec9dadb3ea78118a4db10eab558f0ccc3eb79b60082781c699d2cf2f81e89fa", "exact_match": 1.0}
{"doc_id": 2779, "doc": {"question": "what is the rate limiting enzyme of kreb's cycle", "answer": ["Isocitrate dehydrogenase"]}, "target": "['Isocitrate dehydrogenase']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the rate limiting enzyme of kreb's cycle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Isocitrate dehydrogenase"]], "filtered_resps": ["Isocitrate dehydrogenase"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "53534f380740e671f2ed35fd1ceaefa82689b569e277fb0c718642adcc0abd42", "prompt_hash": "7952ab41b2c25069e59dbef8db7c1821baa055cc56b2fdac419e5f0e209057a7", "target_hash": "1b6cab4afa6ad31ff3001adcb6b97f9d49865a34ceb9eafba4bc314710125146", "exact_match": 1.0}
{"doc_id": 2780, "doc": {"question": "term limits for house of representatives and senate", "answer": ["Unlimited six-year terms", "Unlimited two-year terms"]}, "target": "['Unlimited six-year terms', 'Unlimited two-year terms']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: term limits for house of representatives and senate?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" two years"]], "filtered_resps": ["two years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8de3eb8c5903c0db7b501297324c198e2cf202d02451c45aa0b92be28a9c0cbd", "prompt_hash": "6c092f8903408e804e8c2a255a389bb69211942ef5652506ba9e5c0c2cc6a1f9", "target_hash": "948e768147f2d666e63b81993aced0d1d0bf883070cca8d07c552164e0e738ac", "exact_match": 0.0}
{"doc_id": 2781, "doc": {"question": "what information is displayed in clear text from the ftp header", "answer": ["all transmissions"]}, "target": "['all transmissions']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what information is displayed in clear text from the ftp header?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" command and reply codes"]], "filtered_resps": ["command and reply codes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8756efbd076e917675475c6a4473dd6d713b250db4dbf039d1735a6e28282819", "prompt_hash": "7eb85c7cc6b8247b300206ec023fe427110d7c637245a870878c44d687fe72ac", "target_hash": "c5219dc39c980aadf507161bf5f4b88fcedd96bc889857f918cfa96a7e9aa1c0", "exact_match": 0.0}
{"doc_id": 2782, "doc": {"question": "who has the most big ten championships in football", "answer": ["Michigan"]}, "target": "['Michigan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most big ten championships in football?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michigan"]], "filtered_resps": ["Michigan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f16c5dd705b5c82ac0b4403fffd6596d7dc7147ee6097d4b6cc6456d0f145448", "prompt_hash": "bfd069fe94485970ad9f0f7dba631ae1ee9fe4ebe49051ca5ab6992fd4a71cf8", "target_hash": "e74655ceb00ecd29195097df2d77eb21e2220645ea21913b8e2d1335882f4c86", "exact_match": 1.0}
{"doc_id": 2783, "doc": {"question": "what does ss stand for on naval ships", "answer": ["Single-screw Steamship"]}, "target": "['Single-screw Steamship']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does ss stand for on naval ships?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" submarine service"]], "filtered_resps": ["submarine service"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7350a137786ac038fc2164e896e86ab33629d4861c7c693616d9862ee15a1e24", "prompt_hash": "295679d82b05c9956b123a1fb19b47f95c765cfa6b90cccafb2dfe8757d7dad3", "target_hash": "3a678e5c22f28777d121fab89f4166dde24f8eedfa15f8fc2a97972b29720c08", "exact_match": 0.0}
{"doc_id": 2784, "doc": {"question": "which hormone is released from the ruptured follicle or corpus luteum", "answer": ["estrogen", "estradiol", "inhibin A", "progesterone"]}, "target": "['estrogen', 'estradiol', 'inhibin A', 'progesterone']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which hormone is released from the ruptured follicle or corpus luteum?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" progesterone"]], "filtered_resps": ["progesterone"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b78adce33dec4a691257eac1d8a415c11f6cb86cf3fb11f74a3ec86feac16398", "prompt_hash": "d8fba2b0f187111238387a19cc7dc80fce88865829b068868d8fc021e85831b8", "target_hash": "50872b9703761b95c5b2360adba7caebb7ebf80c1cb1c418b00d62df506e3769", "exact_match": 1.0}
{"doc_id": 2785, "doc": {"question": "where do the elves go on the boat in lord of the rings", "answer": ["Valinor"]}, "target": "['Valinor']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the elves go on the boat in lord of the rings?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Grey Havens"]], "filtered_resps": ["the Grey Havens"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8aca0cefd84e51988b08bf3dbb11d7967287493c74a4555d331e0ad67922c5eb", "prompt_hash": "43b6facbdb7f135c411d4d6a3c06075be361d8de6a5c96d2a9c4471416efa3d0", "target_hash": "6e1350147b2bf4ab10aa0fb5cd820250aa439689ff3aa42c841edc09ca06b571", "exact_match": 0.0}
{"doc_id": 2786, "doc": {"question": "how many times has psg won champions league", "answer": ["46"]}, "target": "['46']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many times has psg won champions league?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 0"]], "filtered_resps": ["0"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7c8a8790feabd537b5034799bdcec420ee7b6909b1ef013649cc5de3e1e4a26a", "prompt_hash": "9d586349b5d2b059d8e74c15926d30116e7108dc863f99b332d50beaad350312", "target_hash": "737a43a75e48601cf4ce756c0df5ecd34af1913bfb3ff4ef23059bbfeb273625", "exact_match": 0.0}
{"doc_id": 2787, "doc": {"question": "when did ga state get a football team", "answer": ["2010"]}, "target": "['2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did ga state get a football team?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1916"]], "filtered_resps": ["1916"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df477f1280e3b5899d4122178b0978d6393ce45ffc8b90f1dae5ad247c2a5833", "prompt_hash": "73ab76338970f0834448a1644633f02fdb92261a276b581c4c826343bda18686", "target_hash": "2ee1672483fc554adda35e9fdae9ee5db286e7d25dde7bd484842b76ac64c784", "exact_match": 0.0}
{"doc_id": 2788, "doc": {"question": "who has won the mens singles title in the australian open tennis tournament", "answer": ["Roger Federer"]}, "target": "['Roger Federer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the mens singles title in the australian open tennis tournament?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Roger Federer"]], "filtered_resps": ["Roger Federer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8f5966594d080052d7c86d01fbc866677c4267df94b33c9db56a696b2573f68", "prompt_hash": "1a21871ee8bf42a0edf51e312cec5eb51ff87dd9d9db259b2d6ccd13a8fe478d", "target_hash": "6858b4fe714464a9751819ee4b8ef3f3c03ff673126aaaf25b8a748dfc549c38", "exact_match": 1.0}
{"doc_id": 2789, "doc": {"question": "what is the longest english word in the dictionary", "answer": ["Methionylglutaminylarginyltyrosylglutamyl...serine", "pneumonoultramicroscopicsilicovolcanoconiosis", "Pneumonoultramicroscopicsilicovolcanoconiosis"]}, "target": "['Methionylglutaminylarginyltyrosylglutamyl...serine', 'pneumonoultramicroscopicsilicovolcanoconiosis', 'Pneumonoultramicroscopicsilicovolcanoconiosis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the longest english word in the dictionary?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pneumonoultramicroscopicsilicovolcanoconiosis"]], "filtered_resps": ["pneumonoultramicroscopicsilicovolcanoconiosis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f8b095ce63e20c9bce58ceacac158392f988ef1572cc3e23e7e4a9040fe3182", "prompt_hash": "58b767fe9392c00686db18a6be08cd194b02e85c661345569ead6460de564801", "target_hash": "fe4a09447c5d846e29eb59f1e5770aa814d66d774cd79bb46f9a195a7d5e16b4", "exact_match": 1.0}
{"doc_id": 2790, "doc": {"question": "in which city are the wimbledon game held", "answer": ["London", "Wimbledon, London"]}, "target": "['London', 'Wimbledon, London']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in which city are the wimbledon game held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" London"]], "filtered_resps": ["London"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d4df9f32676ec953c962a01afe06c2d39ca22c401540f286a7a0193aae8169dd", "prompt_hash": "e5cde448a0af7b45e4e6a39d9604eabd8450afb6dae4e1823d34a4d145f5153b", "target_hash": "1844f4df783251671f3f9b6eb94181ab619d50cf585cf7bdd84503ed84a25bdb", "exact_match": 1.0}
{"doc_id": 2791, "doc": {"question": "who won the oscar for best actor when titanic was nominated", "answer": ["Jack Nicholson"]}, "target": "['Jack Nicholson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the oscar for best actor when titanic was nominated?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kevin Spacey"]], "filtered_resps": ["Kevin Spacey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d70b9bcc13bf8ff588edf000ac534f583f4850d543f403eed9b01b11c942c40", "prompt_hash": "af341fc87e089d28d3e6d549eb484605b1f4ce13ba27ed0031dc818aa15d0588", "target_hash": "b40228a7805f434603beb35fcb6940d1e07277c4a9cab3d19fb396a07027e4e7", "exact_match": 0.0}
{"doc_id": 2792, "doc": {"question": "when did australia 2 win the america's cup", "answer": ["1983"]}, "target": "['1983']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did australia 2 win the america's cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1983"]], "filtered_resps": ["1983"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "908fb7e7daf644c924670fa33f2aa318bd5cbf5a7095becf92a30ca2a3206dd3", "prompt_hash": "933631da44d755694e3a0cc360579e03333eddff69699b9d6ae3eeeba90dc2d8", "target_hash": "0e762ec36fa410e205903e938523d534249d38027c286e0b2efd2df9399a61ca", "exact_match": 1.0}
{"doc_id": 2793, "doc": {"question": "how many points did lebron james scored in his career", "answer": ["31,038"]}, "target": "['31,038']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many points did lebron james scored in his career?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 28"]], "filtered_resps": ["28"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2dd76e13823a6c22e367e04dd1de063d092605c2dfbbe61119cf599446ef43eb", "prompt_hash": "b93e44019d805273a191b9d26263d487498ffd64272cf6d2b5f2848b8022de60", "target_hash": "099f8f60925649168ea7d92dea8bab5d9b69516915e4eccdcfa0fb3209867d8e", "exact_match": 0.0}
{"doc_id": 2794, "doc": {"question": "where does sound travel fastest solid liquid or gas", "answer": ["solids"]}, "target": "['solids']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does sound travel fastest solid liquid or gas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" solid"]], "filtered_resps": ["solid"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "004b7ae70fa7b012a6df3d940615b2028164c88b55998afea7ad6d5e11a5a64f", "prompt_hash": "a105934d1e174f1df45b7f6f014616c1b375c6054738168e21ff16c2351d9c1b", "target_hash": "508cd71bb3f54b0e16c55f65ae9405fa36383e18ac00df190fa2612b8ba358b6", "exact_match": 0.0}
{"doc_id": 2795, "doc": {"question": "cat in the hat knows a lot about space movie", "answer": ["Space is the Place"]}, "target": "['Space is the Place']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: cat in the hat knows a lot about space movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Outer Space (1968"]], "filtered_resps": ["Outer Space (1968"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3211d78410c5c2db5b7c833f7997bd1d017dca84e994cae525be4d5addf472a6", "prompt_hash": "3dfb06686137c73b18e3e2158d21fe61e677e7cd08f6607cd3275aa4167563e0", "target_hash": "b3c7d54c63e03ff15c6f6cc71f3ccbf67f0574a5b8e3f4c1532b5e15c91cda72", "exact_match": 0.0}
{"doc_id": 2796, "doc": {"question": "who sang on the back in black album", "answer": ["Brian Johnson", "Malcolm Young", "Cliff Williams"]}, "target": "['Brian Johnson', 'Malcolm Young', 'Cliff Williams']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang on the back in black album?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brian Johnson"]], "filtered_resps": ["Brian Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d920251cb9aae2da6de56f1912caa8488a5a08243b9cb53e60ca7ae0a925359e", "prompt_hash": "7dac6237afaade05696b3455065b1ff4164fb49062779ca9a3fdb3808339c5d2", "target_hash": "47a788069cfb624fc26fc44a875d29be8a69eb805c0054b7353709bc63a83b48", "exact_match": 1.0}
{"doc_id": 2797, "doc": {"question": "where did the race in grease take place", "answer": ["Thunder Road"]}, "target": "['Thunder Road']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the race in grease take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Las Vegas"]], "filtered_resps": ["Las Vegas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "86028013c7d57e84d5e4fc321c283cad77dda64f372ad03b9e99c18843743935", "prompt_hash": "ecd78310d2578c4572422d0bb3d5b446bca43dfa961e9feb3da07f511aef69a9", "target_hash": "2b2094f92ade0eaaa1180047a7baf9be8b2bf97ae5aa20048171db1a352a7190", "exact_match": 0.0}
{"doc_id": 2798, "doc": {"question": "what year did bennie and the jets come out", "answer": ["in 1973", "4 February 1974", "1974", "1973"]}, "target": "['in 1973', '4 February 1974', '1974', '1973']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what year did bennie and the jets come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1975"]], "filtered_resps": ["1975"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "86cf8568107c5427db735c025e545c645f6912fffa4de46a9aba3acc56979253", "prompt_hash": "048805f7775b11869c41910313beaa0c1a2e1474d95c2d6c600b7936d2fddf68", "target_hash": "4531e456fc04750f2fee074607c917d817c7fa4a4c57bfed2f1ed9b120d20191", "exact_match": 0.0}
{"doc_id": 2799, "doc": {"question": "the witch and the hundred knight 2 us release", "answer": ["March 27, 2018", "March 2018"]}, "target": "['March 27, 2018', 'March 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the witch and the hundred knight 2 us release?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "adef91b1c664295831f5b1a771b28554c0b2f99424f1c6af37b6b79f9db53ba0", "prompt_hash": "bbe8cd75c9bbf38585a68a6251b89ad39495d196da18b987b40700c3ff4ed320", "target_hash": "b6e88112f37b3fd0334e68e10c9f03a1700065518490236f2e8268fee2b56f6c", "exact_match": 0.0}
{"doc_id": 2800, "doc": {"question": "who plays jack skellington in nightmare before christmas", "answer": ["Danny Elfman", "Chris Sarandon"]}, "target": "['Danny Elfman', 'Chris Sarandon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays jack skellington in nightmare before christmas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chris Sarandon"]], "filtered_resps": ["Chris Sarandon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1463f2db1b0d0803c0d61efd5c14a9ea3e475c4309550e19a057b2b5b041af62", "prompt_hash": "4ada74f26a1d85c14e5d19261fa767fe3daba4a2324bcb6ace5c5079862799b5", "target_hash": "0c4f80c0530c63e09c88526f4faa64405e143d1a719f8b76c2522b5a7d5651ee", "exact_match": 1.0}
{"doc_id": 2801, "doc": {"question": "when was the last time the denver nuggets made the playoffs", "answer": ["2012–13", "2009–10", "1994–95"]}, "target": "['2012–13', '2009–10', '1994–95']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the denver nuggets made the playoffs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b0025419c740f0b7197b32d68eeb54ada9f64bb726944c2dde9d24471bcefa56", "prompt_hash": "7417d81ae530fe9464d139c1d2ab02743483007b1335953bfff897effc7010a1", "target_hash": "8401a29ef17c150341a56e164354bb8c727370d636eadc308ef1a4bb0731e1b4", "exact_match": 0.0}
{"doc_id": 2802, "doc": {"question": "who was the winner of the first indianapolis 500", "answer": ["Ray Harroun"]}, "target": "['Ray Harroun']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the winner of the first indianapolis 500?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ray Harroun"]], "filtered_resps": ["Ray Harroun"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "221f5b30c2366ec597b2414aac02da86ff31bba04cb3f139b5c5e4eb3e095024", "prompt_hash": "d60b5bcb577c327e3d141769876c76378cd17a2c051f41fd00a265ca55b6ebf6", "target_hash": "7566889759ade4691fbf0702e0383bfca897770ed20ff413f7d0ecd2ed14544e", "exact_match": 1.0}
{"doc_id": 2803, "doc": {"question": "when will the next episode of my next guest needs no introduction", "answer": ["May 31, 2018"]}, "target": "['May 31, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will the next episode of my next guest needs no introduction?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ce11f4c7a1bc3d98fa2053d34b5e3dbdfc268b8ad464268b28246c302d4c82e", "prompt_hash": "3b8865002f16705d66fc3a1c840d8db8fbfa8d3fb6672ca70c9c99a13c886de9", "target_hash": "da8e95349eefbaa7e9328a53e30af1855d1a27906fccc63b1449c13e38adbcf7", "exact_match": 0.0}
{"doc_id": 2804, "doc": {"question": "where does the last name hidalgo come from", "answer": ["Spanish"]}, "target": "['Spanish']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name hidalgo come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spanish"]], "filtered_resps": ["Spanish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "899e779a10cc5c72978f7eb53756c777080cc1495016417c6e74b08876fb6d8a", "prompt_hash": "c449c7112e03544679ff22b8b9d51331caa307a66bcdb6f86498182d61cd6c66", "target_hash": "542c2526bea4ce7a094e660928867975f00301abd0e8f05dc248c994a7aeb6f1", "exact_match": 1.0}
{"doc_id": 2805, "doc": {"question": "where in the bible is the book of revelation", "answer": ["last book", "in the New Testament"]}, "target": "['last book', 'in the New Testament']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where in the bible is the book of revelation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New Testament"]], "filtered_resps": ["New Testament"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e76c21d7f9cbee16eae31f1e49f6d6a2055438697c577adcd17ec5f2ea4158d9", "prompt_hash": "dc54fd9549fbb56755a2cc10a04c819fd170a1efd1a7d0b85f12457f88545f20", "target_hash": "dca69e4f812aba058d67a1d3db89612705a5f61bb41f2735ea93b7a76d3e894a", "exact_match": 0.0}
{"doc_id": 2806, "doc": {"question": "who has played in the most premier league derbies", "answer": ["Neville Southall"]}, "target": "['Neville Southall']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has played in the most premier league derbies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wayne Rooney"]], "filtered_resps": ["Wayne Rooney"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df1b8b49c0134c6967b08ef57cb44815934de37b202f2863ca11776b9de7579f", "prompt_hash": "adc62987762d44aeb5d2ce07328ee29264c6882823cc7bef701c8695747e093b", "target_hash": "2dcebfcb486ce2d5d54e71121edff454fff8550450ecddddd9f57c162d5cb174", "exact_match": 0.0}
{"doc_id": 2807, "doc": {"question": "who has become the new chairman of sebi", "answer": ["Ajay Tyagi"]}, "target": "['Ajay Tyagi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has become the new chairman of sebi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ajay Tyagi"]], "filtered_resps": ["Ajay Tyagi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "29c2423b56fc92cececd214fdfe3ac2802fa5d25db7266d1ceaf634af90fb99c", "prompt_hash": "0f0c32cd68c40cb9140148ea09b2811c865f127eca880e64a168fedd427fa62f", "target_hash": "4dc0a06132c3763c480262826ab19a800eb5b827b60d0873efef5e1df7ed0f4f", "exact_match": 1.0}
{"doc_id": 2808, "doc": {"question": "who present the central budget in lok sabha", "answer": ["the Finance Minister of India"]}, "target": "['the Finance Minister of India']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who present the central budget in lok sabha?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Minister of Finance"]], "filtered_resps": ["Minister of Finance"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "188c5aafea2c979e651d42c642396054c92d59642857bb82c3dbbc4a8180aadb", "prompt_hash": "938f5f3bd0b9fdd2f69bbc67fd6bd04cb000778d92e263ad3a27be6a387764c7", "target_hash": "cb166faf094774d916dd616a6536f31881345fe88bdf8e4db3792f14946f64e5", "exact_match": 0.0}
{"doc_id": 2809, "doc": {"question": "what is an example of a government monopoly in the united states", "answer": ["West Virginia American Water", "State Bar of Arizona"]}, "target": "['West Virginia American Water', 'State Bar of Arizona']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is an example of a government monopoly in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" post office"]], "filtered_resps": ["post office"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b5b099fbcc609eed7115272f58b9480a5370a6fd8216b9ecd13ffb51f1d0cb56", "prompt_hash": "ee5429a42333481bdbd462136226ca83291f7b0335a90469ce1d1c55d82456ec", "target_hash": "bc888527406b77679fdf08d96dc0f246122ddd6d413bef198270359ef3871754", "exact_match": 0.0}
{"doc_id": 2810, "doc": {"question": "when did world war one begin and end", "answer": ["28 July 1914", "11 November 1918"]}, "target": "['28 July 1914', '11 November 1918']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did world war one begin and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1914"]], "filtered_resps": ["1914"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "415d2cad75abd5ee547a818ed14d8efed0f803bc9855e7cfbd7930022506e3e5", "prompt_hash": "6de94fb058b331cff4e92bb84553712d9c0590a8e33c7eb19b8365a0b54abd7f", "target_hash": "ac46598f1381499f62f6e0086d38f9923faa138e40cec234abc41c53a694a457", "exact_match": 0.0}
{"doc_id": 2811, "doc": {"question": "who played ethel on the doris day show", "answer": ["Carol Worthington"]}, "target": "['Carol Worthington']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played ethel on the doris day show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" June McCarroll"]], "filtered_resps": ["June McCarroll"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f038c00612c49d2016349849b500ce3d992d64565f664b3e3571f5eac62084a8", "prompt_hash": "97b3aebc7781a97be22fa12eb6dd2e474d22a4d633ec877d74bb10980671273c", "target_hash": "5332892b1fec99278ab7cc54748046267e0e60170551afa84fcdc8002327444b", "exact_match": 0.0}
{"doc_id": 2812, "doc": {"question": "who wrote somewhere there's a place for us", "answer": ["Stephen Sondheim", "lyrics by Stephen Sondheim", "Leonard Bernstein"]}, "target": "['Stephen Sondheim', 'lyrics by Stephen Sondheim', 'Leonard Bernstein']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote somewhere there's a place for us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Lennon"]], "filtered_resps": ["John Lennon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b8286f0267930b7bb5c22cd8d1d10734b243e51ff76376e422014c0044ba91c", "prompt_hash": "e0a7abc31260f2cff95d0f6e16b633993040d7855d5210684fa739f06e908b71", "target_hash": "af092b19c2e8ce410aa06d41b0a7da133bf3e851bce1b8760a16ef53788ed5aa", "exact_match": 0.0}
{"doc_id": 2813, "doc": {"question": "where did the peanut butter jelly time song come from", "answer": ["Buckwheat Boyz", "The Buckwheat Boyz"]}, "target": "['Buckwheat Boyz', 'The Buckwheat Boyz']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the peanut butter jelly time song come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" YouTube"]], "filtered_resps": ["YouTube"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b2818a8fea1512fe8d283b8634cfd8ba35b2162b5e5b54af4aa31f174a987e5f", "prompt_hash": "0524207dcfc6e67452021e16504df319ed5df91f36d1a594b578fdf353f983c9", "target_hash": "452e09e933fa7a609410e6d97b842ba4434ae5b1abac4e1daa9d077951ae2373", "exact_match": 0.0}
{"doc_id": 2814, "doc": {"question": "who played the jewish man in coming to america", "answer": ["Eddie Murphy"]}, "target": "['Eddie Murphy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the jewish man in coming to america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arnold Schwarzenegger"]], "filtered_resps": ["Arnold Schwarzenegger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "70daf5dce3ac86b476c570348d9646323d2820f3bbcaa3110de781eabb9e3ae0", "prompt_hash": "b59a072b120e45ad34efb618ef86ae8596b94a857557b71c6c2c7149ab65bc31", "target_hash": "5a1efc27f762ed67b03e36a754a5736c8271c9602ade701e62cfb33f22343c59", "exact_match": 0.0}
{"doc_id": 2815, "doc": {"question": "who talks for belle in beauty and the beast", "answer": ["Julie Nathanson", "Jodi Benson", "Paige O'Hara"]}, "target": "['Julie Nathanson', 'Jodi Benson', \"Paige O'Hara\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who talks for belle in beauty and the beast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paige O'Hara"]], "filtered_resps": ["Paige O'Hara"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad33616feada38710e0ad74aea01dd44a294b533d77d48a6ad6dbc17cfe24240", "prompt_hash": "0c64636039ab28961bc7245012ac32571720652b39dfb5fc21715de56c7c23d7", "target_hash": "abd4d540bdee6ed9ac8381f96abff973c4aa383fdbfb69be0330c40a25673617", "exact_match": 1.0}
{"doc_id": 2816, "doc": {"question": "who has a ring of power in lotr", "answer": ["Sauron"]}, "target": "['Sauron']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has a ring of power in lotr?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gollum"]], "filtered_resps": ["Gollum"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9bfa0adf3139c5ca1c661d7af5d27a6d3cc1550f00e4b64f409d5f691dbeb4c", "prompt_hash": "99c368c31e46f5cf10e1cac21bde9caa4ef2f6909db8b2a92078507df149b7d3", "target_hash": "08d8647f626be00716d7c9eedb571299b082ef3665f4702de8828303a7ee5b49", "exact_match": 0.0}
{"doc_id": 2817, "doc": {"question": "who introduced the first chrismas tree to the uk", "answer": ["Charlotte of Mecklenburg-Strelitz"]}, "target": "['Charlotte of Mecklenburg-Strelitz']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who introduced the first chrismas tree to the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Queen Victoria"]], "filtered_resps": ["Queen Victoria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4eefbdb37a970396a3ca8b859e4ced05f9849c97d4e1036354696a5f24cfa2ba", "prompt_hash": "f11d5fa04c98e6e12367074ee1a2bb7b6749ecbadb540851acf1e99d746e6cc4", "target_hash": "b8c7be7138b4d09bbb0814a70cbaa6d7b7b3d9ca9292b41dde54ea3470d528ec", "exact_match": 0.0}
{"doc_id": 2818, "doc": {"question": "how many writes does a flash drive have", "answer": ["up to 100,000"]}, "target": "['up to 100,000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many writes does a flash drive have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1000"]], "filtered_resps": ["1000"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a4fd80a1a1fe3ec9dfd19a8a74ff7869b2632396cdde8900278ed1856894acc8", "prompt_hash": "401631ab42823e351f29f25c66bf645656b08e803043fc67aef69f6122e8e119", "target_hash": "3dc449d4a8f033003fe65b362d74368197d977a83f19a64aa7f5b717bc5ebc42", "exact_match": 0.0}
{"doc_id": 2819, "doc": {"question": "what is the inner core of the earth made from", "answer": ["some other elements", "iron–nickel alloy", "an iron–nickel alloy"]}, "target": "['some other elements', 'iron–nickel alloy', 'an iron–nickel alloy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the inner core of the earth made from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" iron"]], "filtered_resps": ["iron"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "71903796ccb09f81fb5189499ccb96af9466c828a395ba22798bb6e5b779cef8", "prompt_hash": "a0f51b7a9196f5db758df8da9020f350a9876edfaa3e6f64df2abcc8bdaa30f6", "target_hash": "db8d4942ffd240c01cf1c5a840c522a8dc25d33e857d91d8522b26a16b85fc46", "exact_match": 0.0}
{"doc_id": 2820, "doc": {"question": "how many episodes is season 4 of the flash", "answer": ["23 episodes", "19", "23"]}, "target": "['23 episodes', '19', '23']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes is season 4 of the flash?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 23"]], "filtered_resps": ["23"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6fc3f596ffea36208cca81d3eae161ff6ce6d52a782c46117e27dad5c87cddba", "prompt_hash": "16841ff2473e9533593e293789161e69c9596a12a033f06b4e2d3afbf1dd69b0", "target_hash": "0efdd6bee4c62b5f616da9465a94445dda7a3017f9392fe035de9c4df9616978", "exact_match": 1.0}
{"doc_id": 2821, "doc": {"question": "who plays in the new mary poppins movie", "answer": ["Meryl Streep", "Nathanael Saleh", "Emily Blunt", "Colin Firth", "Joel Dawson", "Emily Mortimer", "Lin-Manuel Miranda", "Ben Whishaw", "Pixie Davies", "Julie Walters"]}, "target": "['Meryl Streep', 'Nathanael Saleh', 'Emily Blunt', 'Colin Firth', 'Joel Dawson', 'Emily Mortimer', 'Lin-Manuel Miranda', 'Ben Whishaw', 'Pixie Davies', 'Julie Walters']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays in the new mary poppins movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Emily Blunt"]], "filtered_resps": ["Emily Blunt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ded81df1801935716cec9c3c5ca46454b0d711b759ffbc0e6fd4cdd97e5982a", "prompt_hash": "fe3ee92592887f878ea617353ab1548d2f46a6896f6d64a46c3f893b3ed397f2", "target_hash": "388cb6bef698bcc81ff45d85abe59d3f4c886a6e7ee5cb3d1adcecdc3f20887a", "exact_match": 1.0}
{"doc_id": 2822, "doc": {"question": "the old man and the sea page count", "answer": ["127"]}, "target": "['127']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the old man and the sea page count?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 109"]], "filtered_resps": ["109"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "566a76c43fcb142827c824458fd0b24134637470cc8afca9acf8e96c52e2d273", "prompt_hash": "6908c759d26464e9366c20c74df22ec0329a6476c59842af8059cdeff82219d1", "target_hash": "670a5ca3621f3bd051912e521074030851a4a11429b89519a65a5112bbb47e46", "exact_match": 0.0}
{"doc_id": 2823, "doc": {"question": "mount and blade with fire and sword time period", "answer": ["1648-51"]}, "target": "['1648-51']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: mount and blade with fire and sword time period?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" medieval"]], "filtered_resps": ["medieval"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d36bce052d37f43cb8551ae7eda8efc9385e6d62cf4ed9b08f27a21c8827f8e", "prompt_hash": "f15378d0ad7ce799c492e5c5ad4593b68505a010920b73b4d673df0c7096eb23", "target_hash": "0ddc9736ae2f7db7675ddd7860ca798b4156a2cbc9d128717a52b64819dadadb", "exact_match": 0.0}
{"doc_id": 2824, "doc": {"question": "nearest metro station to gandhi nagar in delhi", "answer": ["Shastri Park metro station"]}, "target": "['Shastri Park metro station']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: nearest metro station to gandhi nagar in delhi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gandhi Nagar station"]], "filtered_resps": ["Gandhi Nagar station"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d07c022880b68109453722e3e474264259c4a7666d6aa0a593e4355a7e510e11", "prompt_hash": "ca22b003ca7f07f77c5e5b185541732c3e3594e3276f3ca9beb4be1f8bab8b04", "target_hash": "02cb23af2217a114085a666c1fc35f362af112533ed06b84adc6d77a45a2ee4f", "exact_match": 0.0}
{"doc_id": 2825, "doc": {"question": "when was looking for a sunset bird in winter written", "answer": ["1923"]}, "target": "['1923']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was looking for a sunset bird in winter written?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1991"]], "filtered_resps": ["1991"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "de4f8b18d9926e2953f260a94b43471dcb23964db209e4f2b1ae5726e1955689", "prompt_hash": "dc31f258fab687e7f2552b354e553a303d5dd6bf939263d7c114de3235c35d64", "target_hash": "137ba74f3b64278e1d3c962cd05a4a2afd554a8be58641ee162988ff5de77a24", "exact_match": 0.0}
{"doc_id": 2826, "doc": {"question": "where are the winter olympic games being played", "answer": ["Beijing"]}, "target": "['Beijing']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the winter olympic games being played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pyeongchang"]], "filtered_resps": ["Pyeongchang"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dceeeb90de5be85ccae1831db3d1c7fbc283caa182f07885c6b286f5dbe387eb", "prompt_hash": "b923cb4705c116b693b67d980046bb31889ba4a1deecadf6c1954a871d3bd8c2", "target_hash": "2ebaddd7275884dd95a344be5c006341be06d6043599420b08a7902ad486a4ba", "exact_match": 0.0}
{"doc_id": 2827, "doc": {"question": "why was the plague that struck athens so devastating", "answer": ["close quarters and poor hygiene"]}, "target": "['close quarters and poor hygiene']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why was the plague that struck athens so devastating?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Athenians were not prepared"]], "filtered_resps": ["the Athenians were not prepared"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ad415293f177f03b66fa367737a6d2e2e57d9331a5ba7e5cbfd3ef39850e81c", "prompt_hash": "ed63f7703c896b805e0b1ce7ceb8d809dcb1054d4b20ac36cd5d10f99a269335", "target_hash": "2c15dd0b3c383217d7101daad66aed8f9e97dfbe79ed5a8a174c252e18f216ec", "exact_match": 0.0}
{"doc_id": 2828, "doc": {"question": "baseball teams who have not won a world series", "answer": ["Washington Nationals", "Tampa Bay Rays", "San Diego Padres", "Milwaukee Brewers", "Seattle Mariners", "Texas Rangers", "Colorado Rockies"]}, "target": "['Washington Nationals', 'Tampa Bay Rays', 'San Diego Padres', 'Milwaukee Brewers', 'Seattle Mariners', 'Texas Rangers', 'Colorado Rockies']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: baseball teams who have not won a world series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Texas Rangers"]], "filtered_resps": ["Texas Rangers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "30d1c31d500c7ec843b2fc1e81394090fa7d95f0b040a55bcbaa265da99f6981", "prompt_hash": "cdd9efcba6c69699f998e9023140b05347c3419bcddeddce691894c5c84c676b", "target_hash": "de6aa417b7f0a6f7344afdf18e6877470f57b63a6fd0a3de0a766ef5874f56b7", "exact_match": 1.0}
{"doc_id": 2829, "doc": {"question": "who wrote ain't living long like this", "answer": ["Rodney Crowell"]}, "target": "['Rodney Crowell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote ain't living long like this?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dwight Yoakam"]], "filtered_resps": ["Dwight Yoakam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8783b53b94778d2586a558bdbb8a47946d9b554504c5fb707576be5041d861e2", "prompt_hash": "f8df0f8a1b32d3c7832e3e3e988b7f7f942ba2489eb9e23eeca92f5360cbf601", "target_hash": "c63d58eb089f52d8df7237aeaca7759e6989965d4522b574fdb8b50dbc14bfb6", "exact_match": 0.0}
{"doc_id": 2830, "doc": {"question": "what group of animals do octopus belong to", "answer": ["class Cephalopoda"]}, "target": "['class Cephalopoda']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what group of animals do octopus belong to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" molluscs"]], "filtered_resps": ["molluscs"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b656752b50b19dcae2f640740b25c34b752fca698bd2a9651a3e2644c392107c", "prompt_hash": "f3bb3950a63115156c28b235e2d1a40399fbc5a7af09cdb1621d42fc4146e8eb", "target_hash": "2ea3089da7f60cfff9772b0954d2152c4e803848e4bf5c1fc12728465b2bb95b", "exact_match": 0.0}
{"doc_id": 2831, "doc": {"question": "where is the new football stadium in atlanta", "answer": ["Mercedes-Benz Stadium"]}, "target": "['Mercedes-Benz Stadium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the new football stadium in atlanta?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mercedes-Benz Stadium"]], "filtered_resps": ["Mercedes-Benz Stadium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e7165bcf123e61408420757612fc48717afea93179f1fa31e82b028b56e84769", "prompt_hash": "545d2ef014599b19558d8136670a6e4a06fcffbeb66831678f1345f3f7e34645", "target_hash": "e17fd59d093242c0ab68c2864570eebc8e8b2f103ab453811e6d8d2c515c756a", "exact_match": 1.0}
{"doc_id": 2832, "doc": {"question": "when was the last god of war made", "answer": ["2018", "March 12, 2013"]}, "target": "['2018', 'March 12, 2013']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last god of war made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" God of War III"]], "filtered_resps": ["God of War III"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "29120be5ecf165c3edd4b8653882657e6508bf44c64518cd4a1748579203801c", "prompt_hash": "722ddf914e59584a98d452426c8c55f7185037c6c87c424dbb6ba76a99141a1d", "target_hash": "733a7408fda15ca4dd16cd3787e91db294e9015ec932a6d4d6383e062f56fe8a", "exact_match": 0.0}
{"doc_id": 2833, "doc": {"question": "what are the zords in the power rangers movie", "answer": ["animal-themed Ninja Zords"]}, "target": "['animal-themed Ninja Zords']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the zords in the power rangers movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Megazord"]], "filtered_resps": ["Megazord"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4a5361019d67a7eafa956665e979a6910b2a2b901df4e6459c9f04f0f82aca76", "prompt_hash": "fe6a2a1484d53c4c6c51feec8a4213be838391525819864629ee1704f19ff5d5", "target_hash": "1d2c687b24b00da68ac285548b672c4c9de87de634fe18a9ff41ab083004a2dc", "exact_match": 0.0}
{"doc_id": 2834, "doc": {"question": "why the hole in the ozone layer over antarctica", "answer": ["bromine", "chlorine", "methyl bromide", "CFCs", "halons"]}, "target": "['bromine', 'chlorine', 'methyl bromide', 'CFCs', 'halons']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why the hole in the ozone layer over antarctica?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chlorofluorocarbons"]], "filtered_resps": ["Chlorofluorocarbons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b07c54eeddd81ba765cadba2b63f9bbe8c0bd788e71987bfb5b79efe5db1b322", "prompt_hash": "0127f009756fb295365d2499a9fbb21deafe997a9a695ad6eadb9948cbebf215", "target_hash": "9f800e32d604faf1a3b05ca18497310dddddab643c9e5bb6f5971ee06eba324b", "exact_match": 0.0}
{"doc_id": 2835, "doc": {"question": "where does the last name waller come from", "answer": ["Anglo-Saxon origin"]}, "target": "['Anglo-Saxon origin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name waller come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" German"]], "filtered_resps": ["German"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f44c3e1930badd7b56a83c449c9c0d976e3158dc57e5dd191f32a63e67c527e6", "prompt_hash": "0fe7ffdbf5725e6ca4de3e9587c02eec9e71b29266d3f7f9c679b8650c5902c7", "target_hash": "a8788402c611f8d132699ad1a6dabc84d52d5b50c36338aaa565ee504d915ba9", "exact_match": 0.0}
{"doc_id": 2836, "doc": {"question": "who wrote the music phantom of the opera", "answer": ["Andrew Lloyd Webber"]}, "target": "['Andrew Lloyd Webber']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the music phantom of the opera?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Andrew Lloyd Webber"]], "filtered_resps": ["Andrew Lloyd Webber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cc9698bbe43d9683442336eadd774124bcd79b3f3d89fbad54afbfa9406cff54", "prompt_hash": "1967d976eef9aced76e1669706f2e9934db057fd68529a70d83dfb6e762021ad", "target_hash": "219ffc6e3d042e3362d23b463feba1d180877f8e51518a9582b4d306921734f3", "exact_match": 1.0}
{"doc_id": 2837, "doc": {"question": "when's the last time michigan won a national championship in basketball", "answer": ["1989"]}, "target": "['1989']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when's the last time michigan won a national championship in basketball?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1989"]], "filtered_resps": ["1989"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "88ea26c9227c8017f6be34ed71c50e353215e5249695b0614be13dcc986655c4", "prompt_hash": "2c93568f1b9a9b4dbb23b3b0d0439ca9d8e2627def69597ee4f6360e30785f23", "target_hash": "f29cbf85fafcf80ace6273501b873d82d249a9b85142c12822207ef416c8728f", "exact_match": 1.0}
{"doc_id": 2838, "doc": {"question": "who won the icc under 19 world cup 2018", "answer": ["West Indies", "India"]}, "target": "['West Indies', 'India']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the icc under 19 world cup 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" India"]], "filtered_resps": ["India"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fec6f9bb9a8383ab106f918077e924ffe6946fd9d738046554578a74d70ef512", "prompt_hash": "36ef9df6577e1e797cfec56155fb1a52b1392a54964827c5465642c3e2791739", "target_hash": "2d4085e74f2fc13e01344b51991528354c999a0a78a1e0f1c1b85bce34ee3c68", "exact_match": 1.0}
{"doc_id": 2839, "doc": {"question": "in the honour of which god is anant chaturdashi celebrated", "answer": ["Ganesh"]}, "target": "['Ganesh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in the honour of which god is anant chaturdashi celebrated?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vishnu"]], "filtered_resps": ["Vishnu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3334ec4c12afdf5d8a9172d18d489b19bffbc3e467a686f2315e98ca23b7d8da", "prompt_hash": "fde48f7837fa8cb6534028420b56331c78dc10a9af945afc4c95d761bf0bfa79", "target_hash": "9c8be3aae2c57803d8aaf723a6927b89378a05543f99a5442c5f6e35b41e115a", "exact_match": 0.0}
{"doc_id": 2840, "doc": {"question": "when did the us stop trading with japan", "answer": ["1939", "1940"]}, "target": "['1939', '1940']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the us stop trading with japan?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1941"]], "filtered_resps": ["1941"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2070bd78dd8b949f30d2df7fdbdb5bbf753142387e5ace362df9f7a9ef988d08", "prompt_hash": "6909285a02b5454eebe252fc407a99473cf4f0a48870a395e06235cdd94b734b", "target_hash": "4bd0a42260f823c3542675e2d21bde27ac4d2b8b1074f7ffe73e2f3c48f89502", "exact_match": 0.0}
{"doc_id": 2841, "doc": {"question": "what territory is spain and the united kingdom in a dispute over", "answer": ["Gibraltar"]}, "target": "['Gibraltar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what territory is spain and the united kingdom in a dispute over?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gibraltar"]], "filtered_resps": ["Gibraltar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "444d9300fefb7601dd7949aabd273f1ea42857f181931969f042a721f37a98c8", "prompt_hash": "d4dda0facddef56cf0847ef19e574661db65bc3663ccb7d304e88683452320e7", "target_hash": "0b2093e7ecf7a645c09e8ae2cda54fbff5fa5d056bf4e135b2fe12a2ec975e7b", "exact_match": 1.0}
{"doc_id": 2842, "doc": {"question": "when was you'll never walk alone first released", "answer": ["1945"]}, "target": "['1945']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was you'll never walk alone first released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1963"]], "filtered_resps": ["1963"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "93a4e571769f55999c4ebba04bc1001c56203ff7b02dfb9641fba1e8b230eeac", "prompt_hash": "628cd52b3e1dce5baa09ab308d5ef36aca687f1e77c5d2a826b2b823556f55f4", "target_hash": "a4ccf6516c2d80e68c765c49139a5a9f6cd3fb06aded9ea5366468369ddac125", "exact_match": 0.0}
{"doc_id": 2843, "doc": {"question": "atlantic ocean's shape is similar to which english alphabet", "answer": ["S"]}, "target": "['S']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: atlantic ocean's shape is similar to which english alphabet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" M"]], "filtered_resps": ["M"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9a79545de97d990757f9b09dc69d869ec78467ac25d4ab491dd76d30265db455", "prompt_hash": "7b5535a9930909e3d90f5662f68c2f74d67fa54a1fde2b366dd0116e39296077", "target_hash": "743c2bbb3f0160d3f48cc7417cb10db6d29852976c1c3889baa3190e66b28645", "exact_match": 0.0}
{"doc_id": 2844, "doc": {"question": "where did the titanic sink at what ocean", "answer": ["North Atlantic Ocean"]}, "target": "['North Atlantic Ocean']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the titanic sink at what ocean?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Atlantic Ocean"]], "filtered_resps": ["Atlantic Ocean"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4afd9f9e46d9ad9da2249ed356fcce2b315a963ca101091d4ee34bf8bbdde621", "prompt_hash": "82b5b781859320276b26a33b6dbe9b4f00e9493f0e30e07c0709c5ebe65ec850", "target_hash": "52050eedd2edbf23a2a76867576d62f808d09e998c236c0b2306407de506c021", "exact_match": 0.0}
{"doc_id": 2845, "doc": {"question": "dogs name in the grinch who stole christmas", "answer": ["Max"]}, "target": "['Max']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: dogs name in the grinch who stole christmas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Max"]], "filtered_resps": ["Max"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44d72c6d8d009e7124b8e0b22862fdec1ea75e23500a95fc214e520676fe3fbc", "prompt_hash": "149a959e53ff01439cdaa8c06e87e7bd13be7d2667d5630507c061ae586b6545", "target_hash": "85f65cf95b7c147f1730e2a57fb0a6c6d35e04c846b9a6083cef81a8851ad0c9", "exact_match": 1.0}
{"doc_id": 2846, "doc": {"question": "how many gold medals won by india in 2018 commonwealth games", "answer": ["26", "66"]}, "target": "['26', '66']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many gold medals won by india in 2018 commonwealth games?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfc9354e5f5ecf96df0f495067c754f91838650c49b90f3029d9f854a667412a", "prompt_hash": "a140b1b2f8fe0c3cebe6202e23a21d6d3982fd2fa2856d144d86e9929f690b15", "target_hash": "107b1e60eeb68af6c127bacf1668697395886c3585c3ff8518df257cab5ba95d", "exact_match": 0.0}
{"doc_id": 2847, "doc": {"question": "civil war what did the north fight for", "answer": ["maintaining the republic", "loyalty to the U.S. Constitution"]}, "target": "['maintaining the republic', 'loyalty to the U.S. Constitution']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: civil war what did the north fight for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" slavery"]], "filtered_resps": ["slavery"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e7bb62d0af5d5613ffc14f38e63e67b9be1c91ceb29aaa5177ff5065faffc7d0", "prompt_hash": "8e95351020585fe907d5293c41f769dd747ef51708c64e4a0c78864022837ec2", "target_hash": "2810eb984c393ad321274d0876d02d1f97744d98311a1a284921451eb84ca53c", "exact_match": 0.0}
{"doc_id": 2848, "doc": {"question": "when does the next episode of rebels come out", "answer": ["February 19, 2018"]}, "target": "['February 19, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the next episode of rebels come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 13"]], "filtered_resps": ["January 13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "adb735dfccaa5a4d09abe98969a273d17e59cc9798153a6715301f08d3a6cac9", "prompt_hash": "51d7ca506d39804837fbd5a62338f2472af102f06659b69ee82a3fa41d044ea2", "target_hash": "7e045dcf2f7743ec0ae85823205992023e8a27f9fbcd20b7c45ccb2e8fe5c2a7", "exact_match": 0.0}
{"doc_id": 2849, "doc": {"question": "who dies in transformers revenge of the fallen", "answer": ["Ravage and the Decepticon Rampage", "Jetfire"]}, "target": "['Ravage and the Decepticon Rampage', 'Jetfire']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who dies in transformers revenge of the fallen?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bumblebee"]], "filtered_resps": ["Bumblebee"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb413c7451ec19b62af5990aff32964c42d7bff77ac09e7794ee418d8538a7ff", "prompt_hash": "213f9ac37a97f29eb9b8d399b9c106ad0f7adb44ebd99e39134d08502928918c", "target_hash": "2294b4ba730976597efe57058f9c7b1c5df1e135dc09dee13f339cb81d07696b", "exact_match": 0.0}
{"doc_id": 2850, "doc": {"question": "perth is the capital of which australian state", "answer": ["Western Australia"]}, "target": "['Western Australia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: perth is the capital of which australian state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Western Australia"]], "filtered_resps": ["Western Australia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4772184331f823b6ebbb20b26affd40119edb467d16dd35628787b562c2c8d3f", "prompt_hash": "f0216d54a28783ff634ce51f499506086054208f0a1a4f9ddf161c7208de7e8e", "target_hash": "16477bf7db9989ad8d96ba8ed23007754a61907cfaab4c58cccfc771f27ec149", "exact_match": 1.0}
{"doc_id": 2851, "doc": {"question": "who gets the golden boot if its a tie", "answer": ["shared"]}, "target": "['shared']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who gets the golden boot if its a tie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cristiano Ronaldo"]], "filtered_resps": ["Cristiano Ronaldo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58f377b36a0943225e0600478ae2e4b26b5918d2a15a2959a72c6fec34a5f5d6", "prompt_hash": "09eeb6162a718cb22e60cc3c79f0e70e79e9ad9e4dce6226494f361041ed6393", "target_hash": "8f9f9e8b6bc83b64085e636dde5ba6a20d2a4683a89520e84847991b82463ecf", "exact_match": 0.0}
{"doc_id": 2852, "doc": {"question": "three movies made from agatha christie's novels", "answer": ["Alibi", "Die Abenteurer G.m.b.H.", "The Passing of Mr. Quinn"]}, "target": "['Alibi', 'Die Abenteurer G.m.b.H.', 'The Passing of Mr. Quinn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: three movies made from agatha christie's novels?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Murder on the Orient Express"]], "filtered_resps": ["Murder on the Orient Express"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "445b410d73e3c25008e2601f433cc1c611025a3809ae3893fe554e861c1122c7", "prompt_hash": "69b54993af8f8b7f40bb84a2d3ae10ad283f3f23ac4fde2d56e8aa355b4bbbba", "target_hash": "b791becc8913e706bc22be1bd01a22b1397d61e91c80a6810f01524645059208", "exact_match": 0.0}
{"doc_id": 2853, "doc": {"question": "when does a cell have condensed visible chromosomes also known as sister chromatids", "answer": ["metaphase"]}, "target": "['metaphase']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does a cell have condensed visible chromosomes also known as sister chromatids?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" during mitosis"]], "filtered_resps": ["during mitosis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "187f16a036311e1516f54552f8d82b4fe3d755094da19a72b2adecc950b6bb6b", "prompt_hash": "63feac6f16a90aceba6d88e9cc3dab06bfa5ab4b6f22ff9f80d03f5d6c351305", "target_hash": "579c3d521b42800d71f03cab05619804aeea807de32589dbbc00e4fbee3ceb8f", "exact_match": 0.0}
{"doc_id": 2854, "doc": {"question": "is greenland part of europe or north america", "answer": ["North America"]}, "target": "['North America']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: is greenland part of europe or north america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" North America"]], "filtered_resps": ["North America"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1d8efaa6f37270837eab4a1122e7606936f2b3caf6cc13be9cc47fb0cd43c759", "prompt_hash": "80d64dcad40de5bcee11f06938f11b69cce71b7b9a0de2c06135c4648fddfe30", "target_hash": "806f3201949145231245313a3cd88ac1c074eab1cd9fb64c806cfc9bb78f8a29", "exact_match": 1.0}
{"doc_id": 2855, "doc": {"question": "the boiling point of water is 100 degrees celsius express this in si units", "answer": ["100 °C"]}, "target": "['100\\xa0°C']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the boiling point of water is 100 degrees celsius express this in si units?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 100 degrees celsius"]], "filtered_resps": ["100 degrees celsius"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b27d70a6bd55e82ebadaa4e73cf6b8d9dea3dd83468ba9de62aec35e62459c5a", "prompt_hash": "c29bd9074b5568807348464950823216215e66f5bbd562b6151c89936d07c523", "target_hash": "55914627aa24ab2396f5b061e4de42847bdca1b67a352b9a03df6308a4158797", "exact_match": 0.0}
{"doc_id": 2856, "doc": {"question": "what's the nba record for most points in a half", "answer": ["59"]}, "target": "['59']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what's the nba record for most points in a half?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 79"]], "filtered_resps": ["79"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "48285496709e8900b263b5eafd3e7d74c30eee99a3919ce29246c85c1aa795fb", "prompt_hash": "4a14d93dddf53388b3997cdc7d4b5870a86edd2692c319388d8bb1285f808006", "target_hash": "e8835bf5a28f8a8d67799479ffe2e5734f316d2e6788de288bd8f185618d39a2", "exact_match": 0.0}
{"doc_id": 2857, "doc": {"question": "where does the last name tavarez come from", "answer": ["Spanish"]}, "target": "['Spanish']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name tavarez come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spanish"]], "filtered_resps": ["Spanish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b197ae73970f90c43e7dc58682fb4e9a0015b6406e8cade3a2c3d370c9068597", "prompt_hash": "c5a39f717e14bc39f0f0a0bf9cfea04180e1ac51a19aac114ee48e7a614ed33e", "target_hash": "542c2526bea4ce7a094e660928867975f00301abd0e8f05dc248c994a7aeb6f1", "exact_match": 1.0}
{"doc_id": 2858, "doc": {"question": "who owns and operates the panama canal today", "answer": ["Panama Canal Authority", "Panamanian government", "Panama"]}, "target": "['Panama Canal Authority', 'Panamanian government', 'Panama']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns and operates the panama canal today?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Panama Canal Authority"]], "filtered_resps": ["Panama Canal Authority"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c9a116954b0fda3f651ead6846c9ad129d8021e1c0b96e669a76000dacdb9f9b", "prompt_hash": "5886df6c2c5286d0071859c6c7bc757ec0424c1d8acf83536107ff6bb875517c", "target_hash": "d327c6d57628597496b0fee16f84dbde2375d7772eefd9c6185f771e90dddb95", "exact_match": 1.0}
{"doc_id": 2859, "doc": {"question": "the vast interior rural area of australia is known as the", "answer": ["The Outback", "Outback"]}, "target": "['The Outback', 'Outback']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the vast interior rural area of australia is known as the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Outback"]], "filtered_resps": ["Outback"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d91262534616bf443151229220a04866098bc821035f131b8f2cd95aeec18880", "prompt_hash": "847a5df92be2822bd2556351022cecc0fc3f7c03e0bf9759ba25d3224b6d55ef", "target_hash": "0ba83b303a220e7f2ddcd7d34cfdfe3a7f590ea9988333d6b75077683dca7e7e", "exact_match": 1.0}
{"doc_id": 2860, "doc": {"question": "who sings god gave rock and roll to you", "answer": ["the British band Argent", "Petra", "Kiss"]}, "target": "['the British band Argent', 'Petra', 'Kiss']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings god gave rock and roll to you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Elvis Presley"]], "filtered_resps": ["Elvis Presley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e8aaada933c6df72b954a18f038a7dd1fad98a45034593683cd5e3af97c38e13", "prompt_hash": "9aeea558293b926a3277d82d7ec88022b190b0883583264b53cacd6cad5700ba", "target_hash": "c10edf9ea6dc5367cbe78497f1899c43c417d6e930a1eb6e6cd30b7967c2ee31", "exact_match": 0.0}
{"doc_id": 2861, "doc": {"question": "who shot first in the shot heard around the world", "answer": ["Americans acting under orders"]}, "target": "['Americans acting under orders']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who shot first in the shot heard around the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" British soldiers"]], "filtered_resps": ["British soldiers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a261455aea1868d4d2caa1e4c9ddd6380109cee8432c5c7496c33867b6b05532", "prompt_hash": "edbdb19b1224dea3076255a26843f135efb3da3224dadb0c5e8d27983d191cc5", "target_hash": "5045a9475941bcb0e616535b85ff5c80736fdb3d54072a87fa3f79120dd6eb15", "exact_match": 0.0}
{"doc_id": 2862, "doc": {"question": "who has won the 2017 mens singles mutua madrid open tennis", "answer": ["Rafael Nadal"]}, "target": "['Rafael Nadal']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the 2017 mens singles mutua madrid open tennis?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rafael Nadal"]], "filtered_resps": ["Rafael Nadal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91717c2f61e3978b93fc40c7af5bfe8aaf07db73b148432dc3c8c1a5f655bef5", "prompt_hash": "c85aeb269ddf508dd938f0418f1b6806c8196404583b1b8264cbd252b5f09a15", "target_hash": "b7d195251a4b3f626fc43d60f7a8f31653df1dc358e3c647ec40669c451bcf3d", "exact_match": 1.0}
{"doc_id": 2863, "doc": {"question": "method used by a writer to develop a character", "answer": ["Anthropomorphism", "Personification", "Hamartia", "Pathetic fallacy"]}, "target": "['Anthropomorphism', 'Personification', 'Hamartia', 'Pathetic fallacy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: method used by a writer to develop a character?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dialogue"]], "filtered_resps": ["dialogue"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9ff42c8607871771ba7e6a48f1f12005263588881578e187ff4ed3b7dfc62617", "prompt_hash": "23045b48f2cd141e7db2b26b087e55efc1f7702b9b962f86053cb53fe8ae4d5b", "target_hash": "2b8a262e0bd58b78fc3a002f7bc4482bc08616f7ce17efc6013b5d5e2b06ef14", "exact_match": 0.0}
{"doc_id": 2864, "doc": {"question": "when did linkin park release one more light", "answer": ["May 19, 2017"]}, "target": "['May 19, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did linkin park release one more light?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012"]], "filtered_resps": ["2012"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "da3bb0b543ee794a2fb25d7fde2b293ca0571eb6fcadd9a74131e2b6f0a6eac3", "prompt_hash": "86842898bae7e56fd5e4daef463f1cd5d213b3ef20f9cb14d68f6ed4d91e944c", "target_hash": "bcfb4f68a04556cf4b9e33daaa9aee74525344c88bb1c71fa7c26cabc587006b", "exact_match": 0.0}
{"doc_id": 2865, "doc": {"question": "created propaganda to support the war effort & encourage the sale of war bonds", "answer": ["the government"]}, "target": "['the government']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: created propaganda to support the war effort & encourage the sale of war bonds?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the government"]], "filtered_resps": ["the government"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "941f3dd592eb51fed8be3163de4245f5b9dbdded9729abea4473782a4dc0213c", "prompt_hash": "ab761a7386d74f76f4b51e38bb7c7473e6412de10b22bfe6ea7d3b16f5cd03ce", "target_hash": "51eeb5cab09e0ba173f7e9d08b6b8e0bf0561f03602706e84dddb41d96f39705", "exact_match": 1.0}
{"doc_id": 2866, "doc": {"question": "what us state forms the western boundary of montana", "answer": ["Idaho"]}, "target": "['Idaho']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what us state forms the western boundary of montana?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Idaho"]], "filtered_resps": ["Idaho"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "567158708d44bd9de055a6df76f9b5bf7a7e533d1b9304526357858a31604b0f", "prompt_hash": "b3f70766f98bf9480912cabec9045f850c21fcf125867bc56237cddae22c215f", "target_hash": "f834701ae5dfba1b4e292eeb9fb85e1b7192084eacb0fe6cbca2a7d820058ca3", "exact_match": 1.0}
{"doc_id": 2867, "doc": {"question": "where does the edinburgh fringe festival take place", "answer": ["in Edinburgh, Scotland"]}, "target": "['in Edinburgh, Scotland']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the edinburgh fringe festival take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Edinburgh"]], "filtered_resps": ["Edinburgh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ea0e213686df24ad612fc3817372476f2d93b507a4a9a8906089c7860b835ab", "prompt_hash": "511d3d64d525c492f5ab7b0892701dad12dd2619848f6dc17ac2887930e94a91", "target_hash": "d81a96ff9c89ebff89ab1964bdcefadbb0d18890c25386259a33b866c263828b", "exact_match": 0.0}
{"doc_id": 2868, "doc": {"question": "first dynasty to issue gold coins in india", "answer": ["The Gupta Empire"]}, "target": "['The Gupta Empire']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: first dynasty to issue gold coins in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vijayanagara"]], "filtered_resps": ["Vijayanagara"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "104bd58c1796e507da9976983ec8ebf561fcb3c9cbf8b0666272ccaf230923ee", "prompt_hash": "b4f1ad0c35f29bb360a1dd6d9ee35527489c2105c560f98fb52b0b4ea9de30e0", "target_hash": "6571349e98ce5180693b05763fa91b6f85d672c0c47ee0c560d880e12d86aaef", "exact_match": 0.0}
{"doc_id": 2869, "doc": {"question": "how many games does a football team have to win to be bowl eligible", "answer": ["5"]}, "target": "['5']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many games does a football team have to win to be bowl eligible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" six"]], "filtered_resps": ["six"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "668b328b036ebd2e248c38387a2a7fdeb5f869c902072e239f263aa6c72941be", "prompt_hash": "4eb99e850078af40caafcca32b3bd72cede8c67eb7311043ac229231e164014f", "target_hash": "494a843c2b5d43b794ff50c14c5647bf4ff556a365c58f564af6c235a53602e3", "exact_match": 0.0}
{"doc_id": 2870, "doc": {"question": "where does the journey start in the canterbury tales", "answer": ["London"]}, "target": "['London']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the journey start in the canterbury tales?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Southwark"]], "filtered_resps": ["Southwark"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "85d505dcdeff2c116c5a57b3f4680a4b7a870f40ef126f212f4eb02f82de9d71", "prompt_hash": "1356fb8bb81a49a93bc9086d3f7d8ba159fe692104b3aa8e10dd7c4e7bd897ae", "target_hash": "76591b5b3bd1e8c779278746b1bc0bd4d5be42a7c3d5c52acff7ca21f908354c", "exact_match": 0.0}
{"doc_id": 2871, "doc": {"question": "when was the first time lebron went to the finals", "answer": ["2007"]}, "target": "['2007']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first time lebron went to the finals?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2007"]], "filtered_resps": ["2007"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "31127ab57223cb071a574b4aa9193b0d0b2aeb0ee1ae73fc2cf6dbe975873675", "prompt_hash": "4b4b156d7da5ce0c72137bc7f200cddb6a296bf634c06bc4e2340181b592180b", "target_hash": "168cf105f77be5aea9191ddc62c60660b18032919072f8a487cbb3b43a480193", "exact_match": 1.0}
{"doc_id": 2872, "doc": {"question": "who has made the most premier league appearances", "answer": ["Gareth Barry"]}, "target": "['Gareth Barry']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has made the most premier league appearances?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wayne Rooney"]], "filtered_resps": ["Wayne Rooney"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "544c21327fff64a2380f025900910fd2c750fcfcfa2f61f54be42aaa1cd591bb", "prompt_hash": "000f742cbd7c6ce21f7841b79c55b2d6aeabdcc87b6e1bdf3b7d12df949fa544", "target_hash": "8205df73a2b476b4ba0bc459c874003ff3071789caf93a54038f9f2bf1010921", "exact_match": 0.0}
{"doc_id": 2873, "doc": {"question": "when was the last time it snowed in england on christmas day", "answer": ["2009"]}, "target": "['2009']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time it snowed in england on christmas day?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1875"]], "filtered_resps": ["1875"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "090ec6ab13221ed82cfb8bfd008d5c77a8e5dfe4877982346385410b9dcb4901", "prompt_hash": "08f4f7f79d0dd048a7c43e9b38626244ca15f7a41b0d2287d0dc1b78716db17d", "target_hash": "785f41b900e53a7e9a110e70b294735b0cb9d4d9bacc74be5ad8e56c8a243d27", "exact_match": 0.0}
{"doc_id": 2874, "doc": {"question": "when was the first star wars film released", "answer": ["1977", "May 25, 1977"]}, "target": "['1977', 'May\\xa025,\\xa01977']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first star wars film released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1977"]], "filtered_resps": ["1977"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "50cc938e858279d71ca44834d0a42b2a562f803991c54f9ad5f71d5da935379f", "prompt_hash": "323e32423cb3a89744c6f97360a0dc42da459a2f5d965d1dafe954b56bd5d4e8", "target_hash": "2c63d4a929de0a905382cc64e1d6d03a53a3a3ce519848fd8edc06f3e4ab148a", "exact_match": 1.0}
{"doc_id": 2875, "doc": {"question": "when does the movie the star come out", "answer": ["November 17, 2017"]}, "target": "['November 17, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the movie the star come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0dee7cded9dc9be27a6dfb748ee015e0f21e85b39809cfb6a73a34468f297691", "prompt_hash": "9d55ec89525672032fbf011f0e7b8d8fe04797fc5c4a29f1d83ca984aedfa384", "target_hash": "7aa4a3612064375e21eeb81995b6e705e2ca62d159ca98c4c208187cb605b713", "exact_match": 0.0}
{"doc_id": 2876, "doc": {"question": "who carried florida by 537 votes in 2000", "answer": ["George W. Bush", "Bush"]}, "target": "['George W. Bush', 'Bush']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who carried florida by 537 votes in 2000?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George W"]], "filtered_resps": ["George W"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35af171337e4d2dce27899a009775bc1ac5bffa48e65db8c688555f89bbfe5d4", "prompt_hash": "68b003e57ee2535cdbe3ff9ffbfa38a6704ff9a2f3adce0603795bdeb770dd4f", "target_hash": "ba52aead0ae2af5a334711536f96dfd4bc0eb74923eca5afebc79ba2987c85fd", "exact_match": 0.0}
{"doc_id": 2877, "doc": {"question": "first who wants to be a millionaire winner uk", "answer": ["Judith Cynthia Aline Keppel"]}, "target": "['Judith Cynthia Aline Keppel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: first who wants to be a millionaire winner uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carol Vorderman"]], "filtered_resps": ["Carol Vorderman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58ff397ada952ccc2643a12117a87537a0c17a3ab9acc700b0ee59d24bb2a7de", "prompt_hash": "0f170f772c2d7db76f3f09fbafddcc7039d310eb5d61f9b270b04b4631a3d78b", "target_hash": "27d8ae5b58cdbd8ae3b05d7f41d1896a00083b6903fc505f6a6c14f40e53cbcd", "exact_match": 0.0}
{"doc_id": 2878, "doc": {"question": "actor who played caesar in dawn of the planet of the apes", "answer": ["Andy Serkis"]}, "target": "['Andy Serkis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: actor who played caesar in dawn of the planet of the apes?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gary Oldman"]], "filtered_resps": ["Gary Oldman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e8e345a4dfbbdd00eb2e59f3a9107f7b2ca3ec7e1d3bbad7b07ec12325ec6e4b", "prompt_hash": "3f2c01a491ddc3843ad11e6bc139345a8291163771866d6073f7bd9230cdcd77", "target_hash": "68a452147a514270edb8f4be4410f8bc9fc99d58d1fb52d680dd7201717ca54d", "exact_match": 0.0}
{"doc_id": 2879, "doc": {"question": "when does wentworth season 6 start in australia", "answer": ["2019", "19 June 2018"]}, "target": "['2019', '19 June 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does wentworth season 6 start in australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c777b74a3a5dc7898a8478928526764bef47e9dd738f7377ef1c62cef2f168bf", "prompt_hash": "0e6d7c5b481f77ef873c4f4cdbd8712088312b56bfe17c40b0df12316461d14d", "target_hash": "b1a91e81a6bd68509defb46d9b1abe8025188f583758469be8e502c6f7fff5ee", "exact_match": 0.0}
{"doc_id": 2880, "doc": {"question": "where is the north pole on the world map", "answer": ["latitude 90° North"]}, "target": "['latitude 90° North']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the north pole on the world map?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arctic Ocean"]], "filtered_resps": ["Arctic Ocean"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5f03971330730bab278f4a959483fb74349c865406eadd8b1f18de64cb2a71ba", "prompt_hash": "a685ff6b2892879717a15e9d884a1ea5ea147227ba17476b320f3004f731ddbc", "target_hash": "2a4889fee1091cb966d24654a8e5150857da5db01caebd32b6786293583853a2", "exact_match": 0.0}
{"doc_id": 2881, "doc": {"question": "who played sam in clarissa explains it all", "answer": ["Sean O'Neal"]}, "target": "[\"Sean O'Neal\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played sam in clarissa explains it all?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sam Lavender"]], "filtered_resps": ["Sam Lavender"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "499f803cc96de4610ac57b11011a1cbd9886447779a62d866b2fc378686677e5", "prompt_hash": "d956a2c703eb55694c0ae7648718e6638af3d108884ce1b3f3612b819eb6c58c", "target_hash": "3cbea81e12c2f6bca49dea928acf9cabd899b5dd1117778a5d3446254d00d46c", "exact_match": 0.0}
{"doc_id": 2882, "doc": {"question": "who did puss in boots grew up with", "answer": ["Humpty Dumpty", "Kitty Softpaws", "Humpty Alexander Dumpty"]}, "target": "['Humpty Dumpty', 'Kitty Softpaws', 'Humpty Alexander Dumpty']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did puss in boots grew up with?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Humpty Humpty"]], "filtered_resps": ["Humpty Humpty"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "da1a1709e440b78cfdede3db972bb7700104c12482cf08f69bd331edbe473cdc", "prompt_hash": "01b1f3a438516505eda49b2f1cf441b294c5c9f2bd5c31a8dd88d7589b666326", "target_hash": "b259e2f4649d224a6eec229b5809353522f4e5bb3449a34a5f9e254f3f66caac", "exact_match": 0.0}
{"doc_id": 2883, "doc": {"question": "who commissioned the first christmas card in 1943", "answer": ["Sir Henry Cole"]}, "target": "['Sir Henry Cole']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who commissioned the first christmas card in 1943?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Queen Victoria"]], "filtered_resps": ["Queen Victoria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18354e1f1052f8fd030f85132232e03c3838fc5bc7c72fee2a1179d077ef5cbf", "prompt_hash": "1950a5cdfa32fbe57b9cefd34790d1225d5f2b06c9371d919222995f1a60410c", "target_hash": "afb3e7063729352af09acee3093e7699a659a205848916750e01106ed37365ae", "exact_match": 0.0}
{"doc_id": 2884, "doc": {"question": "who can be called a man of god", "answer": ["beloved religious leaders", "prophets"]}, "target": "['beloved religious leaders', 'prophets']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who can be called a man of god?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" prophet"]], "filtered_resps": ["prophet"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "86b08c51a7ae6259ea2efda086f6af5c6a83517a68a467af0329441d2b47c3eb", "prompt_hash": "f10b0ff52bfe99c4b80b9daf73fae7b6d0797b64a0242980dae604f72ef6dd79", "target_hash": "9df4856adcf633b83a3333b9a99c89447654dab597d38d61a07ea4216603296c", "exact_match": 0.0}
{"doc_id": 2885, "doc": {"question": "what nfl player has the most nfl rings", "answer": ["Charles Haley", "Neal Dahlen", "Bill Belichick", "Tom Brady", "\"Mean Joe\" Greene"]}, "target": "['Charles Haley', 'Neal Dahlen', 'Bill Belichick', 'Tom Brady', '\"Mean Joe\" Greene']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what nfl player has the most nfl rings?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jerry Kramer"]], "filtered_resps": ["Jerry Kramer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ef822fb50df5bac1184a54320d2e3394e3153d4c6f4e42d973d75734de5f008a", "prompt_hash": "1ab33a91178ea61c59f5bc3ba3c2ff7ef6880768d632f952e1765dc057ef7e01", "target_hash": "eff3494759803455783be53817cf64d39fca261e7d8ab40b9bf7c3ea9e968e36", "exact_match": 0.0}
{"doc_id": 2886, "doc": {"question": "most passing yards by nfl qb in a game", "answer": ["Norm Van Brocklin", "554"]}, "target": "['Norm Van Brocklin', '554']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: most passing yards by nfl qb in a game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 518"]], "filtered_resps": ["518"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a7368cbae975d0b26e597bd387917f154378cd553357e376a13044b7ba6a7420", "prompt_hash": "e533df90ba409b087e6d8e0c759a29e3f67343f66890e8f98c97f375cfeeecaa", "target_hash": "35f1e16999f5b7f1b77a859a625a13a21cc44b98e5e1c2440114fd25e0a7c84f", "exact_match": 0.0}
{"doc_id": 2887, "doc": {"question": "who sings jungle book i wanna be like you", "answer": ["Louis Prima.", "Louis Prima"]}, "target": "['Louis Prima.', 'Louis Prima']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings jungle book i wanna be like you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Harry Belafonte"]], "filtered_resps": ["Harry Belafonte"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ec9b17ea36f8cd75d6c983b5a756a7fffe9ed8dd7b555f03f99bc3ab4c1d1b77", "prompt_hash": "45b0ab5a3d3fd484fa3ab7996471781ecb59f48b006f43fdbab898f39e152b3e", "target_hash": "e51227db98d572cddc843d139dbcb0fbb2336d8301d26c035e43babd116a99c1", "exact_match": 0.0}
{"doc_id": 2888, "doc": {"question": "where was the new pete's dragon filmed", "answer": ["New Zealand"]}, "target": "['New Zealand']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the new pete's dragon filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hawaii"]], "filtered_resps": ["Hawaii"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b593f1c25dbb14ea3ebfbfcb85153100b5483aef67ded72426e8e6e9d48807b", "prompt_hash": "edafc4be34cd22724c9a661de19da4eae2cf51f168ec946423416c061883674e", "target_hash": "b820da97c134f64b2ebafdc15f5a2f78fc082d85779bfe35737dfe3e9c883807", "exact_match": 0.0}
{"doc_id": 2889, "doc": {"question": "total goals scored by ronaldo in la liga", "answer": ["309"]}, "target": "['309']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: total goals scored by ronaldo in la liga?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 311"]], "filtered_resps": ["311"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ed149369ca01a238c9f2a9eeba15619ba3e79a4aa9373d5697574827f7752782", "prompt_hash": "9ec0f0e194e6bd680eba59c335e31b0f8542c1728a83f9f302d3aef2e969f10e", "target_hash": "38f8decfb6f07229bbae5473492603829ee85cdb06190d07eba50362fba3f55b", "exact_match": 0.0}
{"doc_id": 2890, "doc": {"question": "who's opening for foo fighters at fenway", "answer": ["N/A"]}, "target": "['N/A']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who's opening for foo fighters at fenway?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Imagine Dragons"]], "filtered_resps": ["Imagine Dragons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a5e59e2d03446c0e669a19f66f79158c8aa26c15656610324b3180a80b762d91", "prompt_hash": "3f9e0c09057fda1c56c8d275c6a24c2eda175db341de8b03d1c8ff5633d29d82", "target_hash": "7d259586e41984799dbecab4c3b9ff9e5d1a89cc069004e8ab08cb2b473b28c8", "exact_match": 0.0}
{"doc_id": 2891, "doc": {"question": "when did the eagles last play in a superbowl", "answer": ["2017 season"]}, "target": "['2017 season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the eagles last play in a superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d766fdb868111d9947160c2409ad4288103ebf454172eed084e690cbb6cc5b1", "prompt_hash": "0cb582d23fa3788b7197199fb708a32f3ad3379e4154faa5442280fecb492e1e", "target_hash": "f0e56f14ed8516a890ca89f671e6f504eac0f8c83cdbe4844f8bdd6b24d1d368", "exact_match": 0.0}
{"doc_id": 2892, "doc": {"question": "when did south sudan join east african community", "answer": ["April 2016", "2016"]}, "target": "['April 2016', '2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did south sudan join east african community?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012"]], "filtered_resps": ["2012"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e934d444c1af47f70f28a5efdfa768bce9f3d805d01cd73c082113f502b98b08", "prompt_hash": "922c63f856412a839ca9152f2ebd031176bb53eccb4d10ac5f839039a9fd6685", "target_hash": "47634b91a7e76b03d2162df1de6e83b91d0312a0d02999e6bf3f5707353aaf51", "exact_match": 0.0}
{"doc_id": 2893, "doc": {"question": "when did the nfl start playing in london", "answer": ["since 2007", "October 28, 2007", "2007"]}, "target": "['since 2007', 'October 28, 2007', '2007']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the nfl start playing in london?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2007"]], "filtered_resps": ["2007"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d3d8714780908b2029b0fe8c62d1c3191e3065cc5efc87b041003ccd5f94342", "prompt_hash": "0ebfd6bc7129d9a48ecfe75a651bbb06d4770c40073fc5bee0523d17d027b92a", "target_hash": "2820789e38a28366f98d9a59ca785b125c80c3cc59b66102a0b7bd51260e6cf5", "exact_match": 1.0}
{"doc_id": 2894, "doc": {"question": "the world's tallest jain statue is located in which state of india", "answer": ["Maharashtra"]}, "target": "['Maharashtra']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the world's tallest jain statue is located in which state of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gujarat"]], "filtered_resps": ["Gujarat"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "291a2a0d578339513f7c8a94d23320b8d859dcebcd09792623170a2f6cd0bed1", "prompt_hash": "e27dfb61d51ba9ff18d32cfd8b8cebd908274ce33854bad98cac41eb204f079f", "target_hash": "be766e3d42b88e78d083be31238c9e26a544a149ebf7eb08f3bf1f812ef88eda", "exact_match": 0.0}
{"doc_id": 2895, "doc": {"question": "who said beware of the ides of march", "answer": ["the soothsayer", "a seer", "a haruspex named Spurinna"]}, "target": "['the soothsayer', 'a seer', 'a haruspex named Spurinna']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said beware of the ides of march?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shakespeare"]], "filtered_resps": ["Shakespeare"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "15a52f6ffe15c3a056ade1ecfb4e2cdfa874317a21bfc4a19a5e61937883d33a", "prompt_hash": "3b5f371083f4aef2ccdd75b9de8c883b9bc2d636e51524de3e809a6b19214899", "target_hash": "2613ee932912ee1c05abafe80f421c96eaa2d03ddcbb06dd3b5a58642fe96bfe", "exact_match": 0.0}
{"doc_id": 2896, "doc": {"question": "where do the maple leafs play hockey in toronto", "answer": ["Air Canada Centre", "the Air Canada Centre"]}, "target": "['Air Canada Centre', 'the Air Canada Centre']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the maple leafs play hockey in toronto?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Air Canada Centre"]], "filtered_resps": ["Air Canada Centre"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96ac694eca357d5be23e6033c70b46c031bac3fb949fe28387b6243a7c486298", "prompt_hash": "f2da57a483e03bc56d9a340de9be1b80edc1b12ce952708e02cb9a0a1bfd0f4c", "target_hash": "0e83e23273c71be8a77d86ad0d0fb2169703bd8899211e38643b515b3384991d", "exact_match": 1.0}
{"doc_id": 2897, "doc": {"question": "what kind of dog is nana in snow dogs", "answer": ["a Border Collie", "Border Collie"]}, "target": "['a Border Collie', 'Border Collie']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of dog is nana in snow dogs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Siberian Husky"]], "filtered_resps": ["Siberian Husky"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df6e3e5bd847b8d03d711b476fadd554890d96855f4045c00c3a5c3565cbf690", "prompt_hash": "512e4a586a117455abe30d38acc4d7f9a0299cd6f784482028fd7d4cceb4fdb0", "target_hash": "8a41e5feb061a65c896671d48ee2da7ca219dda42a86f7a498069329b2cd82f7", "exact_match": 0.0}
{"doc_id": 2898, "doc": {"question": "when did hyderabad became a part of india", "answer": ["24 November 1949"]}, "target": "['24 November 1949']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did hyderabad became a part of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1948"]], "filtered_resps": ["1948"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "98c91a74bbe6bea3a024444632bd3d993f22399f7c247fe4ab8b619268616712", "prompt_hash": "a151aea43f6aa097e409c0be07227d4007008c7d2e0b5d5c073044eeb2ee6c8b", "target_hash": "0bba755a6fe1cffd91afe0f4e26ef8569d068a2d0b86a9ed31ddc5554b04d787", "exact_match": 0.0}
{"doc_id": 2899, "doc": {"question": "who made possible the first reproduction of the human voice on the phonograph", "answer": ["Thomas Alva Edison", "Thomas Edison"]}, "target": "['Thomas Alva Edison', 'Thomas Edison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made possible the first reproduction of the human voice on the phonograph?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thomas Edison"]], "filtered_resps": ["Thomas Edison"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d0008f10b7cc3f5046cbc3f6889536967cce2c1c0d0ede1603a2fd7f46607258", "prompt_hash": "45c9a8e298351b3088e2516a356541df31568ec1ad20db637c6c467e6691e7e8", "target_hash": "7091ece1e5b5c16e4d5a74bd73a4c34affd0a2d1704976163d1ddf755893c2de", "exact_match": 1.0}
{"doc_id": 2900, "doc": {"question": "when does wonder woman come out on blue ray", "answer": ["September 19, 2017"]}, "target": "['September 19, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does wonder woman come out on blue ray?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 3"]], "filtered_resps": ["October 3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f9f1865ad15cc6685a9e51130576b6700d548f63919f60a538e88ce7e7483389", "prompt_hash": "d1181b8ac01f0e06bafcbc3c0984ee08d8be618d87a1252bdd0587cc8aece36d", "target_hash": "66406d9f150dc16e8daebd5bab91d7527106613c89bea2a0df932de65d6973d5", "exact_match": 0.0}
{"doc_id": 2901, "doc": {"question": "who sang i dreamed a dream in les miserables", "answer": ["Rose Laurens", "Fantine", "the character Fantine"]}, "target": "['Rose Laurens', 'Fantine', 'the character Fantine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang i dreamed a dream in les miserables?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lea Salonga"]], "filtered_resps": ["Lea Salonga"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "65ae6938fd43738e99686b3e70fa225cb3085071d0547fcdfe5e26e8b194c218", "prompt_hash": "991a8e7fbea14e6b670eaf2ec8b5535f660d1c7ec9a6749658f90a9603a0fba6", "target_hash": "a8dc217b82e017d94ea8ecf69f1ce97af4602ef75073aa021933162d8124794d", "exact_match": 0.0}
{"doc_id": 2902, "doc": {"question": "what time is it in alaska time zone", "answer": ["UTC−09:00"]}, "target": "['UTC−09:00']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what time is it in alaska time zone?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alaska Standard Time"]], "filtered_resps": ["Alaska Standard Time"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5cb164a4964db1f174564ac1086878d1e68cdda331ef4a91d9e1151924826403", "prompt_hash": "467ef84ab1ddc2377c7cb87aee87b9de93f66228f7dc3697fa9ad43949efcec9", "target_hash": "3bf8f3190a47b0e0fae52df0f232500c7b5ecc7a31619a48d47a4200b0a6620b", "exact_match": 0.0}
{"doc_id": 2903, "doc": {"question": "when is winter big brother going to start", "answer": ["February 7, 2018"]}, "target": "['February 7, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is winter big brother going to start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 2017"]], "filtered_resps": ["January 2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "abcb6f404d440cd8749beaf0b44c858347d0733e8483eaebef751c03f67b35c2", "prompt_hash": "66897c823835197429d38723df4b27a275e9e227c526786d29f96cfbe2f9a2b3", "target_hash": "cf6e36d114efb8f1d5b2635cf9ee02e5afdf771a3bf8fe9314505b5ff35ddf17", "exact_match": 0.0}
{"doc_id": 2904, "doc": {"question": "where is the nfl game in london played", "answer": ["Twickenham Stadium", "Wembley Stadium"]}, "target": "['Twickenham Stadium', 'Wembley Stadium']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the nfl game in london played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wembley Stadium"]], "filtered_resps": ["Wembley Stadium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "803b600fda6659b165de0da2abf084e6ad91893cab83cb301a6276dffaa3ebdf", "prompt_hash": "57f1e6fd180b84fece4f50f6192ae9f6f5f3d8003938d08430dddd254fc59ca4", "target_hash": "12f01f239580d37f91da3e4e9cdde59b47467aaa38e37ce97e99030b11e92ab0", "exact_match": 1.0}
{"doc_id": 2905, "doc": {"question": "who is the quarterback for the green bay packers", "answer": ["Aaron Rodgers"]}, "target": "['Aaron Rodgers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the quarterback for the green bay packers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Aaron Rodgers"]], "filtered_resps": ["Aaron Rodgers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c08bfa1901beb59698b5055b19b4dfceaac028ae260650e71dd3fe4e3c65dfe3", "prompt_hash": "20c2165f65b85d02840e44e4c3dd207c662d1a786aa94ce032c2561c62a883ca", "target_hash": "664f7c7a8f35ad9d56d44c76a28e32fa2028c336aaab72798adba19c7e10082f", "exact_match": 1.0}
{"doc_id": 2906, "doc": {"question": "who played the face in the a team", "answer": ["Dirk Benedict"]}, "target": "['Dirk Benedict']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the face in the a team?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Jason"]], "filtered_resps": ["David Jason"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b5cc1e504dd04fdd95d1b082544ec67a8c48f6f8451b04f10df19eeb3ac6cd0d", "prompt_hash": "2825de620bc584f9d523957adb8415b301ed085c0ae9d508289c776f85339211", "target_hash": "a201c3ed90c20579b8873747228d609584db0fb08e161c76790d05656d467774", "exact_match": 0.0}
{"doc_id": 2907, "doc": {"question": "where did the battle of bonhomme richard take place", "answer": ["near Flamborough Head"]}, "target": "['near Flamborough Head']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the battle of bonhomme richard take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" off the coast of Portugal"]], "filtered_resps": ["off the coast of Portugal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae46eab916eb53b280c5b5ac4b3d8960ececa43ae508138c4dd8aca112b926d7", "prompt_hash": "408d71b77eca34b62532738017be63f3a6ed5a0adc7f583df91a9e15014ec64d", "target_hash": "273049ffecafafdf206fbe73e7e5203f4e915378a81e8245b1aebfbf0ccb0a66", "exact_match": 0.0}
{"doc_id": 2908, "doc": {"question": "when was the last time that the eagles were at the super bowl", "answer": ["2017", "2017 season"]}, "target": "['2017', '2017 season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time that the eagles were at the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d1bcd10f269bb5c7a30dfa20c615ed66d3f0f66847fc1eadd803314980cd693", "prompt_hash": "6855a13c2e35ea23179c45a0834ce45f135f90e6ddb8d24f27aac451774aa9aa", "target_hash": "f78ef7ea3ad061c17c9a6ddf47f6bf8fc94fb46e14b8c0fe87d551f01dc97fb1", "exact_match": 1.0}
{"doc_id": 2909, "doc": {"question": "who is the original singer of fool if you think it's over", "answer": ["Chris Rea"]}, "target": "['Chris Rea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the original singer of fool if you think it's over?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Billy Joel"]], "filtered_resps": ["Billy Joel"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3deca3d5c870c5fc8e90f238848fdbd26362b517b88dc927a0e93dd2b70a9ac8", "prompt_hash": "6ddf0c5fba3de42216ff7052d65e5ffda9ce31ee3faa442d28967f331bda496c", "target_hash": "88063e17eab760455506a549e917f296c879b4a9e1e295eefcf8ae46b274bb0f", "exact_match": 0.0}
{"doc_id": 2910, "doc": {"question": "who stars in kevin probably save the world", "answer": ["J. August Richards", "Kimberly Hebert Gregory", "India de Beaufort", "Jason Ritter", "Chloe East", "JoAnna Garcia Swisher", "Dustin Ybarra"]}, "target": "['J. August Richards', 'Kimberly Hebert Gregory', 'India de Beaufort', 'Jason Ritter', 'Chloe East', 'JoAnna Garcia Swisher', 'Dustin Ybarra']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who stars in kevin probably save the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Steve Carell"]], "filtered_resps": ["Steve Carell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ebfa6dafb3fec797826ea296347119d8d6f90d7f10a21434977065804cfdf470", "prompt_hash": "735cfa2adf9a4759db8d3f648344ca2f427587beccfa72269d9dc8320d46a445", "target_hash": "a57cf4621d2e876da7d1985437bf5f32ea7938878ca076322433adefd4ffe88b", "exact_match": 0.0}
{"doc_id": 2911, "doc": {"question": "what is in a pat o brien hurricane", "answer": ["fruit juice", "syrup or grenadine", "rum"]}, "target": "['fruit juice', 'syrup or grenadine', 'rum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is in a pat o brien hurricane?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" whiskey"]], "filtered_resps": ["whiskey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8217f1cf31f227a0e4ff5d179a828c497a0e875bf54deaa3d336095f01e2642f", "prompt_hash": "5afdd69f505f78058b0738462641caf731017226f8d1e4803c8be26ad0095dfe", "target_hash": "d4d8c5830288da1f20e858a24cb1aedff1dee04940334b1356803780364a26cc", "exact_match": 0.0}
{"doc_id": 2912, "doc": {"question": "who did the us share the oregon country with", "answer": ["French Canadian", "British"]}, "target": "['French Canadian', 'British']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the us share the oregon country with?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Britain"]], "filtered_resps": ["Britain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c16fd75a12076d25abb27919407ef919d478af2fafb27dcf6e27561f57a0df0", "prompt_hash": "c70c1728ceb147320ccb9111364594742bdb357d7836ad9fd3b4cff7caf67e35", "target_hash": "ec426105fd07b3e4bbb210d0e214c4da75e86b90e67f6685ff4dfac1520b8efd", "exact_match": 0.0}
{"doc_id": 2913, "doc": {"question": "who is eliminated in big boss kannada 5", "answer": ["Shruti", "Niveditha", "Diwakar"]}, "target": "['Shruti', 'Niveditha', 'Diwakar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is eliminated in big boss kannada 5?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sagarika"]], "filtered_resps": ["Sagarika"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3e1765b90e2eade2e5bbca1fe2b518b38d615f5249335b7ac91dbeab2189d1c0", "prompt_hash": "910131365447e28a23309f4733955c212b40da43394ab029657205b15f000f5a", "target_hash": "260dc6ed836aba64adedfce2c0a4e092cfdfb9d2a3afc1df092109bc7b0dddc5", "exact_match": 0.0}
{"doc_id": 2914, "doc": {"question": "who won la liga in the last 10 years", "answer": ["Real Madrid", "Barcelona", "Atlético Madrid"]}, "target": "['Real Madrid', 'Barcelona', 'Atlético Madrid']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won la liga in the last 10 years?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Barcelona"]], "filtered_resps": ["Barcelona"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eb09bc53488ea22c5da02521cd0e215f86c0f56cb4d8f7974d7c5d253c9b1795", "prompt_hash": "1e53195ad15c6d7914ac3992b54d597de45c3dc25d3bc5d634c24433ee06a938", "target_hash": "210a40c843fe05d9c2824068ffa8d2b41f9146ee6421bf2c4d52f4e7e7442858", "exact_match": 1.0}
{"doc_id": 2915, "doc": {"question": "who has played in the most masters tournaments", "answer": ["Gary Player"]}, "target": "['Gary Player']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has played in the most masters tournaments?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tiger Woods"]], "filtered_resps": ["Tiger Woods"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e5be19510f813c8d66c5ce0754b50c6d7a1ea6aaf3012329689f81447c6f150e", "prompt_hash": "a14c5492104a664ae55c155c9d21cf6350dcb244df3254a35e8e1280bc5e14a3", "target_hash": "9b0e13f53474687400dde6469c00f9387c5b56db13f3bf6ab235457bfb3c3557", "exact_match": 0.0}
{"doc_id": 2916, "doc": {"question": "when did beryl markham fly across the atlantic", "answer": ["September 1936", "4 September 1936"]}, "target": "['September 1936', '4 September 1936']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did beryl markham fly across the atlantic?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1936"]], "filtered_resps": ["1936"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c863211be074a2a257d933a2ec6e4d3cac1dcb1a29e9f0412e2195a1c29451c5", "prompt_hash": "1a2387db0fde64104b35446fa8cfa9f05d1299d115f488af28061e3f38035a01", "target_hash": "c08df59a669a51cf91aa034b495b849c4bc6cd23a41629480d2efc24c8ab8b0c", "exact_match": 0.0}
{"doc_id": 2917, "doc": {"question": "what are some special products made in mississippi", "answer": ["farm-raised catfish"]}, "target": "['farm-raised catfish']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are some special products made in mississippi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cotton"]], "filtered_resps": ["cotton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "90524808f599babc92cabf9558baaeb1f96220a27520230603381c4ad23d4857", "prompt_hash": "d7768fb0e3ee7656012eeb3d61b0141a247e54efd4cc7de5f80e236ca1b67572", "target_hash": "374ec58afca1b23d1675c622ceda3227b41c8d16331328d7b72bc2f9efc15d77", "exact_match": 0.0}
{"doc_id": 2918, "doc": {"question": "how many numbers are in the euromillions draw", "answer": ["7"]}, "target": "['7']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many numbers are in the euromillions draw?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" five"]], "filtered_resps": ["five"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "53d73bc8b0d78bff8aaa9c7afacd9a603902165031a6844620f4ea8fb629a40e", "prompt_hash": "48b96c11b173e8b8a671bb62f6be63ba43c1014d53586b12ca8a68c79e291a2d", "target_hash": "daaadef398cfd18ecc086697bc3b2c12ec6c2386996037c659028ab3e97cd18d", "exact_match": 0.0}
{"doc_id": 2919, "doc": {"question": "where is the heart of palm on a palm tree", "answer": ["inner core and growing bud", "the inner core", "growing bud", "the center core"]}, "target": "['inner core and growing bud', 'the inner core', 'growing bud', 'the center core']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the heart of palm on a palm tree?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" trunk"]], "filtered_resps": ["trunk"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49bc05f30e669185e388b556705622a8f9fb121e56ca4e911a72c3e1fa924d31", "prompt_hash": "5fe78fa78dac959ec0ebdeb5dc5f72bc0b4aae9210d14c99fcb1c3a27becca59", "target_hash": "b6f2b8b83a890ed6f722aafdc91617c2dcddb782127b1b21f9ffdbc25b334ae0", "exact_match": 0.0}
{"doc_id": 2920, "doc": {"question": "who enforces the charter of rights and freedoms", "answer": ["Supreme Court of Canada", "the courts", "courts", "The courts"]}, "target": "['Supreme Court of Canada', 'the courts', 'courts', 'The courts']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who enforces the charter of rights and freedoms?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Supreme Court"]], "filtered_resps": ["Supreme Court"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a03bc7baba658b40b2bdaee918cc64064bff6366eddd91905f035931ed034163", "prompt_hash": "1e7eba7fe7027597a898eba38d7e4a4db87accb5f02a84d5c943f706a494b4ea", "target_hash": "8040b54a417b7953be043edf36903465d1986f9ad31cea40d81b5a2fcc055233", "exact_match": 0.0}
{"doc_id": 2921, "doc": {"question": "who was the buccaneers qb when they won the superbowl", "answer": ["Brad Johnson"]}, "target": "['Brad Johnson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the buccaneers qb when they won the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chris Simms"]], "filtered_resps": ["Chris Simms"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "29890868c6e9dedf12ca7ce14d1baeb35076e3eb5744a95aeac7b6e2eee30238", "prompt_hash": "94bb1d7fb2c9d4dac11018fa3d63d610cb461316425ca4fa4e059546108d9bcb", "target_hash": "bb684d7b813bcd5dfe823893cb987d0cbfe9fc54d3a62872ac1122ed67472d18", "exact_match": 0.0}
{"doc_id": 2922, "doc": {"question": "how many episodes in season 3 of good witch", "answer": ["10"]}, "target": "['10']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes in season 3 of good witch?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 13"]], "filtered_resps": ["13"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a9730588e47cefa047b3fd061c51b9d4f4d61d3d60df869b0b0073b11077780", "prompt_hash": "28bae073c49e65ecd193bd6f198b6d5b355db708d2287d3d3716d79b250d19c7", "target_hash": "9503346d501aeaec3f521be685eb235cda81c7929ba2f1acdae1be91400b787a", "exact_match": 0.0}
{"doc_id": 2923, "doc": {"question": "when was theme from a summer place released", "answer": ["1960", "1959", "September 1959"]}, "target": "['1960', '1959', 'September 1959']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was theme from a summer place released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1960"]], "filtered_resps": ["1960"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cffb4c4a74c9dfaa0b7a9770a1009e79d5cd19a876231c03285718107ab92260", "prompt_hash": "083b80c41856c5077d95def978c9f259ce48ce3b47737f2684ddaa0b65e53762", "target_hash": "b3949a6930880b0ef2874dd2369335abdf408b03c810116d42b2ae354be34eaa", "exact_match": 1.0}
{"doc_id": 2924, "doc": {"question": "what are the two extracellular fluid compartments in the body", "answer": ["transcellular", "intravascular", "interstitial"]}, "target": "['transcellular', 'intravascular', 'interstitial']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the two extracellular fluid compartments in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" interstitial fluid"]], "filtered_resps": ["interstitial fluid"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d816d9aea50b2fdaf94cb9861dab60faa49dd7d6516cf116cd87c53ebea2b5d5", "prompt_hash": "240f82aa1a8651c94ab865e0a41b99659acbc3b058c2d177f0f1a831d4c64d7d", "target_hash": "54da2d6eed9bfe57fe50f229c6354513674a7b3753f75c4e1e2c58ea1f4dca02", "exact_match": 0.0}
{"doc_id": 2925, "doc": {"question": "in 1945 which party came into power in england", "answer": ["Labour Party", "Conservative", "Labour"]}, "target": "['Labour Party', 'Conservative', 'Labour']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in 1945 which party came into power in england?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Conservative"]], "filtered_resps": ["Conservative"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b2601aec6566a758e0a32d07f3bb9f35ed5c40be5b9d3ee3f14d9901045bc956", "prompt_hash": "d6a42672152eda24fa8228d8c41e08824d93e8cb4fa78717e5241c0e343a1a94", "target_hash": "83ccad3c4977d5ad738a35be0716afed137623fb6a2af048660e1ff288c1239b", "exact_match": 1.0}
{"doc_id": 2926, "doc": {"question": "who is the best director in bollywood 2017", "answer": ["Nitesh Tiwari"]}, "target": "['Nitesh Tiwari']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the best director in bollywood 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Karan Johar"]], "filtered_resps": ["Karan Johar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b493961c957a59ae43ec27294d409486883e5bf37c4ee6255f4bc5ef3bbfca89", "prompt_hash": "fd0bc7f3a84d0ed7a9870d3084eaf58b0002352235c076a38ecf8afde1d534ea", "target_hash": "25fe301b18a0cd23f40a921925f575b1c54ab4f83ca518f06272b7d283d71af6", "exact_match": 0.0}
{"doc_id": 2927, "doc": {"question": "what is the population of keystone heights florida", "answer": ["1,350", "1,461"]}, "target": "['1,350', '1,461']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the population of keystone heights florida?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1abce86d0d4e950ca0f68a76c081882ff30e1aad1a73a9fa73be7a805ea22b94", "prompt_hash": "57b79b43ea936d7d0af005f3a1492d69b2d9aa0c45b9d63954e6d64243f65756", "target_hash": "457cc9133432ed22f60f931174e6c212d5673ebc12921fd8876ab4af4c5cbcbd", "exact_match": 0.0}
{"doc_id": 2928, "doc": {"question": "when did the term sustainability start to be used", "answer": ["1987"]}, "target": "['1987']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the term sustainability start to be used?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1970"]], "filtered_resps": ["1970"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5f5c3222cc292bf5f97aa026e273c7842e6e53aee73de8f5935a2cec606456ea", "prompt_hash": "36ce965763dc2e4663dbbb47e2a68ae3a14a439a2bb9e7eb23d406a515cd6168", "target_hash": "8a3fc8b2b5b806619cf115d4aecffac18d0b1b251d4d609835f6b793358bf889", "exact_match": 0.0}
{"doc_id": 2929, "doc": {"question": "when did the cowboys start playing on thanksgiving day", "answer": ["In 1966", "1966", "1978"]}, "target": "['In 1966', '1966', '1978']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the cowboys start playing on thanksgiving day?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1966"]], "filtered_resps": ["1966"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "624e004b4ff8c526bafcce815963add706d4abb5806635b6d47ad6cd852a4337", "prompt_hash": "1092158407e9955b996bc7a8f648dd28ca2552c92417dad936cf3eff8c891702", "target_hash": "0bc5af456f26230b16880c5401b05cdf439f24b334d6d7751d528095be92c1f8", "exact_match": 1.0}
{"doc_id": 2930, "doc": {"question": "who played emma in h2o just add water", "answer": ["Claire Rhiannon Holt"]}, "target": "['Claire Rhiannon Holt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played emma in h2o just add water?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Danielle Campbell"]], "filtered_resps": ["Danielle Campbell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e54c1b6f6582af700330f9b3b76df8729f2c391a09c7a24e72ba9a49e3deb8b", "prompt_hash": "cec590690c3abc5d62511ca1b8508a1bdd0a9afe73812397c18d31f50a8937e3", "target_hash": "3d9d2350e2ef2a531cc846057bb6014a3e54174d63d5eb98c3c85bfba7651a5c", "exact_match": 0.0}
{"doc_id": 2931, "doc": {"question": "what does fancy dress mean in the uk", "answer": ["A costume", "guests dress up in costumes", "costumes"]}, "target": "['A costume', 'guests dress up in costumes', 'costumes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does fancy dress mean in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" costume"]], "filtered_resps": ["costume"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f8a28c17523755ff4f7e182fe757ac4109eea48e1591be3887d8885139a550cc", "prompt_hash": "84d8267426511e900734a1c3ab0cb33dde380a9ff747ad6e12b27fd221670e02", "target_hash": "8c45f0c7e84b7ed23cb53af2920f11575612576ca73534bbe6a7dc8aaff97ca5", "exact_match": 1.0}
{"doc_id": 2932, "doc": {"question": "when is the new tappan zee bridge going to be finished", "answer": ["by June 15, 2018", "June 15, 2018"]}, "target": "['by June 15, 2018', 'June 15, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the new tappan zee bridge going to be finished?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b944cb8978a9b388fd2cf217848c06a7e33436520ab266300346646ef841616", "prompt_hash": "85e41c4e7923dbcfc7afa3c176ef6b38eae1bc598f9acc87039237767b6690c1", "target_hash": "b480c3a58b2780fb57b88ddf4e02c66057e4596c3976bf1d48efa6f04d8011f3", "exact_match": 0.0}
{"doc_id": 2933, "doc": {"question": "what are the 5 pillars of faith of islam", "answer": ["concern for the needy", "the pilgrimage", "life", "self-purification", "prayer"]}, "target": "['concern for the needy', 'the pilgrimage', 'life', 'self-purification', 'prayer']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the 5 pillars of faith of islam?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shahada"]], "filtered_resps": ["Shahada"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4bf51c199298e345580c4b69226640761a3908e8a371e136fcdfde577b91e148", "prompt_hash": "52c1cf29c0f0f933ffd4fedd986c0c5b688b5bac0f579f6d285bf8fd070c77d8", "target_hash": "1c30471dc62cf607ee7a060700575ab7e15368faad678bef826d2d90639d163f", "exact_match": 0.0}
{"doc_id": 2934, "doc": {"question": "when does i'm a celebrity get me out of here start in the uk", "answer": ["2018"]}, "target": "['2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does i'm a celebrity get me out of here start in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2002"]], "filtered_resps": ["2002"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8567af735d8c31fb270deaa2b5f1fba83cb4fe60c220e1c8a56fafdcff689085", "prompt_hash": "9f7e1de3489983f6b4cef5d0aed41dcc953073244c0022201d9f8dae1fd2d0a7", "target_hash": "c2a08bd0d5533c6f50031d08624056732f73b7e301788ef0d882bb0b26a8e885", "exact_match": 0.0}
{"doc_id": 2935, "doc": {"question": "who plays meredith quill in guardians of the galaxy 2", "answer": ["Laura Jane Haddock"]}, "target": "['Laura Jane Haddock']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays meredith quill in guardians of the galaxy 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Zoe Saldana"]], "filtered_resps": ["Zoe Saldana"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9cb13a3371bf34ab1a374f804277f8c7726f14ca0cc871936a426ed72c5ccfd7", "prompt_hash": "ca0fa441a6e7cfe5f48ed8b67e74a2174b02b90b2e612d7c07bd33a91c53512f", "target_hash": "ccd52e9b9d25d3bdcd14b7396036bc283a536656e3729d7969c53bc2f9e5430f", "exact_match": 0.0}
{"doc_id": 2936, "doc": {"question": "how many terms can a mayor serve in texas", "answer": ["two four-year terms"]}, "target": "['two four-year terms']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many terms can a mayor serve in texas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" two"]], "filtered_resps": ["two"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4c35b6d803432b1e368cb6247ae490e0a482a30f68b369c13513084ae0521e1", "prompt_hash": "f80f2c98a6854f1645775b89eb11c63f1b18058884c916288af0138ff09963c3", "target_hash": "322458714f8962bf26225f8069bef4e227e26207419b156a54ba85a767035e8b", "exact_match": 0.0}
{"doc_id": 2937, "doc": {"question": "when was the first time the internet was used", "answer": ["1969", "November 22, 1977"]}, "target": "['1969', 'November 22, 1977']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first time the internet was used?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1969"]], "filtered_resps": ["1969"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "43603fd2a3ba351f497d493399a69260e940bfafb2ed9155a1154fb5000d2658", "prompt_hash": "f45bf554263eaf0ab73867a1cb447e179972c668d19cfb12cfb0d5b755fdaad7", "target_hash": "30fd6701af25815423c863d9db952019e86ad82f74203cc08d460ee36a272406", "exact_match": 1.0}
{"doc_id": 2938, "doc": {"question": "who wrote hallelujah i just love her so", "answer": ["Ray Charles"]}, "target": "['Ray Charles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote hallelujah i just love her so?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bob Dylan"]], "filtered_resps": ["Bob Dylan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eff3a985e91148af385e6d4ce4a51632b4e98892a638754de75745e2b51f2740", "prompt_hash": "1d1c2e762448065ca085923301f316a7f853b12bc80b938cea15014a5fe7eca2", "target_hash": "833ed9157abefd8f301b6d8a606f27eef3b2f7db07d08b6f30fb268b89865f43", "exact_match": 0.0}
{"doc_id": 2939, "doc": {"question": "who was the oldest man elected president of usa", "answer": ["Ronald Reagan", "Donald Trump"]}, "target": "['Ronald Reagan', 'Donald Trump']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the oldest man elected president of usa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Buchanan"]], "filtered_resps": ["James Buchanan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fd45cfa619b10747b569973426c6fa31f897932ed5ca967fc7cf35d9fe8ce77d", "prompt_hash": "2a8f5769ffa6e291d2f092990c92f6a02811aca4fb14bc6b369b7fe80a14265a", "target_hash": "7e04e342f034b409956a88f8a57bf89999670448a71f2bcac9c5ce84c489858e", "exact_match": 0.0}
{"doc_id": 2940, "doc": {"question": "where will the cricket world cup 2019 be held", "answer": ["England and Wales"]}, "target": "['England and Wales']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where will the cricket world cup 2019 be held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" England"]], "filtered_resps": ["England"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "81d5054b7038d7f2a51680d26481bf841f319006de6ad7c4b9dfe96a0b12dfb9", "prompt_hash": "a32c5343db3aa387c786777e8e26887d8222eb81e1938c23d5f8c4364e2c5b74", "target_hash": "252a8c3d1f72946ec9df9825a69c9f4cbaedd3053df986a0b032c4e78675e942", "exact_match": 0.0}
{"doc_id": 2941, "doc": {"question": "who painted the ceiling of the florence cathedral", "answer": ["Giorgio Vasari", "Federico Zuccari", "Giorgio Vasari and Federico Zuccari"]}, "target": "['Giorgio Vasari', 'Federico Zuccari', 'Giorgio Vasari and Federico Zuccari']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who painted the ceiling of the florence cathedral?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michelangelo"]], "filtered_resps": ["Michelangelo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "37e0ebff199a3eaee110ffb57231926e83caa6c647e5432822fe893883d14884", "prompt_hash": "bd39d54db923e5bfe021606f567752602d525161bc6ed6faaf33c233c7d33a63", "target_hash": "ababdf377e0d1e4c6cf701b6323df40cc66178d2b6678e2fb6418eaef6b84876", "exact_match": 0.0}
{"doc_id": 2942, "doc": {"question": "author of the hymn great is thy faithfulness", "answer": ["William M. Runyan", "written by Thomas Chisholm", "Thomas Chisholm"]}, "target": "['William M. Runyan', 'written by Thomas Chisholm', 'Thomas Chisholm']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: author of the hymn great is thy faithfulness?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thomas Chisholm"]], "filtered_resps": ["Thomas Chisholm"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3f8595d48e93eda2c616e728dc315d298dfc2eaded4d153af1be06229b936f19", "prompt_hash": "13423463ae873e84cf49762ca5213d66224a0b7ae4c1b534e7b24a9bc6f10d79", "target_hash": "1b8e90e8eaa37d6a13e57467b32dc90559683d3b0a3a895f689032abe4e43859", "exact_match": 1.0}
{"doc_id": 2943, "doc": {"question": "where does sleeping freshmen never lie take place", "answer": ["J.P. Zenger High"]}, "target": "['J.P. Zenger High']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does sleeping freshmen never lie take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dormitory"]], "filtered_resps": ["dormitory"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "89cabb7934bcbc69d61a5d6797cde9b7bdab908afb31d4cb5e12eff17081b049", "prompt_hash": "cd845736016dd2d6a92c579e7b94a9e153ef4fdc36fd5c47855354b69021bce0", "target_hash": "b02a8250e2ed630bb1c61f0160e01cdb19532f55769377efad80a3813f330a35", "exact_match": 0.0}
{"doc_id": 2944, "doc": {"question": "word that means separation of church and state", "answer": ["separationism"]}, "target": "['separationism']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: word that means separation of church and state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" secular"]], "filtered_resps": ["secular"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3158fa122b8ee93e8c6802b49cdcbc7c893a063b8647aaad2c83962f17c5968", "prompt_hash": "29618004365304910d012fc23d5826eaa8be98fcfacd261f1e48263f0a958151", "target_hash": "17fa69063462ce135ecd8162a6bced0a70cdf28aa35b4f1616a541947fe626ca", "exact_match": 0.0}
{"doc_id": 2945, "doc": {"question": "actor who plays dr avery on grey's anatomy", "answer": ["Jesse Wesley Williams"]}, "target": "['Jesse Wesley Williams']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: actor who plays dr avery on grey's anatomy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Chambers"]], "filtered_resps": ["Justin Chambers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fe493d35c614445c3d2c5ede5ba16080c85d783c3d7e09c4ac8c22c8ca5b8dfc", "prompt_hash": "3919748033d7fe80cd39312262353d6e0640a2171b8a1571bfbe0321420d323e", "target_hash": "7a9987631be88fe18516c2df67dc724c0fcca36fa16eca307bf856dd11b8e054", "exact_match": 0.0}
{"doc_id": 2946, "doc": {"question": "when does season 13 of america's got talent premiere", "answer": ["May 29, 2018"]}, "target": "['May 29, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 13 of america's got talent premiere?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" July 16"]], "filtered_resps": ["July 16"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0319eb5c43be1742a66ff8670722777acbfe39c68b01bc8a21abc3b3d0e2fc91", "prompt_hash": "e51f21494a9825c560365f9d11fbe7baf27e2ff9f2d55dbdcedca0b7a5924e67", "target_hash": "ac747820391fff66d8502ed9a4a80984ecf7fd5af9d664d5d1e2c342a0e1fc0f", "exact_match": 0.0}
{"doc_id": 2947, "doc": {"question": "which animal on earth has the longest life span", "answer": ["tortoise"]}, "target": "['tortoise']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which animal on earth has the longest life span?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" humans"]], "filtered_resps": ["humans"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "98a4d348caad7d29f216916427889e0e3963f21a122f1a733e7215bf92533818", "prompt_hash": "e8ef11e51946a3e6760a25db56e6b15566b1d0042b338f0f29cb5e6721dc862d", "target_hash": "3ef64dd8803f669e086d47298fe912fe6ed286e9408b0e6c75de00b453bbb60d", "exact_match": 0.0}
{"doc_id": 2948, "doc": {"question": "where was held the first session of muslim league", "answer": ["Dhaka, Bangladesh", "Lucknow"]}, "target": "['Dhaka, Bangladesh', 'Lucknow']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was held the first session of muslim league?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lahore"]], "filtered_resps": ["Lahore"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c2def6699e22bbab0d8f43b4d575700f58b0db24b17e50a7a86a62e02a4d05b3", "prompt_hash": "0fd8452b4470ef8d9cbff28006210cc82d050e2931404c38c86b6106b6ccd434", "target_hash": "7773e1452a171d442fb64ad7980427069ff4f7e1442149ffb3f0f8146fd2e0a6", "exact_match": 0.0}
{"doc_id": 2949, "doc": {"question": "who does the democratic republic of congo trade with", "answer": ["Group of 77", "WTO", "Zimbabwe", "Belgium", "SADC", "Kenya", "South Africa", "China", "AU", "Zambia", "IMF", "World Bank", "African Development Bank", "France"]}, "target": "['Group of 77', 'WTO', 'Zimbabwe', 'Belgium', 'SADC', 'Kenya', 'South Africa', 'China', 'AU', 'Zambia', 'IMF', 'World Bank', 'African Development Bank', 'France']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the democratic republic of congo trade with?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" China"]], "filtered_resps": ["China"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c862aea2f630e95426a5167a8e72ea06d1681d08bb263ccf5318bd96bec796fe", "prompt_hash": "29aac9d659dc0e1866cdc3fa8c5cb7edca89359b00a7a68b0b78912eb3e3aa51", "target_hash": "3ff97d5a67610da7da72c819060c144fa4d5f3bb3f78f1fc3c05de966eb37f84", "exact_match": 1.0}
{"doc_id": 2950, "doc": {"question": "what is the name of india 29 state", "answer": ["Rajasthan"]}, "target": "['Rajasthan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of india 29 state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Uttar Pradesh"]], "filtered_resps": ["Uttar Pradesh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "08a32c7d1da82de77052d124d8080e45c4cdc140826c7eac9ab0f77844935946", "prompt_hash": "edb0ecdfa20c26929532df82de1097b6588137481993e2a9bff7fbbe5d19dd2e", "target_hash": "0422f10b34583dbe6be4c29794992bc2dbaca358f0e67eb9131465bdc9c31e30", "exact_match": 0.0}
{"doc_id": 2951, "doc": {"question": "when was the last time miss texas won miss america", "answer": ["1975"]}, "target": "['1975']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time miss texas won miss america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1990"]], "filtered_resps": ["1990"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "52f3efd214808b07e0966a00430e327dbea61261db49a2d19fb49359dc5200f7", "prompt_hash": "47da198f418b1da118691f686b6abf0fb9705ebb4f2261625d8807bd7f13130d", "target_hash": "30547784fdb6aac7ba2fd06b246ee5c7e1d5fb3c600cdce64155caba0d079427", "exact_match": 0.0}
{"doc_id": 2952, "doc": {"question": "what new deal program limited production in industry", "answer": ["National Industrial Recovery Act", "NIRA"]}, "target": "['National Industrial Recovery Act', 'NIRA']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what new deal program limited production in industry?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" National Industrial Recovery Act"]], "filtered_resps": ["National Industrial Recovery Act"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6338c4c41fb81b701390072ef180517d098a7528b52222f320918d81772fa5f7", "prompt_hash": "ef419b5778df255b6d30f53e0995995b8aed7accd449dcf642e3dd45c976e4cb", "target_hash": "e66fecf34e0735db6612e14e04cc3116ceb8e25bd788743d12c5bc9e5a68537f", "exact_match": 1.0}
{"doc_id": 2953, "doc": {"question": "who beat university of virginia in basketball this year", "answer": ["Virginia Tech", "UMBC", "West Virginia"]}, "target": "['Virginia Tech', 'UMBC', 'West Virginia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who beat university of virginia in basketball this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Villanova"]], "filtered_resps": ["Villanova"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d090117837bfc46eb620255ab60c32cc4d3ffeaa2a5ea69dbef7ad4afd53781", "prompt_hash": "5e468d68250f67035d43d38bcd3902c864d0bf299cec70221dbd9737b23669de", "target_hash": "4e9efbd065d4c956c4f655ce6fa13bc773a3e157299a358a8f6570b0e18162a2", "exact_match": 0.0}
{"doc_id": 2954, "doc": {"question": "two atoms of the same element that are covalently bonded", "answer": ["shared pairs or bonding pairs"]}, "target": "['shared pairs or bonding pairs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: two atoms of the same element that are covalently bonded?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" molecule"]], "filtered_resps": ["molecule"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f9ee5c1ae8f59ac2feb73d29972f02d248f512566ca12af00ea202c46e083ad9", "prompt_hash": "d848155cd0384e63d825dd027dd7304ce1ffaff724d28c088dfdc50551909a53", "target_hash": "a5382704eb451447e0c8d8018c9216eba285f9251c563d0dc33b7aea9e53b5f2", "exact_match": 0.0}
{"doc_id": 2955, "doc": {"question": "what is don quixote's horse's name", "answer": ["Rocinante"]}, "target": "['Rocinante']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is don quixote's horse's name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rocinante"]], "filtered_resps": ["Rocinante"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb685540cd6c9d38236f89798f425334857ba33af3c88c30f0821aa746c07a60", "prompt_hash": "2350984be9987a5462fd0a8436acdab261c8323297ab4e7f92bd9227814ea04d", "target_hash": "0186a0b55bc1e9a7e133d8679a75026b8913f3d9c203c6e8d6e0819625679970", "exact_match": 1.0}
{"doc_id": 2956, "doc": {"question": "who played morticia in the addams family tv show", "answer": ["Carolyn Sue Jones"]}, "target": "['Carolyn Sue Jones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played morticia in the addams family tv show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carolyn Jones"]], "filtered_resps": ["Carolyn Jones"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1f7539e30a0939473392d9e335cb030b395ef91ad75e4246dcc410740550e8a9", "prompt_hash": "fcf8c590fc87424538ae48788fda55df8841f18df53072437ba7eb07424ee405", "target_hash": "10f3fd083ab833eb66cdf5fc56e89115dd607989a268d0e14d4026445cf52988", "exact_match": 0.0}
{"doc_id": 2957, "doc": {"question": "when was pac man released in the us", "answer": ["October 26, 1980", "October 1980", "in October 1980"]}, "target": "['October 26, 1980', 'October 1980', 'in October 1980']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was pac man released in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fa5a906478a822ded77591446abc60779b885d9db7c4abe7c031fd934fe9ee2b", "prompt_hash": "66938dd209d5e1fce1701cb5bcdd966697b9a7ef92463391302184cafb17c263", "target_hash": "0fba5a651f107859140b353f86e1cd14c9969088249caaa8ab3f71d2d034d867", "exact_match": 0.0}
{"doc_id": 2958, "doc": {"question": "how much is the united states in debt to china", "answer": ["$1.06 trillion", "1,200.5"]}, "target": "['$1.06 trillion', '1,200.5']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how much is the united states in debt to china?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $1"]], "filtered_resps": ["$1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c1be1bd973c3402c3453b79ea6dc112535b7271ddefe17ae8b540c2ec1b56009", "prompt_hash": "2e3f0e7eb7819980055922183bb4841fe689d2f7c9f802be5aa0a7e1c359bad9", "target_hash": "97049c619f1f58b275c67f944ee1714c5d9c2377374543ff9634b896c2d6ed25", "exact_match": 0.0}
{"doc_id": 2959, "doc": {"question": "which type of hematoma is a result of torn bridging meningeal veins", "answer": ["A subdural hematoma", "subdural hematoma (SDH)", "subdural hematoma"]}, "target": "['A subdural hematoma', 'subdural hematoma (SDH)', 'subdural hematoma']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which type of hematoma is a result of torn bridging meningeal veins?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" extradural"]], "filtered_resps": ["extradural"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "460ab63f5d802651f9cbd28f42f467cfa71fb58341ce7294e44d881f101579fc", "prompt_hash": "728b3af699b2627163e04a211803f978c50a24c9148f220a73890539463640d3", "target_hash": "3c949e6b7b6a423ea29908da8456e647afc43b4054eefbe5488d719b089dbbe2", "exact_match": 0.0}
{"doc_id": 2960, "doc": {"question": "who sang i'm gonna run away from you", "answer": ["Tami Lynn"]}, "target": "['Tami Lynn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang i'm gonna run away from you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pretenders"]], "filtered_resps": ["The Pretenders"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "374e8c06aa235ec950126544e489588509b21ff65a39a3f96d2cb9d4d9791c04", "prompt_hash": "159a1c691f7378c133589f11ae2e97be8464f6aa3f9a88028c87696e7cc0fd0d", "target_hash": "afe9fb663e92ce60967c6cd2ba9941a02abceec1511e827db96c9866528e5dc0", "exact_match": 0.0}
{"doc_id": 2961, "doc": {"question": "what kind of sentence contains an independent clause and a dependent clause", "answer": ["A complex sentence", "complex sentence"]}, "target": "['A complex sentence', 'complex sentence']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of sentence contains an independent clause and a dependent clause?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" complex"]], "filtered_resps": ["complex"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6cceea91f340eff7c25dea046b609294379848cec212e19ea0f78df71f9b86c5", "prompt_hash": "5274c03c23c607df75947da6a11ea8563a1d6aede56b05a0a6ca4fc6031307ab", "target_hash": "1c8b4ed12e6978af3b44fd1645ed18f25942851f5d42e88e8b87aed97bebaa75", "exact_match": 0.0}
{"doc_id": 2962, "doc": {"question": "where did remember the titans camp take place", "answer": ["Gettysburg College"]}, "target": "['Gettysburg College']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did remember the titans camp take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Virginia"]], "filtered_resps": ["Virginia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49943b094e3e047f93f2836a26d3588db72e8ff481b375fe7182155d1faee0b8", "prompt_hash": "9013e19c4204c002bc1b48b0d6efc454b05a40cbe8cebb34f8354333edc98f16", "target_hash": "9b9622e65de73a33696af9687e2f07df52a3fe75335819d1b5571e65fb1394d0", "exact_match": 0.0}
{"doc_id": 2963, "doc": {"question": "criminal minds episode where jj becomes a profiler", "answer": ["season seven premiere", "The season seven premiere"]}, "target": "['season seven premiere', 'The season seven premiere']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: criminal minds episode where jj becomes a profiler?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The episode \"The Mentor\""]], "filtered_resps": ["The episode \"The Mentor\""], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5edaf72c65afb40505b7d2af7aa0fdd4b6f033c6bdf17921739e33a5d7861a76", "prompt_hash": "987f8d1470f6da2d60062f11d5b08b0981271671efc26868bdc3e8eebc0db233", "target_hash": "8cd4ca3fff0deaa3afc45f8d30a0bc8adc05862c129c29f55c452a9a27b67f49", "exact_match": 0.0}
{"doc_id": 2964, "doc": {"question": "what is the most fundamental unit of life according to the cell theory", "answer": ["cells"]}, "target": "['cells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the most fundamental unit of life according to the cell theory?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cell"]], "filtered_resps": ["cell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "719566ec0dac85c322a611ceaae56abf67374f637aa05a71044fab0eb7f515e0", "prompt_hash": "b430bf4bd80c5725f82630cdf601c25d8887b517633f51b5b63a2cb3504db669", "target_hash": "bdee41ac053217f822442b69c1ce90bbde72b1b4876ba7297b823ac5ce18ea58", "exact_match": 0.0}
{"doc_id": 2965, "doc": {"question": "how many national parks are present in india", "answer": ["103"]}, "target": "['103']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many national parks are present in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1e6aa927f669f1f73988cdc29e8f608fd00e585b68daa023c541d6ed0139794f", "prompt_hash": "e9b870c62ae0337f5457301528188d7b9b6ddaa02d82646f24f345c4f2f1a2af", "target_hash": "be125be1da931f231bdfedcb4e29f770723a61bd33861ba8b2cda73fe5c71c85", "exact_match": 0.0}
{"doc_id": 2966, "doc": {"question": "where was the remake of wake in fright filmed", "answer": ["Broken", "Broken Hill", "Broken Hill and Sydney"]}, "target": "['Broken', 'Broken Hill', 'Broken Hill and Sydney']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the remake of wake in fright filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Australia"]], "filtered_resps": ["Australia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7dcf97dd8e8bebda5cb1479f4ee79f599a48b88a4936c4229cc2c07625fb6b47", "prompt_hash": "a3a4ad23e1a0818a617371f36a098f9c5d23b3f4206529cce34fc6b806239d52", "target_hash": "cab5d3b0b0e2db69f910249cb49c45a9b85ec1b8acc850222f97b92234c8c692", "exact_match": 0.0}
{"doc_id": 2967, "doc": {"question": "the atomic number of indium which belongs to 5th period is", "answer": ["49"]}, "target": "['49']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the atomic number of indium which belongs to 5th period is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 49"]], "filtered_resps": ["49"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a3296259efb57634ac066b88e2ed7e99341eccd803eb4a2e3fc19de8437c9ec5", "prompt_hash": "830bc887f5edc09cae207e456ef143b72d960c8154a76ac623a0bfebe5a04889", "target_hash": "2b690380fb86d30aef6f8c891642870b0029a978b3a227044c779c18bedd89e6", "exact_match": 1.0}
{"doc_id": 2968, "doc": {"question": "bowler to take hattrick in both inning in test match", "answer": ["leg spinner Jimmy Matthews", "Jimmy Matthews"]}, "target": "['leg spinner Jimmy Matthews', 'Jimmy Matthews']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: bowler to take hattrick in both inning in test match?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wasim Akram"]], "filtered_resps": ["Wasim Akram"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2108efd5c4f8c70bf4afb128ed88cde27992c703add44ba4a469c9d14c3929f4", "prompt_hash": "7662688d76efe27ebc413a82cd0a06afe9276d2857038eb45ba322668776b926", "target_hash": "dfeba8b10e29b0552afdb4bcbd7c3aff26e3248fad1a0d909476278ee886512e", "exact_match": 0.0}
{"doc_id": 2969, "doc": {"question": "where does the synthesis of new dna from existing dna occurs", "answer": ["origins of replication"]}, "target": "['origins of replication']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the synthesis of new dna from existing dna occurs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" DNA replication"]], "filtered_resps": ["DNA replication"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3824690acc673567ff4a6cab76d7bbcc8b90c0aed1252f6cc75514f943c74988", "prompt_hash": "8c59c8f8f77fead0450d7cb3b33411fb498efc29ed0c18d38f1bcbd2d84d4490", "target_hash": "a9e8f33eae176a80cbf3f590ca68abc66f63b14d670d27cbaf12cff761fa98f3", "exact_match": 0.0}
{"doc_id": 2970, "doc": {"question": "who plays the prosecutor on law and order svu", "answer": ["Raúl Eduardo Esparza"]}, "target": "['Raúl Eduardo Esparza']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the prosecutor on law and order svu?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mariska Hargitay"]], "filtered_resps": ["Mariska Hargitay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b198183f961f3189c18eaf8c9bc2b4a0829771509075201c8198673e31c76a41", "prompt_hash": "2593cdf56adedaf3f07985b1f58630881199f7ac754ae055e7ee7c0b62607946", "target_hash": "da078c4d6dbbb9685c3e56c3f9f3695597568ebe2d8d45345b93cf4d41cbc2d9", "exact_match": 0.0}
{"doc_id": 2971, "doc": {"question": "when did the ship hector arrived in pictou", "answer": ["1773"]}, "target": "['1773']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the ship hector arrived in pictou?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1773"]], "filtered_resps": ["1773"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0a8c9bb4fff6a76007d0db765deb3d60b12d52b9fb5843407341436c3aae96b2", "prompt_hash": "1eac44c479ca161d07921ca50e16dfc838260dbcc23ac7c2cc1b80589e008684", "target_hash": "3cae0c9e76e5cb83ee73e225d1c0c4bd3bd8321ad16dd3d6b478d50444d8ea2d", "exact_match": 1.0}
{"doc_id": 2972, "doc": {"question": "who plays the mom on the tv show mom", "answer": ["Allison Janney", "Anna Faris"]}, "target": "['Allison Janney', 'Anna Faris']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the mom on the tv show mom?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Allison Janney"]], "filtered_resps": ["Allison Janney"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c764a9646644bd5d2c8bee8563fb6857f2ad04eccafaec34d871c5d6993a864a", "prompt_hash": "1495f5b4f903ee43238d8401076d837adcea15cb254142e293c152e7af14670f", "target_hash": "eaff5eb76835ca719cd84c5fc2d09dad88ab1a0e4dfd6bda1fad84b9d6476b7c", "exact_match": 1.0}
{"doc_id": 2973, "doc": {"question": "when did the song the joker come out", "answer": ["1973", "October 1973"]}, "target": "['1973', 'October 1973']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the song the joker come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1989"]], "filtered_resps": ["1989"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0be03c2c35020142ce1e2b020f3c84f580abdfc3d049fdb61fd963677e3c1638", "prompt_hash": "9138055cf7ab602bde483d75f94f33bd44e08b8c3e2044af6173b63f3d1e6614", "target_hash": "774399fd0df63fa671f4c28dfa6ee8d68bb4d323bceece1fe0b43b0141e3fa8a", "exact_match": 0.0}
{"doc_id": 2974, "doc": {"question": "where is bigg boss tamil house is located", "answer": ["Chennai"]}, "target": "['Chennai']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is bigg boss tamil house is located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chennai"]], "filtered_resps": ["Chennai"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e55bbe319782ac6bfc86c5f92bd5d5eed1804ffecdd66ee5cc08571151db5905", "prompt_hash": "80ca9fa1f45c9b31ead15876d80f6a9b598558d659ef3e45f6fbe3ccefd86ec3", "target_hash": "1e155e0e5e1c5c33b612794f5fbe9cab39931464e5cfa2c4f992ac47538ea519", "exact_match": 1.0}
{"doc_id": 2975, "doc": {"question": "who wrote yakkity yak don't talk back", "answer": ["Jerry Leiber and Mike Stoller", "Jerry Leiber", "Mike Stoller"]}, "target": "['Jerry Leiber and Mike Stoller', 'Jerry Leiber', 'Mike Stoller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote yakkity yak don't talk back?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chuck Berry"]], "filtered_resps": ["Chuck Berry"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "045c2cd6fd9738d2219f14539eb1b8318f650a36b21f8b3e1700d66feeb43c74", "prompt_hash": "828ebbd4b43e0aba64c691e04c80cc420eeda0aabbf04bfc041fdbd473acb405", "target_hash": "1037ed00faa6a762cae0dd8901ef56d78377927ca51949513ed79a64a373e350", "exact_match": 0.0}
{"doc_id": 2976, "doc": {"question": "who played doctor smith in lost in space", "answer": ["Jonathan Harris"]}, "target": "['Jonathan Harris']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played doctor smith in lost in space?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Guy Williams"]], "filtered_resps": ["Guy Williams"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7326e4259270fa16507ee24e68034abe9724d3c169cb843f62498c2a02930dd4", "prompt_hash": "7ada0642050de0b772fa7d7c7f5da92160f9031cd155207725064a2f51434110", "target_hash": "173af1aa54b52a35488a309b92fdf85399e5ccc341898a402fab4f4a22c6ca8b", "exact_match": 0.0}
{"doc_id": 2977, "doc": {"question": "who won the ncaa basketball championship in 1994", "answer": ["Arkansas", "Arkansas Razorbacks"]}, "target": "['Arkansas', 'Arkansas Razorbacks']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the ncaa basketball championship in 1994?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kentucky"]], "filtered_resps": ["Kentucky"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f606821b8edbdc8a1bc5a33fd658e2c1dff61f5e04144cde008387c01b40c62", "prompt_hash": "f5c740d184906bfda1ba2160feda97f5e8a3cb2bcb1ed5e2ea8bd23dec8314a7", "target_hash": "cad13c02b5d6de2ae12ca73e7bbfee988e1ee39bf01890d9474d40428fe5c22e", "exact_match": 0.0}
{"doc_id": 2978, "doc": {"question": "where was the original flight of the phoenix filmed", "answer": ["20th Century-Fox Studios", "Buttercup Valley, Arizona", "Pilot Knob Mesa, California"]}, "target": "['20th Century-Fox Studios', 'Buttercup Valley, Arizona', 'Pilot Knob Mesa, California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the original flight of the phoenix filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arizona"]], "filtered_resps": ["Arizona"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e22110fbb58d01a75bc7abc6e9c26f009e566e5b04196f124336a1be793c9c7d", "prompt_hash": "43c4839d0af5d47323a6d628ba4c4dff415ecde85f1e1dca4de2ed6dd7526bdb", "target_hash": "b96758df143fd2c92e2f4760e8b766578b14d401db32c40df1d959828fed0f93", "exact_match": 0.0}
{"doc_id": 2979, "doc": {"question": "what nba team did derrick rose play for", "answer": ["Minnesota Timberwolves", "Cleveland Cavaliers", "New York Knicks", "Chicago Bulls"]}, "target": "['Minnesota Timberwolves', 'Cleveland Cavaliers', 'New York Knicks', 'Chicago Bulls']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what nba team did derrick rose play for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chicago Bulls"]], "filtered_resps": ["Chicago Bulls"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f9e1c718e4bb0d5911868e68f93754f9a5921f23570241bf5b51520e8ba60df", "prompt_hash": "10ef70d2ea13f516c9a0d0b385ccbc6e3c802600cdbb03ec5475df762965e7ed", "target_hash": "8d623a217142bc0fda1257b312e3161494082b96dbc0fa94fbb28050ffd3a1fe", "exact_match": 1.0}
{"doc_id": 2980, "doc": {"question": "points outside the production possibilities curve represent combinations of products that are", "answer": ["unattainable"]}, "target": "['unattainable']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: points outside the production possibilities curve represent combinations of products that are?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" unattainable"]], "filtered_resps": ["unattainable"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aee9bfaae4c9a8406fc7e199c5ec0f5b2fdb90595a8bdcf7986dc02ae7f2be17", "prompt_hash": "348de64236be1eba3306f708b6f464fb1e1c8c1e41f2d6a18bc7642ffdf986f1", "target_hash": "53192af106a41c727f9eda2528d30efa377827167cb62097dbd7509b90d5537e", "exact_match": 1.0}
{"doc_id": 2981, "doc": {"question": "where is dancing on ice filmed this year", "answer": ["RAF Bovingdon"]}, "target": "['RAF Bovingdon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is dancing on ice filmed this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Milton Keynes Ice Rink"]], "filtered_resps": ["Milton Keynes Ice Rink"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a5f7fe9d6c4fbf3f860907c8d9238af574478b9922bc90e939338a1d824f548", "prompt_hash": "52eb05bbdf8456e98c6884c16fa23b66e8edd402430c24d232f76d2288e8f2b5", "target_hash": "a164eb91936de8228ae8bb80677723c61084ee030b64180bd0e2f8ab225f8a59", "exact_match": 0.0}
{"doc_id": 2982, "doc": {"question": "another name for a hairpin bend crossword clue", "answer": ["hairpin bend", "switchback", "hairpin corner", "switchbacks"]}, "target": "['hairpin bend', 'switchback', 'hairpin corner', 'switchbacks']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: another name for a hairpin bend crossword clue?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" S-shaped"]], "filtered_resps": ["S-shaped"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dbabc7625426e7dcc7f33517a4036fabad0c11713f8aaf3843804f1467576cb6", "prompt_hash": "69232295cdd8b166817d5631aeabaefdac7b96ec7e37b616cf26a41378010238", "target_hash": "d674a2d5bc30ce7fe89811111178bc3d937521479a56652f0d83b5d24f08438b", "exact_match": 0.0}
{"doc_id": 2983, "doc": {"question": "who sang the theme song for laverne and shirley", "answer": ["Cyndi Grecco"]}, "target": "['Cyndi Grecco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the theme song for laverne and shirley?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Jackson 5"]], "filtered_resps": ["The Jackson 5"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "478ecb6be610f6dfda74e088554a7ebf64a77fb997cb9bf84f11a3b9900339b4", "prompt_hash": "31df0ef606e3d20e4c2c9b05eb53e7a13e1fd6d698a7230e3583eea29bceec6c", "target_hash": "9feb70975f6dad5f04468511614206d59ddf9f166f46ccf64e29501ad5f962ce", "exact_match": 0.0}
{"doc_id": 2984, "doc": {"question": "who are the dallas cowboys playing on thanksgiving", "answer": ["Los Angeles Chargers", "the Detroit Lions"]}, "target": "['Los Angeles Chargers', 'the Detroit Lions']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the dallas cowboys playing on thanksgiving?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Detroit Lions"]], "filtered_resps": ["Detroit Lions"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9cf0da003269188e6e873ee6aac50d6ed25e807e15604b634b67e80c17f2e105", "prompt_hash": "1cb933dd310fd42ac402aa04a1714e5a09398c2bd8e7bbab5dca7eaf663ac02b", "target_hash": "3945fee8d989b8d63cd9136d175d1e6da19d94f4cd8fcc31693d2ea1f2d098ba", "exact_match": 1.0}
{"doc_id": 2985, "doc": {"question": "a request to the supreme court that it review a case that was already decided is made via a(n)", "answer": ["a writ of certiorari", "writ of certiorari"]}, "target": "['a writ of certiorari', 'writ of certiorari']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a request to the supreme court that it review a case that was already decided is made via a(n)?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" writ of certiorari"]], "filtered_resps": ["writ of certiorari"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "536f0d3a50981521ae4b15f12d1f97bb775dfd3e4b9e43e20ef52b632b1f39c7", "prompt_hash": "f0d6fb077ab605a4e6b6d9fdb6eaec1250d75483d6770e2d076511158eb577d7", "target_hash": "d5f77edb0804ccff3b1fb12f8d36e0501717b5410052cc867999b1be2739385e", "exact_match": 1.0}
{"doc_id": 2986, "doc": {"question": "when did the last volcano erupt in iceland", "answer": ["2010"]}, "target": "['2010']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the last volcano erupt in iceland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2014"]], "filtered_resps": ["2014"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb08b9c8a038153d5d8c081b7b7b8d0f4a373feecbd44190e2e687a4e43e8b37", "prompt_hash": "d98df4b211bf1c763e1f87a685395185cb8321a3479f2a6d71d4f3c171bf6ba3", "target_hash": "2ee1672483fc554adda35e9fdae9ee5db286e7d25dde7bd484842b76ac64c784", "exact_match": 0.0}
{"doc_id": 2987, "doc": {"question": "what nfl team is robert griffin the third playing for", "answer": ["currently a free agent"]}, "target": "['currently a free agent']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what nfl team is robert griffin the third playing for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Washington Redskins"]], "filtered_resps": ["Washington Redskins"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b3237053b5d069730749cf244df867d985866dba2a88bb1dc469b3ffe897162", "prompt_hash": "d346c506ab9a3ff1350728d75640eb0d4673542ddbdc3ee5d0b5eef3aa4b91c7", "target_hash": "f86924552a45a5ceb910b09b0275146bb813dae29cb7d05cc4479fd836435baa", "exact_match": 0.0}
{"doc_id": 2988, "doc": {"question": "who did the united states fight in the war of 1812", "answer": ["United Kingdom"]}, "target": "['United Kingdom']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the united states fight in the war of 1812?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Great Britain"]], "filtered_resps": ["Great Britain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9905159598299d4c3ab916235b546dd5e349454575c6cd27b5a8548500e8775b", "prompt_hash": "76ee5b4b798b0e8f7cdd22857304570772ea552326d4aee2996516114c233376", "target_hash": "e1f0e94d1cacf16d9316103aebc358ef7dbc9ed7c63ea7550c58c3f5bc3fbb4e", "exact_match": 0.0}
{"doc_id": 2989, "doc": {"question": "india's first arctic research station set up in 2008 what is the name", "answer": ["Himadri Station"]}, "target": "['Himadri Station']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: india's first arctic research station set up in 2008 what is the name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Maitri"]], "filtered_resps": ["Maitri"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fa9d058c0dd2c09f1652ebedd545d8e73a2a3b84ea95c7982f7b29f377434ce9", "prompt_hash": "78c963fccab29562826461ca8a3032e525dc477137b6e3a1b3453fb74d043dc4", "target_hash": "34e57b2f2472d4e1c071c40fca54450920d8237de2631eeaf74d5e3abea94e87", "exact_match": 0.0}
{"doc_id": 2990, "doc": {"question": "name of black man in to kill a mockingbird", "answer": ["Thomas \"Tom\" Robinson", "Tom Robinson"]}, "target": "['Thomas \"Tom\" Robinson', 'Tom Robinson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name of black man in to kill a mockingbird?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tom Robinson"]], "filtered_resps": ["Tom Robinson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2d05d44e05bdbbc9f09645814b35916efd87ad859d5f6b16b41d6e0d1b9eb0fc", "prompt_hash": "5e914ff36467ff3e1dc7003b6d39ffc41ffcab8c67b97885abb6ab9306ccb5d0", "target_hash": "82987cb9819db0d9f1ed28b9640c8bc5205d680332e2b69ab97b73eed3dcaf20", "exact_match": 1.0}
{"doc_id": 2991, "doc": {"question": "why does cooling water run through the condenser", "answer": ["condense the steam"]}, "target": "['condense the steam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why does cooling water run through the condenser?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to condense the steam"]], "filtered_resps": ["to condense the steam"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77bb036b69afcf9bfe15241e75a8003d2493e13c044e8ca01d4d6a85dbceec48", "prompt_hash": "cb603b96ca45fce1042c5508481eb6bcfa9653f075f59d596f2791ef691cd583", "target_hash": "e6b3c56ac6ec9902d73521bc1334035dc74a5004e3db606e44b61dd7fb5c9720", "exact_match": 0.0}
{"doc_id": 2992, "doc": {"question": "what is the name of son of lord krishna", "answer": ["Pradyumna"]}, "target": "['Pradyumna']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of son of lord krishna?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Balaram"]], "filtered_resps": ["Balaram"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "366b687125336ae909a331baa5e87906d992a02dbabd096291223776ccebf2ac", "prompt_hash": "3372d128b26dbe17d87a0ae85b1930569e88222bf3a8d8b8a414ea345a3588e2", "target_hash": "ae562ad9114fd6c42b9366df06f56c302afe6868aa1bcc740797dfbec63b8021", "exact_match": 0.0}
{"doc_id": 2993, "doc": {"question": "what nfl team has the most expensive super bowl ring", "answer": ["the New England Patriots", "New England Patriots"]}, "target": "['the New England Patriots', 'New England Patriots']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what nfl team has the most expensive super bowl ring?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dallas Cowboys"]], "filtered_resps": ["Dallas Cowboys"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b69a13e6eda50ec6fd562ccebcd9379bf08022d83c2f4f93c60ea500470c81b", "prompt_hash": "09d00cc2b4bfd5022cd331f18ee13b89d5fff365a8c89fa1fed182451eaa697b", "target_hash": "e3daf16cb940144e7409ee73b15197f3fdf0531918a55f1e90b91c99c1a24387", "exact_match": 0.0}
{"doc_id": 2994, "doc": {"question": "who sings i'm going to soak up the sun", "answer": ["Kidz Bop Kids", "American artist Sheryl Crow", "backing vocals by Liz Phair", "Sheryl Crow"]}, "target": "['Kidz Bop Kids', 'American artist Sheryl Crow', 'backing vocals by Liz Phair', 'Sheryl Crow']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i'm going to soak up the sun?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Pointer Sisters"]], "filtered_resps": ["The Pointer Sisters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2770818d0e8edc06bbe8ba8eb177441bab942aff4d62595510d5618b9e95fb8", "prompt_hash": "840d8a9ee858d3b4fd93c53b55ba05919c8d95935d8c46caf98ab3eaa7351e18", "target_hash": "f608384af2b2d76a82b0b27981ce86073f039131434235251d7f3c245abf9888", "exact_match": 0.0}
{"doc_id": 2995, "doc": {"question": "what does rt mean on a dodge car", "answer": ["Road/Track"]}, "target": "['Road/Track']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does rt mean on a dodge car?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ram Truck"]], "filtered_resps": ["Ram Truck"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5ef89fe019f687c729bfce091527519993d5b0a5b08437b5e260b55b0f69f08", "prompt_hash": "39bb59f3823a3ec439207132c9e84373cc6c2b3d31839df5189710f539afc427", "target_hash": "90eebb163e68273aa13b73e6838e7e3d692fc95e47b79bb4d0cf24ba3644dc70", "exact_match": 0.0}
{"doc_id": 2996, "doc": {"question": "where does water come from in new york city", "answer": ["The Catskill Aqueduct", "The Delaware Aqueduct", "The New Croton Aqueduct"]}, "target": "['The Catskill Aqueduct', 'The Delaware Aqueduct', 'The New Croton Aqueduct']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does water come from in new york city?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Catskill Mountains"]], "filtered_resps": ["Catskill Mountains"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d0d1c499a912343ba86c33b4ffc7182d95ba3de49d8a2fae20de1c3b324ca9a0", "prompt_hash": "1477c70fe6b6de02203b9c870e1591af7f87304f9c01b545899621c1be6d69c2", "target_hash": "2cab2b3b36c314b7225ce90e31e11fcccc8270c50b4d6fa2703af3cf3f0dae3b", "exact_match": 0.0}
{"doc_id": 2997, "doc": {"question": "who played mike stivic on all in the family", "answer": ["Rob Reiner"]}, "target": "['Rob Reiner']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played mike stivic on all in the family?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Steinberg"]], "filtered_resps": ["David Steinberg"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6e5ada846b8be8fa5f064480cc5a5f6aaaacdd01784f7fa4aadf4fdb3cee2499", "prompt_hash": "525adfc652df01ea9428a3b340284b18d83eaf2989bdaf2a0d114d05d4b76fda", "target_hash": "b598b80e6f6533ce334151f93aee1a686b7099ca40d1bf6174dbddeb132d1fa2", "exact_match": 0.0}
{"doc_id": 2998, "doc": {"question": "who plays the grandmother in game of thrones", "answer": ["Rigg"]}, "target": "['Rigg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the grandmother in game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lena Headey"]], "filtered_resps": ["Lena Headey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1827f1a6a7f22fdc5b2dcce2039b7906a9a779e78e9e9a994ed185e933d50cd7", "prompt_hash": "a6c7a06eaee57f35c80e56897bd7dc9e4da7f4dea8ba37011fe9ccfdb343cce5", "target_hash": "9414ab5e31b5965bcc5f7b418f743f763b7a5c27ae30fea96a26a5bc52b830b8", "exact_match": 0.0}
{"doc_id": 2999, "doc": {"question": "what was the main characteristic of post-world war ii american society", "answer": ["strong economic growth"]}, "target": "['strong economic growth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the main characteristic of post-world war ii american society?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" suburbanization"]], "filtered_resps": ["suburbanization"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f1c1505ada4ef3dee51c58336fcdbcc3e8579a7e57b95fa37e25f44a60132f77", "prompt_hash": "f68640c84354f4ca25d4e77cca198f04bbeebea594d442b66459d217180d175b", "target_hash": "5272c1424037ab207a7dfb54deb222ad0f22b3b2456df99d72912c3be26645a5", "exact_match": 0.0}
{"doc_id": 3000, "doc": {"question": "what is the name of the skin between your nostrils", "answer": ["the nasal septum", "septum"]}, "target": "['the nasal septum', 'septum']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the skin between your nostrils?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" nasal cavity"]], "filtered_resps": ["nasal cavity"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f89a3cb9be7409ff4c726b5c3dce72c51f3cefd5fdc94c9a3da6892163362a65", "prompt_hash": "ee35be1accb86b988666b03dc1924fec830715025f528d67fc793e4072537ce4", "target_hash": "5eb91a32557ca901a16e311f315841403b1808ea570f0258b8580b1e9a0cc51a", "exact_match": 0.0}
{"doc_id": 3001, "doc": {"question": "who was the leader of the zulu in south africa who led the fight against the british", "answer": ["Cetshwayo", "Cetshwayo kaMpande"]}, "target": "['Cetshwayo', 'Cetshwayo kaMpande']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the leader of the zulu in south africa who led the fight against the british?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shaka"]], "filtered_resps": ["Shaka"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f4c3ac874860d90128f4f90cc7622e1683e2db303e9f215146c2f5ecf2c17be6", "prompt_hash": "fb7333ec798a2477fecabc2b4c0b63eb082325fa2fb0e0e95b8b1fc767de3b44", "target_hash": "2e32882b36a94907e42e395a5f2c74dac49e066e743e9f03a0fac3cc9ba7912f", "exact_match": 0.0}
{"doc_id": 3002, "doc": {"question": "who is playing halftime at the pro bowl", "answer": ["Jordan Fisher"]}, "target": "['Jordan Fisher']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is playing halftime at the pro bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f91a6a4134f1fb4c453bbe909da8cf5c4734b052fdb57694cd374130c9560b9e", "prompt_hash": "c57af86f25bab74805ea9cae730659c9594c11e3b6f21d5c3688821b71250c33", "target_hash": "189effca359951ab7568beaff496faacc3d0aa51678f5c0a9e7418fcec8ad56c", "exact_match": 0.0}
{"doc_id": 3003, "doc": {"question": "what are the importance of rigor mortis in meat processing", "answer": ["tenderness of meat"]}, "target": "['tenderness of meat']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the importance of rigor mortis in meat processing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rigor mortis"]], "filtered_resps": ["Rigor mortis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a67eeaa3459c648f7e28b17c6261205d823b2f856caf4841c5cfda31c1738105", "prompt_hash": "181814a00dda4e620a773c7919751708086d3e92021198f8bbc3123886e7bff1", "target_hash": "ab7305880d1a5360f45c8fcefa0599897c3676beaac6bd20bd9009052d173022", "exact_match": 0.0}
{"doc_id": 3004, "doc": {"question": "right to property according to the constitution of india is a", "answer": ["constitutional right"]}, "target": "['constitutional right']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: right to property according to the constitution of india is a?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fundamental right"]], "filtered_resps": ["fundamental right"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06da751e3dfdd371cbb137e7420016b1152defa673804cc23b244147601b9aff", "prompt_hash": "dde846e5bd2cf1abde41efb82d7ee986ba61933bfb34cc95a1dd401612440ba9", "target_hash": "1a91061ca1c1a9cebe6ad16dd038f95ce2f0f242ba2645e3247bf6a9216f1d02", "exact_match": 0.0}
{"doc_id": 3005, "doc": {"question": "today gestalt psychology ideas are part of which branch of psychology", "answer": ["Berlin School of experimental psychology", "experimental psychology"]}, "target": "['Berlin School of experimental psychology', 'experimental psychology']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: today gestalt psychology ideas are part of which branch of psychology?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" cognitive"]], "filtered_resps": ["cognitive"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "538f62172d7dfa4b1aab08806079d19cdcd872c0b1134df6bfd4aee488ce0cf1", "prompt_hash": "b3164628e01a5209aaa84ec2ac66ee4688d8c59922b07da5da4a7581815b8e6c", "target_hash": "dc1def7a18b334d6bea3d57534b4055d9ca89066ab25ee2f6df6b6aad56b5cae", "exact_match": 0.0}
{"doc_id": 3006, "doc": {"question": "cls bank deals with transactions arising out of", "answer": ["foreign exchange market", "the foreign exchange market"]}, "target": "['foreign exchange market', 'the foreign exchange market']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: cls bank deals with transactions arising out of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lending"]], "filtered_resps": ["lending"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d064f0dec075b31fbb7a68bf769ba6d9f7b6a236957a80abc0ce59dae597cfb2", "prompt_hash": "a14d4735072044139adf398f3b2f895a475682211df7650f5551222938428806", "target_hash": "6409bce80f9e439362f1c82c3002a59a96c08409af46fe2acb31325fa15601f3", "exact_match": 0.0}
{"doc_id": 3007, "doc": {"question": "is aluminium a ferrous or non ferrous metal", "answer": ["non-ferrous"]}, "target": "['non-ferrous']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: is aluminium a ferrous or non ferrous metal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" non ferrous"]], "filtered_resps": ["non ferrous"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9ac6dc072409c485c436f3e4e1fa1579bb4cf96627aa5cfc5227dd9d3bb99d04", "prompt_hash": "ba3290053f48a2c687b47187760b985c1b843dbc54b278f8cf259f1ffe16485d", "target_hash": "7218993c06fa0ac186795581eb1a8e01f397eb8c04ead682a0e921d3ddee218c", "exact_match": 0.0}
{"doc_id": 3008, "doc": {"question": "how many grams of alcohol in one beer", "answer": ["14", "about 14 grams of alcohol", "14 grams"]}, "target": "['14', 'about 14 grams of alcohol', '14 grams']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many grams of alcohol in one beer?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12 fl oz"]], "filtered_resps": ["12 fl oz"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0371c44fe0e7c6d6ab84186f96fdd691d5417a8dcfdae14acbd5e0b7a11855a7", "prompt_hash": "9afaf78ad9620496fe31c7b534196d2007a3f9b0b66d3e298c7ab8cd94280a81", "target_hash": "a4855dd25c3229412161488400bf141c3754d928034192efe22aca60304775b3", "exact_match": 0.0}
{"doc_id": 3009, "doc": {"question": "what does a yellow 35 mph sign mean", "answer": ["advisory speed limits"]}, "target": "['advisory speed limits']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does a yellow 35 mph sign mean?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" speed limit"]], "filtered_resps": ["speed limit"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ee4d970a757b3b702d9aa1b1e20f692846b066eeec3c577053330efa02ecadec", "prompt_hash": "3ab8bb6b5c0779fb551a34d68ac9e23179528e4907e3b0543cd6ce588d63d52b", "target_hash": "269f9eacad998407754ddfc4752f7938a184d7385fea8173776af305bdf4289f", "exact_match": 0.0}
{"doc_id": 3010, "doc": {"question": "who came in last place on amazing race", "answer": ["Kevin & Jenn"]}, "target": "['Kevin & Jenn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who came in last place on amazing race?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Team Canada"]], "filtered_resps": ["Team Canada"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a96f5e98f182ba375605719371c405b20ddf402842fcc37da35e0ea9ee867696", "prompt_hash": "a32a5e1f6f8f1cdbf09572b53d8651d64d0dde28168e53e96fddc2c9ac42a875", "target_hash": "d9d527b728bcb489913d36cb895be6098ec4ff2fb05837773dd6fea07862e755", "exact_match": 0.0}
{"doc_id": 3011, "doc": {"question": "where is the extensor pollicis longus tendon located", "answer": ["dorsally on the forearm", "located dorsally on the forearm"]}, "target": "['dorsally on the forearm', 'located dorsally on the forearm']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the extensor pollicis longus tendon located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dorsum"]], "filtered_resps": ["dorsum"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e2efb1c3eb31d96154393786b6c64c8fc3fb80a7ad6e4bf3ecacb6829a343952", "prompt_hash": "699f5ed5890dcdd9aa1f59e8ccb3c54432df01d542e2dbbfad16fb694661b4bd", "target_hash": "3bfb4e53a8212488eba876c85e707d02515b657286a1af67961a45f58cae3cb6", "exact_match": 0.0}
{"doc_id": 3012, "doc": {"question": "when do you pray the chaplet of divine mercy", "answer": ["Good Friday"]}, "target": "['Good Friday']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do you pray the chaplet of divine mercy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Friday"]], "filtered_resps": ["Friday"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a15e87cbccabdf8e358133ba893aeb0085ba5bad192b53308fb1212b70c4ec99", "prompt_hash": "a038cb9a4ee400880c9cf324c9ebd0d9d41e921b201769756c28b190353a21f3", "target_hash": "b72a4df9b2ec093cf59cfee14d628cdfc05e8f1862c3b0968e3467f6800a0edd", "exact_match": 0.0}
{"doc_id": 3013, "doc": {"question": "where was the world chess tournament 2017 held", "answer": ["Tbilisi, Georgia", "in Tbilisi, Georgia"]}, "target": "['Tbilisi, Georgia', 'in Tbilisi, Georgia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the world chess tournament 2017 held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" London"]], "filtered_resps": ["London"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "693603063e610c1265e11d90505b77bd0ef28360c5a302c26a4a349efb6081e9", "prompt_hash": "78591901f73e8975e7f6b14c65de8ffb2b034397f9cafbfb0fded69a219c3646", "target_hash": "6a03be7339639ad4edf61de1a3b01c4e867316a5848bb3e3a5e989a1274677fa", "exact_match": 0.0}
{"doc_id": 3014, "doc": {"question": "who was the dj on in living color", "answer": ["DJ Twist"]}, "target": "['DJ Twist']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the dj on in living color?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Alan \"Dave\" Chappelle"]], "filtered_resps": ["David Alan \"Dave\" Chappelle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b9c4831690b985f75835a7cfdbba0ca9bfa55f50d3e98639cc89a64393c05ee", "prompt_hash": "354818b49e9a7514112758704bb8929a02bd2cf7729f2ffe331b1ab4401eeaef", "target_hash": "1ec2e1c5a0234a80bac4d34a71fb261a6d82954d90e869a1d435b7e38786ade5", "exact_match": 0.0}
{"doc_id": 3015, "doc": {"question": "who made the first to record with the electric guitar", "answer": ["George Warren Barnes"]}, "target": "['George Warren Barnes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who made the first to record with the electric guitar?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Charlie Christian"]], "filtered_resps": ["Charlie Christian"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f6dd609ac05a401d909ab08efb08bf2e7bc58eb0e7eaed02e317cc0582506ee", "prompt_hash": "8043ad9dc4e29331888adfdfd9e55d33353903c585dfda455e1d79e2ca89ea81", "target_hash": "9b07fa5b4a6174e0460b8192bf89726cdecf80c4c537d2d8ad277362c08708c7", "exact_match": 0.0}
{"doc_id": 3016, "doc": {"question": "the grand tour season 2 episode 2 celebrity guests", "answer": ["Brian Wilson", "Kevin Pietersen, Brian Wilson", "Abbie Eaton", "Kevin Pietersen"]}, "target": "['Brian Wilson', 'Kevin Pietersen, Brian Wilson', 'Abbie Eaton', 'Kevin Pietersen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the grand tour season 2 episode 2 celebrity guests?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Bowie"]], "filtered_resps": ["David Bowie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "88a6415103e0ca48346911495466934e02605000e2b55056148d44ac27be00d9", "prompt_hash": "9c07a3da8bc584ea63360306d81fbd3be49e30140cac3195de0eb19bd18ccb26", "target_hash": "c7b4342a51f7e8911dc6262e74f512f636f0aa9c3f4b02db51749f7c8a3834ce", "exact_match": 0.0}
{"doc_id": 3017, "doc": {"question": "when does the new season of law and order svu come on", "answer": ["September 27, 2017"]}, "target": "['September 27, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new season of law and order svu come on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" September 25"]], "filtered_resps": ["September 25"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a58a483358bd0bdbae16573f950357de0e1b7e43c1b325d90f42501943ef195f", "prompt_hash": "c495a62c61b18faa80b0c5e05b85320ba50bd5d693ae8c76094df7c26e71a3c5", "target_hash": "5865d73804e6229eb21de1665d8c77848d9ab9b6383c5c40a2dd429d2f31e471", "exact_match": 0.0}
{"doc_id": 3018, "doc": {"question": "who sing say you won't let go", "answer": ["James Arthur"]}, "target": "['James Arthur']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sing say you won't let go?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Arthur"]], "filtered_resps": ["James Arthur"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ed74279a7174df12bb24e5c86c6cd1dff4fd13798fa8c991035c447111303cd", "prompt_hash": "a53b59844aa61cddac7ef6250d2c44d0f48a1815ec73ce7814b223c62ea872df", "target_hash": "8ab5cb87ee18b521579af8c0e3c8ee4a89cc58471483135e5023ad3e0afe561e", "exact_match": 1.0}
{"doc_id": 3019, "doc": {"question": "who is credited with developing the geocentric view", "answer": ["Ptolemy"]}, "target": "['Ptolemy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is credited with developing the geocentric view?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Aristotle"]], "filtered_resps": ["Aristotle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f257e939231804f767b2918822632df06dee545b0cce21c29f2cceeb415ea287", "prompt_hash": "5001e5a3c64dd33b821ebffbd99bf3d6bd665ea8d5281d68dce95315471ac3ac", "target_hash": "707be503b2339633f56c56396bacf9278135eaaac7b6badcdfa9035b215e6801", "exact_match": 0.0}
{"doc_id": 3020, "doc": {"question": "where was the first high level language operating system created", "answer": ["at IBM"]}, "target": "['at IBM']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the first high level language operating system created?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" IBM"]], "filtered_resps": ["IBM"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7905fbd6b6bfda49055097f3269f2ee7028faa0100ff5c80d68acc1b0ca98a5", "prompt_hash": "107b1fc012e1a613cd34ac857481b96adb1ca2c5bb7886ee61ae649dcec9f962", "target_hash": "35279f18036b1eb39bab409cb77c02e803580c7b9fc58f07fb2651c91a6f165a", "exact_match": 0.0}
{"doc_id": 3021, "doc": {"question": "a legislative act passed by congress is an example of", "answer": ["statutory law"]}, "target": "['statutory law']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a legislative act passed by congress is an example of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" law"]], "filtered_resps": ["law"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e56f0c09880c795e37557e278f0b1b088eed77654ee3fca65a1559646254b0e7", "prompt_hash": "25a4761734a04dc61cba898c5884138b96c2bd549fb91c3fdf548d98f55de630", "target_hash": "a7e8b503238a8b246baf721bd0f846d389f3e350ee68da3e02ecb0f45f25674a", "exact_match": 0.0}
{"doc_id": 3022, "doc": {"question": "what was the last episode of the flash", "answer": ["The Elongated Knight Rises", "\"Finish Line\""]}, "target": "['The Elongated Knight Rises', '\"Finish Line\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the last episode of the flash?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" \"The Reverse-Flash War\""]], "filtered_resps": ["\"The Reverse-Flash War\""], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "46096e5f09922df3c80a9022cf436272d99c180120ef9e0d13e9b1c986642600", "prompt_hash": "51bdcd731e29b92955fc42c8d84fc548eb916af3cbbfe2050e75e66264f861fc", "target_hash": "e63dba6cb1454b9160bdf05abc94bcc311c81bc21938d81eb05d9655f595b693", "exact_match": 0.0}
{"doc_id": 3023, "doc": {"question": "who wants a heart in the wizard of oz", "answer": ["Tin Woodman", "the Tin Woodman"]}, "target": "['Tin Woodman', 'the Tin Woodman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wants a heart in the wizard of oz?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dorothy"]], "filtered_resps": ["Dorothy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b95e72faa4a65ff9720d33c08d1e93371a04ce997eebeef0c88869fe3e0bc568", "prompt_hash": "eb1376392e48214d796cbd421960dabf68c010a07b89047f9cbd3b989d7a5d29", "target_hash": "4f58f7e59682659e5e1b0457ed25820f7e82bb7fd97429bbe42d85bc61d2a46a", "exact_match": 0.0}
{"doc_id": 3024, "doc": {"question": "who plays the voice of chucky in seed of chucky", "answer": ["Brad Dourif"]}, "target": "['Brad Dourif']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the voice of chucky in seed of chucky?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Tilly"]], "filtered_resps": ["Jennifer Tilly"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7c43d78d188fa463bc4f5867def63aceb47d33ccfa6143be6b6e47e6696c6163", "prompt_hash": "e7d3f0cb8e4855cd7c014a3c8ac973a67cc57a4c6cbaf2835f27c0fe17ddd822", "target_hash": "958fbc90e7ff125bfb81022b38da8890b47b168047eb2dd7f89a313d54820c13", "exact_match": 0.0}
{"doc_id": 3025, "doc": {"question": "what is the latest version of microsoft office 2010", "answer": ["14.0.7182.5000"]}, "target": "['14.0.7182.5000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the latest version of microsoft office 2010?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 14"]], "filtered_resps": ["14"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f8c42766f2a3a68377b2378b85423e2b565705c74755c0d724c838396a8a968f", "prompt_hash": "a6cac7f23e3575429b96d36ee640fae0fa85863b2fdfc92d2a778f802848b376", "target_hash": "dd0f1d6b33610c07ee9c9f157f5627a09f8760f18d10399dd00d75329cd70169", "exact_match": 0.0}
{"doc_id": 3026, "doc": {"question": "who won the battle of saratoga in 1777", "answer": ["Americans", "the Americans"]}, "target": "['Americans', 'the Americans']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the battle of saratoga in 1777?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" American"]], "filtered_resps": ["American"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1fe4df5682c7ab3f8ed26797c133ee71fd82bef0867ae1debd147c3fa389eb03", "prompt_hash": "039fa1f9ecad3d59f3baf718e52c2a4451a91f60088382b5add2b4425c4bf708", "target_hash": "55c75aadb2822a34f783fefa95de727870d15510cec8262347306eb6eda893f5", "exact_match": 0.0}
{"doc_id": 3027, "doc": {"question": "where does creatine come from in the body", "answer": ["kidneys", "liver", "the liver and kidneys", "arginine", "glycine"]}, "target": "['kidneys', 'liver', 'the liver and kidneys', 'arginine', 'glycine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does creatine come from in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" liver"]], "filtered_resps": ["liver"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c4fac713f2a1411049c707992e097e199383c79ee99b53063a170806e8b9d65", "prompt_hash": "91b1586ec91f7ea4f6b7259620efa81ebd05c1a22d9ebed1f06a0ea14b5b5f43", "target_hash": "80e817c1ee8ce6bfd5ebd4de74628a440c61715493bcb4a7a6c104f63de14aa9", "exact_match": 1.0}
{"doc_id": 3028, "doc": {"question": "where is the battle of britain flight based", "answer": ["RAF Coningsby in Lincolnshire"]}, "target": "['RAF Coningsby in Lincolnshire']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the battle of britain flight based?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" RAF Northolt"]], "filtered_resps": ["RAF Northolt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8aadf5b96c300e72ce838bc696eb484984ac7003b6124219c95418a59eeabaa5", "prompt_hash": "a9631e4308c065eb0349d8cbabe5ab0e763b026313abcdef46ecaf3ad93e550f", "target_hash": "cf97e2a06c03616d8e6739a891abbf1454b92a45876cec08c3d3d58e82c767a5", "exact_match": 0.0}
{"doc_id": 3029, "doc": {"question": "who is the president of usa right now", "answer": ["Donald Trump"]}, "target": "['Donald Trump']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the president of usa right now?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Donald Trump"]], "filtered_resps": ["Donald Trump"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aa2db21608cbae0e0db08d71ba1859761b91b7f8bad432fbd87fed3011c90c5c", "prompt_hash": "3986739317e3857c9ba7b726e53edf51b01651e43dc292f267008bbcc33c46d6", "target_hash": "1fcc52a6487444a047da2b30957f03261279dddf27070a8e431cae08a8d2055e", "exact_match": 1.0}
{"doc_id": 3030, "doc": {"question": "who sang the song suddenly with olivia newton john", "answer": ["Cliff Richard"]}, "target": "['Cliff Richard']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song suddenly with olivia newton john?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bertie Higgins"]], "filtered_resps": ["Bertie Higgins"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fa5cdf4655ac2ade8297616c09fa6baa5a7cd82919224972730e59e68e51f7e5", "prompt_hash": "025d4083df7f1d203d01c58d4b6581c4b296f55871a7bb9db17e63dd422b1a08", "target_hash": "b1722847dc4607686cfd71a6593632bf587b11a08e887e3d3a59708c00396b73", "exact_match": 0.0}
{"doc_id": 3031, "doc": {"question": "where does the movie proof of life take place", "answer": ["The Republic of Tecala"]}, "target": "['The Republic of Tecala']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the movie proof of life take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ef82d1462f6f715eebeae47bc0d272805083a8ba95f42fc522f6b6f7787ef074", "prompt_hash": "0eb9cdd4e3104024840205ff18ea22d2c0ed5c9e7a7601f43db564f31db97b33", "target_hash": "ccd2afd3963b6c7de3efde4302c9bdc5ec0f5703e6a7449e05342ecbaa030264", "exact_match": 0.0}
{"doc_id": 3032, "doc": {"question": "who was involved in the mapp vs ohio case", "answer": ["Dollree Mapp"]}, "target": "['Dollree Mapp']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was involved in the mapp vs ohio case?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mapp"]], "filtered_resps": ["Mapp"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c2c6daaaae91b56962096d246a2a88394e0ee37a8d3b543f8403bddb57e4177d", "prompt_hash": "2106fe8e6b0b4906982d8607668c38be78e31a9583349a8c1421ea5c10c71fe3", "target_hash": "9cf00ca4218badad4307d9449e45bd3bb5f3c45da13be85863fc11d4543a40c2", "exact_match": 0.0}
{"doc_id": 3033, "doc": {"question": "who was the sixteenth century physician who published", "answer": ["Andreas Vesalius"]}, "target": "['Andreas Vesalius']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the sixteenth century physician who published?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Harvey"]], "filtered_resps": ["William Harvey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0cb5b56863a9681fd94352767fb654c2ffeba2bed62c120e23fa5773ee1c4ad2", "prompt_hash": "461cbab2fa953c1b1ae13064bf4fd08fbe8c4ba2d8d539b3a99377eaf9edd735", "target_hash": "6b8e1c9bab6e3ecba291592a32308c92b5611876ae0884cf577506f43f4d777c", "exact_match": 0.0}
{"doc_id": 3034, "doc": {"question": "what is the cross on a letter t called", "answer": ["arm", "crossbar"]}, "target": "['arm', 'crossbar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the cross on a letter t called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Transom"]], "filtered_resps": ["Transom"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bfa0da26ecef62ab0b26c081c2149958f1ad056aef7e5c1dc4209e2d607c0bd3", "prompt_hash": "3708114cbcef0ab1b14e882b54d9cececb38fa7d0301200f13dbf616647f6843", "target_hash": "cb8bf9ef53c8b14540beac463207ccf63c820380c5539cadd5dbe39e51cb7e41", "exact_match": 0.0}
{"doc_id": 3035, "doc": {"question": "how many episodes of the killing on netflix", "answer": ["6", "six"]}, "target": "['6', 'six']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes of the killing on netflix?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e67ac69f04e2a9d4b92e0c12d15427d75b33ace019694c0f208215eeaa5adcc9", "prompt_hash": "4468c4f37d77fb5d95c7d0ceb021b810888883bde8610e5f13c0c8c6c0bfa5ca", "target_hash": "25b67b0377e96bcf611a32e31c108b5a330c4b9724a4d4d5b11182a8902f643c", "exact_match": 0.0}
{"doc_id": 3036, "doc": {"question": "how many episodes are there in season six of nashville", "answer": ["16", "16 episodes"]}, "target": "['16', '16 episodes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes are there in season six of nashville?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 16"]], "filtered_resps": ["16"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e5902374e2768f1c7a93e191a51c1e4fa7a3b1c931a4a311e8b27fdf91be5481", "prompt_hash": "3f33f4baf44638e458dbbe4818ee2db79351b4c8e0364197a17a1a0991ab9ed3", "target_hash": "a3a321293b4e5af88fc7e20576c185b79e3286479c610bdf6b49a968250a0ec1", "exact_match": 1.0}
{"doc_id": 3037, "doc": {"question": "what breed of dog is the teddy bear dog", "answer": ["Zuchon"]}, "target": "['Zuchon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what breed of dog is the teddy bear dog?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pomeranian"]], "filtered_resps": ["Pomeranian"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b21d60e6f448c4cbe5733ec9df799be54f3eff3a741ff1581019153ccaecd532", "prompt_hash": "95ca240f9b3e369925b6c5cb8f523476dcfbc904d7bd7e61d2aa9c054b157e0e", "target_hash": "46aea1afb450dfd657b52b52ba50ddf10230890a01580eea5683538a0f534acb", "exact_match": 0.0}
{"doc_id": 3038, "doc": {"question": "how many levels are there on the eiffel tower", "answer": ["three"]}, "target": "['three']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many levels are there on the eiffel tower?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 3"]], "filtered_resps": ["3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f61b11c7c26c8588c409db578735a40d12f897780ef6ddfbcad042b982c90888", "prompt_hash": "aef00be0e410fc9eb32dd2de6f6416c1f77fd82c77817e43f97e8bcc7aa051d3", "target_hash": "141a32643fe9577b35f39034369dfd8915892451c7d2aadc2c7af5642262e23f", "exact_match": 0.0}
{"doc_id": 3039, "doc": {"question": "how many episodes in 1st season of the crown", "answer": ["10"]}, "target": "['10']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes in 1st season of the crown?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 6"]], "filtered_resps": ["6"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b6c8cdfce32d2feb8cccd2933ea031fe4812941cb6c0f3a3629fa831a4ccb4a", "prompt_hash": "13218794f3c31dc68b61346a0e175806abddaafd435f0df7064a03041307e7ab", "target_hash": "9503346d501aeaec3f521be685eb235cda81c7929ba2f1acdae1be91400b787a", "exact_match": 0.0}
{"doc_id": 3040, "doc": {"question": "who was the first chief minister of west bengal", "answer": ["Prafulla Chandra Ghosh"]}, "target": "['Prafulla Chandra Ghosh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first chief minister of west bengal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sarat Chandra Bose"]], "filtered_resps": ["Sarat Chandra Bose"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "00077ceea360d362208531b15e26f4ad4512d1ed28b26628a522ad188be7137e", "prompt_hash": "b02a1221bce91088465a5f7e22f758ac14e30671f5976691f900d83ff742e307", "target_hash": "cb85a64f7d65c0dfdf15b406147f669c257655ca6ed449ad30344cfbc4654990", "exact_match": 0.0}
{"doc_id": 3041, "doc": {"question": "when do the walking dead comics come out", "answer": ["October 2003 – present", "2003"]}, "target": "['October\\xa02003\\xa0– present', '2003']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do the walking dead comics come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" monthly"]], "filtered_resps": ["monthly"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4f702f5d6efcb2a0814ff77b796508a4231ce17c8ef8f78dae413db5c7e4b3ed", "prompt_hash": "df19927eaaab8ad038b409e2abbf3b1d167b674f6ccd22ff590ff170db086d73", "target_hash": "76e32cc7936dbeafb5e9cb0fbda13e7359050eb2594a1f7ea35192d7f7142f51", "exact_match": 0.0}
{"doc_id": 3042, "doc": {"question": "who is the originator of the plan-do-check-act model of performance improvement", "answer": ["W. Edwards Deming"]}, "target": "['W. Edwards Deming']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the originator of the plan-do-check-act model of performance improvement?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walter A"]], "filtered_resps": ["Walter A"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "172c5074c87b8c58e3ff22a693217e318e1fbd0749849ed6237368287e92690a", "prompt_hash": "24035b78b296f571b6c59583957911e63119b86d58dd59331951275a2766139b", "target_hash": "920b16cdca84323109a6313a0d9e7d26d87d9b6c71af2c705f09b92c7d98a450", "exact_match": 0.0}
{"doc_id": 3043, "doc": {"question": "which supreme court judge has surved in international court of justice", "answer": ["Dalveer Bhandari"]}, "target": "['Dalveer Bhandari']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which supreme court judge has surved in international court of justice?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stephen Breyer"]], "filtered_resps": ["Stephen Breyer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "473d5d782a87aff1ee0050600a6e951b2014ef378afe76d5fd4a2b5adee39a77", "prompt_hash": "0631a0c95487a73c557dee4d2be1b720eab2dc4e14688e4acb7c2b293b6e0b7e", "target_hash": "81d2e07739c7401d8ba52e7632d0d91b7e935118fbb9498f89b8d76302845e20", "exact_match": 0.0}
{"doc_id": 3044, "doc": {"question": "what are the main crops grown in the united states", "answer": ["Corn", "Hay, (non-Alfalfa)", "Cotton", "Alfalfa", "Sorghum", "Wheat", "Rice", "Barley", "Soybeans", "Hay", "Tobacco"]}, "target": "['Corn', 'Hay, (non-Alfalfa)', 'Cotton', 'Alfalfa', 'Sorghum', 'Wheat', 'Rice', 'Barley', 'Soybeans', 'Hay', 'Tobacco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the main crops grown in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" corn"]], "filtered_resps": ["corn"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "36419418e8e4098b002e6ccea5357c850305715682c926c0277ac098461d9903", "prompt_hash": "7040a8ae75dee14b4264c1203039b6018719f853b2146fcb3ce223ae7ba133cc", "target_hash": "34c35aa37188548c760bbf970c3322e85261add245a75af854b5fa8e98064033", "exact_match": 1.0}
{"doc_id": 3045, "doc": {"question": "how many players in football hall of fame", "answer": ["318", "318 individuals"]}, "target": "['318', '318 individuals']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many players in football hall of fame?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 250"]], "filtered_resps": ["250"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "16e7ff5c51c81123d7ecf1f6fad777a18915184f15948ee4f644e92f445adf23", "prompt_hash": "3184e5a7d07acf88960d0627fd6f64109eb16881c348c1235cfa796b59c61688", "target_hash": "c11227a3937b63f54a4d89bc9ef7691265e7a25f945eab98f60141e341e6abe9", "exact_match": 0.0}
{"doc_id": 3046, "doc": {"question": "who gave the famous objective co relative theory", "answer": ["T.S. Eliot"]}, "target": "['T.S. Eliot']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who gave the famous objective co relative theory?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Niels Bohr"]], "filtered_resps": ["Niels Bohr"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f123ed231663014b9b7ccc12e896824bff2f1539eb86eea0e8680a082ccd2c72", "prompt_hash": "62ae93a181704e299df259644a02357abb28d4d910199734e48fb2e2d9cfc909", "target_hash": "89e7bee861847e54376f8e9cdaba87fc1774fa4963ffd5a39ca1cd5c41b8ab7e", "exact_match": 0.0}
{"doc_id": 3047, "doc": {"question": "who wrote the song photograph by ringo starr", "answer": ["George Harrison", "Ringo Starr", "Richard Starkey"]}, "target": "['George Harrison', 'Ringo Starr', 'Richard Starkey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song photograph by ringo starr?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul McCartney"]], "filtered_resps": ["Paul McCartney"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5beafb677bf7b2262ad93b32e2f003360ddef6572205b4d5855f85b1836c0f5c", "prompt_hash": "07c1a4d537a84fd43364570ab1e8f476ccdd8f61e995539cf5055258da2a4b1b", "target_hash": "d5e0053e1e7736154f3eaa02007165a3b3e79187cef4d4728829f758581a8baa", "exact_match": 0.0}
{"doc_id": 3048, "doc": {"question": "who is the first prime minister of france", "answer": ["Michel Debré"]}, "target": "['Michel Debré']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the first prime minister of france?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Georges Clemenceau"]], "filtered_resps": ["Georges Clemenceau"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4aea338befd3e6aa426ae434d0fc0cda480a55831d0cb3144c38d7f8d0a421c2", "prompt_hash": "7f59e0846c350151244cf7990ba7a418968dc13601ed211945a4c1553a18b78d", "target_hash": "6c028b9c1e60760c6083a68b22f4f21611d63922c0f328e9d6a5ba595ae6f75f", "exact_match": 0.0}
{"doc_id": 3049, "doc": {"question": "who played truman capote in in cold blood", "answer": ["Philip Seymour Hoffman"]}, "target": "['Philip Seymour Hoffman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played truman capote in in cold blood?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Philip Seymour Hoffman"]], "filtered_resps": ["Philip Seymour Hoffman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ba279fd718c72fa93ac441e5729d522c8339ae81c4cfdf86e40ac8c272d861b9", "prompt_hash": "80503be7979dd6fe6b1e597dc063ad692d745dd0010249c23f667e698439708e", "target_hash": "5eb80336fd9f386e9f23f390839869ebaf6912244f1008e88a1762ae5e2fda0f", "exact_match": 1.0}
{"doc_id": 3050, "doc": {"question": "where was the film the remains of the day filmed", "answer": ["Powderham Castle", "Weston-super-Mare", "Dyrham Park", "Badminton House", "Corsham Court", "Limpley Stoke"]}, "target": "['Powderham Castle', 'Weston-super-Mare', 'Dyrham Park', 'Badminton House', 'Corsham Court', 'Limpley Stoke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the film the remains of the day filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" England"]], "filtered_resps": ["England"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9bc262f743df6af7c955a49a017c5af09287ba72eb239987d4e22a92894af3c", "prompt_hash": "a53ceb587369f0508817e86cdbd28d172e73b68adf5debe4359ae0da9df5a280", "target_hash": "fa8b7d5649aaf3cb2545a4c3fbae7fc68f74fc7e7703894ad811c5f854dc202b", "exact_match": 0.0}
{"doc_id": 3051, "doc": {"question": "who eliminated costa rica in world cup 2014", "answer": ["Netherlands"]}, "target": "['Netherlands']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who eliminated costa rica in world cup 2014?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Belgium"]], "filtered_resps": ["Belgium"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9f9055214f59821791ded2ce378cb18c26d0b2aaa892de775c2cbb45a2868346", "prompt_hash": "f0454ad331c4a4a9694bafc2975fcd58edc574efa12b4dd160bea15114d998cd", "target_hash": "fec76019feaf369a415f3ff3ff3aa9b989ac8d3d0e31f4918ed6029667e9edb5", "exact_match": 0.0}
{"doc_id": 3052, "doc": {"question": "what is the name of the weapon the grim reaper carries", "answer": ["scythe"]}, "target": "['scythe']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the weapon the grim reaper carries?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" scythe"]], "filtered_resps": ["scythe"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "95d89b3442fe1ea543005fe98663d2f0f6d03c47fba1b57bb3daf7df797b6194", "prompt_hash": "197ab07ddcb9db10711f795ef23eb6f3716c17d69057bc94fc0472a5c067d2f7", "target_hash": "01a908087eb5745b8a814165e959acde570788a58461e6e1a9c3caeefbeaceb0", "exact_match": 1.0}
{"doc_id": 3053, "doc": {"question": "when did the battle of bull run start", "answer": ["July 21, 1861", "July 21, 1861", "5:15 a.m."]}, "target": "['July\\xa021,\\xa01861', 'July 21, 1861', '5:15\\xa0a.m.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the battle of bull run start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" August 28"]], "filtered_resps": ["August 28"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7ddd0b6b1155bd8aba6fc4755ab2ba31cc106b27d8fc4bfe1f8aeee811f8336", "prompt_hash": "d23c6732048f2f8346a7affbca45ee0d96256d1ecb06dba7ee94f4ad6de3e3b1", "target_hash": "b3e99f083a2d730d6492528dd40d67175e79e3ebb717754101bf984cdb2b8bfc", "exact_match": 0.0}
{"doc_id": 3054, "doc": {"question": "what was the first kind of vertebrate to exist on earth", "answer": ["believed to be the Myllokunmingia", "the Myllokunmingia", "Myllokunmingia"]}, "target": "['believed to be the Myllokunmingia', 'the Myllokunmingia', 'Myllokunmingia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the first kind of vertebrate to exist on earth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fish"]], "filtered_resps": ["fish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "18c42ddb82babde7c17450fa95c17b88b2b00d0598a16a309cc30f31523fcc30", "prompt_hash": "a1583ec3f8f0e6e3ed8178b0154b6d426ddc3d995b369c3f578daafeaf1429c2", "target_hash": "93ca415694b4b18a3c0d0c4c97c8ccb307280627d41a5b4bd73ae18091aa13eb", "exact_match": 0.0}
{"doc_id": 3055, "doc": {"question": "who wrote shes always a woman to me", "answer": ["Billy Joel", "Joel"]}, "target": "['Billy Joel', 'Joel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote shes always a woman to me?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lionel Richie"]], "filtered_resps": ["Lionel Richie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1ca831ee228c9a49ace447007784c27fe0b6faff339b11142b25603e4caefefc", "prompt_hash": "8e63e48137ec97a22e9ee43cf68481d7d6601cec0aa7aa4e3a785d01af8143dc", "target_hash": "81f41b3db90e282df99f8c47e40b21ccacfd4c6de39d56a1ba00627a38dbcec3", "exact_match": 0.0}
{"doc_id": 3056, "doc": {"question": "who won the first season of america's got talent", "answer": ["Bianca Ryan"]}, "target": "['Bianca Ryan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the first season of america's got talent?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Heidi Montag"]], "filtered_resps": ["Heidi Montag"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "75dc23ca1b318545c9cbf680261185228b9a5a89c293ecc68e8eed2dd062ecc5", "prompt_hash": "a6a39156e1da73fc9a45f366be6c1c7de25ec18c16fca0ae0eff387857095f4e", "target_hash": "c5b5d1c74c81f454629398ef9c310685c04557072a37bc17d7e7fcef3b19f09b", "exact_match": 0.0}
{"doc_id": 3057, "doc": {"question": "in the early 1800s california society was dominated by which group", "answer": ["Spanish"]}, "target": "['Spanish']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in the early 1800s california society was dominated by which group?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spanish"]], "filtered_resps": ["Spanish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b41a1cd67b898991e741efad74c7cbee8967ec6706954f06d846e24458c861a", "prompt_hash": "be30fdbe1d75029995e99b05d25dad00f6a4d5d78ce7e11ccf160a02affc9c8d", "target_hash": "542c2526bea4ce7a094e660928867975f00301abd0e8f05dc248c994a7aeb6f1", "exact_match": 1.0}
{"doc_id": 3058, "doc": {"question": "gestational sac and yolk sac but no baby at 7 weeks", "answer": ["an anembryonic gestation"]}, "target": "['an anembryonic gestation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: gestational sac and yolk sac but no baby at 7 weeks?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" missed miscarriage"]], "filtered_resps": ["missed miscarriage"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "806ef1670329d2b8aaa39f0fe3a110e21fba7ed6370c851ffa0cff02973901c3", "prompt_hash": "6a1500e4f18199fce7fdd97813560ae12d5b347cee24abe9f195224827cdf261", "target_hash": "abfbb72379f3f004bb1164d42f1034e9bbbe323711e9a2ccc0ed7f842f89adc6", "exact_match": 0.0}
{"doc_id": 3059, "doc": {"question": "when is the last time the us lost the olympics", "answer": ["2018 Pyeongchang"]}, "target": "['2018 Pyeongchang']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the last time the us lost the olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1904"]], "filtered_resps": ["1904"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "63c3898d1dec08af67ba4d1030b5188a7300331d81937f1104123408b53aad26", "prompt_hash": "0093738dfa51f5a67fca95bbb969e3a8c8cc1c103700ad4dbaabab714fe5bfe6", "target_hash": "63a5ec89b8383e127c9cb72d1b93a4d3e3e0a8c574ef451729a3b87cbb5377d7", "exact_match": 0.0}
{"doc_id": 3060, "doc": {"question": "where does this season's survivor take place", "answer": ["Mamanuca Islands, Fiji", "Ghost Island"]}, "target": "['Mamanuca Islands, Fiji', 'Ghost Island']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does this season's survivor take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Panama"]], "filtered_resps": ["Panama"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6143b3a5af5acc1eb658e3709be56f4f9dcc9338ff55535a8acd7a1a611d1726", "prompt_hash": "6c7cfb5f332b4b7c133914d2412722e878bdcac39ef312f858346428203964d3", "target_hash": "0d0918d951e29e0ac0801af292c4589c14d7d3090fe1f191b28291c0c1a0a282", "exact_match": 0.0}
{"doc_id": 3061, "doc": {"question": "who is singing in something just like this", "answer": ["Will Champion", "Coldplay", "The Chainsmokers", "Chris Martin"]}, "target": "['Will Champion', 'Coldplay', 'The Chainsmokers', 'Chris Martin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is singing in something just like this?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "82c0c1f9c61dd2f52a7c7699945644ad9a93bafa76f73d6c1fd6ff0554ceb1d4", "prompt_hash": "a8fda5be84599d9f4a53e495f3d955cb7944c5149d879692a9328d5675308fbe", "target_hash": "df5a7ec2ad5dd2f22c232a9780291da5e61b664a58efd9128455ba2231cc765f", "exact_match": 0.0}
{"doc_id": 3062, "doc": {"question": "why dry ether is used in woods reaction", "answer": ["to form a higher alkane"]}, "target": "['to form a higher alkane']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why dry ether is used in woods reaction?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" to remove water"]], "filtered_resps": ["to remove water"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e9d962c40203e27c0aa06969297cf45c554d0481f3fdc6a87d0ec9bdb66583d5", "prompt_hash": "ee8d80ef33fe62094563676b89e1c29546d7c01a68bebdff6b17fd7165d80c8f", "target_hash": "8f9af2b3d74739d56156b28b0e9b7b43250d00c5412ce73fd26ca726e78721ce", "exact_match": 0.0}
{"doc_id": 3063, "doc": {"question": "who plays lady talisa in game of thrones", "answer": ["Oona Castilla Chaplin", "Oona Chaplin"]}, "target": "['Oona Castilla Chaplin', 'Oona Chaplin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays lady talisa in game of thrones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rose Leslie"]], "filtered_resps": ["Rose Leslie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d38f06663d21239e7567d693e179f5ef78848e2f7bf0d7c59f5c0be88c3dc34", "prompt_hash": "d080f7ead43901b51d32725baa6d7c012d5b286b264af3272da07e05e5e87536", "target_hash": "11d67f64cdd279464a6086b4f86f6f7594bf68fc161809253e41f871042f7ed4", "exact_match": 0.0}
{"doc_id": 3064, "doc": {"question": "what was the population of the roman empire at its height", "answer": ["50 to 90 million inhabitants", "70 million", "55–60 million", "an estimated 70 million people"]}, "target": "['50 to 90\\xa0million inhabitants', '70\\xa0million', '55–60 million', 'an estimated 70\\xa0million people']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the population of the roman empire at its height?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 50 million"]], "filtered_resps": ["50 million"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a11357da473ca6621b47b9b167139c77c47bb2df1bdf32be49aa942bbf9f4d34", "prompt_hash": "cfedefb256a71302683937b9c96b7203ba046afd8850f50211492c27e8705911", "target_hash": "cd8efb2862999937245b97b20fee8a8a075b1db0051700e564df32882a6f659b", "exact_match": 0.0}
{"doc_id": 3065, "doc": {"question": "where does the phrase train of thought come from", "answer": ["Thomas Hobbes in his Leviathan"]}, "target": "['Thomas Hobbes in his Leviathan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the phrase train of thought come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" English"]], "filtered_resps": ["English"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1f7bb4a74996f538de25330b4db445443518e51f385c1898857cc17cba283fe0", "prompt_hash": "afd6294e8f36dbd75115d15520e10fccf1790a68d6807723113e7a4cb66e74e1", "target_hash": "8668aa88a204734d18cf36a3873728ec4fe2574a3d1bb2544717fda8e128a542", "exact_match": 0.0}
{"doc_id": 3066, "doc": {"question": "which body system differentiates a male from a female", "answer": ["the external genitalia", "the sex hormones", "reproductive", "the type of gonads"]}, "target": "['the external genitalia', 'the sex hormones', 'reproductive', 'the type of gonads']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which body system differentiates a male from a female?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" reproductive"]], "filtered_resps": ["reproductive"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5736a86527b97cfdccb6ff8bb637bf65f452079952b652c125a53af39cd03bfb", "prompt_hash": "6015f75f689890f4d75c13395c713229ebdcc8b2c57c86357c7b82cf2eaad67d", "target_hash": "d107df4759cfcfc309fb83acc1b2e3a11146e774e293215aa1d680a4a454c0c4", "exact_match": 1.0}
{"doc_id": 3067, "doc": {"question": "name the process of fusion of an egg with a sperm", "answer": ["fertilization", "Human fertilization"]}, "target": "['fertilization', 'Human fertilization']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: name the process of fusion of an egg with a sperm?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fertilization"]], "filtered_resps": ["fertilization"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c463621b9f13741637e5aeab5372cd6b359c06ec4f3fa761457ff42ea3e7d6ff", "prompt_hash": "d623bbe68d4ffdaa1a518c119a2d7b4dab1ee6eeae261abd8dd936dea36bedac", "target_hash": "51eda08511f2cc1d7595c027b628e4ce2fabec5c1126328380b7eb6a84260b3a", "exact_match": 1.0}
{"doc_id": 3068, "doc": {"question": "who were the first kings to issue coins bearing their names", "answer": ["the Indo-Greek kings"]}, "target": "['the Indo-Greek kings']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the first kings to issue coins bearing their names?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Romans"]], "filtered_resps": ["Romans"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "64af765532119b4211056fb1d09e498c9bc89eaf13cffe3897106ce2cac6dc59", "prompt_hash": "65c9449523a59e768385794232b70bba74106fd80120003aae813cfabe4a57cf", "target_hash": "e52cd35bf15b0a79c745fed1e2a8ee7133c56de2debfacfb7c1e3f7bb2ce0c54", "exact_match": 0.0}
{"doc_id": 3069, "doc": {"question": "who played michael jackson in jackson 5 movie", "answer": ["Alex Burrall", "Abolade David Olatunde", "Wylie Draper", "Jason Weaver"]}, "target": "['Alex Burrall', 'Abolade David Olatunde', 'Wylie Draper', 'Jason Weaver']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played michael jackson in jackson 5 movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Jackson"]], "filtered_resps": ["Michael Jackson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3526f5165d716f1bb8a8733bff823d0f843ca77c60612a8b4145cac03444bb5b", "prompt_hash": "f1893dac0a8e41c6e1e935ab914d49d3c9485fe7a8e28da6537482e946fa2456", "target_hash": "9aa5205b0b22152461f2211fdad163f02d2635782f6c47f679b4e3328e680c24", "exact_match": 0.0}
{"doc_id": 3070, "doc": {"question": "who are the stars in dancing with the stars", "answer": ["Jennie Finch Daigle", "Jamie Anderson", "Arike Ogunbowale", "Mirai Nagasu", "Adam Rippon", "Josh Norman", "Tonya Harding", "Kareem Abdul-Jabbar", "Chris Mazdzer", "Johnny Damon"]}, "target": "['Jennie Finch Daigle', 'Jamie Anderson', 'Arike Ogunbowale', 'Mirai Nagasu', 'Adam Rippon', 'Josh Norman', 'Tonya Harding', 'Kareem Abdul-Jabbar', 'Chris Mazdzer', 'Johnny Damon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the stars in dancing with the stars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carrie Ann Inaba"]], "filtered_resps": ["Carrie Ann Inaba"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c61ced180cd31087e74d5db5a1d7fd1ea5134db31eaffa944edf2d4498738d3b", "prompt_hash": "2422815cd93ac718c71a057d8118febc5de76959231ff12b27cd33ccc59d9adb", "target_hash": "c6c5dcd4bdcf73b360ed1cea7cb0e00ddaa70d0a1f862ffb47b2e6f188fd2276", "exact_match": 0.0}
{"doc_id": 3071, "doc": {"question": "who hosted they think it's all over", "answer": ["Nick Hancock", "Des Lynam", "comedian Nick Hancock", "Lee Mack"]}, "target": "['Nick Hancock', 'Des Lynam', 'comedian Nick Hancock', 'Lee Mack']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who hosted they think it's all over?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gary Lineker"]], "filtered_resps": ["Gary Lineker"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ec317051643c817225741ae6c15d12fe0dbe26475c87582cfd677662831fefec", "prompt_hash": "53af676fcf2689d7a54ec2059d465f146b6b1db8a3358d02dc981306aa0c1d8c", "target_hash": "10a8ff037ca5e2b6153b8a17eb9f377f968fad9db5f8504d7cf19baa4291ba79", "exact_match": 0.0}
{"doc_id": 3072, "doc": {"question": "when did red bull come to the united states", "answer": ["1997"]}, "target": "['1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did red bull come to the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2007"]], "filtered_resps": ["2007"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e8fe1b5801e727a3b79b64bbcea15ab4d29f820d7bcb8aff1be40198e42cfedc", "prompt_hash": "b8bff3109f67508e63b3435bd0fa54b928c0c380adc8d1b8e48aa9a6d8d97f93", "target_hash": "982df96f59043cb182196071792c2904ff376f72b15cb9b414721d5b74a296b0", "exact_match": 0.0}
{"doc_id": 3073, "doc": {"question": "how long has tom brady been the patriots quarterback", "answer": ["2001", "16 seasons"]}, "target": "['2001', '16 seasons']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long has tom brady been the patriots quarterback?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2000"]], "filtered_resps": ["2000"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4b43e30f74498e060bce42e4f2234e9c101074781b1e341da5500cedaf954ae3", "prompt_hash": "36510a0b66e46689e2164209392083d440450c65e2d697b74f93583416dc8b2f", "target_hash": "5ca06f57dea15e27b1fc05574c979eb8c19ef3d5f3e3cc92916be636a686e9a8", "exact_match": 0.0}
{"doc_id": 3074, "doc": {"question": "how many casinos are in atlantic city new jersey", "answer": ["eleven", "seven"]}, "target": "['eleven', 'seven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many casinos are in atlantic city new jersey?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c13fdc397b4f022ea927629cb0677a87eb4fc484c0a1fe1c1b0c61931091dcbd", "prompt_hash": "27b8e6cdb035121e621d862b85fa42e7d924218186b2f1f60bbd2ab52689a56b", "target_hash": "58e40b1f6b84e051d5ac7954651b48b9d3646f39c427a71f6f8306c55a6f3f60", "exact_match": 0.0}
{"doc_id": 3075, "doc": {"question": "when did the great fire of london end", "answer": ["5 September 1666", "Wednesday, 5 September 1666"]}, "target": "['5 September 1666', 'Wednesday, 5 September 1666']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the great fire of london end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1666"]], "filtered_resps": ["1666"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "153d1c38c9d6e82cc11b96bea915855043f42e0df1a441ba46225a9ccccd7b24", "prompt_hash": "01e66043ad4e63d624e4f2ec43f153d050bea4991473406fda79df1d206dda63", "target_hash": "97b2e80d11be7559689a9b4b121a0081fea783e358068a99d25f8eb0a3808b25", "exact_match": 0.0}
{"doc_id": 3076, "doc": {"question": "who is the singer of kal ho na ho", "answer": ["Sonu Nigam", "Richa Sharma", "Alka Yagnik"]}, "target": "['Sonu Nigam', 'Richa Sharma', 'Alka Yagnik']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the singer of kal ho na ho?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arijit Singh"]], "filtered_resps": ["Arijit Singh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3f44d7f452aa23868309129835a87afdb03243c1d0ab3a6f1d9e317bb2e57c0d", "prompt_hash": "8bd1ad54af175d594d7d4cae366f49d2a5842c414102bb96fadb5eddc15b2a14", "target_hash": "c09a1db05cfff561b2a0993cb15cd7924ea042a04bc4d4ec47a0a38319c69476", "exact_match": 0.0}
{"doc_id": 3077, "doc": {"question": "when did the passion of the christ come out", "answer": ["February 25, 2004", "February 25, 2004", "2004"]}, "target": "['February 25, 2004', 'February\\xa025,\\xa02004', '2004']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the passion of the christ come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2004"]], "filtered_resps": ["2004"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2b38c8fd59fc6799d6ead979729afd8c78427b49867395907f89d8dc073588a", "prompt_hash": "62d550d40144b84d1e6bee5d7f69387f784eca2286399ea9c3747b8b8bb80488", "target_hash": "3e0b4d4c292c1460777d545e3bb49af0c4dd310170c2610d36cd251725838c59", "exact_match": 1.0}
{"doc_id": 3078, "doc": {"question": "star wars the clone wars season 3 episode 1", "answer": ["Clone Cadets"]}, "target": "['Clone Cadets']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: star wars the clone wars season 3 episode 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Lawless"]], "filtered_resps": ["The Lawless"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db4c1b2408c21813278e59e1d8789ce556bf79b0c95c67695829dedc8f89c04b", "prompt_hash": "1ad8ce13c832f5d459b25652146d8b806991a7029578782441894099c7521488", "target_hash": "e1b549e4e0fdcecf34c01e4661d8c5253c6e0680aa1d97f65bd06ae5df584ba8", "exact_match": 0.0}
{"doc_id": 3079, "doc": {"question": "is the united states a country or nation", "answer": ["country"]}, "target": "['country']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: is the united states a country or nation?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" country"]], "filtered_resps": ["country"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "df5168dee0415f7bbfbc62080db62b3491c2301817c176a2a90e2146e2d1d8da", "prompt_hash": "34426aee5126e140e1cee054acdfe80de0688297cd904737404c5e37d5609fd6", "target_hash": "562b6dac6ec864ab14f7b56dd1c2cc7d2932c4794abbfe899bd23e9f84edfe22", "exact_match": 1.0}
{"doc_id": 3080, "doc": {"question": "who has the most podium finishes in australia", "answer": ["Michael Schumacher"]}, "target": "['Michael Schumacher']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most podium finishes in australia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mark Webber"]], "filtered_resps": ["Mark Webber"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dfa02b49debaeb6a61c8a6fe41683f27e913e2b470a44b38b84fd3a69b486253", "prompt_hash": "ff9249f8efbcfb8167d3737b558766ff1d6cd600e077b40697b0220af9c2aeda", "target_hash": "3fd1e16eccf5a75746e8a6d94b2104275bc132c8daf123beefd2c6f53806fcea", "exact_match": 0.0}
{"doc_id": 3081, "doc": {"question": "when did john steinbeck write of mice and me", "answer": ["1937"]}, "target": "['1937']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did john steinbeck write of mice and me?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1937"]], "filtered_resps": ["1937"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1381cd25429520fb7ec8f5380f4519dde55b3c43caa97cc616897517f8391644", "prompt_hash": "c8ef53e41303df6201130fe3f57a5d9e8fd238d38ff01eda78c024b6ddf1af36", "target_hash": "d54b29aabf6d74a46298310cd4c7b8f3570aaba12b198b7b982f647882761ffa", "exact_match": 1.0}
{"doc_id": 3082, "doc": {"question": "who plays faith on when calls the heart", "answer": ["Andrea Brooks"]}, "target": "['Andrea Brooks']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays faith on when calls the heart?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Love Hewitt"]], "filtered_resps": ["Jennifer Love Hewitt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b6cc755acdde05fe0c12e8ef471f9cff34acdc1f17ddc15a91d668c5e9bd10e", "prompt_hash": "b23dfcc3ff428f06ecb19f3250cf131373da078c3c0793e093cdfb1b45456761", "target_hash": "907473f03d1ae08c05a6f262d1149bb1c70ba3822413d78c4602a36624433217", "exact_match": 0.0}
{"doc_id": 3083, "doc": {"question": "where was the movie the glass castle filmed", "answer": ["in Welch, West Virginia", "Welch, West Virginia"]}, "target": "['in Welch, West Virginia', 'Welch, West Virginia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the movie the glass castle filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tucson"]], "filtered_resps": ["Tucson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b696c9350f2bc907760bd52b35987beb87b369b3182caed99097d831ad3b240f", "prompt_hash": "f6e2915d70a15ecda639b433c7257cd83fcc53246efec84dbf3eb0ef8faba030", "target_hash": "5ea3612a5fe7021cee9f00ed70f2084c9e30967d7a3f4e7ef137f961e99e5791", "exact_match": 0.0}
{"doc_id": 3084, "doc": {"question": "how many seasons of prison break are on netflix", "answer": ["five"]}, "target": "['five']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons of prison break are on netflix?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 4"]], "filtered_resps": ["4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eeac896fae4e4e6cfafa561a390662c694c68ecf6f37e402979a2efb2a375e2c", "prompt_hash": "f9e5e2f71eeb4f20e2350eb61c3558d348f94e9203bd265ffe7c3c8f663c995a", "target_hash": "f2c5cc9345e0b74e87324716ab150568729c21826fed4d9d2f95d4b5ee6dfdd7", "exact_match": 0.0}
{"doc_id": 3085, "doc": {"question": "when does the turn of the screw take place", "answer": ["1898"]}, "target": "['1898']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the turn of the screw take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1897"]], "filtered_resps": ["1897"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "05e1420f3793a2160a64ad8bcac865d26a4862b516ee5cc585138a64c29525b3", "prompt_hash": "7f7c884e09c8c51e47048e1ff9ecadbae2fa2ccac6170bda8def95e79280ac3d", "target_hash": "d24377e00d603d57f6b06cf8fe63e117c28df36077e7854da03067d8611ebbfd", "exact_match": 0.0}
{"doc_id": 3086, "doc": {"question": "who is the longest serving member of the house in history", "answer": ["John Dingell"]}, "target": "['John Dingell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the longest serving member of the house in history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joseph McDade"]], "filtered_resps": ["Joseph McDade"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7e70af39d94dff5f1cb6cd3c4f8b38951b322713b4ee71391bf24e221c8bdd3a", "prompt_hash": "c49ec1f03af3a99db2cde4bc44215ec818b76a39cff62a1ba3738a9b17f699d5", "target_hash": "48ba00b7769e9385a08ca532dfdc91c92eef07478fc4b72cd93c31e7fd5a065d", "exact_match": 0.0}
{"doc_id": 3087, "doc": {"question": "when did first fast and furious come out", "answer": ["2001"]}, "target": "['2001']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did first fast and furious come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2001"]], "filtered_resps": ["2001"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "20e00edc43ca5744fed7c5cd5706ee1600f477be935feefdca4db50b00bf1936", "prompt_hash": "c5bfa33e5bd1fb80bc10f3cd3df7fb402657fbabddeb556334974fb62addc7f8", "target_hash": "a1b3b1d0a85b30b08b5612ad5c0b0247b678a69f475941a1f022a4a8638a9f0b", "exact_match": 1.0}
{"doc_id": 3088, "doc": {"question": "when does the second half of vikings season 5 air", "answer": ["2018", "in 2018"]}, "target": "['2018', 'in 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the second half of vikings season 5 air?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" February 11"]], "filtered_resps": ["February 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a4cdcc5a5d6cd502f1bbc5a37fd470a84780609d4757cfdbe22466ed8f101112", "prompt_hash": "54f826e6d959f722a0d0ce038ac89adfd396308a4796d11202ec385dced59678", "target_hash": "9e524537facfb2dc4ea7fff76a005be136bc3a77b7f1029343f3bbc9fdd65d04", "exact_match": 0.0}
{"doc_id": 3089, "doc": {"question": "what kind of fish live in the salton sea", "answer": ["tilapia"]}, "target": "['tilapia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of fish live in the salton sea?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tilapia"]], "filtered_resps": ["Tilapia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a5821f69e23e0ef9cf2740f6c03e3229e04d5ff88a2bc3864bfbaf65e1a551da", "prompt_hash": "41eb6776b4d609d9c7ba89c424344ba2449e1bbf7cca1663e0d384c025d4c1b6", "target_hash": "20a39a41931b3511e334e5d1c89725185ee5c767b2d4eeb8940e95493424b2b9", "exact_match": 1.0}
{"doc_id": 3090, "doc": {"question": "who sang the song you got a friend in me", "answer": ["Lyle Lovett", "Randy Newman"]}, "target": "['Lyle Lovett', 'Randy Newman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the song you got a friend in me?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Randy Newman"]], "filtered_resps": ["Randy Newman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ca03056232845f4369313b138750a2e344386ffe4be88e4964c7a7262265b40d", "prompt_hash": "92f9d57aea12e8dd8444d334cd397f44f5495df01b34cd12892814b5d3e5a1a3", "target_hash": "3fcd771fbb75be2347d140a39ce0ade1700f31bbd9a306e79b0073c15fb1c0a1", "exact_match": 1.0}
{"doc_id": 3091, "doc": {"question": "what is the strongest earthquake in the united states", "answer": ["1964 Alaska earthquake and tsunami", "1700 Cascadia earthquake"]}, "target": "['1964 Alaska earthquake and tsunami', '1700 Cascadia earthquake']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the strongest earthquake in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1964 Alaska earthquake"]], "filtered_resps": ["1964 Alaska earthquake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6bc9e03e9ae1953df8d9bcc601f4e6e7d7e1e1737d641b08d2cbb0824b531584", "prompt_hash": "18d12b188724b7c4e1eefcebe05476fd281b4f0f428fda92d205b6bfdcee7bbd", "target_hash": "4cbf0b8cdf40b51b47eeb3f4c9329c9d3a76ae8aff7711776f06c61c5de7155a", "exact_match": 0.0}
{"doc_id": 3092, "doc": {"question": "who wrote put your hand in the hand of the man who stilled the water", "answer": ["Gene MacLellan"]}, "target": "['Gene MacLellan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote put your hand in the hand of the man who stilled the water?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Lennon"]], "filtered_resps": ["John Lennon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "318237da6d866a708aaf3f81901922aac990510d7999e7d32f62f21c8f5ea067", "prompt_hash": "6f01295ffb64fce9360fb4b8a3d4217810902c7045a9c86fb820e5c07d9edf92", "target_hash": "e09ad3dd9e73f622d8f61f856ff32e2dbb6d9d0d1ceec5bdd3abae28689224a0", "exact_match": 0.0}
{"doc_id": 3093, "doc": {"question": "where is the white castle that harold and kumar go to", "answer": ["in Cherry Hill"]}, "target": "['in Cherry Hill']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the white castle that harold and kumar go to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f72923c6a34efd964ce19467e4ee97bf2dd31be36dba06056459c6b9dce4ecd4", "prompt_hash": "91d0b96ce9567997ee4ab19300055de35024f6ea218b660297ac4f8cd05b5a97", "target_hash": "7e5d17faeca8fe337a52278dc2b39a51eb5baf5da0352cb2e5ee5cf9c0651517", "exact_match": 0.0}
{"doc_id": 3094, "doc": {"question": "who headed the 7th central pay commission of india", "answer": ["Justice A.K Mathur"]}, "target": "['Justice A.K Mathur']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who headed the 7th central pay commission of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justice S"]], "filtered_resps": ["Justice S"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbfeb5482905295e6697c8dc6f875532d076ac486efe5fa56daee65e0ef8d801", "prompt_hash": "c459854f2c4554b8eab1b354945c6b48b7a95327c27e5f9f9e86102c98b42136", "target_hash": "9aa38c5672200926aec75a44e3d1d98697680770787aaa04dcb5dc4a76f66c0d", "exact_match": 0.0}
{"doc_id": 3095, "doc": {"question": "where is season 3 of the detour filmed", "answer": ["Alaska"]}, "target": "['Alaska']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is season 3 of the detour filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vancouver"]], "filtered_resps": ["Vancouver"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ef84f41edf34a46439c49946cd0ba6924d0ce8394c5da49bdad7a39103c04e06", "prompt_hash": "07dcdf07fc96d34c8629e711c43a4b8ecda88e1a09112230c2c68f545299c932", "target_hash": "298485455a3a4c3c12529c9b0fa7a692e09593047973aff01b35d9e5b2ae59d2", "exact_match": 0.0}
{"doc_id": 3096, "doc": {"question": "who plays the voice of sulley in monsters inc", "answer": ["John Goodman"]}, "target": "['John Goodman']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the voice of sulley in monsters inc?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Goodman"]], "filtered_resps": ["John Goodman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb852a602cc5619abb8d4e011d955753e7a9ae84fd41192f9d689f90f576374a", "prompt_hash": "f2eda94f96b6995c6eef3548f52763364a6cf94ae79bc9a676c5ff8800a077a6", "target_hash": "99775f73807bc207015ba375611b499e799df3a939bc5420bfd0827a6d466251", "exact_match": 1.0}
{"doc_id": 3097, "doc": {"question": "largest state in the us by land mass", "answer": ["Alaska"]}, "target": "['Alaska']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: largest state in the us by land mass?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alaska"]], "filtered_resps": ["Alaska"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7c7599a81fb61030b4a574e5f470f85f37c596558f1b14a3d64da6893fb5df4", "prompt_hash": "56ae1dfe5913addb045e5b9ff2db1f878f941fe0a753384167ccb6b4c6912358", "target_hash": "298485455a3a4c3c12529c9b0fa7a692e09593047973aff01b35d9e5b2ae59d2", "exact_match": 1.0}
{"doc_id": 3098, "doc": {"question": "when do you get your dress blues in the army", "answer": ["during initial entry training", "when commissioned"]}, "target": "['during initial entry training', 'when commissioned']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do you get your dress blues in the army?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" after basic training"]], "filtered_resps": ["after basic training"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a8eae1758ab5228e256fb0c09f607f5b6f2bf9d46ec6d72cc48d48e392f83b78", "prompt_hash": "50a8fd4b812d6c97f07249d58ad07b8bc2ff8c3383878c3fb5c5a238905b3dd0", "target_hash": "4c8220737a3fa85eee2b69fbf204900f2c70e24ec63f2dc7b1f9b71fb2df67e2", "exact_match": 0.0}
{"doc_id": 3099, "doc": {"question": "who used to present i'm a celebrity now", "answer": ["Ant & Dec"]}, "target": "['Ant & Dec']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who used to present i'm a celebrity now?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dermot O'Leary"]], "filtered_resps": ["Dermot O'Leary"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad45f70257a20461c9fb1d274b8937fea0a0f512ad9e5ab83e842e97eb2d6d1b", "prompt_hash": "30aa86d104c31ac6493b8a1d8f89ee46b82e6da07d7b2101251ca53618775f72", "target_hash": "214e40ffea67ef1ca3c68db62e53cd5fee27a5e053a4163ff935db74d1d452a8", "exact_match": 0.0}
{"doc_id": 3100, "doc": {"question": "when does congress declare war for the first time", "answer": ["June 18, 1812", "War of 1812"]}, "target": "['June 18, 1812', 'War of 1812']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does congress declare war for the first time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1798"]], "filtered_resps": ["1798"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "90168565192bbc80f5bf7de443bcaa3ddcf544253c90854d7e8edf04bcd12abb", "prompt_hash": "44e95cecc3268011e444ac1ad8fa46e935937f316621f52712256cf79bbf15db", "target_hash": "952b7907c52269083cd36c1bc921493a544f2052f656cf0025f3fe2ff07d63c5", "exact_match": 0.0}
{"doc_id": 3101, "doc": {"question": "what is the longest panic at the disco song title", "answer": ["Bohemian Rhapsody"]}, "target": "['Bohemian Rhapsody']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the longest panic at the disco song title?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Death of a Bachelor"]], "filtered_resps": ["Death of a Bachelor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a27a85950a9ae9722f57f1e22551a288327c3a1b50f1d77351642391b9f4ba30", "prompt_hash": "8f82be17b41b5cd4913a3215cd652f6820cdb6726c6a97ca047152eb96a55eed", "target_hash": "49f0fd48048bda9293eb54a37f3a63f8ce2126312afdf6af492b4a554900608d", "exact_match": 0.0}
{"doc_id": 3102, "doc": {"question": "who played mark on the show the rifleman", "answer": ["John Ernest Crawford"]}, "target": "['John Ernest Crawford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played mark on the show the rifleman?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chuck Connors"]], "filtered_resps": ["Chuck Connors"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c6eddf78625f69530d871f45b002ced74b3829a9d30b7dfd5b4bbd82e3606b1b", "prompt_hash": "c7adcd9063750046a470996b763ec22e740075eb11cd7bf8f785087d7ea716fa", "target_hash": "f5af4a55a6d64f58160a171e3d03c11949abd03527a87702e22c0345111106a9", "exact_match": 0.0}
{"doc_id": 3103, "doc": {"question": "where does the last name orozco originate from", "answer": ["Spanish/Basque origin"]}, "target": "['Spanish/Basque origin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name orozco originate from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mexican"]], "filtered_resps": ["Mexican"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "463b0d4d2b4f49e623a7a022288e1e42664321e8d3efdfb9efe5d68ccf862410", "prompt_hash": "356b4fbda0ea229e2f08044b2a4ae602d8d1ff22dc89ae1c1c4cd6e065c8ffa5", "target_hash": "4f350b0f1b056c1e2455fea30b67b6ad394bb1dcdd2056f9385d988de15985b7", "exact_match": 0.0}
{"doc_id": 3104, "doc": {"question": "when did marley die in a christmas carol", "answer": ["December 24, 1836"]}, "target": "['December 24, 1836']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did marley die in a christmas carol?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Marley died 7 years before Scrooge's Christmas Eve"]], "filtered_resps": ["Marley died 7 years before Scrooge's Christmas Eve"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5e65eac0812ec787945fd1715a4ecd238e4b10a448a7285842acd9e8ea4ca526", "prompt_hash": "4ce727d8a00e221c76cec894503b02379c85fa306aeece994f0d351b55b5699a", "target_hash": "8269bd401862acdee4bdbd1948adf3047991775b10d75c8f9c33923ba3901c5c", "exact_match": 0.0}
{"doc_id": 3105, "doc": {"question": "where does a girl wear an engagement ring", "answer": ["left hand ring finger"]}, "target": "['left hand ring finger']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does a girl wear an engagement ring?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" left hand"]], "filtered_resps": ["left hand"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cc0b539997460958cd548531bdb01f3bdad1d7cb8d2b3054bafa14a3bbeeb6ac", "prompt_hash": "3df709b4b2493b7ce4d09fd9b0898f0df4edb88b67cbe715a16e179fe0e1fbe8", "target_hash": "9f1370b00a0055147370f8f49e54ff52d7b3584d6adf924b035b8da0e9ec9b60", "exact_match": 0.0}
{"doc_id": 3106, "doc": {"question": "the art and science of making objects from clay is called", "answer": ["Ceramic art", "Ceramic"]}, "target": "['Ceramic art', 'Ceramic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the art and science of making objects from clay is called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" pottery"]], "filtered_resps": ["pottery"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "540d059c11165c4b0efc5d047fabd02eec7bd02b64370a36fd6a36c56e7f04e0", "prompt_hash": "8b66b7cb7250084762348ac7f209becd4beae193b5dbce12412e35093c46b186", "target_hash": "2936aefda73b91c9f78afb5474ad7b0da3704f4afbaa5df8d8ae5c241109c540", "exact_match": 0.0}
{"doc_id": 3107, "doc": {"question": "when did i ain't mad at cha come out", "answer": ["September 15, 1996"]}, "target": "['September 15, 1996']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did i ain't mad at cha come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1992"]], "filtered_resps": ["1992"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "058e4232040b0aa8522ab495b57f1476c73caabef3498f25bf0fc6f30a8bf7d0", "prompt_hash": "bb796cf83dafd2ded7434086a404485900227d8f39c7a549e9fcdd9eb5ce23fb", "target_hash": "b5013979102f7facdd2418923721d01728c251316963135d4981731b9dab0e8c", "exact_match": 0.0}
{"doc_id": 3108, "doc": {"question": "communities of similar organisms found in a particular environment are called", "answer": ["species", "habitat"]}, "target": "['species', 'habitat']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: communities of similar organisms found in a particular environment are called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ecosystems"]], "filtered_resps": ["ecosystems"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9eefa963c30c85929e879c599bb879e6b4359e3d2837548b75d75c1c799b6979", "prompt_hash": "5dd44e5794a20ffe883a897a08051f83e6164a2da41651dfc0151587d1dea827", "target_hash": "0f562d3474e1791408b85d6fd064e8792e8ae56e9a6dbad9af4c9a02fad34b81", "exact_match": 0.0}
{"doc_id": 3109, "doc": {"question": "what happened to the west wing of the white house during a christmas party in 1929", "answer": ["electrical fire"]}, "target": "['electrical fire']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what happened to the west wing of the white house during a christmas party in 1929?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" it caught fire"]], "filtered_resps": ["it caught fire"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b7ccbf14304f209052b36f0836011b0d1a25d7294e6b21867ad3f53a913b9d0", "prompt_hash": "0ff27d1946acf04dbe73e2c3387526c14965aa8786c0418df1b91f458417b7bb", "target_hash": "776752b1196b411daede39b7f0c1ed16d1846dc9cd90701a52f9f8c5496bac04", "exact_match": 0.0}
{"doc_id": 3110, "doc": {"question": "who played tom hagon in the godfather series", "answer": ["Robert Duvall"]}, "target": "['Robert Duvall']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played tom hagon in the godfather series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Caan"]], "filtered_resps": ["James Caan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "14dc7f72b8c598635e4601fa5f34e0e67fae294618d5d40ec20eca873a2d6438", "prompt_hash": "901b141a71c16f53ddd70cea4f3ae620c63da9e1040b2cb335cb252855f2dba5", "target_hash": "6bc6be295413c04820e57cee4ecc3b58e5931114b263a573ff34453cb4da62a8", "exact_match": 0.0}
{"doc_id": 3111, "doc": {"question": "who wrote the song i hate you i love you", "answer": ["Garrett Nash", "Olivia O'Brien", "Angel Benitez"]}, "target": "['Garrett Nash', \"Olivia O'Brien\", 'Angel Benitez']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song i hate you i love you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Bangles"]], "filtered_resps": ["The Bangles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8be35b55081e0997185589b34823332faaf9e7be9ed87d4a0a2da23f2436752d", "prompt_hash": "8055337d1ce2ab09141f508d0eebd52da7c9587bdd344c49efffa5f67592dfe8", "target_hash": "4a02b5d629bc2a8b5408813c5b46bcc0ab70bce78aeb35ae0f4c36965ee598c8", "exact_match": 0.0}
{"doc_id": 3112, "doc": {"question": "who sings the theme tune to mum on bbc2", "answer": ["Lulu and the Lampshades"]}, "target": "['Lulu and the Lampshades']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the theme tune to mum on bbc2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Bay"]], "filtered_resps": ["James Bay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c8a686cc548111f34dcb7f379f261dd319aedc33187015e00d62616c4af1046", "prompt_hash": "2027e3718a1d6a66772e77da56c7cfb1d33583959cef04489af7d6dbcd416fe3", "target_hash": "407ea2a02cc1de82dfc7bfa9aa894d7c4d237ff4424d36acbd5a10e48315c602", "exact_match": 0.0}
{"doc_id": 3113, "doc": {"question": "the general term for software that is designed to damage disable or steal data is", "answer": ["Malware"]}, "target": "['Malware']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the general term for software that is designed to damage disable or steal data is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" malware"]], "filtered_resps": ["malware"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f83fd2c902e093762ab7399def267a7c953f89b3b751cb97931741b9132182a", "prompt_hash": "0d794face001e73458c4c4078e7a4590f888133c904e48cd1d264b9d61d1cf31", "target_hash": "c44efebd258700c7be39b24746cf74b25aa908be6c0502f0a00edbd77e596b2e", "exact_match": 1.0}
{"doc_id": 3114, "doc": {"question": "what kind of plate boundary is nepal on", "answer": ["orogenic belt"]}, "target": "['orogenic belt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of plate boundary is nepal on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" convergent"]], "filtered_resps": ["convergent"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ca4241220b52d33f40800d54fd773afe5c2c13fd50b9faee0685e1035dccafc", "prompt_hash": "354a2d1f45722ff92078caf81a34c026160844ac519e765d07f978079f2256c8", "target_hash": "8fb4426b422c99754625f34c03aa7f0f05ae4271fdfbf6ac353f8e94438a4ae8", "exact_match": 0.0}
{"doc_id": 3115, "doc": {"question": "chemicals produced in one part of a plant that affect the growth and response of other parts", "answer": ["Plant hormones", "hormones"]}, "target": "['Plant hormones', 'hormones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: chemicals produced in one part of a plant that affect the growth and response of other parts?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" hormones"]], "filtered_resps": ["hormones"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "687dc8e27e1438be85f31fcec8d5dc8382d3a5490693b07d6e75bee977038294", "prompt_hash": "346ec1e39ec66108b5b5fad2636eb8d7de8a89b0dee32754d8fd381215a413d7", "target_hash": "82ef53fc728dd86f35adcb7f2d2567b9bdf15404c6e5cdd534377ec529fc8d76", "exact_match": 1.0}
{"doc_id": 3116, "doc": {"question": "the basic steps of rational decision making include defining goals and", "answer": ["Implementation of plans/policies"]}, "target": "['Implementation of plans/policies']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the basic steps of rational decision making include defining goals and?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" identifying alternatives"]], "filtered_resps": ["identifying alternatives"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "26e59bbd2296334f348998c9626fc19d2124d2234c0c0b444dbc8ba4d26ed135", "prompt_hash": "57dfb8a41a27b67bb0628781ef60dfb46deccae716d54d1c43e42beb4e128795", "target_hash": "eaef62cb517799731f9912db627bd8ef5590fd51a06638e41c4c67fd01152b82", "exact_match": 0.0}
{"doc_id": 3117, "doc": {"question": "who wrote if she knew what she wants", "answer": ["Jules Shear"]}, "target": "['Jules Shear']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote if she knew what she wants?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Taylor Swift"]], "filtered_resps": ["Taylor Swift"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58ff043de9f083387b6dba82ffdb65e79666b3505eaa3180e72168045d86a7ed", "prompt_hash": "3db20c4135aaf918f6a75e59eb784864146750ffdf26118a0433554ef768abac", "target_hash": "3fb7b59d5660565566a29a17ea5b8924215997fd051bc0aace75a0c0524b5933", "exact_match": 0.0}
{"doc_id": 3118, "doc": {"question": "who won the medals in men's figure skating 2018", "answer": ["Yuzuru Hanyu", "Javier Fernández", "Shoma Uno"]}, "target": "['Yuzuru Hanyu', 'Javier Fernández', 'Shoma Uno']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the medals in men's figure skating 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yuzuru Hanyu"]], "filtered_resps": ["Yuzuru Hanyu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b40f0e88fdd75f53fd8823cc92012a2f86a18dcab2940922ad48802ca1f9dbb0", "prompt_hash": "876d021e8bef7c843b1a31ec8a167d8af06cd08cbbf319ef089202f89267d865", "target_hash": "e8726f23b8265ee586fabc7e51cac6c372fbd8f1e8a79e0d98332df023187293", "exact_match": 1.0}
{"doc_id": 3119, "doc": {"question": "who ruled the ottoman empire in the 1500s", "answer": ["Selim I"]}, "target": "['Selim I']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who ruled the ottoman empire in the 1500s?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Suleiman"]], "filtered_resps": ["Suleiman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f4fa135b90c6d2b0a27bb9f4406de82eeaaf66503c0450f6e6dfb9d68983595", "prompt_hash": "f95a9d6af67b9e8319c9bb1f3f615168b04018eeeda94d5876d3d399c505bd9a", "target_hash": "2e70344c3de455fefffac6bbac58a01ad0d429582c8c21a15c5d7a2147de02e4", "exact_match": 0.0}
{"doc_id": 3120, "doc": {"question": "who appoints the chief election commissioner of india", "answer": ["President of India", "the president"]}, "target": "['President of India', 'the president']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who appoints the chief election commissioner of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" president"]], "filtered_resps": ["president"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d9e37260d684dd8cda1f758df2db5a754a0ef75c2037946b6834b904e38167d6", "prompt_hash": "3d5f54d7b985edd9170e73b0742ec5eae66c6cb8bdbe5e2328424b7244b030c1", "target_hash": "f88adb4e07dc369cd6ca29d513da1e961e44b46aaf3683b949786a0e705e9de9", "exact_match": 1.0}
{"doc_id": 3121, "doc": {"question": "song have i told you lately i love you", "answer": ["Have I Told You Lately"]}, "target": "['Have I Told You Lately']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: song have i told you lately i love you?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Have I Told You Lately"]], "filtered_resps": ["Have I Told You Lately"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "495c159fea6069b6ec4eef769cf5dd387ab9f1abd01d44d6734b141a9553b986", "prompt_hash": "6b7ce5b250c36a7177f230de6eb2879074bc3ee59b42fc5628b1f7b0a41cde93", "target_hash": "bf547653e351aedecdfef0975bcc1b0b264caae25ac35a43450eb8498dfbbdaa", "exact_match": 1.0}
{"doc_id": 3122, "doc": {"question": "when did the legal drinking age changed to 21", "answer": ["By mid-1988", "October 1986"]}, "target": "['By mid-1988', 'October 1986']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the legal drinking age changed to 21?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1984"]], "filtered_resps": ["1984"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb9f7d4fd79756cdd3b46a876690456835be56117c909936ee43ef9c24007e12", "prompt_hash": "5b3659e38512c6f0fab985570cfe9dad1bbc564ae7b73f39bcf85eb47c8f1d87", "target_hash": "54ed613636f0554188f59e72ba37b0e9126d16a353a2f4ac03893d1711834291", "exact_match": 0.0}
{"doc_id": 3123, "doc": {"question": "eukaryotes have how many origins of dna replication", "answer": ["up to 100,000"]}, "target": "['up to 100,000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: eukaryotes have how many origins of dna replication?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" one"]], "filtered_resps": ["one"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9821937d56a2197e970fbd39a7f2e7a53e02326de26c5eb5e97716f7476509e4", "prompt_hash": "ccfc92aaa719bedcda30350064bbf412ce7b63426e02c671a93eeaf16c657dff", "target_hash": "3dc449d4a8f033003fe65b362d74368197d977a83f19a64aa7f5b717bc5ebc42", "exact_match": 0.0}
{"doc_id": 3124, "doc": {"question": "who performed the halftime show at super bowl 51", "answer": ["Lady Gaga"]}, "target": "['Lady Gaga']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who performed the halftime show at super bowl 51?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d8da641cccc802800799443aec5252c681512943e8e8a42dea4d9554f6ffb23", "prompt_hash": "3a33364d6e35c743f812ecef8e334a5bf72657339c63626baa7bd03767c1bc01", "target_hash": "3f9f37b05b94bb689a144395119c01dbc061e4de7ed0fab577e940aa6383a20e", "exact_match": 0.0}
{"doc_id": 3125, "doc": {"question": "when did world war 2 officially start and end", "answer": ["1939", "1945", "2 September 1945", "1 September 1939"]}, "target": "['1939', '1945', '2\\xa0September\\xa01945', '1\\xa0September\\xa01939']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did world war 2 officially start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1939"]], "filtered_resps": ["1939"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "12e369fb2bd3fca347324999f3ef1150f3cbc51caafd5b0f54855c57e4cc8e84", "prompt_hash": "46a40737dbd7647cc662c001da102a417df040db0d755208a3e1b46160313dce", "target_hash": "ebc9c80127da26759cb815e1a00399d1cd80c46cc6f728b7f3d7ef8ee3ec70ef", "exact_match": 1.0}
{"doc_id": 3126, "doc": {"question": "when does the new death of superman come out", "answer": ["2019", "2018", "in 2018 and 2019", "late 2018"]}, "target": "['2019', '2018', 'in 2018 and 2019', 'late 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the new death of superman come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" December 23"]], "filtered_resps": ["December 23"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d5c2465b904745b5166263b7e79c6674b176a6f1e2cb8ec4cb1649938a68476", "prompt_hash": "56598296518a9744ff39b0f5800afb96425fcb39a1034b9cc58dbdd9fe63960b", "target_hash": "b8ded8f67f4d4cb1f0ddcadce19ac39f4a8cb71d98c84b5bc6f9c6a1b494a6a0", "exact_match": 0.0}
{"doc_id": 3127, "doc": {"question": "who win road march in trinidad and tobago", "answer": ["Superblue", "Machel Montano"]}, "target": "['Superblue', 'Machel Montano']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who win road march in trinidad and tobago?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Trinidad and Tobago"]], "filtered_resps": ["Trinidad and Tobago"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6676090de93e6350d34eb93d316392138adf99fa4c71c47fa84cf4ce3583c07d", "prompt_hash": "639a2d81e801bf2dc8a3434981ac5cf0bead3de3d8868da6f92e7c6dc25d1224", "target_hash": "66e02be03c7c35306841c4edfc9d803aba8bc33dbeb36fb524765479f558aaa5", "exact_match": 0.0}
{"doc_id": 3128, "doc": {"question": "who was elected president of the united states in 1928", "answer": ["Herbert Hoover"]}, "target": "['Herbert Hoover']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was elected president of the united states in 1928?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Herbert Hoover"]], "filtered_resps": ["Herbert Hoover"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5a39e752e885991bb21bbf88aa3a0ff1fb4de93117aeeec1f2bbfb3158655f2d", "prompt_hash": "3bd2e2d70c5c6df4a5598b4556225daede239cbd1fe39dff76fa7d7987d75576", "target_hash": "12e90b4275a7ab46a5ef3a8bcfd69376107183af2f9e8b80646c28a767863946", "exact_match": 1.0}
{"doc_id": 3129, "doc": {"question": "who wrote the theme song to law and order", "answer": ["Mike Post"]}, "target": "['Mike Post']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the theme song to law and order?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Richard Gibbs"]], "filtered_resps": ["Richard Gibbs"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbe6e2855f0ca1461549f50efe900cfa61cdf9de8a44ec5d8600b2ab41e4f460", "prompt_hash": "e4670e849dd012556b511ad8680bbb3078272db09bb2b81d028f490e025e6d05", "target_hash": "76087a808fc42df0c234733be94dd95ceac878fe63acd148abed5adc880e5657", "exact_match": 0.0}
{"doc_id": 3130, "doc": {"question": "when did the angel of the north get built", "answer": ["1998", "1994"]}, "target": "['1998', '1994']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the angel of the north get built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "39a0167db51cd993bfd41a684f2766afcdfff798fddb09fac91982ffdf20c5f4", "prompt_hash": "b2027ced7e156ee64f554013e957594e07f6c5ddfcb7361b4d64fd883a367652", "target_hash": "0b3ccba060657590f3aeb4716c90dfa6a2dce395f3dc5b771b97636884b95291", "exact_match": 1.0}
{"doc_id": 3131, "doc": {"question": "who plays lois's dad on family guy", "answer": ["MacFarlane"]}, "target": "['MacFarlane']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays lois's dad on family guy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Frank Grimes"]], "filtered_resps": ["Frank Grimes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "527b46620ae5d51a8be7ae78188ae5dfbc83ea24cbfb4f4e253150bf58f98022", "prompt_hash": "95365e7ba5c99be5a15d14258b08cbab60e91bea82f92edbe5912f0ead35500f", "target_hash": "d3b3c08df32eff10494c01d94a1f7a9142a83906911a3c613381ed26b90fb84d", "exact_match": 0.0}
{"doc_id": 3132, "doc": {"question": "who plays david in alvin and the chipmunks", "answer": ["Jason Lee"]}, "target": "['Jason Lee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays david in alvin and the chipmunks?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Cross"]], "filtered_resps": ["David Cross"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "067435b29ca7c16ae46bcba03c577089193e8417e5b5f944411d0acc36e1655e", "prompt_hash": "7e65a08524f7359a7c0ebdd3f608a8994e6d193386bfd69838db03f71c7d0a48", "target_hash": "93e1f2aa7f35d5eb828bed738cd4e35e55a2c0bac0c83b10bc04c810de007aa9", "exact_match": 0.0}
{"doc_id": 3133, "doc": {"question": "where is the tennessee titans football stadium located", "answer": ["Nashville, Tennessee"]}, "target": "['Nashville, Tennessee']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the tennessee titans football stadium located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nashville"]], "filtered_resps": ["Nashville"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cc0a55d626c96c43a794844063cea2476b226ba8f25c1cf9d99efc9d6dc9d189", "prompt_hash": "aa578b02c9ffe11ea9dfd275be1c41f7d9768374b23e300fa65cc98b3e7dd960", "target_hash": "b59444d531fdab926cf0d0f51072742a8bc03d66c2e09d7407d53db2d84ba040", "exact_match": 0.0}
{"doc_id": 3134, "doc": {"question": "what is the salary of the ceo of microsoft", "answer": ["$84.3 million", "$84.3 million (2018)"]}, "target": "['$84.3 million', '$84.3 million (2018)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the salary of the ceo of microsoft?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Satya Nadella"]], "filtered_resps": ["Satya Nadella"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7be18347eae1d8602e419d9b69b182c713f0548fd896dcbfbfb57a7c6796aa2e", "prompt_hash": "cf4a7a79ab09c5cbe52898192168ae873b15d6d18013d44ae4f93980968245a3", "target_hash": "d1aa9e480230dad21f7f9ce529c0bae7fa58c593c828faff05c1c2c4ea293cd8", "exact_match": 0.0}
{"doc_id": 3135, "doc": {"question": "who is directly elected according to the constitution", "answer": ["senators"]}, "target": "['senators']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is directly elected according to the constitution?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the President"]], "filtered_resps": ["the President"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0e1f7f87db9b61a4e39526911e1a88bebf85050908f1f21cb4d721d8fc4a063f", "prompt_hash": "76e7c530a0f4080f8e3fd3aaad65692e5e399835a0da960f3ad175a5c3dca1c9", "target_hash": "69b22a09eb792faca53d80efa4eff7653a3e62afb833aa6a450e5a4e511edc99", "exact_match": 0.0}
{"doc_id": 3136, "doc": {"question": "where is the suez canal located on a map", "answer": ["Egypt"]}, "target": "['Egypt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the suez canal located on a map?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Egypt"]], "filtered_resps": ["Egypt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "717148c583006abbefcaa515d232e44d20806817b0bec39257386d9252a3c804", "prompt_hash": "97cec76e10da3add5dd439f8668c71c71629885a0b98137fe2dd7290021b81c1", "target_hash": "3805f88755fac2aedf4b6e3e90c22ac89dd56ddc838f719efbeb1dbbe090f2d4", "exact_match": 1.0}
{"doc_id": 3137, "doc": {"question": "baga beach is in north or south goa", "answer": ["North Goa", "North"]}, "target": "['North Goa', 'North']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: baga beach is in north or south goa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" north"]], "filtered_resps": ["north"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "32b25fd88cbff3ca85f1b352034b0d4f05ed3abf46e924c705a43ee49c7d458f", "prompt_hash": "a0c9d561c4d2c0ccc36f390e19fd3ee85f802510a2b0e02c673b3e6590c9e1af", "target_hash": "7f9e79f276880ebc900530dd8d30ad8b507a669683db1094237b7ea6bb06da8f", "exact_match": 1.0}
{"doc_id": 3138, "doc": {"question": "what disney cartoon character's middle name is fauntleroy", "answer": ["Donald Duck", "Donald Fauntleroy Duck"]}, "target": "['Donald Duck', 'Donald Fauntleroy Duck']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what disney cartoon character's middle name is fauntleroy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oliver Twist from Oliver & Company"]], "filtered_resps": ["Oliver Twist from Oliver & Company"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e980be50f64010e62318d287a16253539022ac89b114678400ac6fecde832fd5", "prompt_hash": "68c812721893b5f30de8cb08fa5d6639e2c11c5dff9b4ba0f8f37bcd95d26036", "target_hash": "ef1af7ea20f1d1229add3f8ea061f9f0c9a1e1ac5e713c1fdbed531eeb1aa393", "exact_match": 0.0}
{"doc_id": 3139, "doc": {"question": "when did the romanticism period start and end", "answer": ["1780–1830"]}, "target": "['1780–1830']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the romanticism period start and end?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1780 to 1860"]], "filtered_resps": ["1780 to 1860"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae5b967f35893554069c5d82542ab7042188130690963bca265b07eface43ed9", "prompt_hash": "d06714e18f5839030aa0604f8e99d787b49b81684328130407d56b0bf5bba7bc", "target_hash": "138671516c0c30db4bd9ddd1dcdbd3847a47541ba2928d431d2e33d955fe064b", "exact_match": 0.0}
{"doc_id": 3140, "doc": {"question": "three act puccini opera first performed in 1900", "answer": ["Tosca"]}, "target": "['Tosca']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: three act puccini opera first performed in 1900?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tosca"]], "filtered_resps": ["Tosca"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a017c15e8c5b890dbe33afddcff8c37f6c901d0a1a9d35829832a1484de007bb", "prompt_hash": "03e790e0450d49806d8ad9702facd782d9ace6088cf01ff2c0c8d41241f6cc1a", "target_hash": "757d0e422da765163078acf542261b3884301bfe31ff8726f896f677db1008d9", "exact_match": 1.0}
{"doc_id": 3141, "doc": {"question": "which is the largest great lake in north america", "answer": ["Lake Superior"]}, "target": "['Lake Superior']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the largest great lake in north america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lake Superior"]], "filtered_resps": ["Lake Superior"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "be820e45566b23212ed967ffbf43fddd75a3e225dfab48370923917ba75553df", "prompt_hash": "3540faa876abffa0bf9bf7e8218ab8c9ebc7bd9418654c043bd537fb4b9ea284", "target_hash": "ea51c15bf4a6e9bf59e62a6ee4b3aa56f2b84cc72137bb104def3a870f54200c", "exact_match": 1.0}
{"doc_id": 3142, "doc": {"question": "who wrote the song what child is this", "answer": ["William Chatterton Dix"]}, "target": "['William Chatterton Dix']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song what child is this?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Dix"]], "filtered_resps": ["William Dix"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fb5c899710e30a1d237a0b2536d1c34982878b71170479d64d8676feaee7d62c", "prompt_hash": "93a257d702f3d34ead4669d4bdcecb866043126469addd95173cf92c540efb0f", "target_hash": "251d60e1a1bd379919c7218d0ba4d81b7f9ec461fe308d66e76322db30a3e6d8", "exact_match": 0.0}
{"doc_id": 3143, "doc": {"question": "who is credited with creating the gothic art movement", "answer": ["Abbot Suger"]}, "target": "['Abbot Suger']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is credited with creating the gothic art movement?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Otto von Bismarck"]], "filtered_resps": ["Otto von Bismarck"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "49703b006ccf00f484f9b1e0f608317255142ce24c0e18983b6ee3db5f0a9d30", "prompt_hash": "47ef81e711e76edcbe196f6c215d9180be4574b1c25a04c38704c1cb2e4f0d5b", "target_hash": "e10c18ffef659364f3b216124ae96ade03734a2ecdb327cc79d61ff6cbce92ac", "exact_match": 0.0}
{"doc_id": 3144, "doc": {"question": "when did they stop using aluminum wiring in ontario", "answer": ["mid-1970s"]}, "target": "['mid-1970s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they stop using aluminum wiring in ontario?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1974"]], "filtered_resps": ["1974"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e77d7deea0d2a83bd491c6b6938cf060d28bbb105808a2e7eac1035feec8b5e0", "prompt_hash": "89d5a3fd42fc1c244324ba0d0c48dec6aaf3fd78783e0558e71e22a00dd9b3ba", "target_hash": "5d7f1c087a69eaacf72fc3ca23c608cddbfec5c8cf97ffceef703f1108817b08", "exact_match": 0.0}
{"doc_id": 3145, "doc": {"question": "where is nuclear energy used most in the world", "answer": ["United States", "France"]}, "target": "['United States', 'France']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is nuclear energy used most in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United States"]], "filtered_resps": ["United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "08b697b4c7ff7e500075da0ddbdce4141cf1e03eb0236548d3d68d8e43f717de", "prompt_hash": "51d2cbe20e1c4e0e84e375ae3ac8c47c2aab8c382c7c560aaa455a72e96a9f7f", "target_hash": "735cf84202ab4300b3917fc679aeb89f5e756b441929d06de26238c7502e69e8", "exact_match": 1.0}
{"doc_id": 3146, "doc": {"question": "who carried the us flag in the 2014 olympics", "answer": ["Todd Lodwick", "Julie Chu"]}, "target": "['Todd Lodwick', 'Julie Chu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who carried the us flag in the 2014 olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shaun White"]], "filtered_resps": ["Shaun White"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c486aa1c4ef1be15ceaf147c05644a1575f18f3386a84ac77697343aca4eb7e0", "prompt_hash": "f9e8ab272470857a8dc2f42aa75ee4debb8e73047e19e71f51ea43c88e2eefa5", "target_hash": "fdd74afe4c3aa5b3b4559d3bbc422a8f5c3ad77cab099ada82c0d9ba91f945fc", "exact_match": 0.0}
{"doc_id": 3147, "doc": {"question": "when does the next apollo book come out", "answer": ["May 1, 2018"]}, "target": "['May 1, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the next apollo book come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f5002e4dd441f0c787aced83eeec882c41eda9e3ef21468c724c7aceeced6c96", "prompt_hash": "ccffece3abb6afc3ad02162a5fb1001b994f2a9a4af97873cefce35a113c491b", "target_hash": "74ba98453c92bba4d581a9e611b59c6d5e8a4ef7d49bb60efcdd54b1d084ec9f", "exact_match": 0.0}
{"doc_id": 3148, "doc": {"question": "when was the last time lebron scored 50 points", "answer": ["March 5, 2008", "November 3, 2017"]}, "target": "['March 5, 2008', 'November 3, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time lebron scored 50 points?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2018"]], "filtered_resps": ["2018"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "837e55ec201a0a710b8481476ecbc0bb4871e04d63b163e35554c28387005568", "prompt_hash": "654eaa35bf971c526522008a110fd1660890c92889b27fb3d9b7681579ea7994", "target_hash": "40950580203e6136756cc99c18057d5e5779586107d21731397074cb7131987d", "exact_match": 0.0}
{"doc_id": 3149, "doc": {"question": "who created the borders of africa's countries", "answer": ["European imperial powers"]}, "target": "['European imperial powers']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who created the borders of africa's countries?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" European colonial powers"]], "filtered_resps": ["European colonial powers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "66a38f58074586e4c6dbbd23fe0a4217bcedcc67ae66d7f33e27e7937f9e27ee", "prompt_hash": "01d0a8477edb503ba4f9898bce2e691b3f77676b837c30e4c8cfb5ffe7e01247", "target_hash": "3e1e42998de524be95019e0163c5321d1b3771285e2a4d832dc92fea3a6bee5b", "exact_match": 0.0}
{"doc_id": 3150, "doc": {"question": "where did the british empire control an entire continent", "answer": ["Australia"]}, "target": "['Australia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the british empire control an entire continent?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Australia"]], "filtered_resps": ["Australia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2596003f5f6229b2c3c3fc647df3c2558e863c1beecd1675debaa1c88edb0db", "prompt_hash": "ece0bc9cf97ca6b632febd3485687878a91b043638973f6820d2f6679a71c805", "target_hash": "e93046d7b9b679fac171cfe1c97d475a9a91f20f122092146289e932cf18c258", "exact_match": 1.0}
{"doc_id": 3151, "doc": {"question": "who won best boy band i heart radio music awards", "answer": ["BTS"]}, "target": "['BTS']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won best boy band i heart radio music awards?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" One Direction"]], "filtered_resps": ["One Direction"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f08f3665312526253344898124e0226cb9fd00d8500ed75d13d38444ca528c0", "prompt_hash": "c0d691b4507a108555a812894c974f17c77c665dbce0dc670f4d0201aca86271", "target_hash": "a1bae24eb2278e5b66e75f7c78810e1d11831439df0d0524557b5090b50c61bf", "exact_match": 0.0}
{"doc_id": 3152, "doc": {"question": "when did it become law to stand for the national anthem", "answer": ["June 22, 1942"]}, "target": "['June 22, 1942']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did it become law to stand for the national anthem?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1942"]], "filtered_resps": ["1942"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e0df7d1d7a710ea8adef142c750b9365507abb1e7abf9c3cc3e30ad01d8b7d50", "prompt_hash": "f67a0f7a7db95d9ed0cfb28ee64e545069465dae8cdb199d377df11b66d0d0ce", "target_hash": "e486de3a73565820d1a2d487f41f6634508d38248a00226428324466137b667b", "exact_match": 0.0}
{"doc_id": 3153, "doc": {"question": "who is the character of santa claus based on", "answer": ["Saint Nicholas", "Father Christmas", "Sinterklaas", "Wodan"]}, "target": "['Saint Nicholas', 'Father Christmas', 'Sinterklaas', 'Wodan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the character of santa claus based on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Saint Nicholas"]], "filtered_resps": ["Saint Nicholas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "52fb836bdb0d23fa90efc5c2173b391d6a99a865fb6a2e9be4f0aeb41a6a9b95", "prompt_hash": "bae1803471750552d69d3dd3e71346c91cb6e01b4899d255d0c89b325ec4b2b4", "target_hash": "804f5e8dee88556d08d145f4bc2b0e3ca9b621d2d2fc0fb78b185117734c1fce", "exact_match": 1.0}
{"doc_id": 3154, "doc": {"question": "who created the convention on the rights of the child", "answer": ["The UN General Assembly", "The United Nations"]}, "target": "['The UN General Assembly', 'The United Nations']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who created the convention on the rights of the child?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United Nations"]], "filtered_resps": ["United Nations"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f76965936b79c82e899e2eca9e64277c89bc26d62aeacc4839bf82bae25bcf76", "prompt_hash": "4e3f4c6eebafe6d7d9decd8ba7bb13284a794925a3a0eebba9687097e2f15636", "target_hash": "be4af3e32a861a2b037e97f2d87956200684e138930ed03f1f241e543d454c62", "exact_match": 1.0}
{"doc_id": 3155, "doc": {"question": "who wrote the song the glory of love", "answer": ["Billy Hill"]}, "target": "['Billy Hill']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song the glory of love?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Lionel Richie"]], "filtered_resps": ["Lionel Richie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35332cf058f63753eb5ddcf07b6685f14172c0d6c2277d984914c9cdc4e95599", "prompt_hash": "280cf405427398a4a97be7e08af7f4e52cae9689db34cff6bf56e1f923eba037", "target_hash": "4661380eba5b3a4f51945bea8608b4ba8707a7d60422201c580f732d9bc7ee2a", "exact_match": 0.0}
{"doc_id": 3156, "doc": {"question": "who spoke the words ich bin ein berliner", "answer": ["U.S. President John F. Kennedy"]}, "target": "['U.S. President John F. Kennedy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who spoke the words ich bin ein berliner?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John F"]], "filtered_resps": ["John F"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3f3319fa9ecfdd81e2d63b6635c9f2d3673ffc3c554fa56af7e9dcd4417330b6", "prompt_hash": "e99198fc6e91d20c5f5b6f2d92832e247ff289152be31a1a14bc0e373a3ecb52", "target_hash": "9de18e90b47b384065f40de9510b96e26bd6f875767e0853bbb0230325891e23", "exact_match": 0.0}
{"doc_id": 3157, "doc": {"question": "who sang the american national anthem at the super bowl", "answer": ["Pink"]}, "target": "['Pink']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the american national anthem at the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3994105a167f93fc40f85c3dac933cffdeafebd7171e4a3f972e3f894672374a", "prompt_hash": "683b347f59ccf340aaad0916890b2eb7569ac75e9f08f27cdf9a45b18970ed76", "target_hash": "491e226c43d39720230f8307c51a67ba28b0b68501d88129e96093803b7f227e", "exact_match": 0.0}
{"doc_id": 3158, "doc": {"question": "capital of georgia the former soviet republic 7 letters", "answer": ["Tbilisi"]}, "target": "['Tbilisi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: capital of georgia the former soviet republic 7 letters?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tbilisi"]], "filtered_resps": ["Tbilisi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f2e1a9e70fe206ad3f7481b909ff56701e121acd7419e431faba1c4133c0354f", "prompt_hash": "50c7aaff1c1cac61849e0a5d901b514506ad0c8eea0c61fb6e2cf7bd76df85ac", "target_hash": "ba7128c845303f5c520411b7cdd061be21410622817636ac2dca0c25b561d19f", "exact_match": 1.0}
{"doc_id": 3159, "doc": {"question": "who owns the four seasons hotel in las vegas", "answer": ["MGM Resorts International"]}, "target": "['MGM Resorts International']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who owns the four seasons hotel in las vegas?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" MGM Resorts International"]], "filtered_resps": ["MGM Resorts International"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "43fe8656bb31b9c8310fbe7f0fce00d4c94fd1279ff5bd2f314c03a56a63126e", "prompt_hash": "764de850495fa6f660f75aa80640e3a24f93d54501c0b5bfbbc629b6ea6b5638", "target_hash": "c88630fa60e326917cb42577bfec1f076a474cab17aeed12f74f2bee3a1f4360", "exact_match": 1.0}
{"doc_id": 3160, "doc": {"question": "who was allowed to vote in the roman republic", "answer": ["unknown", "citizens"]}, "target": "['unknown', 'citizens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was allowed to vote in the roman republic?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" male citizens"]], "filtered_resps": ["male citizens"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "30eeeb39cfaddf17e0b34ded836abc92aed5b06b7c17bad04199a1c7c143ff54", "prompt_hash": "60e8c57b4585369a00008c5eb2146b02c4b0e0fafafa42eed60e04e8816f97d5", "target_hash": "1e9e9eaeaea248c0dc05611878bfa72783a1b0885b3fca01a06384de1cabeabe", "exact_match": 0.0}
{"doc_id": 3161, "doc": {"question": "form from material that has accumulated on the earths surface", "answer": ["Sedimentary rock"]}, "target": "['Sedimentary rock']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: form from material that has accumulated on the earths surface?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sedimentary rock"]], "filtered_resps": ["sedimentary rock"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "80f2fc1642f853aa756e03fe3fecc6905e743c58e677d59bc2fe7ca983f2c97e", "prompt_hash": "0a77af9cc323874c367b343d6dcdf605145cb5f4920c0b29b5728f0bb2520c82", "target_hash": "1c8a60847708be3b25a5f3dcac05ee2524bbd9d4daf3e7f6cdda1eea44382bfa", "exact_match": 1.0}
{"doc_id": 3162, "doc": {"question": "the sport psychology sub-field of performance enhancement deals primarily with", "answer": ["Arousal regulation"]}, "target": "['Arousal regulation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the sport psychology sub-field of performance enhancement deals primarily with?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" improving athletic performance"]], "filtered_resps": ["improving athletic performance"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4a3be6ecd7ebf620d764c4fe06e2d981657e2832bdb20296e551eb02d9823a41", "prompt_hash": "42cb4c784f13750ea9c13cfebd1b6abcf701fd55bbc840fd4f09e6903a066542", "target_hash": "0b72b49e6c2054352dce5227a7422479c8e16d658314cff2b307aaeb97842cc7", "exact_match": 0.0}
{"doc_id": 3163, "doc": {"question": "when did the prr first begin its passenger service between pittsburgh and philadelphia", "answer": ["by 1854", "1854"]}, "target": "['by 1854', '1854']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the prr first begin its passenger service between pittsburgh and philadelphia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1852"]], "filtered_resps": ["1852"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a5348b012473596592b6d23661b27d5e6b887c44c897b15987f66ea6efc5ff1a", "prompt_hash": "76920c06988778374e1ef99b370899735c3988672cdcadadbb5dd9a5e62f16f9", "target_hash": "9b43ec52b2ad5a0459ba9308a47444b3cd91bd88706a1adcbc2078e707a1c7d6", "exact_match": 0.0}
{"doc_id": 3164, "doc": {"question": "when do luke and lorelai get back together", "answer": ["Say Goodbye to Daisy Miller"]}, "target": "['Say Goodbye to Daisy Miller']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do luke and lorelai get back together?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 4"]], "filtered_resps": ["season 4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8a24dd0495aef85e2529eab584857958d602c8938f5f628796cd0c82441583d0", "prompt_hash": "17104099abdb130ccc8db524c89108b1ea81d6170dce410d0ce29e79ab93f86e", "target_hash": "38b51fd1e0c657a7864d1bb7a496c4f6122d9c43ab38996215b8321ff1c67a09", "exact_match": 0.0}
{"doc_id": 3165, "doc": {"question": "who is mowgli's main enemy in the jungle book", "answer": ["Shere Khan"]}, "target": "['Shere Khan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is mowgli's main enemy in the jungle book?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shere Khan"]], "filtered_resps": ["Shere Khan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "346f691dd6bb2fd4c5b066758552d8f0c5fa538a0301730bff47da05632a9544", "prompt_hash": "c34770226d37be0aa2b2141d179346e65141b4c01d75d8d6eb4395711bd78696", "target_hash": "30e82f4d81920f5fdb07d795fe31414bedc516ba220ac45efd408a9fac0b0ba5", "exact_match": 1.0}
{"doc_id": 3166, "doc": {"question": "who drives the number 95 car in nascar", "answer": ["Michael Christopher McDowell"]}, "target": "['Michael Christopher McDowell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who drives the number 95 car in nascar?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jimmie Johnson"]], "filtered_resps": ["Jimmie Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "952b3dbecf61c10f8a2e115575c2177b67f7cfd91f5bda191a0acf3384f92548", "prompt_hash": "946291b64932ec6f648141eae89d0134488130167dd2e64c3c1f3fc44dd8a821", "target_hash": "25fdff767978c64cac0010bb4d49b9a9760dc7a0715a6f95b884498218c052ce", "exact_match": 0.0}
{"doc_id": 3167, "doc": {"question": "who sings the wire season 5 theme song", "answer": ["Steve Earle"]}, "target": "['Steve Earle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the wire season 5 theme song?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kanye West"]], "filtered_resps": ["Kanye West"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0136640f5e51d712b1f1291f8f09ed006e82fccc895d22ae14db0788e891780e", "prompt_hash": "f0d9f3313bda7009941f9e74b9f9623df7c8e368b0b88edb7b2ba393678a6269", "target_hash": "03d36a0e549eda4034c023ef06d7b906f0775088a4a5f1e899c1c2c7a3545cc1", "exact_match": 0.0}
{"doc_id": 3168, "doc": {"question": "who plays dorian tyrell when he puts on the mask", "answer": ["Peter Greene"]}, "target": "['Peter Greene']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays dorian tyrell when he puts on the mask?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Peter Dinklage"]], "filtered_resps": ["Peter Dinklage"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ad032ed65a4d5b3b315d0d683effb32083f520013e2b05c0b93bac003a80d246", "prompt_hash": "3ab7c99e4d9971070b4e28dbb5e4f7f5ac89311722bbf691059be08961a86f85", "target_hash": "93bbf1de16a88005c70401f809f5828fd683dd769dbd326f1ba8f1e457e035b1", "exact_match": 0.0}
{"doc_id": 3169, "doc": {"question": "who has won india's next super star", "answer": ["Natasha Bharadwaj", "Aman Gandotra"]}, "target": "['Natasha Bharadwaj', 'Aman Gandotra']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won india's next super star?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Akashdeep Singh"]], "filtered_resps": ["Akashdeep Singh"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "31dc03a142c4924feafc149def281c3da5c095b7ea686b651e40bb8fa1516c78", "prompt_hash": "c7007b2da70d3b8e85dd883588ac38b6ea5e7b0c98ca2a56d4f7b3a9f00db364", "target_hash": "0f01a246f477b10eb7dc6574f43efd811e9892dc8d742fc9baf388ecb2380046", "exact_match": 0.0}
{"doc_id": 3170, "doc": {"question": "where did an independence movement occur because of the congress of vienna", "answer": ["Italy", "Norway"]}, "target": "['Italy', 'Norway']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did an independence movement occur because of the congress of vienna?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Italy"]], "filtered_resps": ["Italy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d28c14592f03e6bc149cf19b25021ad904cc4368c2c674e5a4b54693d4420a3", "prompt_hash": "7521cecc63bd6fdefedcbebe1ec338e44ee0eab0d4df870d48b106efdaea646b", "target_hash": "d357ddcfe5816848131d53178a20870d80233965036d02dbd52937f8685ba9f4", "exact_match": 1.0}
{"doc_id": 3171, "doc": {"question": "where was it happened at the world fair filmed", "answer": ["Seattle, Washington", "The Seattle Center", "Camarillo, California"]}, "target": "['Seattle, Washington', 'The Seattle Center', 'Camarillo, California']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was it happened at the world fair filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York City"]], "filtered_resps": ["New York City"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "293e9cf11fb18590c7ae8297c8569d6e841d9aba9cb832d568399eb8df79aeb7", "prompt_hash": "790007dbf1cd381cba0e43614e05cccea36f43005a10c3dc44614bfdbcaf0111", "target_hash": "40d998b49935baeb48d70e346fccb03137e19f575864ccf1ebf9814671cbf896", "exact_match": 0.0}
{"doc_id": 3172, "doc": {"question": "who wrote the song to make you feel my love", "answer": ["Bob Dylan"]}, "target": "['Bob Dylan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song to make you feel my love?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bob Dylan"]], "filtered_resps": ["Bob Dylan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a8a79074396e15b087aba27ac505500184654971bcccd39240e8cd5d7b2e8e5", "prompt_hash": "6f44d017a1c2f86e8f0b553587002eadd993f7b1d3c3ae1c8529c31defbf49c0", "target_hash": "183d2854d32090092acfd7e78bc5499b9b3fb04b27e2963a1269b70bf1812be7", "exact_match": 1.0}
{"doc_id": 3173, "doc": {"question": "who plays harry's mom in harry potter", "answer": ["Geraldine Margaret Agnew-Somerville"]}, "target": "['Geraldine Margaret Agnew-Somerville']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays harry's mom in harry potter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Emma Watson"]], "filtered_resps": ["Emma Watson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "17ee3394716e45b6b6b98085abb19b51482ad0c03114d481b028087e171e7d06", "prompt_hash": "d6243a10d812373d4658bedeafc621a0f714cb20361344bbd0f21cae61c0c565", "target_hash": "33ee0d244bf66e07e8de643fafcb50c810d6156431db55081a80dc06a13be8c2", "exact_match": 0.0}
{"doc_id": 3174, "doc": {"question": "what is the membrane on the surface of the stomach called", "answer": ["the visceral membrane", "Serous Membrane"]}, "target": "['the visceral membrane', 'Serous Membrane']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the membrane on the surface of the stomach called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mucosa"]], "filtered_resps": ["mucosa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2433b20460ea01d3b91a08fd149d722d916e54fd8c346ee5296567df1efa2da8", "prompt_hash": "bb1a3d1756b173d443fcdc63c96b4e4166a8ee5e7292ca574216944fdfaaedac", "target_hash": "9d6dc34ae6aa7e358a91fd34675d253694415b07c997d1fd68834ed75865d38e", "exact_match": 0.0}
{"doc_id": 3175, "doc": {"question": "who won the men's ice skating 2018", "answer": ["Yuzuru Hanyu"]}, "target": "['Yuzuru Hanyu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the men's ice skating 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Javier Sotomayor"]], "filtered_resps": ["Javier Sotomayor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3a5b67cc7a754b34016a7abcdb6439dafa1b0c81050c013ed2102b9ab8562153", "prompt_hash": "8ab2a83dca3d967157589b04a560341dd3e3bff469a9066bd95b6a64b3885927", "target_hash": "f6935a29a19a36c37c3c4f200de042c11f3690a07ff9848ed93d24787842dbfc", "exact_match": 0.0}
{"doc_id": 3176, "doc": {"question": "who sings i'll sleep when i'm dead", "answer": ["Warren Zevon", "Bon Jovi"]}, "target": "['Warren Zevon', 'Bon Jovi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings i'll sleep when i'm dead?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Metallica"]], "filtered_resps": ["Metallica"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b544d1c6170c98c403a36bfba2c242a5c916fa6fdf966adea2eeb10010c2239", "prompt_hash": "dff8e97672618e2ad2c39d25a7118fdbfcb441352c9a9a98bd42beaedee83a89", "target_hash": "e8c9ba0e92f24ca28db6d73d1926fa96d57e9228eab22abf917eefb80564a13a", "exact_match": 0.0}
{"doc_id": 3177, "doc": {"question": "when did they stop putting lead in paint", "answer": ["1992"]}, "target": "['1992']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they stop putting lead in paint?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1978"]], "filtered_resps": ["1978"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f57b4bd864dec137f2247177e288fcdad435af9eb91e42f58b17c7d98527e94b", "prompt_hash": "48110f4b67fc9da73006d99a1e41e4e1f6aadae89f98f0b35f605666fd1698f7", "target_hash": "5e56bf386b12a9e6abeaa107866348aa8879a9334b5f4c1e1cb16df85ac73ec6", "exact_match": 0.0}
{"doc_id": 3178, "doc": {"question": "what episode does caroline get turned into a vampire", "answer": ["Brave New World", "The Return"]}, "target": "['Brave New World', 'The Return']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what episode does caroline get turned into a vampire?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Vampire Diaries"]], "filtered_resps": ["The Vampire Diaries"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "86385541e6ddc1407c53cf00f6cb88a7b9031214113a4872a915d1c3a35d84e9", "prompt_hash": "bf26714c7c1cc9d7e660b05af8228c22aa031641e3a4e31cb8fbad535fc2cb39", "target_hash": "61fd746e11171ebf7ae0533be84887011b554409df018e927a35a1715e310894", "exact_match": 0.0}
{"doc_id": 3179, "doc": {"question": "who has won the cma entertainer of the year the most", "answer": ["Garth Brooks"]}, "target": "['Garth Brooks']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has won the cma entertainer of the year the most?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Strait"]], "filtered_resps": ["George Strait"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "08c3c1ddc11860f72778839932b69336b90a12a89fdb30a43806dceda6ee14e1", "prompt_hash": "af94aa5884d258d6aba5c6904f75c422b2930a8ea0c6d05552c253f83931cdf7", "target_hash": "5ef068ead7c95ec8c5454c0d5d483778539a6263e8123fc832f73c404d57de9b", "exact_match": 0.0}
{"doc_id": 3180, "doc": {"question": "who won season 2 of great british baking show", "answer": ["Joanne Wheatley"]}, "target": "['Joanne Wheatley']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won season 2 of great british baking show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mary Berry"]], "filtered_resps": ["Mary Berry"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c445863dc05af40938cde31bc001e3fa4775770f17edd98fc73a1f942fa2367", "prompt_hash": "1b2d5bf842d9c08f61966a044584e753bfd587f2a4886b0d8059682860de6105", "target_hash": "9d6bbfe5f2bcd8236f8fd471ea345324e08d21b5afb987ba78718fc5b969c29e", "exact_match": 0.0}
{"doc_id": 3181, "doc": {"question": "where is this years army navy game played", "answer": ["Lincoln Financial Field in Philadelphia", "Lincoln Financial Field"]}, "target": "['Lincoln Financial Field in Philadelphia', 'Lincoln Financial Field']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is this years army navy game played?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Philadelphia"]], "filtered_resps": ["Philadelphia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7498fbba348818480e723ed2e3742494df4bba20bc0cc04a84ed4b3da38857e", "prompt_hash": "d89bd5740e9ae85d5cb167f06e665102600d8dafdc48968da3bfb17b6e0c2a92", "target_hash": "5e4c7050f894f8a54adf261f24a926826ceec1074ec2eb1f4ff9a25f318b2000", "exact_match": 0.0}
{"doc_id": 3182, "doc": {"question": "who sings somebody's watching me with michael jackson", "answer": ["Jermaine Jackson", "Rockwell"]}, "target": "['Jermaine Jackson', 'Rockwell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings somebody's watching me with michael jackson?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paula Abdul"]], "filtered_resps": ["Paula Abdul"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "feca0aa582a733428f4c24fde2a0d720d7c93bd4a43a191c480bdccc4657db54", "prompt_hash": "dcff5ce8c970a97557efeb52f311f15ed5574092e4a8bf1a4d95fe589f22acfb", "target_hash": "927b711223d49ab968ef9b2be678d4e096ab0e9a41196ac70758660420eef98a", "exact_match": 0.0}
{"doc_id": 3183, "doc": {"question": "who were the first settlers in san antonio", "answer": ["Spanish", "Spanish explorers", "varying cultures of indigenous peoples"]}, "target": "['Spanish', 'Spanish explorers', 'varying cultures of indigenous peoples']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who were the first settlers in san antonio?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spanish"]], "filtered_resps": ["Spanish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "107e59cfdd58424d3fee1ba643fffd61d6c7bd9b25a0c80d253ef128d05c6459", "prompt_hash": "6429083a630e34c61356122a54e77156c4f57a78635edd1e53edf62d9d158ff0", "target_hash": "ab445c8655461b7dc921cb0e4c971d50f5a48277912ae6ec8837d9c4148a23c4", "exact_match": 1.0}
{"doc_id": 3184, "doc": {"question": "who played cory's older brother on boy meets world", "answer": ["Will Friedle"]}, "target": "['Will Friedle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played cory's older brother on boy meets world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Matthew Lawrence"]], "filtered_resps": ["Matthew Lawrence"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3c77f15b40256058772cda7280e4dac35755c27bc3a903c9513700110838cf7e", "prompt_hash": "e7b6f070fcf21f689ce7ae2ea0b9ae026973fe95529aa3cefec0205b48995ae9", "target_hash": "a78c055014c0479612d680b70a90f3af539793b8fa0bbbe4468232e2cc3bc4ef", "exact_match": 0.0}
{"doc_id": 3185, "doc": {"question": "who has the most receiving yards in the nfl history", "answer": ["Jerry Rice"]}, "target": "['Jerry Rice']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most receiving yards in the nfl history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jerry Rice"]], "filtered_resps": ["Jerry Rice"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "efd6ec4237d18b1417bde75b277a791bc3a67247c4949b7b2102e0e2f7a85c8d", "prompt_hash": "e1cc1ec168d1b3a7c4f6b11b83572f59c3c61dc4b7e2438382a41052d4811fd3", "target_hash": "6c4ad64d33b8c6d1c82ca10fcc4597275147fc99bf6626240aa072bc826189f0", "exact_match": 1.0}
{"doc_id": 3186, "doc": {"question": "where are red blood cells made in adults", "answer": ["in the bone marrow"]}, "target": "['in the bone marrow']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are red blood cells made in adults?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" bone marrow"]], "filtered_resps": ["bone marrow"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "db63b28d27ad8a50aa958b786736dde788681cf67af2ee9ff6f3a0d0f7ad0c7d", "prompt_hash": "d633740905a8b931dc58bd05697d88e82dcb910e854bade76b0b9bbe85f65957", "target_hash": "4a36a6a198775a6c9193c0107385f62caf38ca7954a7e70279e78cc91885929c", "exact_match": 0.0}
{"doc_id": 3187, "doc": {"question": "who is playing halftime show super bowl 2018", "answer": ["Justin Timberlake"]}, "target": "['Justin Timberlake']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is playing halftime show super bowl 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Justin Timberlake"]], "filtered_resps": ["Justin Timberlake"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3e5ca37f6f6f707e0cabae826c6dc38e12b365660d45c8aa0337929149dd9312", "prompt_hash": "12a28b93053938f904bb189df32f57542b448d8551d43d023fe6cfeddf50133c", "target_hash": "5d3c152075c166190023989a2621c5a25323ea4325675ced16b03032cec5a254", "exact_match": 1.0}
{"doc_id": 3188, "doc": {"question": "how many episodes in game if thrones season 7", "answer": ["7", "seven"]}, "target": "['7', 'seven']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes in game if thrones season 7?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7e63bea6af25eeb1f68ae77f561d3308f743d72fdc50311878db9ffe9ce158ab", "prompt_hash": "8c3266e44add4d918632b560d26346699cf9199f3b1b8cf1e3901429966ec700", "target_hash": "47f94e48a0b9d4503db57f1a49661eb2671f9fd3c109285b1dddf1e46f90fa36", "exact_match": 0.0}
{"doc_id": 3189, "doc": {"question": "when was the last time unc did not make the ncaa tournament", "answer": ["2003"]}, "target": "['2003']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time unc did not make the ncaa tournament?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1997"]], "filtered_resps": ["1997"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fd87e0525bd044e543354dbdfeaad5673be2cc6b7cccc1750a8bdf9272edeb2b", "prompt_hash": "a844527479ac3f1eba162faa92e88077e46461e24e7fa6426f41389ce74d58f5", "target_hash": "5978614d1e7ad640786f985df33971f764165cd361438c50f60ef546e9e83ff4", "exact_match": 0.0}
{"doc_id": 3190, "doc": {"question": "where was one direction what makes you beautiful filmed", "answer": ["Malibu, California", "a Malibu, California beach"]}, "target": "['Malibu, California', 'a Malibu, California beach']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was one direction what makes you beautiful filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The video was filmed in the grounds of the 16th century Belton House"]], "filtered_resps": ["The video was filmed in the grounds of the 16th century Belton House"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dfcbf6c758c055e4df1a7fd6252d54adf5eadd82bb33e1370513f43a7d113547", "prompt_hash": "9ae21a334dfc7667bea168b24e010c678b16c63189272cbecd93b53e7d631ee9", "target_hash": "a26aca33b1bfdde660664bda8f6d17eee763e1f9533d98d64805997ba40e1644", "exact_match": 0.0}
{"doc_id": 3191, "doc": {"question": "where does the name de la rosa come from", "answer": ["Spanish"]}, "target": "['Spanish']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the name de la rosa come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spanish"]], "filtered_resps": ["Spanish"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d1196efa812ffa5c16c3f6510e3ff6f9762aee3e74f87dffe9734814854d117", "prompt_hash": "2576cf4b521d7c5eb71bf2ec86a42d7b946da68a96156338021514ca3e1042b7", "target_hash": "542c2526bea4ce7a094e660928867975f00301abd0e8f05dc248c994a7aeb6f1", "exact_match": 1.0}
{"doc_id": 3192, "doc": {"question": "what does sip stand for in the ngn / ims context", "answer": ["Session Initiation Protocol"]}, "target": "['Session Initiation Protocol']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does sip stand for in the ngn / ims context?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Session Initiation Protocol"]], "filtered_resps": ["Session Initiation Protocol"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "be9d4746d71c6537854cbf00ff43253cb254bc4e5b8f6c24c5947d0642c604a8", "prompt_hash": "beef7c0584d78b9146fb28aaed791f20f58bccb9856bc1f8048090bf59a6309d", "target_hash": "e3bef6621c4bc34b61353d086c47d504ace5ce0027b38d9df561d9f03a6bfe04", "exact_match": 1.0}
{"doc_id": 3193, "doc": {"question": "who makes the important government decisions in an autocracy", "answer": ["one person"]}, "target": "['one person']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who makes the important government decisions in an autocracy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the dictator"]], "filtered_resps": ["the dictator"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a7ae787b6ce5374f757fdc19b4fbf51574f95f0a2bed579ce2ac817bc6c6d78", "prompt_hash": "b75b258084359ef66e4cd5a723afc67ef6a7fca3e924f91b3408510730d06306", "target_hash": "bb6875700decb58fa9d994518feccb9fd7a82adafa2f3c6f033032a0b169d52b", "exact_match": 0.0}
{"doc_id": 3194, "doc": {"question": "what is the meaning of the name habib", "answer": ["\"beloved\"", "beloved"]}, "target": "['\"beloved\"', 'beloved']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of the name habib?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" friend"]], "filtered_resps": ["friend"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "28d005fd5896130577c2352b5cfa022010b1349f6015cb2ced9e57eed75d4b3e", "prompt_hash": "5bd84438971898d9137426e846435207ea819b07b61b8a30a3a0dcd012ab4269", "target_hash": "aa2be4778762e4614042cfd071d3a9d2b25c92287aac959dc3f0b948ab2ec739", "exact_match": 0.0}
{"doc_id": 3195, "doc": {"question": "who won the most gold metals in olympics", "answer": ["American swimmer Michael Phelps", "Michael Phelps"]}, "target": "['American swimmer Michael Phelps', 'Michael Phelps']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the most gold metals in olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Phelps"]], "filtered_resps": ["Michael Phelps"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "397c6a15253ce47450c72646d7231ef7888cf821885b6414165733b58004f0b8", "prompt_hash": "b3c52991d9b2dcfbe01aa9c08bfe5f1e815ac30b748219708f0729c99595f837", "target_hash": "ccf5ae586df17f5e7d3343910c12c7557bee82222a80cc97c21034290d269514", "exact_match": 1.0}
{"doc_id": 3196, "doc": {"question": "who came first second and third on the grand national", "answer": ["Pleasant Company", "Bless The Wings", "Tiger Roll"]}, "target": "['Pleasant Company', 'Bless The Wings', 'Tiger Roll']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who came first second and third on the grand national?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Red Rum"]], "filtered_resps": ["Red Rum"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "133717eb26211a69da05afdc54c33fc43e2424d280070b8dc415cc11f4676192", "prompt_hash": "49ceeb930165a7be00180b1685994e8fb293dd63cc6417e58506a778ec551557", "target_hash": "d5557522248fcd60b4397c503e94dce6549a67c61f45abbe2816697b7a03166c", "exact_match": 0.0}
{"doc_id": 3197, "doc": {"question": "when did the book thief movie come out", "answer": ["November 27, 2013", "2013", "November 27, 2013"]}, "target": "['November 27, 2013', '2013', 'November\\xa027,\\xa02013']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the book thief movie come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2013"]], "filtered_resps": ["2013"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42c4feaa551c2c2055a20e07b44b06a7281bc0faa41d2e00cddc99031ebca5c7", "prompt_hash": "128fb9c3f9c044da535a9edb06454cd7dd59a5e92f125eddd31908dcbfd88199", "target_hash": "c9cf243d5f5a432003d3528b413fe203280dd91fa99f4cfa8340e5e795183ec3", "exact_match": 1.0}
{"doc_id": 3198, "doc": {"question": "who played the princess in kingsman secret service", "answer": ["Hanna Alström"]}, "target": "['Hanna Alström']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the princess in kingsman secret service?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tilda Swinton"]], "filtered_resps": ["Tilda Swinton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "864d2490cef968de10b56609af7df6bc9791d5d65fb8cbcbe0f1a6bdf7fc1ed2", "prompt_hash": "74ea3e0e778f05677e217a64053a49eb5a44735292e1bd80c44694c490da88a3", "target_hash": "71b0512f847f62f455b3753496bec86e5af8a37bdf80736dd74194ae92efce8f", "exact_match": 0.0}
{"doc_id": 3199, "doc": {"question": "who was the editor of the journal jugantor published in the time of swadeshi movement", "answer": ["Bhupendranath Dutt"]}, "target": "['Bhupendranath Dutt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the editor of the journal jugantor published in the time of swadeshi movement?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mohini Mukherjee"]], "filtered_resps": ["Mohini Mukherjee"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "31ecda2b0595873d04379e2911a147d40154411f2fea58810b184613998a9d07", "prompt_hash": "7b81dbb1fe0abac5c29dcc5b30d8784f2d498be0e27063dc4c0589fd22f7daa6", "target_hash": "84210de8c2ed8f7653775b37f2a87bdd4c07fd182541f12beb1210decc4f73a7", "exact_match": 0.0}
{"doc_id": 3200, "doc": {"question": "who sang the theme song for the man with the golden gun", "answer": ["Lulu"]}, "target": "['Lulu']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the theme song for the man with the golden gun?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Bowie"]], "filtered_resps": ["David Bowie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6765e0f3e5c284d51e42bf3b5f298c9a3180b0fabab641ccd13c7e2a4b8b3481", "prompt_hash": "bff9d810a1539a89bfd3f39cfcc45acfcbf115e85d675738f2d13d689ac7483b", "target_hash": "1e2672124d3eb321473115e84596133340324f779f611860ecf22552faf8ca8a", "exact_match": 0.0}
{"doc_id": 3201, "doc": {"question": "where does the movie road to perdition take place", "answer": ["Rock Island, Illinois", "Chicago"]}, "target": "['Rock Island, Illinois', 'Chicago']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the movie road to perdition take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bf2fd2fe509043088eb14f1c6af23f8aa66edfc497c4de458474968b3e09dad8", "prompt_hash": "f13ef9f37ab53c15a6ab72eb02d57320db23592530adca99ededb30d1e806ba0", "target_hash": "22e4df84f50884ac4444b148035f55c9a82f9f9eb6ab31f1a2e3d98b1b81384d", "exact_match": 0.0}
{"doc_id": 3202, "doc": {"question": "where are the hormones of the anterior pituitary created", "answer": ["The pars distalis"]}, "target": "['The pars distalis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are the hormones of the anterior pituitary created?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" adenohypophysis"]], "filtered_resps": ["adenohypophysis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8b6e66d14849451fc40d7a66b7ebcc85ed6e6b8eb85f15c76bff95332eb9e610", "prompt_hash": "78b1fe778f789493bae7a548c6a08fdd292c5ce5250f619433a3208cff51a9a9", "target_hash": "8dc852ed4bb0522a573b165c6a4522ded50171e166ae0776db0e7771f40c16e0", "exact_match": 0.0}
{"doc_id": 3203, "doc": {"question": "who voiced mewtwo in pokemon the first movie", "answer": ["Showtaro Morikubo", "Fujiko Takimoto", "Masachika Ichimura", "Philip Bartlett"]}, "target": "['Showtaro Morikubo', 'Fujiko Takimoto', 'Masachika Ichimura', 'Philip Bartlett']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who voiced mewtwo in pokemon the first movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bill Nye"]], "filtered_resps": ["Bill Nye"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cbc7c4f9b70822d9d4ab8dcbe007217c48d06c1c86d8a167834ba6f2897f792a", "prompt_hash": "5c5e41f0fdd0b951942cd80bd800a92bbfdc801e6c64dc00093c4d60f0d5516d", "target_hash": "182cf85e75315189a35c7cf2922cec9571cc023771b5b67d97b0975801c5de30", "exact_match": 0.0}
{"doc_id": 3204, "doc": {"question": "who proposed the first accurate model of dna", "answer": ["Watson", "Crick"]}, "target": "['Watson', 'Crick']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who proposed the first accurate model of dna?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Watson"]], "filtered_resps": ["James Watson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8973466ed3328ab4f875337d999ca1a065f65bcb6b4a917bfdbdd275d0338a16", "prompt_hash": "14bfe67a5bbf74634bba9611b6450f52fd604de8ac6e1a363a3cd9f8093ff89d", "target_hash": "e270b168c05cdc21f64ba8c9a40bc53d8eda61684931d3fd04add2fd195db42b", "exact_match": 0.0}
{"doc_id": 3205, "doc": {"question": "who said i will not go quietly into the night", "answer": ["Welsh poet Dylan Thomas", "Dylan Thomas"]}, "target": "['Welsh poet Dylan Thomas', 'Dylan Thomas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said i will not go quietly into the night?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Edvard Munch"]], "filtered_resps": ["Edvard Munch"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "961eff0ff39626b90760b5a4cdf6bb930ba45ebae6b6ab7e2f0b727aec201824", "prompt_hash": "f4e8279f1317dd00b13bd91e1b9858b400a4465696dacaad6f38a0ae752172ab", "target_hash": "75ba9e95a5956b9105e50bbf568d247d0ee3228a90cb9d8be36ef9916afc2739", "exact_match": 0.0}
{"doc_id": 3206, "doc": {"question": "what are the joyful mysteries of the holy rosary", "answer": ["The Nativity", "The Annunciation", "The Visitation"]}, "target": "['The Nativity', 'The Annunciation', 'The Visitation']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the joyful mysteries of the holy rosary?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Annunciation"]], "filtered_resps": ["Annunciation"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "977387b698818c4c6f55e8506b978f5c4c0ba12d8e685fc6703f3fa4af88702a", "prompt_hash": "95f3197d41d4bdcf6884146e4240ff657dcc8fe71729bfcc21721e82e12c58a5", "target_hash": "c8cdc5059f575469b9d84b58716547e2e6017f6c40effa9befd0c8925e84a28c", "exact_match": 1.0}
{"doc_id": 3207, "doc": {"question": "the very last episode of that's so raven", "answer": ["Where There's Smoke"]}, "target": "[\"Where There's Smoke\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the very last episode of that's so raven?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Raven graduates"]], "filtered_resps": ["Raven graduates"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbf7e96d54464b8d1c013b0c123b70be4b8e9ddde98244d63913991ca4f98eda", "prompt_hash": "b2b9b294742e446f49682549774f392e2240599c2345fa429a1199e18d948d53", "target_hash": "280b002ac6c5600003697e5eaf9188b53370013c26927a4e57bda450afb4a90c", "exact_match": 0.0}
{"doc_id": 3208, "doc": {"question": "what is a coherent set of values and beliefs about public policy called", "answer": ["a political ideology", "political ideology", "ideology"]}, "target": "['a political ideology', 'political ideology', 'ideology']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is a coherent set of values and beliefs about public policy called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ideology"]], "filtered_resps": ["ideology"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "230a61c8c8f0a903b74fbe22ef9ad9770385b26f9cd003ec954a24d250607c87", "prompt_hash": "18ca1522f2b5d4058c44a987be8dae9ce809bd4b2517c7354ccd0074aba36feb", "target_hash": "9ef1dbc181d3077168db367298e1e85c4ff90bd131df7fe0b3f77b7e033de356", "exact_match": 1.0}
{"doc_id": 3209, "doc": {"question": "how many stores does lowes have in canada", "answer": ["62"]}, "target": "['62']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many stores does lowes have in canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 17"]], "filtered_resps": ["17"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c2cb2a967a0aef111b06856d6101916dff90e25b0321a74a6db1af55502d4e21", "prompt_hash": "af12c51a2b05e92010819d7f993bab5c3ef8fa327fee1b500ad485dc06400bb8", "target_hash": "8105ed5bd6d41284a79b950e52cf384c24e7f1bfe97c2894a3b201addc8b2911", "exact_match": 0.0}
{"doc_id": 3210, "doc": {"question": "who does brant daugherty play in pretty little liars", "answer": ["Noel Kahn"]}, "target": "['Noel Kahn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does brant daugherty play in pretty little liars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Toby"]], "filtered_resps": ["Toby"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8d95df26eb60bcacefe1644393289fed0d18505ccbf5b320105a2068411c31f0", "prompt_hash": "eb9e4a12206fd438d6afee5154dca1f6acf8647a3916f1efcec6e0b54339653c", "target_hash": "5ca4ccd543c885114c05a29a9aff4ccbfe3800537e5801038a8a7d4b019fa032", "exact_match": 0.0}
{"doc_id": 3211, "doc": {"question": "when did the new day win the tag team titles", "answer": ["2016"]}, "target": "['2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the new day win the tag team titles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015"]], "filtered_resps": ["2015"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "424d5d819e4e8d5648984997e18fd68541ef73ed002aabb5aa21e22d744d8621", "prompt_hash": "828c126a9bce2d884554bdb8bf888bc1552b2e96f4440dd530e9c4bfc7506eb1", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "exact_match": 0.0}
{"doc_id": 3212, "doc": {"question": "who is young george bailey in it's a wonderful life", "answer": ["Robert James Anderson"]}, "target": "['Robert James Anderson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is young george bailey in it's a wonderful life?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Stewart"]], "filtered_resps": ["James Stewart"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "62dba8535653d8a96f4731d7e6d21c03187811438d8cc3aae6d6ed44e7382fb4", "prompt_hash": "8aba6ae1cf90dbf3ed2a1dbd7205d0d8eeb7cdc5c16d1a8461e55a99a7cfbd2a", "target_hash": "c27de76544eabd5e4c77a0da87219d4195406320e4b85d5b9b7da65af178acf3", "exact_match": 0.0}
{"doc_id": 3213, "doc": {"question": "who missed the plane the day the music died", "answer": ["Tommy Allsup", "Waylon Jennings"]}, "target": "['Tommy Allsup', 'Waylon Jennings']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who missed the plane the day the music died?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Buddy Holly"]], "filtered_resps": ["Buddy Holly"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b2df24aadfaf0771b15c15b423d7b09776afabc90960f56c2a1a5fe5542dff2f", "prompt_hash": "93a38e08f4c5d8ac19fea84fcc290ab0ca72fcc5c8afe9589cdd398d2422c44e", "target_hash": "fe7ed7e5ad2094dff1106a081ec6f1cd113c3dbb086ba49ddb5373134aca2518", "exact_match": 0.0}
{"doc_id": 3214, "doc": {"question": "when will the flash season 4 be released", "answer": ["October 10, 2017"]}, "target": "['October 10, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will the flash season 4 be released?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 10"]], "filtered_resps": ["October 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "edeb759731564fb758c697e7b26a838ed483df0b7dfa6bbf6a2d2d82d6269405", "prompt_hash": "14738eb59df9d36c980eda467b2f90e2e060dfdd262dc028b03b0c68aec933a6", "target_hash": "320115ec58cfa45d5c00b0c004591571c1730f2a491e4777b63cfa83f3715269", "exact_match": 0.0}
{"doc_id": 3215, "doc": {"question": "new movie of ajay devgan and sonakshi sinha", "answer": ["Action Jackson"]}, "target": "['Action Jackson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: new movie of ajay devgan and sonakshi sinha?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Duet"]], "filtered_resps": ["Duet"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "70ff019e95458d200fc35e36219e71b2144039982a8fa5105170981310dbd3dd", "prompt_hash": "444815ffb1668cb9664b879bbb7f1aebe4012ea4d49725f8c1dcea548264333a", "target_hash": "586b19f21fd71d16d86764207122238e12f393c13103ecfe8707ddd23abf4315", "exact_match": 0.0}
{"doc_id": 3216, "doc": {"question": "when did ireland gain its independence from england", "answer": ["In 1922", "1922"]}, "target": "['In 1922', '1922']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did ireland gain its independence from england?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1921"]], "filtered_resps": ["1921"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "63f56a4c37523d660d27c81610a08104f64b9b6ffb2952f02036c7b234c61d68", "prompt_hash": "c56aa12c47c6d88541bf1c86e4c07bebd3cbb2dc940d452e7099d1488bfb852e", "target_hash": "0e613a4e9bad485d08794350d2d9c6797d5a2efc5e545105a2ccbaa69732fe1e", "exact_match": 0.0}
{"doc_id": 3217, "doc": {"question": "who played the girl in silence of the lambs", "answer": ["Masha Skorobogatov", "Jodie Foster"]}, "target": "['Masha Skorobogatov', 'Jodie Foster']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the girl in silence of the lambs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jodie Foster"]], "filtered_resps": ["Jodie Foster"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2485eea4bf3c44812ca86c055d58e4180976ba11555040f1d1c15cee1ebd42d7", "prompt_hash": "22e0bd7f23232c71fe264bf06671c2f2c3c9a65099c4fae20856000173c1287c", "target_hash": "0b6678bdf53ca4fcebdf09035e0b305c5ace95311f8724f217ebf3861704fba6", "exact_match": 1.0}
{"doc_id": 3218, "doc": {"question": "tallest building in the world of all time", "answer": ["Burj Khalifa"]}, "target": "['Burj Khalifa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: tallest building in the world of all time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Burj Khalifa"]], "filtered_resps": ["Burj Khalifa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e0a128350fa53aff4a52ace9f35ae738a829378be789eff60433d602c5eaa712", "prompt_hash": "10e7c49670aec27864870272052ee2c5c675a7a84d31f8aba4973a8ee357485d", "target_hash": "44d7cba5f31afd278e0bb1a160b352b17869194d84f4a6ea68e4135a5a6b0702", "exact_match": 1.0}
{"doc_id": 3219, "doc": {"question": "who played mrs. trumbull on i love lucy", "answer": ["Mary Elizabeth Patterson"]}, "target": "['Mary Elizabeth Patterson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played mrs. trumbull on i love lucy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vivian Vance"]], "filtered_resps": ["Vivian Vance"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b631696c59fcb5ae4d44a9383a4fe9ba010e6567e7d593d965ecd4b784aa4cd5", "prompt_hash": "ceb81bfea8ef117e2a3194381ae8e053a03a25e79eb4fb092eb4f0c5e3725616", "target_hash": "c188a0ff1b815fdfabdcc24cb78df60ac66e441b4ab0741d774d48fc5f1b91fb", "exact_match": 0.0}
{"doc_id": 3220, "doc": {"question": "who was the successful commanding general of the northern forces in the civil war", "answer": ["George B. McClellan"]}, "target": "['George B. McClellan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the successful commanding general of the northern forces in the civil war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ulysses S"]], "filtered_resps": ["Ulysses S"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9861d12d18e90ee4eb4b2b6f43e65628bf560dac381f43038b5294b960c52fde", "prompt_hash": "24c375748bc52c4d5bfc8f1776fcb3333b216a6f86a01e8ac1d77d673b6c3049", "target_hash": "ec31ba290e1eaf9dee4896426480c558490fa30b14628b8cba833cfd23af4e6c", "exact_match": 0.0}
{"doc_id": 3221, "doc": {"question": "where does the red wolf live in the world", "answer": ["the southeastern United States", "southeastern United States"]}, "target": "['the southeastern United States', 'southeastern United States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the red wolf live in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" United States"]], "filtered_resps": ["United States"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "25a69500847030482c12375b63c078676ac79c7e58f5a7f44ed0496444e7ce24", "prompt_hash": "5d2cf18b8733bedb82bb729f0c942f4caff42c7ae06f96059a55178f17422500", "target_hash": "6234d829ae570d464a019a25e5d26f6db27ff84f12c01e72bfcdbce3b58aded1", "exact_match": 0.0}
{"doc_id": 3222, "doc": {"question": "what does v sign in front of mouth mean", "answer": ["signify cunnilingus", "cunnilingus"]}, "target": "['signify cunnilingus', 'cunnilingus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does v sign in front of mouth mean?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" victory"]], "filtered_resps": ["victory"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "65d0e0d12536bef41b8914eefd9b1a99adb82fc1d6624410b47b9f683dd95d8c", "prompt_hash": "3b65b58a849469727c0c308e9b299136f0cce1c2f0f041f44d91d6fded5df6b9", "target_hash": "852890a2e86a23e975f2ac3c2d9f9502492127701bf4fa300cb0a51577e8e1b1", "exact_match": 0.0}
{"doc_id": 3223, "doc": {"question": "who dies at the end of den of thieves", "answer": ["Merrimen", "Levi", "Bosco"]}, "target": "['Merrimen', 'Levi', 'Bosco']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who dies at the end of den of thieves?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Frank Castle"]], "filtered_resps": ["Frank Castle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "545f6acff0ef735b4098a64bb6b98ae0a6430054a0dec11b1124803404faddb9", "prompt_hash": "cde5c3f0c9c57e5a1a9f62668c47a682e2269b047527bb15c8c6a3e171a2e751", "target_hash": "437a8a24f73e6f6a7167b0272086bb6284aaa92fa76e2302663c69a5a77a651c", "exact_match": 0.0}
{"doc_id": 3224, "doc": {"question": "the xix of the social security act of 1965 established", "answer": ["Medicaid"]}, "target": "['Medicaid']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the xix of the social security act of 1965 established?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Medicare"]], "filtered_resps": ["Medicare"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7c800e5bc2bad427ec27175445832742ac58c1effe978d22386be4ff37ea677e", "prompt_hash": "8c118cc62e1ba5171dd352baa85131372661ee226879d091785fd9434b77ceec", "target_hash": "f64704780218439a3bc0f3ae428cc46008d466ca33dff7aebb901585f78d7aa3", "exact_match": 0.0}
{"doc_id": 3225, "doc": {"question": "the winner of the best actor award in the 64th national film festival 2017", "answer": ["Akshay Kumar", "Manoj Joshi"]}, "target": "['Akshay Kumar', 'Manoj Joshi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the winner of the best actor award in the 64th national film festival 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mohanlal"]], "filtered_resps": ["Mohanlal"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "29d317444ae7ea1de4ec1b44076a830ccb980a3f30ddaac60b891e1eae6969d3", "prompt_hash": "6f6b0d347b9174a04a5d9a626b2bb14b1bd3c0e421b0c13e171e5302a8722a02", "target_hash": "57908b67621991bac459d1952640754df0a2cd4deedc876b51de6ad35b3bcd38", "exact_match": 0.0}
{"doc_id": 3226, "doc": {"question": "how many times have real madrid won the european cup.in paris", "answer": ["twelve"]}, "target": "['twelve']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many times have real madrid won the european cup.in paris?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 12"]], "filtered_resps": ["12"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6b636bddcf94271716c80f857030b972712aa32d54e2cbaec6ecdf21b99c0308", "prompt_hash": "0e1da01c1148ab89888bee62196904d0ca092409db4875fe5fa9aaab6128d466", "target_hash": "a82964817cdb7abc68c90b78908d928165cd41f032c9f42df82d9c775e08d828", "exact_match": 0.0}
{"doc_id": 3227, "doc": {"question": "what is the limit of resolution of the human eye in micrometers", "answer": ["~55-75 micrometers", "~ 0.116 mm"]}, "target": "['~55-75 micrometers', '~ 0.116\\xa0mm']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the limit of resolution of the human eye in micrometers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 200 µm"]], "filtered_resps": ["200 µm"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "65672e3ce5a379b0e77688a98729291f7cedb92535085cba49f296672a18caf6", "prompt_hash": "b155f7800b89c7c8567ea41fffa6b06ba274495fcf1b8df8e5c482f86cdb0216", "target_hash": "87655c44d6e89961c5fbc017bb47b017498437b168c850db8b40ef1416303d3d", "exact_match": 0.0}
{"doc_id": 3228, "doc": {"question": "who laid the foundation of school of possibilism", "answer": ["Strabo", "Marshall Sahlins"]}, "target": "['Strabo', 'Marshall Sahlins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who laid the foundation of school of possibilism?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alfred Espinas"]], "filtered_resps": ["Alfred Espinas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "972f8b0f191938dfd787f4509c0bb0d00cb47318dd7537076bd1eea464b45d8e", "prompt_hash": "f20274be0bca28276c350eba94627e6db1d955a514cc54ea26ec5095fbb2ecf6", "target_hash": "a7c248f18f09815d219f6cb429e3d07e042d8d0bbea3fb12d869b88eb4753094", "exact_match": 0.0}
{"doc_id": 3229, "doc": {"question": "what was the purpose of the mall of america", "answer": ["shopping"]}, "target": "['shopping']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the purpose of the mall of america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" entertainment"]], "filtered_resps": ["entertainment"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a536ff5e6a623068efc896016002b5c6d86d79c50fce60cc9e887b0a42f20cbd", "prompt_hash": "c2fb8d16509680d8083eb0d1e7b504196a3c5fb15195b233f942045b053386cb", "target_hash": "feadff5f6e2d07e3cfcaeaf8a99a49fb8ed0a693692bb8255e8d4c8932c25531", "exact_match": 0.0}
{"doc_id": 3230, "doc": {"question": "when did television come out in the us", "answer": ["928,", "uly 2, 1928,"]}, "target": "['928,', 'uly 2, 1928,']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did television come out in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1939"]], "filtered_resps": ["1939"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0a3216af377cf2bac69ffc1f1ef7c42ca8f338c0d02be062b3ef11d4e8a8a190", "prompt_hash": "4b60a85273b13269eb7fa9f774517a56fe54b22d3016e53c3e8c590b9994d98c", "target_hash": "b1d755aef404f84af4e7ae199ecc5d70a9e891908dea59c602dcbfba9bcbfa78", "exact_match": 0.0}
{"doc_id": 3231, "doc": {"question": "what is small business tax rate in canada", "answer": ["10.5%"]}, "target": "['10.5%']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is small business tax rate in canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11%"]], "filtered_resps": ["11%"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "59cdbbf6d4f40bb9648fd73b777ceffc6156b48afd82f15729695e1fe037408a", "prompt_hash": "cb748cd3057fb2222d9d9419b1a1f324a1ab88fd169a36e9b90d0e111cad619d", "target_hash": "3def6d54da8339bae0780db50b4b5a4931bba2d7a73a1514ca582e9d12f86dba", "exact_match": 0.0}
{"doc_id": 3232, "doc": {"question": "who is the present president of assam sahitya sabha", "answer": ["Dhrubajyoti Bora"]}, "target": "['Dhrubajyoti Bora']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the present president of assam sahitya sabha?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bijoy Lochan Brahma"]], "filtered_resps": ["Bijoy Lochan Brahma"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cfc47dc8ea90c3fed0b3c670ff5b1a12b749583b40ba742e5990908b130c2a8e", "prompt_hash": "397757b8debb198a9001c15d00585ae98c4f87922bde03f87b31b2c35249b91f", "target_hash": "33fbe56394f6a1bab357e879cb5dd0b0d5a5d7462ed059644a9ecd107f3988e7", "exact_match": 0.0}
{"doc_id": 3233, "doc": {"question": "when does closure of the ductus arteriosus occur", "answer": ["at birth", "birth"]}, "target": "['at birth', 'birth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does closure of the ductus arteriosus occur?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fetal life"]], "filtered_resps": ["fetal life"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fbeddb75a9c13a906ba9337ad2d6b25ae5b3944fb8dfe4023636145eb3b1f3ae", "prompt_hash": "2b389479ca6b30eeeccda0062626c84d8498ecfb72a79dfbed7d34c316729810", "target_hash": "8d4d2c79598a09197c871927357a2cf5df6219be05d96c27442d9307503530ad", "exact_match": 0.0}
{"doc_id": 3234, "doc": {"question": "love is not love that alters when it alteration finds meaning", "answer": ["Sonnet 116"]}, "target": "['Sonnet 116']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: love is not love that alters when it alteration finds meaning?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shakespeare"]], "filtered_resps": ["Shakespeare"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4302cf154f4b47592a595cdcfb44c5b7115574a488824069c40991ec1ba8a3cb", "prompt_hash": "3ce8c8c5231d1235869d4871ecf67116b6e50d2ff59fdb56be742ca4f8a0fce0", "target_hash": "0d0f3cf57509453abfc481178149e7c9c60d3071ae5052b71717d8de0460bd0a", "exact_match": 0.0}
{"doc_id": 3235, "doc": {"question": "most runs ever scored in a mlb inning", "answer": ["18"]}, "target": "['18']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: most runs ever scored in a mlb inning?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 19"]], "filtered_resps": ["19"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "240be0709e768dbd06bd30b3811158a4c1735cf82b6ce96c8ec47360881a7853", "prompt_hash": "f9ecb011de38d04aa2fc04c98365349fb5c44dcb88f50563fd9a4bd5bef1a776", "target_hash": "fe29453a161fbb6965ccdbdca03593905c1e66cc520a49919b2d999ddf5c0c18", "exact_match": 0.0}
{"doc_id": 3236, "doc": {"question": "where is south pole located on a map", "answer": ["on the continent of Antarctica"]}, "target": "['on the continent of Antarctica']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is south pole located on a map?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Antarctica"]], "filtered_resps": ["Antarctica"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cc28b7b5e2bab0db87d617045e77b372d0cd685990c269fddb12c6b226cff27e", "prompt_hash": "a09506839b714d5eaf1a6bd5c6318e4e69dc8c76a1e158c1760c2a789a69b407", "target_hash": "0576bff14243b5a0035841b4a25b8d3c6d2975a14015271294135c4bf3a676e5", "exact_match": 0.0}
{"doc_id": 3237, "doc": {"question": "who is considered to be the greatest english composer of the baroque period", "answer": ["Henry Purcell"]}, "target": "['Henry Purcell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is considered to be the greatest english composer of the baroque period?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Henry Purcell"]], "filtered_resps": ["Henry Purcell"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c9f86df30aa913f19de3d842ac97f2841d864c1d7d3d414d78c38d52fb7bd098", "prompt_hash": "d5b6dad7102abcf16e927397d4ba41aeb34edd08b5c155fc8d7c9b7e52bbd86c", "target_hash": "a68764d5267d2e922313210f9fdb4a72b7d9d9c83c27bb291e0876557f7988a7", "exact_match": 1.0}
{"doc_id": 3238, "doc": {"question": "what is the setting of a dolls house", "answer": ["a Norwegian town circa 1879", "a Norwegian town"]}, "target": "['a Norwegian town circa 1879', 'a Norwegian town']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the setting of a dolls house?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Norway"]], "filtered_resps": ["Norway"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e0289d195a466aecee175dc378246f2baa857751b0bb4853a2ec1b3d3a65495", "prompt_hash": "33c18400d566977d8b48baafc189af8d351f9bd2cdaf8834df20d3cf28080570", "target_hash": "d55bc1a51f3e029e02a35e6a6545952fa6d8305e3da1b7c7e2cf95ab7a6e19bd", "exact_match": 0.0}
{"doc_id": 3239, "doc": {"question": "when did the tradition of self government began in america", "answer": ["1783"]}, "target": "['1783']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the tradition of self government began in america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1776"]], "filtered_resps": ["1776"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f6668a1ca4ba46a79a4a070eef3b5ee0ae6798bc6bf40d8a17ddb632c788dd4", "prompt_hash": "6e31c06c850af883d39c8b0a4f3761f5b5e8b70180be1fc3c641e7a1ca29188c", "target_hash": "2ced72da1fe35c431da666aafd6bf95a2b65e385c93a5be8b040d26ce9bcccb3", "exact_match": 0.0}
{"doc_id": 3240, "doc": {"question": "what is the meaning of shalom in english", "answer": ["peace"]}, "target": "['peace']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of shalom in english?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" peace"]], "filtered_resps": ["peace"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0fb150e2680ef57da5011422d6e552d7bd6d5e8fa8538d803451d1f8a8cd5ae2", "prompt_hash": "863e254b35ad494b7ab40214668c65a10aef11d1ed1b5355600a908cf55c354d", "target_hash": "6aa08c04a554905f47504b6cbd74a3aee58c9d9b4a04016bdb401dd047ec445c", "exact_match": 1.0}
{"doc_id": 3241, "doc": {"question": "where does the spinal cord become cauda equina", "answer": ["L1 or L2", "the conus medullaris"]}, "target": "['L1 or L2', 'the conus medullaris']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the spinal cord become cauda equina?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lumbar region"]], "filtered_resps": ["lumbar region"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "777726340186568c5ba86aa5ee8e6e986275fa8e1b2e06635fdc52fb52d569fa", "prompt_hash": "a7b3cd9dea10a1ed03d54ed11691f6145163a54e4a0e17f875e782ae702ecba5", "target_hash": "1c03f5a1ac2b84bab859137cb6095ab6846718f81238a43824fcae027d986267", "exact_match": 0.0}
{"doc_id": 3242, "doc": {"question": "which country has the smallest population in europe", "answer": ["Vatican City"]}, "target": "['Vatican City']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which country has the smallest population in europe?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Malta"]], "filtered_resps": ["Malta"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "97a5dc95d1653877536a16f59a4c56902663d1cb9bcbc759c6292c4f803a9087", "prompt_hash": "1f3a574251a0041cb947edada45dfa346e23334afc39657bcf3808b3500a671e", "target_hash": "b256558ecde05b9f3a9d5a895d0069b606400af6afdd2118fb22cbc144ab9594", "exact_match": 0.0}
{"doc_id": 3243, "doc": {"question": "who has the biggest airport in the us", "answer": ["Atlanta"]}, "target": "['Atlanta']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the biggest airport in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hartsfield-Jackson Atlanta International Airport"]], "filtered_resps": ["Hartsfield-Jackson Atlanta International Airport"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03b3ba8974cd2f89fec385eddc121cd03ff398be6f6edefdd7bde541042c6310", "prompt_hash": "111ac2194d20e4dbeb782eb3700bc0962246ed33e1f885e60c96b4f800ce6867", "target_hash": "ffc829f4d5843302606e07d9090b81ef73f7353b74a6a553b61e87abef8b58eb", "exact_match": 0.0}
{"doc_id": 3244, "doc": {"question": "who does the voice of mrs. wolowitz on the big bang theory", "answer": ["Carol Ann Susi"]}, "target": "['Carol Ann Susi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of mrs. wolowitz on the big bang theory?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Carol Ann Susi"]], "filtered_resps": ["Carol Ann Susi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2ada83f6f769459e53003df9c832624293a632c97a9f23304d095136370ce9d5", "prompt_hash": "a2fddf8a4ebb841e80c0f8978cec42514c170fb3a35586599c5a0cb8d90d513a", "target_hash": "d9b981fa44b817cb8638978da6a2fba73f823cf496985443da0195ba5be11e04", "exact_match": 1.0}
{"doc_id": 3245, "doc": {"question": "who sings war don't let me down", "answer": ["American production duo The Chainsmokers", "vocals of American singer Daya"]}, "target": "['American production duo The Chainsmokers', 'vocals of American singer Daya']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings war don't let me down?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Beatles"]], "filtered_resps": ["The Beatles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "336897b2839aac0469de049c2a8c3705c041d37c1b28e666d5635caa8ae33c2d", "prompt_hash": "73ea496fffbc5d9652016a82010a571ffe77279f29f7f901c70fc54c9c6f94bd", "target_hash": "c830cd160e96922f163731031714b985a8159d7b4eac10ae1056b2fb275bac62", "exact_match": 0.0}
{"doc_id": 3246, "doc": {"question": "who wrote the song after you've gone", "answer": ["Turner Layton", "Henry Creamer", "lyrics by Henry Creamer", "composed by Turner Layton"]}, "target": "['Turner Layton', 'Henry Creamer', 'lyrics by Henry Creamer', 'composed by Turner Layton']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song after you've gone?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul McCartney"]], "filtered_resps": ["Paul McCartney"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b8a502a7e2abb27e72673ae0d3439b05ce56cbd597297921ed42a013ba38d9db", "prompt_hash": "45216b9da3e745b3ed312b4b8527cfda81267f0fb7aab2d1f849e2a1a960fe63", "target_hash": "8081d81fbec30f4aa427671808f63ac12d26e8e408faad7cf23a19748d6573ef", "exact_match": 0.0}
{"doc_id": 3247, "doc": {"question": "where does the saying get out of dodge come from", "answer": ["the drama series Gunsmoke"]}, "target": "['the drama series Gunsmoke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the saying get out of dodge come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dodge City"]], "filtered_resps": ["Dodge City"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6968d0ae163832d0b9bd95e15faa767a13491cc8d28227b1ead6ee7c0349d387", "prompt_hash": "7336df5b166076e87ac38597cd8d3536706b986c886ad94ed2746c64f4a41663", "target_hash": "8b2a6f8a7c72dd384c17fe1e2756bde5450be785aa142bf4128bbc02973724ce", "exact_match": 0.0}
{"doc_id": 3248, "doc": {"question": "when will notre dame played michigan state again", "answer": ["2026"]}, "target": "['2026']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will notre dame played michigan state again?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bee622b7dd988006465ae0af9d1b024d0acdd8f86074af683daf512c152a257b", "prompt_hash": "6b7a67921f9874fdfb6c4e9fe42409b47d9d3e32fdd701fe00c34325061437d1", "target_hash": "fbb300e39994ccd201ab854153a52dfcfe3b45045b96d5679b2da591cf9c2427", "exact_match": 0.0}
{"doc_id": 3249, "doc": {"question": "who sang first line of we are the world", "answer": ["Lionel Richie", "Kenny Rogers", "James Ingram", "Billy Joel", "Tina Turner", "Stevie Wonder", "Paul Simon"]}, "target": "['Lionel Richie', 'Kenny Rogers', 'James Ingram', 'Billy Joel', 'Tina Turner', 'Stevie Wonder', 'Paul Simon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang first line of we are the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael Jackson"]], "filtered_resps": ["Michael Jackson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f031495a101ce2bf07b05408dfd611c8577a3a3abfe5ea5a5a671b4e7f005341", "prompt_hash": "79f1c26c7a206597837010a102e74a47ea0b0cb13ff5f207b4f80cda71311f02", "target_hash": "6272877b6ef2698ef77d5cc48e379566cf0de6627b522804a267b96f69fa80b7", "exact_match": 0.0}
{"doc_id": 3250, "doc": {"question": "who has the most gold gloves at shortstop", "answer": ["Ozzie Smith"]}, "target": "['Ozzie Smith']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most gold gloves at shortstop?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ozzie Guillén"]], "filtered_resps": ["Ozzie Guillén"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7e3c6208a7ab10f37c3880647ed06381404657ef501f35ab3ea086c50174799b", "prompt_hash": "6ca67e7650869a62488cf56e92653ab4d0e7b2a01ba988b90eda2ff7bdce79e4", "target_hash": "d335dd1ed156a3cc510b8f598d36d4f6d0a2ae043cc75472875f209f585fb9ee", "exact_match": 0.0}
{"doc_id": 3251, "doc": {"question": "who wrote i'm in the mood for dancing", "answer": ["Ben Findon", "Mike Myers", "Bob Puzey"]}, "target": "['Ben Findon', 'Mike Myers', 'Bob Puzey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote i'm in the mood for dancing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cy Coleman"]], "filtered_resps": ["Cy Coleman"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7e384eb5907aa1d6eda01c510bdd3c6cc290b497d5cf9c45085c22d3a5231b10", "prompt_hash": "cab01202eb593ce2dfd18a26ed87337ed2084e4777bf66926678a3355ada53aa", "target_hash": "ba39632fc5a88c9dd5718e2ce9d32e356b090487442f8dcbac0aaf289842b147", "exact_match": 0.0}
{"doc_id": 3252, "doc": {"question": "who plays the robot maid in richie rich", "answer": ["Brooke Wexler"]}, "target": "['Brooke Wexler']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the robot maid in richie rich?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rosie"]], "filtered_resps": ["Rosie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbba95c2542c738dd2c0dffed079d13b78b1134aead72bdac3606bd3cf4ecb49", "prompt_hash": "0fb05dddefb9ccc5746ce5e0cc43ad914b351c506f4831648ffdae56b8e754fd", "target_hash": "259edcca4e5054d284b4bd6e7c50974600ce30d71d61d803736a18f946bca970", "exact_match": 0.0}
{"doc_id": 3253, "doc": {"question": "how many hospitals are there in the united states", "answer": ["5,534 registered hospitals", "5,534"]}, "target": "['5,534 registered hospitals', '5,534']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many hospitals are there in the united states?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 5"]], "filtered_resps": ["5"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dedb05260e22848adaa48207a6c7968601e2f72f567d60f5bec67e4129d85e0e", "prompt_hash": "9f08f4dfb7db8569cfcc683ce8660ca5ab2b6b4eb59710ad359cbedfb72c28d8", "target_hash": "aae030133cdfd44455bdebc08613fd98c7e8087bee70244696e276eb0bbdb630", "exact_match": 0.0}
{"doc_id": 3254, "doc": {"question": "university of michigan school of public health ranking", "answer": ["#4"]}, "target": "['#4']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: university of michigan school of public health ranking?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "be194532400c19cf1abaa9de8e79ad910b97a9c9b30d55de5f5b8686cc1ec27b", "prompt_hash": "0be32dc05517eb92a973ee3cabd6997936c0cde0902fefeb70cd59e1231d97e3", "target_hash": "99723b92aa5839ad31ae027e028893e007b582ee144019e22f9ea25cdb32868d", "exact_match": 0.0}
{"doc_id": 3255, "doc": {"question": "african countries that begin with the letter a", "answer": ["Algeria‎", "Algeria", "Angola‎", "Angola"]}, "target": "['Algeria\\u200e', 'Algeria', 'Angola\\u200e', 'Angola']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: african countries that begin with the letter a?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Algeria"]], "filtered_resps": ["Algeria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "856199c7876625a2dda58334fc0812732a37b99f6124155535de3b9ed01d2e4a", "prompt_hash": "b69c30aed5d2e883df964e82fdc19b2c53697a1eae58260a06f494f22543e408", "target_hash": "3f3a6aba4e177c9efe6882ce31ad3c587821d55223818f1c1f8ff51ac4b4376c", "exact_match": 1.0}
{"doc_id": 3256, "doc": {"question": "when was the immigration act passed in canada", "answer": ["1923"]}, "target": "['1923']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the immigration act passed in canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1976"]], "filtered_resps": ["1976"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bad8a1ad41192b679aef610742ef2178b1cdff918e2dafc3c236928dcc011241", "prompt_hash": "f8c248402bbbca07fe4e7762e22612e32f49cdac58babf8d81ac488a9f61dda5", "target_hash": "137ba74f3b64278e1d3c962cd05a4a2afd554a8be58641ee162988ff5de77a24", "exact_match": 0.0}
{"doc_id": 3257, "doc": {"question": "where did the indian myna bird come from", "answer": ["native to Asia", "Asia"]}, "target": "['native to Asia', 'Asia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the indian myna bird come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" India"]], "filtered_resps": ["India"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "033f469ee6285f1412fc2c2f12fbdfdee4d80e61d7db58bfd081ad19cf3d30a0", "prompt_hash": "1d71d8dc08ad88484c61b2075393ffbf0ff0ba20f41f12a4e73949800a44cf41", "target_hash": "df5af1f1a55bc0a00f4dc97769b75aabfe10d270e3ff1019e8163e485866ef29", "exact_match": 0.0}
{"doc_id": 3258, "doc": {"question": "who is the guy on keeping up with the kardashians", "answer": ["Jonathan Cheban"]}, "target": "['Jonathan Cheban']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the guy on keeping up with the kardashians?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kris Humphries"]], "filtered_resps": ["Kris Humphries"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0f680caf2c5a9ceafc9e314a5b5f3951b8eaa1f741072cecfd97c4f14b1eabfd", "prompt_hash": "f06e6bc9f1d1b100b7913038616c333beab91b2fec3b3ca33b527ecf2c55af52", "target_hash": "ac4333e0175af813d5aa538ff7c238c646cbcf5219676dc25fd66193bf2f469b", "exact_match": 0.0}
{"doc_id": 3259, "doc": {"question": "when was catch me if you can made", "answer": ["2002"]}, "target": "['2002']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was catch me if you can made?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2002"]], "filtered_resps": ["2002"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "82286748a3611f2591d999d1e8644cfb9a2f2b8c8886255fca0cb0ddd1d4ed70", "prompt_hash": "153563a16e065920dfc11641728f7ad2d804167d249edb9363f91e1d197579a1", "target_hash": "811c99e4b123b056cb7feb471fcea1b52924b0dd25f2af7628a976ec8411a59d", "exact_match": 1.0}
{"doc_id": 3260, "doc": {"question": "who used the word physiology for the first time", "answer": ["Jean Fernel"]}, "target": "['Jean Fernel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who used the word physiology for the first time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hippocrates"]], "filtered_resps": ["Hippocrates"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e7eecac3d15788fc29d1d58be3fa15993b85aa39532d0dbeefef7570d68b95dc", "prompt_hash": "430e0569430c6054a3e3fbe017408bfc2e897de2c14c61ad816cd4353517a9ae", "target_hash": "49695425873893656611f26b437558dc6c29a48ea3aafd66428430d0a7816fc0", "exact_match": 0.0}
{"doc_id": 3261, "doc": {"question": "how many dominoes do you need for mexican train", "answer": ["A double-twelve set"]}, "target": "['A double-twelve set']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many dominoes do you need for mexican train?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 288"]], "filtered_resps": ["288"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "448d4ae90f3c03dbc9cec45f70095d2d5bf01424f6e1a1d8f24e50da9a5f6c57", "prompt_hash": "eae8c3d8257b05988a6a129f7a0afbf1a35323e0a22519c99775fd9339b96c41", "target_hash": "ce503400b97b09bcb918ad87cb70be243aad7bc31cc955ab4140cb31484e76d1", "exact_match": 0.0}
{"doc_id": 3262, "doc": {"question": "what is the filename extension used for all java source files", "answer": [".java"]}, "target": "['.java']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the filename extension used for all java source files?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" "]], "filtered_resps": [""], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44093666435a12d580f13ddcae904e9bbe58f45d276c6da6609cafcf7de648a3", "prompt_hash": "29287d30fc46b3e00a3afbc10572519acef88c88eb6e7d128a836b1fa6c95411", "target_hash": "3728f3f08f3fb952133aaae4e5d814752bedc07d78fcfe6ca8b7ee07dabfb59b", "exact_match": 0.0}
{"doc_id": 3263, "doc": {"question": "when was the taj mahal built and completed", "answer": ["1632–53"]}, "target": "['1632–53']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the taj mahal built and completed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1632"]], "filtered_resps": ["1632"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ecf957f64b8fa80300fef7a994463cc90f931956790f88c810ec554de098beeb", "prompt_hash": "00f975c66b7a78c95d920e8aeb493a3460a137d5a4606e79830d8620fd66f669", "target_hash": "e0cf492f66537b0ea9fd9ad1f365b321a43f12dc185a409d954fc28cafc40cbf", "exact_match": 0.0}
{"doc_id": 3264, "doc": {"question": "who is the premier of northern cape 2018", "answer": ["Sylvia Lucas"]}, "target": "['Sylvia Lucas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the premier of northern cape 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stanley Mulaudzi"]], "filtered_resps": ["Stanley Mulaudzi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "69d2fb853f4becea51b2f40f5ba1090b4abaa0a16df75d0cc3ed1281a756af73", "prompt_hash": "eda2f56fd2c292c75b44dac4892a7fb6273472ba73eb3747bf7bf1ca29059678", "target_hash": "cccb2012bdb24e7ea626feab61a705086b795cfee655f95e262158867e0ac8c8", "exact_match": 0.0}
{"doc_id": 3265, "doc": {"question": "when was the last time ny rangers won stanley cup", "answer": ["1993–94"]}, "target": "['1993–94']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time ny rangers won stanley cup?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1993–94"]], "filtered_resps": ["1993–94"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4255f25f2a4ee18079fe4ed582d4b2569670368650917ac405216e37906c33a9", "prompt_hash": "c5e5a11100323e34a75eb8a4baaa3d4e727d91d21bc71f732ab4c579b2783d32", "target_hash": "8861ff90236a547749c9eba3a0386f464ccfa5d5cc4c90e158fc5d8bd95c3269", "exact_match": 1.0}
{"doc_id": 3266, "doc": {"question": "cast of the movie pride of the marines", "answer": ["John Garfield as Al Schmid", "John Ridgely", "John Ridgely as Jim Merchant", "Ann E. Todd", "Anthony Caruso as Johnny Rivers", "Anthony Caruso", "Rosemary DeCamp", "Rosemary DeCamp as Virginia Pfeiffer", "Eleanor Parker", "John Garfield", "Dane Clark as Lee Diamond", "Eleanor Parker as Ruth Hartley", "Ann Doran", "Dane Clark"]}, "target": "['John Garfield as Al Schmid', 'John Ridgely', 'John Ridgely as Jim Merchant', 'Ann E. Todd', 'Anthony Caruso as Johnny Rivers', 'Anthony Caruso', 'Rosemary DeCamp', 'Rosemary DeCamp as Virginia Pfeiffer', 'Eleanor Parker', 'John Garfield', 'Dane Clark as Lee Diamond', 'Eleanor Parker as Ruth Hartley', 'Ann Doran', 'Dane Clark']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: cast of the movie pride of the marines?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Adam Beach"]], "filtered_resps": ["Adam Beach"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "85e21137bc04b25b11c21cbc438136b873e6cabdbe77b3be47e02adfacd60cb7", "prompt_hash": "920a99e158c861d6db4a8b274d082a7c600aecb94646c4af97c97ac0661305e2", "target_hash": "f2ed83a0abb48603a10f4b8e6ac2eff15d16e16ed6eb340c3285f1198387ad1f", "exact_match": 0.0}
{"doc_id": 3267, "doc": {"question": "when did cybermen first appear in doctor who", "answer": ["in 1966", "1966"]}, "target": "['in 1966', '1966']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did cybermen first appear in doctor who?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1966"]], "filtered_resps": ["1966"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "708e966ce7ce1a11c903afdee626e1419f398556f753b5ab03eaec71f07904aa", "prompt_hash": "c1b29e0935c4724ed9bf035cb50ece2b1b62b37c480f3e90d38daeb08a2a6017", "target_hash": "50c60ae9524d6cd3cc112742172e944e408f652e3ab91051d4273924babc126a", "exact_match": 1.0}
{"doc_id": 3268, "doc": {"question": "when is the last time the new york giants won the super bowl", "answer": ["(2011"]}, "target": "['(2011']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the last time the new york giants won the super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1986"]], "filtered_resps": ["1986"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f4250c43b02fceb86cac0a8bf1520c9626e42545a365fdbc88f5eaeaffe0181e", "prompt_hash": "0ddd0297e5e50f2c3e94ae0ca09a13cb1124dd3043f375ab140eddcc94769741", "target_hash": "fa517776a9fc6fd3b1e0236cc6014175aff4ea5d90a18e6ba9c5f80aa2ae1100", "exact_match": 0.0}
{"doc_id": 3269, "doc": {"question": "who has scored the most half centuries in test cricket", "answer": ["Sachin Tendulkar"]}, "target": "['Sachin Tendulkar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has scored the most half centuries in test cricket?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sachin Tendulkar"]], "filtered_resps": ["Sachin Tendulkar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbe42c17b53a78788b33d9fcb8d07c28fa8092c456821dada55282fa694dca35", "prompt_hash": "989c90baa7eaacf02e599ea381fa27c019ea4e583b5c47311ff5a0bc36d0af9d", "target_hash": "8dff6b6c1240428bdf0643cf8d44738545582ff57257aee4d06bcf33a412c22f", "exact_match": 1.0}
{"doc_id": 3270, "doc": {"question": "which way does the earth orbit the sun", "answer": ["counterclockwise", "counterclockwise direction"]}, "target": "['counterclockwise', 'counterclockwise direction']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which way does the earth orbit the sun?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" counterclockwise"]], "filtered_resps": ["counterclockwise"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "275003bfb49917a88b6eff3460c15039f44ab1fd58ea0634333b4b5ffb3e2bd5", "prompt_hash": "c430ff066721730fc03170ffc8d833c5a2f581eeae58f8a6579fe1bc396bda5c", "target_hash": "a6581a21549ed458178a6561e85f57465ef6f8c1ea1dff9e82a0462f4c404702", "exact_match": 1.0}
{"doc_id": 3271, "doc": {"question": "what awards did the curious case of benjamin button win", "answer": ["Best Makeup", "Best Visual Effects", "Best Art Direction"]}, "target": "['Best Makeup', 'Best Visual Effects', 'Best Art Direction']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what awards did the curious case of benjamin button win?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Academy Award"]], "filtered_resps": ["Academy Award"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "10cece550939927504a8e5ca200752abe8a7653d95921daf856b71f4a6f542f4", "prompt_hash": "e61bbc0ec8064526fe9b9d64f4662d9b49c03ed8250fbc8a8cbdb83508f1b0c4", "target_hash": "316e92c4cbba914c4213982c0381a9094b36b5e462edee604911fd01711f336a", "exact_match": 0.0}
{"doc_id": 3272, "doc": {"question": "who had the longest tenure as moderator on meet the press", "answer": ["Tim Russert"]}, "target": "['Tim Russert']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who had the longest tenure as moderator on meet the press?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tim Russert"]], "filtered_resps": ["Tim Russert"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e72c26c6420dfdf5294739063976a1975ad4d9267e48159b3f16024e5cf97a8f", "prompt_hash": "504474b9b1cd456dae9edfe21c39f4df07a83df4872d2ab160663c6c7b754635", "target_hash": "6bab2e8f9db49ea9a4659fadeadfa42e8fbc7a4578013dad18c3e1c13712125e", "exact_match": 1.0}
{"doc_id": 3273, "doc": {"question": "who sings it's my party and i cry if i want to", "answer": ["Lesley Gore"]}, "target": "['Lesley Gore']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings it's my party and i cry if i want to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Betty Boop"]], "filtered_resps": ["Betty Boop"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "56effa4b8db7f8273d9dd8c032d722581d9e29b05705f9afe9d371b039b06d80", "prompt_hash": "3735286a7261976d3f3e9858cdfcc7cece60eb284284cd96d07846b474b26ec7", "target_hash": "429dcc4e93fcd03750f26c0dce64b2681b575834bc19c87e985760882084d64a", "exact_match": 0.0}
{"doc_id": 3274, "doc": {"question": "what is the minimum size of a udp datagram", "answer": ["8 bytes"]}, "target": "['8 bytes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the minimum size of a udp datagram?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 8 bytes"]], "filtered_resps": ["8 bytes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eea3e058e686be6c716d276e1ad4408f43f3860c1639d3d1c715834ae09f1e96", "prompt_hash": "bbfa303ac371d8a9e6221888113d4e4008fb02fbfd48c9b55035f840b5414a81", "target_hash": "13763066a3b9d913c16dd55daa1c97a910f0eb6de5b439c8a128c054a9996fea", "exact_match": 1.0}
{"doc_id": 3275, "doc": {"question": "who represents the insured in the investigation of a claim", "answer": ["Public adjusters", "Claims adjuster"]}, "target": "['Public adjusters', 'Claims adjuster']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who represents the insured in the investigation of a claim?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" adjuster"]], "filtered_resps": ["adjuster"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "20bdce9031107f366ba5b9e747a167c42bf691dcdf2e60aa89f4b1ce3e2202b3", "prompt_hash": "ad32fb8c3b483f09e14b366fa2182ec257f4e2d241a8fd49d53b40ec2659a480", "target_hash": "11121563d18509983e9502a5892ccf62e88cdfbf52f9f9e937d4c993ff83fd1b", "exact_match": 0.0}
{"doc_id": 3276, "doc": {"question": "how long do former presidents get secret service protection", "answer": ["lifetime protection", "lifetime"]}, "target": "['lifetime protection', 'lifetime']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long do former presidents get secret service protection?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lifetime"]], "filtered_resps": ["lifetime"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3c39e2e515280cfacdf20a9b28dccc27909d4504a88ebc770d0d91dbe56db0e", "prompt_hash": "d1e75a695f839fd188ba9d87bae0852f10873e067abbf9406856af5bafed44c9", "target_hash": "4254ce2caffe2bbcccea4f67891ab02592643ecd8f15ecd119a6d270a8722096", "exact_match": 1.0}
{"doc_id": 3277, "doc": {"question": "billy ocean when the going gets tough movie", "answer": ["The Jewel of the Nile"]}, "target": "['The Jewel of the Nile']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: billy ocean when the going gets tough movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "17472b513d65f7a1f0bca745d232ae67bca2747dc6ccdae5a92701e5505d2c20", "prompt_hash": "a554b84207b0e3e808724d9ae2feb2db61ccbbe155cc588332471b05dd383ef4", "target_hash": "50a974c271f6b93810a0c26f0af1a5ca6b41086b9ab1ee029f328234f7d4a3ec", "exact_match": 0.0}
{"doc_id": 3278, "doc": {"question": "when did united states enter world war i", "answer": ["April 1917"]}, "target": "['April 1917']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did united states enter world war i?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1917"]], "filtered_resps": ["1917"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d99a97914b8358854f7257fab2f4dbf6cd431e061303fd5f5912e19fea2165e1", "prompt_hash": "65adb7902549e5006e46d138b3146a8813efdd960a0dc8ca22d2c87ad95a46a3", "target_hash": "750d9af4a5d9255b878748b970d4c002dff99b7ea1abc72e9697acc21f8dd2ae", "exact_match": 0.0}
{"doc_id": 3279, "doc": {"question": "most home runs by 2 teammates in a season", "answer": ["115", "a combined 115 home runs", "Roger Maris", "Mickey Mantle"]}, "target": "['115', 'a combined 115 home runs', 'Roger Maris', 'Mickey Mantle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: most home runs by 2 teammates in a season?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 59"]], "filtered_resps": ["59"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ebee11bc5336fa91c49b992d22342a02c38ea0c0f4012a4660f85f764d66daad", "prompt_hash": "3b798ad9a6241f7c3bbad83118034e6c2e544782093837471259855f0dc78d7c", "target_hash": "664a7444d01fdbf48485cfc3ff757661ee3e60bc169c5b9004fe90d06a741b41", "exact_match": 0.0}
{"doc_id": 3280, "doc": {"question": "all of the states in the northeast region", "answer": ["New Hampshire", "Maine", "Rhode Island", "Pennsylvania", "Vermont", "New York", "Connecticut", "New Jersey", "Massachusetts"]}, "target": "['New Hampshire', 'Maine', 'Rhode Island', 'Pennsylvania', 'Vermont', 'New York', 'Connecticut', 'New Jersey', 'Massachusetts']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: all of the states in the northeast region?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "481690541953acac09d8139f969418b993ff9b235211d71f37116f66a5772b6a", "prompt_hash": "bef41b4560f95e52eac5621e5241ebc6337954a632b05155066bc1c70be9c511", "target_hash": "a802a7297aa8b563607783657a6598701372b2cd81a80a29de1af0f8d57b378b", "exact_match": 1.0}
{"doc_id": 3281, "doc": {"question": "make it or break it who goes to the olympics", "answer": ["Payson", "Jordan", "Lauren", "Colleen", "Kaylie"]}, "target": "['Payson', 'Jordan', 'Lauren', 'Colleen', 'Kaylie']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: make it or break it who goes to the olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" gymnastics"]], "filtered_resps": ["gymnastics"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e6858341f4f0eeaa9ce2dddd6c644175d4c69a10ff5a9633c44d4212f89fb22", "prompt_hash": "a662c22fc0819903a745664512221986d7da06283dd086d91c77d8bb17702a3f", "target_hash": "fea315b25982e8b4cd930b3de5177f5a2b0faedc4665d76fad5acfa84b16b50b", "exact_match": 0.0}
{"doc_id": 3282, "doc": {"question": "when was the r10+20 summit in rio de janeiro held", "answer": ["June 2012", "13 to 22 June 2012"]}, "target": "['June 2012', '13 to 22 June 2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the r10+20 summit in rio de janeiro held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2012"]], "filtered_resps": ["2012"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "de991a282d5f7ed69a9f7e55701ec91c2aa7cccab69468a22f2c83ad04eb32bc", "prompt_hash": "5b5815291a64271bd13a53847f5bc4643be1642e4fd2cadde692768736ab749f", "target_hash": "2be27e6d7c86f8d0379940f077409a1b473bf07ec5ae7859ac81410ba815ce48", "exact_match": 0.0}
{"doc_id": 3283, "doc": {"question": "who sold out jesus for 30 pieces of silver", "answer": ["Judas Iscariot"]}, "target": "['Judas Iscariot']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sold out jesus for 30 pieces of silver?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Judas"]], "filtered_resps": ["Judas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2a59a38103979ad667c19ef9bb72a663d924ca2e0b50b958fcf188fdd47cf71", "prompt_hash": "d27197a2519d4193efb6fc8af3dfb0a86310839da341a0147b848f1ba4a394b9", "target_hash": "8a505b8e206d467d70fa976dd308231c8ca7d4a65b709c1b41e5b76f4d0a43c9", "exact_match": 0.0}
{"doc_id": 3284, "doc": {"question": "who has the most 70 point games in nba history", "answer": ["Wilt Chamberlain*", "Wilt Chamberlain", "Booker, Devin", "Chamberlain, Wilt"]}, "target": "['Wilt Chamberlain*', 'Wilt Chamberlain', 'Booker, Devin', 'Chamberlain, Wilt']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most 70 point games in nba history?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kobe Bryant"]], "filtered_resps": ["Kobe Bryant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4335e4b726cf54c3b77c6eae2f83e1c62928062a6d6e194079ba1ed6f44673f7", "prompt_hash": "98dfa22dba7f764add8b051d66833f7ea557713a4f9e2f7e3d1afb27e335530e", "target_hash": "0ee1f439177c35311dd55f20ec63ac78ab942f93d48480c8f217faab769c1d70", "exact_match": 0.0}
{"doc_id": 3285, "doc": {"question": "where can tight junctions be found in the body", "answer": ["Internal epithelia", "the cytoskeletons of adjacent cells"]}, "target": "['Internal epithelia', 'the cytoskeletons of adjacent cells']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where can tight junctions be found in the body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" epithelial and endothelial cells"]], "filtered_resps": ["epithelial and endothelial cells"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "69ad848ff450011775a1a366c33c4476a7e70692e0a940bf8701e02fea7ea960", "prompt_hash": "55cd8bf67a0854ac3ea8251616eb0128595099c7eb48a838d216a8c3f5e78151", "target_hash": "bda14c7e3608cd368cac03d8304af0824e370782316f6047cb64293efb796d3a", "exact_match": 0.0}
{"doc_id": 3286, "doc": {"question": "who plays jimmy's mom in 8 mile", "answer": ["Kim Basinger"]}, "target": "['Kim Basinger']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays jimmy's mom in 8 mile?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kim Basinger"]], "filtered_resps": ["Kim Basinger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3e110061b1291edf99c72b4a0a8eada91d0760bd3b8decd578269bae8fdf181", "prompt_hash": "0c4e9308bbefb46e40c063b783a379a1d3169256956176476d0ae3bd1d7ad023", "target_hash": "0466fee5958f53d1e45a8344567e410755349f4ce8f178c90f0656d0e8d7864c", "exact_match": 1.0}
{"doc_id": 3287, "doc": {"question": "who plays the dad in malcom in the middle", "answer": ["Bryan Cranston"]}, "target": "['Bryan Cranston']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the dad in malcom in the middle?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dean Winters"]], "filtered_resps": ["Dean Winters"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "674257dccddffa8f21297c1b9e8bd92694689c04f03aff2eee67dcd76fd08a80", "prompt_hash": "2fcaf28ac3fd39eda2d051d676838bcc29cea1e3d0c1119e8975b4575fed4bc4", "target_hash": "b48e153e33d106c8a42d2ac763f00692f8c601749386dfde8c7d3c5e2a033caf", "exact_match": 0.0}
{"doc_id": 3288, "doc": {"question": "who won the ladies ice skating in the olympics", "answer": ["Alina Zagitova"]}, "target": "['Alina Zagitova']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the ladies ice skating in the olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michelle Kwan"]], "filtered_resps": ["Michelle Kwan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "414443ade758a89a53922673a6fe1d997c0954cea39bf61d26fb4090db763a36", "prompt_hash": "6cbbbdf71e0c85e1f2543a26db520fd9a7ca8f201f6638008a687ff6b582ad9c", "target_hash": "bd8cc42d4d20db039bc1cafac235ada07d42298babba4492cdf9cc4be94d730c", "exact_match": 0.0}
{"doc_id": 3289, "doc": {"question": "the length and direction of a straight line between two locations or positions", "answer": ["displacement"]}, "target": "['displacement']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the length and direction of a straight line between two locations or positions?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" distance"]], "filtered_resps": ["distance"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "12a8eab37248d75a557768d5d77f66fdaea8e6280f1488692f1ff7f186e4b60b", "prompt_hash": "179acf6e2674f13c085d57dcfefa84120930cecc622bcad4a1080554ed1c5168", "target_hash": "075215b77e0778344cde80d43a27306f3de47806c7942df40c60d17d37ccf12a", "exact_match": 0.0}
{"doc_id": 3290, "doc": {"question": "converting stereo signal to mono signal is called", "answer": ["Panning"]}, "target": "['Panning']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: converting stereo signal to mono signal is called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" downmix"]], "filtered_resps": ["downmix"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e45f8279b0b6037f7b8f15ad5b93efafe26c78b8c2e9fe0744e1872085d0405a", "prompt_hash": "2d6a66ac5a6c37059f90e10e2f463cb5feb2e2ec8303c45be9c01771ddf67dd3", "target_hash": "924e951f612c40e1ac5ea706bf87c7878610424bcffb29c6b715f079386b0f4d", "exact_match": 0.0}
{"doc_id": 3291, "doc": {"question": "what is the system of government in japan", "answer": ["constitutional monarchy"]}, "target": "['constitutional monarchy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the system of government in japan?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" parliamentary system"]], "filtered_resps": ["parliamentary system"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7b88afe82c87b2567e3cadab2a49ad96ccd6d9cba9c79c4a1ac3a9bb610c8a70", "prompt_hash": "3d4dc2807d061784415187e32ecee9ff1544964f4599d0e01b7e537cf8342917", "target_hash": "8f40d3ce55d0da865ae7fa33faedfd178cd5dbfe338b97c52fb9c0783e031161", "exact_match": 0.0}
{"doc_id": 3292, "doc": {"question": "what dynasty completed the great wall of china", "answer": ["Qin", "Ming dynasty", "Qing"]}, "target": "['Qin', 'Ming dynasty', 'Qing']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what dynasty completed the great wall of china?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ming"]], "filtered_resps": ["Ming"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6a4e2200e5e91d7b795c8399b2658dfc47f6e10885d10a128a09b188205299af", "prompt_hash": "1762b19c24d13dbcb214b097b0edf15eb59b438f46522d7388f78e3fabecde56", "target_hash": "63229ba28c83970fcc779db79d060e6ada8c32695083467afca96555de5ac790", "exact_match": 0.0}
{"doc_id": 3293, "doc": {"question": "where is union station in washington dc located", "answer": ["50 Massachusetts Avenue NE"]}, "target": "['50 Massachusetts Avenue NE']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is union station in washington dc located?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Downtown"]], "filtered_resps": ["Downtown"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dd5d9e5997a1be9c070b9bfec965d949f96371729aeab83de6722bcab010eb0c", "prompt_hash": "35c2847018c1d980dd98853d10a5d8f21dd0cc04a1ffde76cc1583f0db845f3b", "target_hash": "0055d6a89dd9c038ad62efc8a5dbfcb50addb8f9b14804c4ae463c4087f9c541", "exact_match": 0.0}
{"doc_id": 3294, "doc": {"question": "who sings song at the end of beauty and the beast", "answer": ["Josh Groban"]}, "target": "['Josh Groban']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings song at the end of beauty and the beast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Celine Dion"]], "filtered_resps": ["Celine Dion"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e319d5cd47ccb4beb49f8fbc39668c91c031fa1c851d8e53f08f925abfb689b9", "prompt_hash": "6691723670bc1eb7da2a594ac8fc5589721b98b851e32d37f16415e6019ec31b", "target_hash": "a961822c6eb9999787b366ec31dbe84c408b15dfa77eb44af721349766960251", "exact_match": 0.0}
{"doc_id": 3295, "doc": {"question": "super robot monkey team hyperforce go fighting games", "answer": ["Galactic Smash"]}, "target": "['Galactic Smash']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: super robot monkey team hyperforce go fighting games?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Super Robot Monkey Team Hyperforce Go! is a fighting game"]], "filtered_resps": ["Super Robot Monkey Team Hyperforce Go! is a fighting game"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3b72eb5b6b54d1387b30cbc9475f20fda787096c7d9ecae81a5828cb00e950d", "prompt_hash": "eb25459560061e30f1c3daf842bbe827ea06b1077ce0e9542edc342b3120d73d", "target_hash": "af4fa39ba838060b785e6fd082073ef8b63b61704f7c5d6cbf66abf50bc772be", "exact_match": 0.0}
{"doc_id": 3296, "doc": {"question": "where did they film the show the crossing", "answer": ["British Columbia, Canada"]}, "target": "['British Columbia, Canada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did they film the show the crossing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Vancouver"]], "filtered_resps": ["Vancouver"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8cb3cd79ad9e950953bbfaa9807bae401b51b10060d8860e7ee1f9dd3af68e23", "prompt_hash": "09c138a656b190167dfce2e96f31b317da4e26a7ef5274043043ce09dfcc4c65", "target_hash": "d1a79b7af2ba1f36e79bee5f26f2e7918c50da2a29a24b03fe1d7a321298c162", "exact_match": 0.0}
{"doc_id": 3297, "doc": {"question": "who wrote lord have mercy on the working man", "answer": ["Kostas"]}, "target": "['Kostas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote lord have mercy on the working man?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bruce Springsteen"]], "filtered_resps": ["Bruce Springsteen"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "da422ecf2fe75fe732a2c447a2ee0da4da5fc9985d7569663a274e0c5a67ffa3", "prompt_hash": "585fb221df25ca7740d37460ad237871f1fcef00ce3417f4cd96a927e1567e18", "target_hash": "c0c79736c6bdf9bf2f8cff8e5255a0c92abbd17f45901c6ac6a6b6e9ffaccc7e", "exact_match": 0.0}
{"doc_id": 3298, "doc": {"question": "in which year was led zepplin refused entry into singapore because the band members had long hair", "answer": ["1972"]}, "target": "['1972']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in which year was led zepplin refused entry into singapore because the band members had long hair?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1972"]], "filtered_resps": ["1972"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "432dbb251c5d7da9343ba6f359ad0c91f8a1a88d9ca6917b0e3c703bf8e0a751", "prompt_hash": "131b185f1c46d7478815c36a2e97c0f8092b41ae3a4284fd6275a082efca667c", "target_hash": "a3229d6bc531436ea36c4e4010310ff57c437f668ee819897593629428d568b5", "exact_match": 1.0}
{"doc_id": 3299, "doc": {"question": "where does the term pop music come from", "answer": ["Britain"]}, "target": "['Britain']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the term pop music come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" popular music"]], "filtered_resps": ["popular music"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e6f897ad69da52e752f9038e97c7016984281bbba48bca63686bbe6bb98dbe5a", "prompt_hash": "9d6ccdb10453f16fd514b76ddef3f4ff0c88c13bac87c1f370c4ba72b83e7d8e", "target_hash": "d2ca663587a83a4801bc00dd59823605c3fdd6263b6f83cdd65e646db565e304", "exact_match": 0.0}
{"doc_id": 3300, "doc": {"question": "who discovered gold in the sierra nevada of california", "answer": ["James W. Marshall"]}, "target": "['James W. Marshall']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who discovered gold in the sierra nevada of california?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James W"]], "filtered_resps": ["James W"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d8ed9bcef5951126e3c9914f70078136d3812d6cca70e2c4a08eb51aeffea32", "prompt_hash": "77baf57a17390f7c551cdf0759f4231fe650a0d1b41cc2e9ee5c4bcb279651ae", "target_hash": "a591feb1cef3d819e5a914f377a6f3e2ae30c306091e5a01dac01d427b23c471", "exact_match": 0.0}
{"doc_id": 3301, "doc": {"question": "list the seven gifts of the holy spirit", "answer": ["fortitude", "wisdom", "fear", "piety", "understanding", "fear of the Lord", "knowledge", "counsel"]}, "target": "['fortitude', 'wisdom', 'fear', 'piety', 'understanding', 'fear of the Lord', 'knowledge', 'counsel']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: list the seven gifts of the holy spirit?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Wisdom"]], "filtered_resps": ["Wisdom"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b105fc3a18ebd17fb696e82ade2df86b7db5b791f4255fa38da8fe3dabfe8b8e", "prompt_hash": "2ee976c8fb2882eac028524c5be93c74156a8af26594317a757ee3c8a9c5e47b", "target_hash": "c2280601571c26f5f256f2bf39b3b97c2351eea7287445943bf163e5ba1b4aca", "exact_match": 1.0}
{"doc_id": 3302, "doc": {"question": "who won season 5 of hell's kitchen", "answer": ["Executive Chef Danny Veltri", "Danny Veltri"]}, "target": "['Executive Chef Danny Veltri', 'Danny Veltri']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won season 5 of hell's kitchen?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Garvey"]], "filtered_resps": ["Jennifer Garvey"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d00a07450a9a334676f16248d98eea68ec0ccc39cf9205e3dfabc80c455ea28", "prompt_hash": "dae28d09d991ae818feb77fa327b5c402d6e74bf6d75c6895a0d1357dd1ec1a7", "target_hash": "2c41dd3daffa38456bfdf31b4d9facf51b0b4a6137782adc62272d891fdc6c61", "exact_match": 0.0}
{"doc_id": 3303, "doc": {"question": "what episode of pll does jenna get her sight back", "answer": ["If These Dolls Could Talk", "Blood is the New Black"]}, "target": "['If These Dolls Could Talk', 'Blood is the New Black']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what episode of pll does jenna get her sight back?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e3089213ef722b244a44f0daabbf77c35672c5a2c0b3e8d7e2ce498f5b277ebd", "prompt_hash": "37a9880929d179342a60cc22276f42097dc9137aac5dfa22ec3c65dd7465173f", "target_hash": "4ed09203a11116ad3b2355ccfe10b6d932df1b7bfdbcf0e5a5ec2d8235bbb2f3", "exact_match": 0.0}
{"doc_id": 3304, "doc": {"question": "when was the last time the crows were in the grand final", "answer": ["2017"]}, "target": "['2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time the crows were in the grand final?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aa0ef8e46ada6d3c56c7ff629fd84654bc4149ee623b20b8f52779848cded828", "prompt_hash": "5c500bd6926b022e6db3ff979d1c676b36f3cfa45465e166889d77ce154b274b", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "exact_match": 1.0}
{"doc_id": 3305, "doc": {"question": "who plays scrooge mcduck in the new ducktales", "answer": ["David Tennant"]}, "target": "['David Tennant']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays scrooge mcduck in the new ducktales?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tony Anselmo"]], "filtered_resps": ["Tony Anselmo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b6e1a0b4d897d902b3e4723ec5947ff34f840b82785a24de4ed58fcf8f9b9b60", "prompt_hash": "5e540db4551d8a6bb3869da6a5c1b1e8874542f530844d47f230729076263fcc", "target_hash": "80efeb734c671ea83e41c5e13cafb2a755ff3840b18d08af427de71e8b68d26e", "exact_match": 0.0}
{"doc_id": 3306, "doc": {"question": "state the process that divides one nucleus into two genetically identical nuclei", "answer": ["mitosis"]}, "target": "['mitosis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: state the process that divides one nucleus into two genetically identical nuclei?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" mitosis"]], "filtered_resps": ["mitosis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f65a50a725de993d80302ce56e11c084a34623ed420b2dae6c546dad9837012", "prompt_hash": "1ba1d3de78222988584e243f7d7f7a09f4e235bb2694ea8fb47cf502a075f0e0", "target_hash": "71b2e2f778fe1866aa50bd206b4c3d50935b67f77d915eace9068ca5477882cc", "exact_match": 1.0}
{"doc_id": 3307, "doc": {"question": "who did kenny rogers sing mary did you know with", "answer": ["Wynonna Judd"]}, "target": "['Wynonna Judd']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did kenny rogers sing mary did you know with?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dolly Parton"]], "filtered_resps": ["Dolly Parton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42b299bb76f763cf3b14f4b71529b7c1f8903d71f6ad4b8def18358ec1fe422b", "prompt_hash": "af04b442ccea0da33aa0f9ca0f5078c0cf6506f62cb0a90c32c7b88106cc7f95", "target_hash": "d7ef69051613ac1209b53ed34d7b978faa32f158e0476684d8845fa858b8f173", "exact_match": 0.0}
{"doc_id": 3308, "doc": {"question": "when did the movie karate kid come out", "answer": ["2010", "1984"]}, "target": "['2010', '1984']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the movie karate kid come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1986"]], "filtered_resps": ["1986"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e1a75591dca81f233370fb6cef7dbac5c259e0314095b45b572213d91b981448", "prompt_hash": "987f054ffef58c169561ef091ab92f3e15b2e6967145312572d1335eb9c99d4a", "target_hash": "6bc74f2bcac9d3ec6241df16b9ffa549e755a0a34752af0ae11589c5ce7e4f3b", "exact_match": 0.0}
{"doc_id": 3309, "doc": {"question": "what is the process to become a santero", "answer": ["a cleansing ritual", "making Ochá (Saint)"]}, "target": "['a cleansing ritual', 'making Ochá (Saint)']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the process to become a santero?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Catholicism"]], "filtered_resps": ["Catholicism"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4160b8e336a20342f7d6e29d47053de5e56b39e82d651a64fa15951fc37999fb", "prompt_hash": "5caa69cd76657339600ead619ab41857ee1a11bc47abe9b769208b64f159085e", "target_hash": "0ffb2c71a95e28f5a596431636501aac359201772669f693c6a0194edbc1f73f", "exact_match": 0.0}
{"doc_id": 3310, "doc": {"question": "which layer of the osi model handles physical addressing", "answer": ["physical layer or layer 1"]}, "target": "['physical layer or layer 1']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which layer of the osi model handles physical addressing?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" data link"]], "filtered_resps": ["data link"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "15c0d99f5dc5de8669f1e85b0fcf6f0538fd20f04e4d35028241607b0f3d04e0", "prompt_hash": "3b9bf6d0ac5b21263a77e12f748d400928591e448aba36e51413a90acbea4ced", "target_hash": "b2cc36af6ced3c1d2306b2b09f14f353e424e63dfff7b8a9c5c699ceea666e52", "exact_match": 0.0}
{"doc_id": 3311, "doc": {"question": "jawaharlal nehru centre for advanced scientific research jakkur campus", "answer": ["Jakkur, Bangalore, India"]}, "target": "['Jakkur, Bangalore, India']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: jawaharlal nehru centre for advanced scientific research jakkur campus?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bangalore"]], "filtered_resps": ["Bangalore"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d030649f473aace8f2f5313e10076ea8718965dafbbf1835d388149985a65963", "prompt_hash": "981d8704f0e2ccc1d94a1956879b66292e1348539e235b646cca0d13699ffd7d", "target_hash": "728473cceb9a9de9900567907ed497eff753cc666e9e5b68c3d8595e03e449ca", "exact_match": 0.0}
{"doc_id": 3312, "doc": {"question": "where is the second largest mall in america", "answer": ["Florida", "Aventura, Florida"]}, "target": "['Florida', 'Aventura, Florida']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the second largest mall in america?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Westfield Mall of America"]], "filtered_resps": ["Westfield Mall of America"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d6d9ac1efc9524a0f61f2ec9d119a98883c5d67d2023705933f34820de3f5472", "prompt_hash": "172353fd9cd497244f75121af7606a8ef5ea380d34b213ae3c3c5419ff34a9a3", "target_hash": "7c904bf5f75aa346f233c61347e800077f3e1ad3f0f577e766adb0504179fcaa", "exact_match": 0.0}
{"doc_id": 3313, "doc": {"question": "when did they stop making jello pudding pops", "answer": ["the 90s", "around 2011"]}, "target": "['the 90s', 'around 2011']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they stop making jello pudding pops?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1991"]], "filtered_resps": ["1991"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "30713d9f16aacfb0d9e4c53c49e1e2ca9eee5e41c34f559e63faf8820668c343", "prompt_hash": "85178a74ad63fe7a4c1ff27c5ddcf6489b0e967f4fe579402e9472f38262e1ee", "target_hash": "8c78561fca094eb28e8ee86f8b35d8812e3aecefcc862d8b2d272708c6417130", "exact_match": 0.0}
{"doc_id": 3314, "doc": {"question": "where did the tea come from in the boston tea party", "answer": ["England", "East India Company", "the East India Company"]}, "target": "['England', 'East India Company', 'the East India Company']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the tea come from in the boston tea party?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" China"]], "filtered_resps": ["China"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1e85615b65ce3c64704a9c4ecfbe945ac1090c1793f13ad5be147a0bdc765b55", "prompt_hash": "163016dd7a947aabfc9604faf167628b82e60c85cc7285a992a0137396c4ca26", "target_hash": "fd2f73da3b3f2ec81c18454f9250b63dbf867579eaeb1af0936591b11253ba62", "exact_match": 0.0}
{"doc_id": 3315, "doc": {"question": "what are the five compulsory prayers in islam", "answer": ["Asr", "Maghrib", "Isha", "Zuhr", "Fajr"]}, "target": "['Asr', 'Maghrib', 'Isha', 'Zuhr', 'Fajr']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the five compulsory prayers in islam?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Salah"]], "filtered_resps": ["Salah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "58cdca99ac2f7b81c2a24068f481213cd24c2fca2f8f970506aa2f45a5a4fe26", "prompt_hash": "fac7671138f13bf6b452a799fa5ec32287755c5d1e6be842aea8839d2945e08e", "target_hash": "47885a23ea688f072edd353dc37c4b3111a15bc1016466be79256ff49c035317", "exact_match": 0.0}
{"doc_id": 3316, "doc": {"question": "who determines the rules of the us house and us senate", "answer": ["United States Senate", "The Senate"]}, "target": "['United States Senate', 'The Senate']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who determines the rules of the us house and us senate?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Congress"]], "filtered_resps": ["Congress"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b0f2b9b1de321c7fa6ee166f9ea0166ccb681e15978192ecda3322c7913445c5", "prompt_hash": "19e1e956b74b312bcdb7629141d1ca24b53131390846482e8c49d6d3bbf3c1d2", "target_hash": "754bc404ee9477a1ea9fae5191f2ee2bf66eb7127573d1336caca5cd705cf690", "exact_match": 0.0}
{"doc_id": 3317, "doc": {"question": "known as the punk poet who used poetry in their music", "answer": ["John Cooper Clarke"]}, "target": "['John Cooper Clarke']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: known as the punk poet who used poetry in their music?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Johnny Rotten"]], "filtered_resps": ["Johnny Rotten"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "af0e91d0ff45846363d71de0e089edad4b01b828eed2ab83f4a2d3554cb9771e", "prompt_hash": "562aa3507180a89f99b129f1f173c5a1faf4a7614e55227e46d5622f63c25711", "target_hash": "5e42fe32799e1f602e1416978d3bdd6915c7ca804b3ec2e5eba2528f3134cf0e", "exact_match": 0.0}
{"doc_id": 3318, "doc": {"question": "when was the last episode of the flash aired", "answer": ["May 22, 2018", "May 1, 2018"]}, "target": "['May\\xa022,\\xa02018', 'May\\xa01,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last episode of the flash aired?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" May 17"]], "filtered_resps": ["May 17"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9dc331746e3187c6fe513624b7313ba07c1ddae585a2425969dab4074aa59301", "prompt_hash": "82c4d2b3838ce20433f5c56284cfd317cffdcca06cc68b24381c7a6178cdb5ca", "target_hash": "fbcbfd78d4364c767c4a7a642a93c5dce052f69344ed174533f50c71a15a3ef4", "exact_match": 0.0}
{"doc_id": 3319, "doc": {"question": "who sang last night on saturday night live", "answer": ["Childish Gambino"]}, "target": "['Childish Gambino']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang last night on saturday night live?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Hudson"]], "filtered_resps": ["Jennifer Hudson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a1c43fac08465ea8b04ec57aad5d5ea16f17ec8c646c22adb5275e2541046631", "prompt_hash": "117e2db9beaf58483f60def3ecf044751c8216051062a9d29337332c19def429", "target_hash": "416cef9787191a0bf680f8db1b42fcb0913d732d6af37bd867ddeef96297cb0e", "exact_match": 0.0}
{"doc_id": 3320, "doc": {"question": "who has scored the most points for wales v france", "answer": ["Neil Jenkins"]}, "target": "['Neil Jenkins']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has scored the most points for wales v france?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Alun Wyn Jones"]], "filtered_resps": ["Alun Wyn Jones"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3eb580391bae9c53d85c807db121a2d18184235fa5f60ca2ed2b618ae52da920", "prompt_hash": "76fe6dc7f4ba55611d5b066f30a4992f89f860fd53077989702d5aebae7fda24", "target_hash": "83329a27c936d0c610094ab73eb6d308a40215db379a6ea647112d92c4ff2fa0", "exact_match": 0.0}
{"doc_id": 3321, "doc": {"question": "who sings oh what a night late december back in 63", "answer": ["the Four Seasons"]}, "target": "['the Four Seasons']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings oh what a night late december back in 63?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Frankie Valli"]], "filtered_resps": ["Frankie Valli"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03bbd2a927e5e5c46d5c4faab5502236521fe322dd0b3a10acaef5edbe7eaa30", "prompt_hash": "c4ae9e0a453179b9f7b84555853c86b0c036b4ab3013bc9fc5f74ab27a79d39c", "target_hash": "08d43a129972bd0b53ec5ba3536167fed301567e02738ea4b44920b89f51f912", "exact_match": 0.0}
{"doc_id": 3322, "doc": {"question": "puella magi madoka magica when does madoka become a magical girl", "answer": ["My Very Best Friend"]}, "target": "['My Very Best Friend']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: puella magi madoka magica when does madoka become a magical girl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" episode 1"]], "filtered_resps": ["episode 1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0d88bca50e40b390effb20dc54987dd4f6ff17444b072ce5d3ea439e6faf3144", "prompt_hash": "4dcbe7b52cf40056062af3a0e8da2bf49eb7e574c3065f7bacc144bb639ae3f4", "target_hash": "f1b95281d6f4c3dc44cb2aae90c209f63099454a71db05179eb274fc019d162b", "exact_match": 0.0}
{"doc_id": 3323, "doc": {"question": "who wrote i want to dance with somebody by whitney houston", "answer": ["George Merrill", "Shannon Rubicam", "George Merrill and Shannon Rubicam"]}, "target": "['George Merrill', 'Shannon Rubicam', 'George Merrill and Shannon Rubicam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote i want to dance with somebody by whitney houston?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Narada Michael Walden"]], "filtered_resps": ["Narada Michael Walden"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9c41c70fa29356e779f23e2b39197acf2dd0788e670ce79cdcb84b2ad1d3807e", "prompt_hash": "a3d3a427ac02e3b67b218aabbc27bc5a6ddecbb4d1f5eb016e87bfc17874ca01", "target_hash": "0c33bb9dc91b8458b31da4661258bf81fd08cff5b950c8b458cc8c01296da00d", "exact_match": 0.0}
{"doc_id": 3324, "doc": {"question": "when did they start adding zinc to pennies", "answer": ["1864", "1943", "1864–1942"]}, "target": "['1864', '1943', '1864–1942']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did they start adding zinc to pennies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1982"]], "filtered_resps": ["1982"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "38689944e3a5e1a86200f93e8f1d17b9c1bb7ea61aa73416fe796b150e25b06a", "prompt_hash": "2579c320703b26ff53ca098b567bcf87d3269a79fdeefa6f79b033faf6982b5f", "target_hash": "5f95ca54c99ae75b0f24a13c1114b3123dcbe2e1d9538ad34fce2f8ed12742ed", "exact_match": 0.0}
{"doc_id": 3325, "doc": {"question": "ahana from dil sambhal ja zara real name", "answer": ["Smriti Kalra"]}, "target": "['Smriti Kalra']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: ahana from dil sambhal ja zara real name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ahana Malik"]], "filtered_resps": ["Ahana Malik"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "364f3416f038351d71c94a7687c1444f962f52423a91af9bd70f92449b4cba9b", "prompt_hash": "e31fe7909d7376f739403b57a4700406c30de71561c16e6e9964c7ee7ab4a386", "target_hash": "6b5199782f3c53f04a42683b81a07948564437fcc0ec1ed685d174f56b4cc67d", "exact_match": 0.0}
{"doc_id": 3326, "doc": {"question": "when was the last year thanksgiving was on the 23rd", "answer": ["2017"]}, "target": "['2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last year thanksgiving was on the 23rd?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1939"]], "filtered_resps": ["1939"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "113441d299f9bd71b203b673623a5ea2bc04ccb26921209a29219641b2c03d2b", "prompt_hash": "6f2a196d273defbf3c7ef7f0ba768a28593d8d3f28c54b6671f2237118b37f6e", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "exact_match": 0.0}
{"doc_id": 3327, "doc": {"question": "what does the air gap do on a dishwasher", "answer": ["backflow prevention"]}, "target": "['backflow prevention']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what does the air gap do on a dishwasher?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" prevents odors"]], "filtered_resps": ["prevents odors"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2c0a6337cb8bef9b2ee52defdba0e0a24c84f07723959e6601c51863e48fb700", "prompt_hash": "6adad32776270f9f71b3a792e900afca066f056855a03d53ebb6678e5646c4c5", "target_hash": "a6afd39244e952dc93130c71c5759869b4dc47f23f661be5139be901e60bd82c", "exact_match": 0.0}
{"doc_id": 3328, "doc": {"question": "who kidnapped ben in the deep end of the ocean", "answer": ["celebrity alumna Cecil Lockhart", "Cecil Lockhart", "the celebrity alumna Cecil Lockhart"]}, "target": "['celebrity alumna Cecil Lockhart', 'Cecil Lockhart', 'the celebrity alumna Cecil Lockhart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who kidnapped ben in the deep end of the ocean?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the shark"]], "filtered_resps": ["the shark"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8958b5e053982afa3b4f319764470a766a75e07b02b49263cdb9c7b4023fbe7f", "prompt_hash": "94ca8fc39029879a22fdc913972cd340d959bb65fa5c8fc4d1a7201b248993ae", "target_hash": "b3c47470c1c3577a724bed4d0da5429283f687dca14e96ae5602e7ec1bee512b", "exact_match": 0.0}
{"doc_id": 3329, "doc": {"question": "star cast of ab tumhare hawale watan sathiyon", "answer": ["Bobby Deol", "Akshay Kumar", "Divya Khosla Kumar", "Nagma", "Amitabh Bachchan", "Sandali Sinha"]}, "target": "['Bobby Deol', 'Akshay Kumar', 'Divya Khosla Kumar', 'Nagma', 'Amitabh Bachchan', 'Sandali Sinha']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: star cast of ab tumhare hawale watan sathiyon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Amitabh Bachchan"]], "filtered_resps": ["Amitabh Bachchan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b4d33096342b37af502c3c1d7f016e00a8df7db0913719ebb9e1c4438c32406c", "prompt_hash": "323a437892b298b595402b888607a41431f2edc6462b4a08a7f110e6e5c11001", "target_hash": "b5796431d6994470117ad433ca17671c5c2f83febb107433bc252c05d7ef0827", "exact_match": 1.0}
{"doc_id": 3330, "doc": {"question": "who sings the theme song to all that", "answer": ["TLC"]}, "target": "['TLC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings the theme song to all that?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Lonely Island"]], "filtered_resps": ["The Lonely Island"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0eee95016fa94b7e46f39180c514079a0d9f613c8dbee25004187c7f0c2df9a8", "prompt_hash": "5bb1f206b076e20ab023ce9e8b8833a9f8001d2774d7d23294848cf638d6294a", "target_hash": "8dd489acecea0cdab44841db392195af5ca81f8df875dd9caaffa6c2b9a7d954", "exact_match": 0.0}
{"doc_id": 3331, "doc": {"question": "the most recent technological change to the u.s. economy was", "answer": ["digitization of social systems", "the digitization of social systems"]}, "target": "['digitization of social systems', 'the digitization of social systems']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the most recent technological change to the u.s. economy was?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the internet"]], "filtered_resps": ["the internet"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3d253390d40354a2111c2c15f98949b1fc026122ad29b5aa23881329d2efb029", "prompt_hash": "d0356c5b5cf72d99576d797e6144751aa53ed21872dbb3994e992f3d41ee458b", "target_hash": "7f9dd682a80d04c57e1b20f48677eee7b1b05c812cb3b27cec85893ecdf88d3f", "exact_match": 0.0}
{"doc_id": 3332, "doc": {"question": "what was the religion in persia before islam", "answer": ["the Zoroastrian religion", "Zoroastrian"]}, "target": "['the Zoroastrian religion', 'Zoroastrian']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the religion in persia before islam?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Zoroastrianism"]], "filtered_resps": ["Zoroastrianism"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "099f64c47c83d01919636dd216934dfd0b75bafe8413fd92eaacc919559ced38", "prompt_hash": "9a6b74a5056d576a1dec689dca481e7f1398c0e04500cb85d7fbfa1bf8428123", "target_hash": "3fd8fc12a92cc2a018ae98ff58c5d759c72399c01743d2ae7687f9bba4a4151c", "exact_match": 0.0}
{"doc_id": 3333, "doc": {"question": "who is often associated with printing the first book using moveable type in germany", "answer": ["Johannes Gutenberg"]}, "target": "['Johannes Gutenberg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is often associated with printing the first book using moveable type in germany?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Johannes Gutenberg"]], "filtered_resps": ["Johannes Gutenberg"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e3e39e6fde74379241644f2c21d53aeb3d7b4dab39a6826a743ebd54ffdeb9f0", "prompt_hash": "cb3f1d4acd70337216312a1b386e62df32c267f7605080e581183ec11c41bf5f", "target_hash": "8c2ecbac7c9f3bb7a9e2c539e1db65dcf21cf923484ebc6d62814f6b0f975554", "exact_match": 1.0}
{"doc_id": 3334, "doc": {"question": "which is the world's largest company in terms of revenue", "answer": ["Walmart"]}, "target": "['Walmart']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which is the world's largest company in terms of revenue?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walmart"]], "filtered_resps": ["Walmart"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "68d3bc6eb85910f4034c394c73bc8541b16e9de2fb481444cecfbf1429eed9b2", "prompt_hash": "f17985fe1d3a165610b8e8fd7bdfad0eccc67408ac025e140706a269f2e866b4", "target_hash": "e5f09ff70d170f7bdabcda3233a50d60327fbbe84246aaaa8ede8d7ec0f23022", "exact_match": 1.0}
{"doc_id": 3335, "doc": {"question": "who was one of the first to chart the course of forgetting over time", "answer": ["Hermann Ebbinghaus"]}, "target": "['Hermann Ebbinghaus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was one of the first to chart the course of forgetting over time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hermann Ebbinghaus"]], "filtered_resps": ["Hermann Ebbinghaus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "806d4e784f0cc14e44bdeb434a10d936af6561196d932f51a496015b67862ece", "prompt_hash": "5e85090d804e74feda2eeabf145b17f596d086d283cbd38407e82c474b46b50d", "target_hash": "c0390d857b07be74644faf45b5bb35cc58d3b292370eaf344726175b354bd42a", "exact_match": 1.0}
{"doc_id": 3336, "doc": {"question": "what is the job of the whip in congress", "answer": ["to ensure party discipline"]}, "target": "['to ensure party discipline']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the job of the whip in congress?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" leader"]], "filtered_resps": ["leader"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6e864225e581e7d28cf8b96852134016682182f96ce0c909fabfc472b5a659d4", "prompt_hash": "e4210db80b5bbf996e201696dd017e409bf49dbeb923bac4bf0bf1ec9ebb4640", "target_hash": "c3cb75c8050f241a9d612280674d0f7567a32f2bc31819f9c5ce66836d8c1422", "exact_match": 0.0}
{"doc_id": 3337, "doc": {"question": "who sang the original scooby doo theme song", "answer": ["Larry Marks"]}, "target": "['Larry Marks']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the original scooby doo theme song?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Casey Kasem"]], "filtered_resps": ["Casey Kasem"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "192ddfd054028727c3107d62162af2fe040e990d16a93fa4376f7b89a46fe5c4", "prompt_hash": "e022d0f95a12de45e2975c89eb1336275d9cbad4d0b8f4b94e675edeae5cfda6", "target_hash": "f0b14e0af7c20db16b427949711d5818db330901788374ed961d86db054c33c5", "exact_match": 0.0}
{"doc_id": 3338, "doc": {"question": "who started the guinness book of world records", "answer": ["Sir Hugh Beaver", "Sir Hugh Beaver,", "Hugh Beaver", "Norris and Ross McWhirter", "brothers Norris and Ross McWhirter"]}, "target": "['Sir Hugh Beaver', 'Sir Hugh Beaver,', 'Hugh Beaver', 'Norris and Ross McWhirter', 'brothers Norris and Ross McWhirter']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who started the guinness book of world records?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hugh Beaver"]], "filtered_resps": ["Hugh Beaver"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f265376327098f123db8063ba4b8dd910074489c3cec4b3fac469d97b2068b5c", "prompt_hash": "baeb4bff7ba279ef262bb5bebedec328bbddbe4c705b5923966512aca0bb6deb", "target_hash": "821450a07d419e60ddce1193637493c511d3328c24e4bca4ef538fec2e12a446", "exact_match": 1.0}
{"doc_id": 3339, "doc": {"question": "where do red ear slider turtles lay eggs", "answer": ["a hole"]}, "target": "['a hole']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do red ear slider turtles lay eggs?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" in the soil"]], "filtered_resps": ["in the soil"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "190a0c1a95cc3f4be4ea2cb951b4b0b3d4cf5db7ca947a1da661ea373956451a", "prompt_hash": "7be99aa393f47d5b7518ad580a8f05dff1feb07c85aca3ce9085f339d17cc1b3", "target_hash": "0b01185c2de968d76f8f078a3697f43d9f96a5c16038b7c45e0c1112fd0c9e35", "exact_match": 0.0}
{"doc_id": 3340, "doc": {"question": "when is season 7 race to the edge coming out", "answer": ["February 16, 2018"]}, "target": "['February 16, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is season 7 race to the edge coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3fce2f2104bb22133ae8953697ca3f423386d299d4560db98ff07198f3bbb443", "prompt_hash": "a261832550629da1693b2920954a2920e677420150712c875ceb693414f14abc", "target_hash": "260f05011bf32343848494aa63725e0d0bd1d953a2ab4e1298c1071ffe374e4b", "exact_match": 0.0}
{"doc_id": 3341, "doc": {"question": "who is darrell brother in the walking dead", "answer": ["Merle Dixon"]}, "target": "['Merle Dixon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is darrell brother in the walking dead?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Eugene Porter"]], "filtered_resps": ["Eugene Porter"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7056eddd015844866f5c1691a660e7e7c03afda0f5371896b72f8c5f160891d4", "prompt_hash": "38b8c247d9ac8b36ca3521a345ed8816e3409bc6723660168bb9712573aef694", "target_hash": "0deb090c66cc7b2cbd502da80e8f8d988706d29ce401fc02d822073e548b647e", "exact_match": 0.0}
{"doc_id": 3342, "doc": {"question": "which mirror is used in vehicles for rear view", "answer": ["rear-view mirror"]}, "target": "['rear-view mirror']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which mirror is used in vehicles for rear view?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" convex"]], "filtered_resps": ["convex"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4aaa7da43380ea2db44b5664f361e5a5974d5c9c01c6d2c60f124b55638070a9", "prompt_hash": "fe6d9e5d04eb0ae8b4d8031dfaa86851facc8b12da2951b27c891ae71a03d3df", "target_hash": "f78496f1405bde89859f433fd8ece3ebb94b48ed29f208d578a607f4e6983e30", "exact_match": 0.0}
{"doc_id": 3343, "doc": {"question": "who did cora marry in once upon a time", "answer": ["Henry"]}, "target": "['Henry']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did cora marry in once upon a time?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Swan"]], "filtered_resps": ["David Swan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96e729f9011109b9e6cc2831ed359dc53878c45c0239eb488395b421a334f653", "prompt_hash": "fabbc292d607e4705fe305f496ac47bab05604245243edf43ee5ca20b4662500", "target_hash": "aeef50edba9d00c25fe9c95fea898eea44686a2482938da4d59b3beb14d58ca8", "exact_match": 0.0}
{"doc_id": 3344, "doc": {"question": "who sings tie a yellow ribbon around the old oak tree", "answer": ["Joyce Vincent Wilson", "Pamela Vincent", "Tony Orlando and Dawn"]}, "target": "['Joyce Vincent Wilson', 'Pamela Vincent', 'Tony Orlando and Dawn']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings tie a yellow ribbon around the old oak tree?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tony Orlando"]], "filtered_resps": ["Tony Orlando"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d7ba7bbc5f1337c6ae080dc5c3c51d68a62f087d88b18d41c8430354a53b0b8a", "prompt_hash": "3ed60c6fa3932d03c331e4b82f487df0bcad34b3fbb7adb433137c298cdeabc8", "target_hash": "f64873acadd629989c9d232130bfdb43e896674615a79c4fea65251992430709", "exact_match": 0.0}
{"doc_id": 3345, "doc": {"question": "where did the crown of thorns starfish come from", "answer": ["Indo-Pacific"]}, "target": "['Indo-Pacific']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the crown of thorns starfish come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Australia"]], "filtered_resps": ["Australia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a880ee5a6e43c1a50675f5d6e2498af117f42ac24003af6bc2cabb24422521d6", "prompt_hash": "a1c2c449fabcba349439ebcb9c68606862577c39d94c5b0b5745538fe8736116", "target_hash": "d89016161637f21110ef1c65b65b9872fd6c8594f77defe7d2432cf2b280fe83", "exact_match": 0.0}
{"doc_id": 3346, "doc": {"question": "where was the diary of a wimpy kid filmed", "answer": ["Vancouver", "Richmond, BC", "Coquitlam, BC"]}, "target": "['Vancouver', 'Richmond, BC', 'Coquitlam, BC']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the diary of a wimpy kid filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Los Angeles"]], "filtered_resps": ["Los Angeles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "00d5295d377624eddd429ac7bb372bc61edd002a670b6385cca3ae3ad0e2f6de", "prompt_hash": "fc79db5a1bdc54835c6f5612c5bdfa7ebdd7b6455ff690c040b9ffb5dae1f816", "target_hash": "e86a3588297b3a89295d27032beca291ffc65e32f37c1304c845d50960c94d17", "exact_match": 0.0}
{"doc_id": 3347, "doc": {"question": "where does the electron transport chain pumps protons", "answer": ["mitochondrial membrane", "across a membrane"]}, "target": "['mitochondrial membrane', 'across a membrane']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the electron transport chain pumps protons?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" chloroplasts"]], "filtered_resps": ["chloroplasts"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fdc34a156b20c50513bdeda46c3cda8ee6e19881fa901bfcb0dbd7316ce7e4ba", "prompt_hash": "11ab211937bfae9c7b3c8cc41b643083e896c594858156cdc41dac8fcdd9e9c8", "target_hash": "03d3681463c63cf1134ac028c8c71a2955c3bbe5362e72948e138524a2442aa2", "exact_match": 0.0}
{"doc_id": 3348, "doc": {"question": "where is the world cup being held 2018", "answer": ["Russia"]}, "target": "['Russia']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the world cup being held 2018?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Russia"]], "filtered_resps": ["Russia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7c3cf5eddbc16d956256243f45d609e653d6f189005a6017cfb6fce383102736", "prompt_hash": "c67a62c31e2d9b2b03ee728324f1bda8bcc70a17ccb376bc9925c9460234bc1d", "target_hash": "59825c07a9ba4fa16c7cea476df51096b732bf4a91071c94bb2141380e95a6d2", "exact_match": 1.0}
{"doc_id": 3349, "doc": {"question": "what features of muscle contraction can be determined from an emg (electromyogram)", "answer": ["electric potential generated"]}, "target": "['electric potential generated']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what features of muscle contraction can be determined from an emg (electromyogram)?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" amplitude"]], "filtered_resps": ["amplitude"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0c8727311cd973453b680ea77afaaaf8d8bc1425bfd0ee153d51e485a5db4236", "prompt_hash": "e55b4b70d8fbdf6c5a2f3e249960c3785cdb3a41ef7f1e448d75ea17028aed78", "target_hash": "2190f648a204b5ac16cfa64561a4f209da4769104baa97bab055919b84a87877", "exact_match": 0.0}
{"doc_id": 3350, "doc": {"question": "what is the minimum dollar amount for federal court", "answer": ["$75,000"]}, "target": "['$75,000']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the minimum dollar amount for federal court?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" $75"]], "filtered_resps": ["$75"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ec7f04930c777ca5c848415337f7d789231b00682cff21a4bace0742fa463887", "prompt_hash": "54e9ee157899cc903b70c07b0277fc471f6d980a1ce5d44f4b3b358317fdcec4", "target_hash": "75f33e2d82630d5b11a2f6d62ebe64ec1b919db4e31ef500e04a8751172d687c", "exact_match": 0.0}
{"doc_id": 3351, "doc": {"question": "french film pioneer who introduced newsreel in 1910", "answer": ["Charles Pathé"]}, "target": "['Charles Pathé']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: french film pioneer who introduced newsreel in 1910?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Georges Méliès"]], "filtered_resps": ["Georges Méliès"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb46028d594d2c0556a90ec00ff685d9cb0ca343c2a0506bc4b1adf7dd40cefe", "prompt_hash": "648d7009a4c4d4adc9a2cfc0347d6ce4a58c8575aa34ae72d48a7242788d73ce", "target_hash": "3267d60500b950fbc37cb9e43b51ec07269cd0799fc198bb4e442b2c8d3e073b", "exact_match": 0.0}
{"doc_id": 3352, "doc": {"question": "the victim was asking for it is an example of a", "answer": ["Victim blaming"]}, "target": "['Victim blaming']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the victim was asking for it is an example of a?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ad hominem"]], "filtered_resps": ["ad hominem"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "088c1f306242c04bb42584a2f8652ecbda31d77db6cff6da00a2ababeea35d5f", "prompt_hash": "fdddb8acc7b24135e1a29325a7b1b75583a3b38451f0d3639c47e1dd161f5339", "target_hash": "9af25a5cf77abc662bef6894717826a8f442c2da5bb5d318aee9a7a8a07457a4", "exact_match": 0.0}
{"doc_id": 3353, "doc": {"question": "where are antibodies made and by what type of lymphocyte", "answer": ["B cells", "lymph"]}, "target": "['B cells', 'lymph']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are antibodies made and by what type of lymphocyte?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" B lymphocytes"]], "filtered_resps": ["B lymphocytes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5be8c539402b2b7d5a3deb0b6f1b926cc98a2aea81633141ddf6a9aad26acfdd", "prompt_hash": "761040ee175da63a5ae4e4785864e5641e5cba44424f9e2c4154c786797cbd84", "target_hash": "b591c36a7be8c7f5d5ba7a7d243e7dd04747c16002913a2484f4058007b41a94", "exact_match": 0.0}
{"doc_id": 3354, "doc": {"question": "where does kevins family go in home alone 2", "answer": ["Florida"]}, "target": "['Florida']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does kevins family go in home alone 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Tokyo"]], "filtered_resps": ["Tokyo"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8ac56ab00b6b80d1337f767c4173ba56987163c22d874d401d361587d7ad5575", "prompt_hash": "8b1921e24cd700232b9b73976eaeb7ccbae530b97898d3eca72d677199069af2", "target_hash": "3a10045e60753262513a93b91b133b58cb1ba7f091839dc5fb133bfd7baa4547", "exact_match": 0.0}
{"doc_id": 3355, "doc": {"question": "list all the planet of the ape movies", "answer": ["Planet of the Apes"]}, "target": "['Planet of the Apes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: list all the planet of the ape movies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rise of Planet of the Apes"]], "filtered_resps": ["Rise of Planet of the Apes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fc782b32f6995f91df060efae10998c7552e88c2c35b58b821aec562b7fba6ed", "prompt_hash": "16962a091da7f9dffe947d2535c692a585dc748f356b7a295456e23181fcc073", "target_hash": "7843bd31f4a1f3e8eee2d095817356df5b4ed9d099c58af54601a41d71430d03", "exact_match": 0.0}
{"doc_id": 3356, "doc": {"question": "who designed the first toy released by nintendo", "answer": ["Gunpei Yokoi", "Fusajiro Yamauchi"]}, "target": "['Gunpei Yokoi', 'Fusajiro Yamauchi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who designed the first toy released by nintendo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Gunpei Yokoi"]], "filtered_resps": ["Gunpei Yokoi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "fabd95a41f58e8a24b7fb3050e61c83849cbcc586173e985039472162a87077b", "prompt_hash": "76b5339118c2bbd47e53ddc2fb576aa8286fe39e3c4d7228d2e6116321840922", "target_hash": "33be381a16ecf9be6096d80edef50da417654de44c25ade102906119a9e9e737", "exact_match": 1.0}
{"doc_id": 3357, "doc": {"question": "who won entertainer of the year country music awards", "answer": ["Garth Brooks"]}, "target": "['Garth Brooks']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won entertainer of the year country music awards?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" George Strait"]], "filtered_resps": ["George Strait"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42368e99195d9659f6c1e1f9b22e03152cb335ffe4670c73dd3f2623936c7708", "prompt_hash": "68d67d369927af0f0ea41709071e9a87c4068afb00a15518b818ff78cdaf66c7", "target_hash": "5ef068ead7c95ec8c5454c0d5d483778539a6263e8123fc832f73c404d57de9b", "exact_match": 0.0}
{"doc_id": 3358, "doc": {"question": "who wore number 7 for the green bay packers", "answer": ["Don Majkowski", "Quarterback Don Majkowski"]}, "target": "['Don Majkowski', 'Quarterback Don Majkowski']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wore number 7 for the green bay packers?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brett Favre"]], "filtered_resps": ["Brett Favre"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d163e7ae4343fa231ac1d7d4d8a6ca4589a71a01130561941b959d526f21e4da", "prompt_hash": "34b807ac56808cba6e554437e190e6765d4c11ddf748def8ea841e5725d4a0fc", "target_hash": "12ed4f7f6c418a34a0c573f8a64f68d9b57c896a618c0e721fe64ef75da32fba", "exact_match": 0.0}
{"doc_id": 3359, "doc": {"question": "what are bulls used for on a farm", "answer": ["as work oxen", "slaughtered for meat", "herd maintenance", "breeding", "work oxen for haulage"]}, "target": "['as work oxen', 'slaughtered for meat', 'herd maintenance', 'breeding', 'work oxen for haulage']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are bulls used for on a farm?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dairy"]], "filtered_resps": ["dairy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "995a95d200f2e156fc28fec5d26676208d3893d82607fc5a97a2a805ac6dab58", "prompt_hash": "7a17ac7dd7cda9a3cc5541746f95d0f59072149e8b59bc66dae281f999bb0b36", "target_hash": "7dcaf3b7d4064220449455842d1ca2f8d80fdb21f14f63fec06c8e4d75f78782", "exact_match": 0.0}
{"doc_id": 3360, "doc": {"question": "when do mr schuester and emma get together", "answer": ["the fourth season"]}, "target": "['the fourth season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do mr schuester and emma get together?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 5"]], "filtered_resps": ["season 5"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1a0f35a4ebedc3101d30055e706f5bcccaaea19137e832c6ffe8353d9046a501", "prompt_hash": "e090f4f7ad4e716c34eaf5924e8c4b29d911b030ccf19a91b5f045d1bdc990c1", "target_hash": "ec86ffff070f28da6d17fc9d3c87a7b5979f8d06ae79616a4a7974de835f15a0", "exact_match": 0.0}
{"doc_id": 3361, "doc": {"question": "who is playing halftime show super bowl 50", "answer": ["Beyoncé", "Coldplay", "Bruno Mars", "the British rock group Coldplay"]}, "target": "['Beyoncé', 'Coldplay', 'Bruno Mars', 'the British rock group Coldplay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is playing halftime show super bowl 50?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bruno Mars"]], "filtered_resps": ["Bruno Mars"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4c87069979afc977a8da01368a6a0186e082a514bbf0b29aff57b853487e23d7", "prompt_hash": "5f9a45312a0b43916fd919693580af6b11ba0d8e2ab8b96a832dd33bb8fc97aa", "target_hash": "946932ad0edc60f8d39b2f18b65b79ce115ef2eb2e18900440b9a8f5426bd52e", "exact_match": 1.0}
{"doc_id": 3362, "doc": {"question": "when will the la sagrada familia be finished", "answer": ["by 2026", "2026", "2026–2028"]}, "target": "['by 2026', '2026', '2026–2028']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will the la sagrada familia be finished?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2026"]], "filtered_resps": ["2026"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f8aa2c39d10bcd20dab67c38bab984e190471df77a4ca3477396bfea70a9cb58", "prompt_hash": "608da7a21ea4eec53d2776fc63070e90db8b79ebf9da858fbbe8ebffe2278e78", "target_hash": "c16481cdfcd471563fddf1d086327d6c619396b7c17cf2a0eb0ce8c643e356a9", "exact_match": 1.0}
{"doc_id": 3363, "doc": {"question": "who wrote from now on from the greatest showman", "answer": ["Joseph Trapanese", "John Debney", "Benj Pasek and Justin Paul"]}, "target": "['Joseph Trapanese', 'John Debney', 'Benj Pasek and Justin Paul']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote from now on from the greatest showman?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Benj Pasek"]], "filtered_resps": ["Benj Pasek"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "51f3d7341e0421bacaf5765e6316136a2fa7893685dc47f0a5972599f6bb6a32", "prompt_hash": "1781dfdae34a1f45745e4a9443127534e41535894597b7dd8ddc36e655133700", "target_hash": "1ebb32f5c61f09a065a3d9101fd634c4fb2f144bf986a9f8a8384ad8cf804657", "exact_match": 0.0}
{"doc_id": 3364, "doc": {"question": "where is final four being held this year", "answer": ["the Alamodome"]}, "target": "['the Alamodome']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is final four being held this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Houston"]], "filtered_resps": ["Houston"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "339dde178afaba164cf8e4ead91611b1eda337fdc75f6559de98b89dc052f8bb", "prompt_hash": "edc2dd65661c968f2ed3e9318b1dc59793deb1ac5693168374664d3b0d508e3e", "target_hash": "18599df012c07e499ad90cc01dbb290ec2f1f56e9e4a05327548c6e1aa6f7605", "exact_match": 0.0}
{"doc_id": 3365, "doc": {"question": "when was the last time arsenal win premier league", "answer": ["2003–04"]}, "target": "['2003–04']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time arsenal win premier league?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2003–04"]], "filtered_resps": ["2003–04"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "57adfbf87dc9d86d2bafceb6b4d9a0ccf4a21313f069e97953d6aa0186c09c60", "prompt_hash": "209f798cb2b3941f2dceca752014449b5d192c9bd8b38e3fbab97e472dc83fbc", "target_hash": "f31ee1bc9aa8d4aba61e2458bfd1eaa67b729cd74caa6b55d003426ce7c3bb51", "exact_match": 1.0}
{"doc_id": 3366, "doc": {"question": "who coined the phrase let them eat cake", "answer": ["Jean-Jacques Rousseau"]}, "target": "['Jean-Jacques Rousseau']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who coined the phrase let them eat cake?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Marie Antoinette"]], "filtered_resps": ["Marie Antoinette"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6717dc45868cb94578af3c6b12d74413688ac9559ca0f93f0a542607a67f3758", "prompt_hash": "153b6a6242c804a0c3bcaf600d8d7fdeb39f1a1e81bbf3353021706ec0db7a36", "target_hash": "d13010b60af4060b17acf902a682fc2f9242e5a0b0671cd248a46f59fc638e33", "exact_match": 0.0}
{"doc_id": 3367, "doc": {"question": "who pays medical bills in great britain where does the money come from to pay these bills", "answer": ["general taxation", "taxes"]}, "target": "['general taxation', 'taxes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who pays medical bills in great britain where does the money come from to pay these bills?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" National Health Service"]], "filtered_resps": ["National Health Service"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "80aafa02457bbd578b7d1554ecf547dcad2f989101216317ce45f926169ea05f", "prompt_hash": "e7e16fe41a0dd1e35156691e3414cea58e291fd65c6b03ada06a4043ef1c2d91", "target_hash": "663c5fcbac2522f6f12188caee5bd808d60ad875044ece014e6bbc84850fe708", "exact_match": 0.0}
{"doc_id": 3368, "doc": {"question": "when did russia join the world economic forum", "answer": ["1997"]}, "target": "['1997']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did russia join the world economic forum?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2007"]], "filtered_resps": ["2007"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eea8422bcf08a350cb4825df595f6c7698766b6f73d4ee6be688d18009967a8e", "prompt_hash": "418374decdcb9d041799cdb9137d72b1c01f96309665d2675dae2b6523eb92cc", "target_hash": "982df96f59043cb182196071792c2904ff376f72b15cb9b414721d5b74a296b0", "exact_match": 0.0}
{"doc_id": 3369, "doc": {"question": "how can you remove the defect of vision presbyopia", "answer": ["Eye glasses"]}, "target": "['Eye glasses']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how can you remove the defect of vision presbyopia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" reading glasses"]], "filtered_resps": ["reading glasses"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2bf0f7e3f50a9529f6eeb2bf706ef38b93e3e55c942f7b9da78edbe33a2f0db", "prompt_hash": "50c76c0c2724fc429b270236be1448ac9e64b8180fc2df10a28a506fae77d113", "target_hash": "2798938928beb4d65a0718c2674b0aaf79d1f2f33d9342dfa1033b4736451851", "exact_match": 0.0}
{"doc_id": 3370, "doc": {"question": "who sang i put a spell on you in hocus pocus", "answer": ["Bette Midler"]}, "target": "['Bette Midler']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang i put a spell on you in hocus pocus?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sarah Michelle Gellar"]], "filtered_resps": ["Sarah Michelle Gellar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7fd5ce34f30603f91eeeb4e00c4e0c6e8a59da8b3bac3fdb67a4e2b13e2fc116", "prompt_hash": "a48da79bd679d31ffc414b3b55eb1fa8beb7149da5a249a70c5cedc7eadc49f7", "target_hash": "836dfe7d4dd1b016f2013643c72fcd2b3f91e029c7d3ffe76b60a0e11cc7744f", "exact_match": 0.0}
{"doc_id": 3371, "doc": {"question": "who was executed for being an american spy during the revolutionary war", "answer": ["Nathan Hale"]}, "target": "['Nathan Hale']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was executed for being an american spy during the revolutionary war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nathan Hale"]], "filtered_resps": ["Nathan Hale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0e68f114d42d26e95d30802100162b01f5844ca95adffb6837afed518d69fe0e", "prompt_hash": "d9aa0a9facc06232acac6b3a60fa67d115d3939075a9b6b04be72015cd617bad", "target_hash": "332fb0d7901eb12c90219e20dd15764d75880ecb53a5694b8c66608509b692f4", "exact_match": 1.0}
{"doc_id": 3372, "doc": {"question": "what is the rank of india in economic growth", "answer": ["12 / 225", "12"]}, "target": "['12 / 225', '12']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the rank of india in economic growth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e2bd124782a725de3857c4a2548c1a83fd7136acc8b92774908c6d87ce3b50ff", "prompt_hash": "7848dce0b75f7ed5319666a40395a6affe043343dac9fe953af31aafc74c7831", "target_hash": "b2961107d9cf220af2718f532490dd08982be5a6d8aa6972842fffa4c1944a92", "exact_match": 0.0}
{"doc_id": 3373, "doc": {"question": "which body part(s) occupy the greatest portion of the primary motor cortex", "answer": ["face", "the human hands"]}, "target": "['face', 'the human hands']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which body part(s) occupy the greatest portion of the primary motor cortex?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" hand"]], "filtered_resps": ["hand"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bac1618644fc3b63f687d61e8788854652fff7b101f3137e98f11b6265ac2514", "prompt_hash": "0b6f4bcbb37df061ac762a4d820301370cc7c3c7bb5beafba79d475cc313008b", "target_hash": "9c161311a4babe148bc1ba3fbf0c2fa0d1a568e3c0601b8ca84df3d3f58965bd", "exact_match": 0.0}
{"doc_id": 3374, "doc": {"question": "how many nuclear power plants in washington state", "answer": ["3"]}, "target": "['3']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many nuclear power plants in washington state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2"]], "filtered_resps": ["2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b11f80deeb847eec37f515d932726477438e1eb62fe88d707e67f11e2c5ecbb8", "prompt_hash": "6f7c56b1d05a7e225a1eb4127c7a9a63b6f273c92fed93292871ed7c8cbfb16f", "target_hash": "e8230ace9f0554c09f639f7a596544b0277f9345d9c2a486d6607bf01c25927e", "exact_match": 0.0}
{"doc_id": 3375, "doc": {"question": "what time can you legally buy alcohol in michigan", "answer": ["7 a.m."]}, "target": "['7 a.m.']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what time can you legally buy alcohol in michigan?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10 a"]], "filtered_resps": ["10 a"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a170194875546057cb27d9f6a56e369a35b278a1f04412d4ebab47336f239d9b", "prompt_hash": "aefcc2c0b6271cbf8f5a76c577638f625e4d1b1db4cd31e35786b204f7a3feff", "target_hash": "66e28e05e5805c0d2838f34750390fdc797928c059b8ec76157bc8f45fff8173", "exact_match": 0.0}
{"doc_id": 3376, "doc": {"question": "who was the book of philippians written to", "answer": ["the church at Philippi", "Philippians"]}, "target": "['the church at Philippi', 'Philippians']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the book of philippians written to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the Philippians"]], "filtered_resps": ["the Philippians"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "26175309eaff5ca29b6dc9ef6830ae9660175f15c388813ed351e8a421b6e833", "prompt_hash": "34416974f2442bd762fa3e63d5480a8f721394f4c437d2cf906114278529f73c", "target_hash": "453729a1fe27ac59616d391116e333d73961ee7993e2fa6858b7c6eef7bc0cec", "exact_match": 1.0}
{"doc_id": 3377, "doc": {"question": "when does the 14th season of grey's anatomy come out", "answer": ["September 28, 2017"]}, "target": "['September 28, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the 14th season of grey's anatomy come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" March 16"]], "filtered_resps": ["March 16"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f3fc3f07b897e1d663dee3d220c165f325e4f2aba9ad8ecd97062d50eb5ba6c4", "prompt_hash": "b06727c494cdc8e6da04e96934432d7f30896181d0570a45fc735413c1f6fbc1", "target_hash": "a4fdd9da745f85ad943d5a99d7a421374e3398ec1f207825487bef2fd9eaa8ca", "exact_match": 0.0}
{"doc_id": 3378, "doc": {"question": "where does the great outdoors movie take place", "answer": ["in Pechoggin, Wisconsin", "Wisconsin"]}, "target": "['in Pechoggin, Wisconsin', 'Wisconsin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the great outdoors movie take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "388d25bb6071dcd1edd537e5e77dcc054de330b372398ad96d47f5ac11de6ab8", "prompt_hash": "c85e7693a64e7604fbe567384e9b054008b8e8883d0fbbd3c01a5153c612d4dc", "target_hash": "537545b9ee32c8a9baf8f9846a94d7537b7a6a94d9a3406ec8f0f737b8c9e74a", "exact_match": 0.0}
{"doc_id": 3379, "doc": {"question": "what is the shannara chronicles season 2 based on", "answer": ["The Sword of Shannara Trilogy"]}, "target": "['The Sword of Shannara Trilogy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the shannara chronicles season 2 based on?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Wishsong of Shannara"]], "filtered_resps": ["The Wishsong of Shannara"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "84bbd57895f7fc3d27d50c66faf535aa218518319c22d560fd5388e7810c69fb", "prompt_hash": "63b6a1c076260078245bb056142fb80dbd8b202f019fa544177bb7f3777999ec", "target_hash": "25224e534fcbbe2183b2b9b831051b0b006f2c3d8f4a38cc38e1639a496c1ee3", "exact_match": 0.0}
{"doc_id": 3380, "doc": {"question": "where do they grow hops in the us", "answer": ["Yakima (Washington)", "Idaho", "Washington", "western Canyon County, Idaho", "Willamette (Oregon)", "Oregon"]}, "target": "['Yakima (Washington)', 'Idaho', 'Washington', 'western Canyon County, Idaho', 'Willamette (Oregon)', 'Oregon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do they grow hops in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Oregon"]], "filtered_resps": ["Oregon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "87c7ab0296fe4c93edd6bfc699e118b35e25edc6fa431c4e220801765b7248f7", "prompt_hash": "51ec2e47291e2ccc84a2b03c9e54d88e17bbb311f2740fe20cdaad35a5ec12fc", "target_hash": "d1c9ba0164a01f4433668c6548f8128cd4074a7f3a3ead9296ee65bb6d683ff2", "exact_match": 1.0}
{"doc_id": 3381, "doc": {"question": "who acts as the chairman of the election commission", "answer": ["The Chief Election Commissioner", "Achal Kumar Jyoti", "Chief Election Commissioner"]}, "target": "['The Chief Election Commissioner', 'Achal Kumar Jyoti', 'Chief Election Commissioner']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who acts as the chairman of the election commission?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chief Election Commissioner"]], "filtered_resps": ["Chief Election Commissioner"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2b5abc36f7b85975243ec768efa2bac77d4f9ad34d113dc2ddf84d268b1795e3", "prompt_hash": "e21194dc0cf83b0a10fdfe48b7d4300a870ebff3915fcf4fbed22d77c3f33f86", "target_hash": "f20e21736bb6e95ea6fa66c202198ea1f2c7fc9373e5460360360edeb4dd0cca", "exact_match": 1.0}
{"doc_id": 3382, "doc": {"question": "when was the seat moved to the capitol", "answer": ["November 17, 1800", "1800", "December 1800"]}, "target": "['November 17, 1800', '1800', 'December 1800']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the seat moved to the capitol?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1819"]], "filtered_resps": ["1819"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6d7a7df770e35a7494f01c403b4cfe79ab77340ef9140602b971bb533dd73e5b", "prompt_hash": "9cb0cd76386f87a78ae656a69efd1c755c457413f86eb9b21e1137f77b0801e4", "target_hash": "e4b03af729ac344a8d920c2013bdc86effe16753709ba8cbd4bd491a6d329ba8", "exact_match": 0.0}
{"doc_id": 3383, "doc": {"question": "where does patience is a virtue come from", "answer": ["\"Psychomachia,\""]}, "target": "['\"Psychomachia,\"']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does patience is a virtue come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" television"]], "filtered_resps": ["television"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f1da5b06e64cddb66799028767dca5692bdfb99d178904ae8793516a9d08cf2f", "prompt_hash": "704c6d845560f885bbe0f9c94151dcd710b3573dea44302952269687f853146a", "target_hash": "a52c2d8ae13f84fb4bd0795549c474b9bb310bcfd7d39067fcde262bc6aed602", "exact_match": 0.0}
{"doc_id": 3384, "doc": {"question": "where was thomas and the magic railroad filmed", "answer": ["the Isle of Man", "Isle of Man", "Toronto, Ontario, Canada"]}, "target": "['the Isle of Man', 'Isle of Man', 'Toronto, Ontario, Canada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was thomas and the magic railroad filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" England"]], "filtered_resps": ["England"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4983302928b8a94b9f99aeedd7369c293d865771b5fc8bc8bd20d8100cc0c06f", "prompt_hash": "e274a9e89bcbc3ea364eb03ccd7b64c4ccc5903fc13653a554895c630ef4baf1", "target_hash": "502f573aa2749ad6f05a032d7d28ea35d4235a83a3e5844708596bb2070d0796", "exact_match": 0.0}
{"doc_id": 3385, "doc": {"question": "how long did the menendez brothers get in prison for killing their parents", "answer": ["life imprisonment", "life"]}, "target": "['life imprisonment', 'life']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long did the menendez brothers get in prison for killing their parents?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 22 years"]], "filtered_resps": ["22 years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a9d6ad4464e84a16e6974001b2fd24c283457eba94a075b2fa3bf7f7610e88a5", "prompt_hash": "e760ef5c676b06c10d573117e56649e733b380194428c4e95d329de27a89ff21", "target_hash": "5d2b47dbccadcfb7bdaf4583db9c198d16d526f991faead2f698892d767eec2d", "exact_match": 0.0}
{"doc_id": 3386, "doc": {"question": "cast of a nightmare on elm street 1984", "answer": ["Heather Langenkamp", "Ronee Blakley", "Nick Corri", "Johnny Depp", "John Saxon", "Robert Englund", "Amanda Wyss"]}, "target": "['Heather Langenkamp', 'Ronee Blakley', 'Nick Corri', 'Johnny Depp', 'John Saxon', 'Robert Englund', 'Amanda Wyss']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: cast of a nightmare on elm street 1984?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Robert Englund"]], "filtered_resps": ["Robert Englund"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "792473a3c27527e83028cccc2e63312ccf4cac6d7cc476cd51926bd483d764f5", "prompt_hash": "a1f36ddd73e75ca0d12728dcd346ec861d1467c8452539a2596e879751d22be8", "target_hash": "6fdc36e80d0e0ba3dd6b447b29ed74dd365d76bd000aa490c0de2c21f448809e", "exact_match": 1.0}
{"doc_id": 3387, "doc": {"question": "when are general elections held in the us", "answer": ["every two to six years"]}, "target": "['every two to six years']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when are general elections held in the us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" every two years"]], "filtered_resps": ["every two years"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "39d205e1bfa808bbbbf9fba448447f8d30f2d76939651d73b4d2696559a449a0", "prompt_hash": "38cbe80e8dc6fedf09b44b6401e67b3a89fb1a6415b6dc4e220e4e005464bb16", "target_hash": "69d5461f91064f80bdce8ceb0fef2e4be754a6f20eb7f115a2f514f376547eeb", "exact_match": 0.0}
{"doc_id": 3388, "doc": {"question": "where was the louisiana purchase signed in 1803", "answer": ["Paris"]}, "target": "['Paris']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was the louisiana purchase signed in 1803?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Washington"]], "filtered_resps": ["Washington"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f04e9b94c6c1b7e5fcaae55671eeb61d2c4f1ac05ca2646a348634f4a9ef59c4", "prompt_hash": "7dae13e4aed902a6daacf59675b3ea85c30859ec8b6fd77fac33ccc3e75a7312", "target_hash": "5c16081dc8c6c87a151bfa0a53e1e0f4f6af5155559b51f0e6e73b14ccaf13b7", "exact_match": 0.0}
{"doc_id": 3389, "doc": {"question": "who is the chief economic advisor of india", "answer": ["Arvind Subramanian"]}, "target": "['Arvind Subramanian']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the chief economic advisor of india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arvind Subramanian"]], "filtered_resps": ["Arvind Subramanian"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "201efaa40742988d4f9cb8e971a742a0f3cc24d539cfad7f0e1ba79e0e40ff95", "prompt_hash": "de767d4c4e3a45d059288a419c24c5ea97c8bac26160829d5cc8ff2c14f02a80", "target_hash": "5a2e9e6297baa6826a96c741f6a6ff78844ac944776420e0aaab62d1be36830b", "exact_match": 1.0}
{"doc_id": 3390, "doc": {"question": "when did jelly beans became associated with easter", "answer": ["during the 1930s", "sometime during the 1930s", "1930s"]}, "target": "['during the 1930s', 'sometime during the 1930s', '1930s']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did jelly beans became associated with easter?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1930"]], "filtered_resps": ["1930"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ecfa51358e8bc27b3539778ec9e2c82f7bdf7853001daf8f846b14a4f2a52aad", "prompt_hash": "c24234ade478d63ac061e066dd605da4b8c997dcb24e36770e924731bd04ff95", "target_hash": "7faf6bb4f0a885ed11dd6b75c6f5721d0e83809bfb2d8f7725b69a2e6375261e", "exact_match": 0.0}
{"doc_id": 3391, "doc": {"question": "where do the astros play for spring training", "answer": ["Osceola County Stadium", "West Palm Beach"]}, "target": "['Osceola County Stadium', 'West Palm Beach']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do the astros play for spring training?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arizona"]], "filtered_resps": ["Arizona"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "317e07bd69f1c031e635ddba093c44dec5b49804429a1f6f7d29f059b6e15686", "prompt_hash": "663435dc5912b7d01952f8028e38285e6f4147af2bed7c277abeb5550ff0f66f", "target_hash": "30dde0d118d852424c03f16098add5e4e50eca313cff50eded15a49f02c9cfca", "exact_match": 0.0}
{"doc_id": 3392, "doc": {"question": "who does the music for shimmer and shine", "answer": ["Bobby Tahouri", "Joachim Svare", "Joleen Belle", "Melanie Fontana"]}, "target": "['Bobby Tahouri', 'Joachim Svare', 'Joleen Belle', 'Melanie Fontana']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the music for shimmer and shine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Bowie"]], "filtered_resps": ["David Bowie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bdfc4944f954d15a2fc172fa45bba523b33aff15fca7956adfae74202a50f09f", "prompt_hash": "5e31ea1fa9a279a666dcc247f02c8922426674fce63f1cdea548591455470ced", "target_hash": "863235f91b295781b55a6102ab7656080fcd3189802df18ab958fe3f69f8bfa5", "exact_match": 0.0}
{"doc_id": 3393, "doc": {"question": "who starred in the film far from the madding crowd", "answer": ["Juno Temple", "Carey Mulligan", "Tom Sturridge", "Matthias Schoenaerts", "Michael Sheen"]}, "target": "['Juno Temple', 'Carey Mulligan', 'Tom Sturridge', 'Matthias Schoenaerts', 'Michael Sheen']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the film far from the madding crowd?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Julie Christie"]], "filtered_resps": ["Julie Christie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "074231e11a3e3ab27fd978c1bf5fb9aa59a0cd81f6a4a0ab56f7e39672305efd", "prompt_hash": "b463649045ec5d29b48dc6c9171929e0d6b88ea1f2f8c5a2fc3385a6020bcd8b", "target_hash": "72caad4198f07299d156a37175b22943356285c9441cc1c6a0241df8e81a4483", "exact_match": 0.0}
{"doc_id": 3394, "doc": {"question": "how do you spell padawan from star wars", "answer": ["Padawan"]}, "target": "['Padawan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how do you spell padawan from star wars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" padawan"]], "filtered_resps": ["padawan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a2b0778d2df3214ea7972b4c98b26e2a015e80c17864e7d565da20b4e856647c", "prompt_hash": "2d78af2ed849e0b1d0478eb133b554434790195e598b8309a26caa1bca1bc202", "target_hash": "9e7f43e96f2aae8fc3d0d3c8506b748fa62b25b6776fc4bc5df4800bcbd1dd89", "exact_match": 1.0}
{"doc_id": 3395, "doc": {"question": "who won the mens figure skating in 2018 olympics", "answer": ["Yuzuru Hanyu", "Javier Fernández", "Shoma Uno"]}, "target": "['Yuzuru Hanyu', 'Javier Fernández', 'Shoma Uno']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the mens figure skating in 2018 olympics?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yuzuru Hanyu"]], "filtered_resps": ["Yuzuru Hanyu"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ce27d863c8ec5fd2f1305ecf6334f4f44ec5eafcde47e81d5e8e795739ecc055", "prompt_hash": "2cc89a822acb01b59fd6613680afce35b243f037b7f700a9d61cdef39dfa608e", "target_hash": "e8726f23b8265ee586fabc7e51cac6c372fbd8f1e8a79e0d98332df023187293", "exact_match": 1.0}
{"doc_id": 3396, "doc": {"question": "who is the minister of youth in namibia", "answer": ["Erastus Utoni"]}, "target": "['Erastus Utoni']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the minister of youth in namibia?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Netumbo Nandewarwa Cornelius"]], "filtered_resps": ["Netumbo Nandewarwa Cornelius"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c46a19af5668571585f0ea942faebd4e04c2e504347ee55eeb46e629cebf3a22", "prompt_hash": "cd02740e198eb16463830340d73bb8fa282627882894934a0f324f386d946cbd", "target_hash": "862b10cb0bff9e78f772ec9c8ab343e7752e07d625244edb644d8aa82479b357", "exact_match": 0.0}
{"doc_id": 3397, "doc": {"question": "which river separates the bronx in new york city from manhattan island", "answer": ["Harlem River", "the Harlem River"]}, "target": "['Harlem River', 'the Harlem River']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which river separates the bronx in new york city from manhattan island?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Harlem River"]], "filtered_resps": ["Harlem River"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6143c38db1a085e61b59c24de3fa7a6be0ade93ef1f529f0255d8cf8d33a1ed3", "prompt_hash": "1f7608f76eecc6a53e7ed168f67063aa81bf9c38f79c9fa15f1efac042a9a64f", "target_hash": "078ea4cb1865fc3aa8aa1a7e5c28a8f651d2978a8d330b4e4381459b735232f9", "exact_match": 1.0}
{"doc_id": 3398, "doc": {"question": "do you cut cards to the left or right", "answer": ["right"]}, "target": "['right']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: do you cut cards to the left or right?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" right"]], "filtered_resps": ["right"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "84ade0d9fd6ccb75c27c026856bdb27559143fe661c5a5fde55622db9ba6dc67", "prompt_hash": "0c9db4370704ee317ead956dfbafc8da82ff7cf1cf07a7a8acaab54f3d72f7d2", "target_hash": "f9d41643baedb2f2784903de641eb10d89a7cdc2c640999dcee0b606c98e2a49", "exact_match": 1.0}
{"doc_id": 3399, "doc": {"question": "when was the dome of the rock completed", "answer": ["691 CE", "in 691 CE"]}, "target": "['691 CE', 'in 691 CE']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the dome of the rock completed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 691"]], "filtered_resps": ["691"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9e1b4785726d8f73f396f876d8a6f0a9de893e2304b01e0ead206c9323145f85", "prompt_hash": "e1b2fa81d979417082fcefb77c26d5c0ead28971b495adac43d07d91c456db18", "target_hash": "e15ebb5299b43365fca345b39c7261b1594888a41f78be7743b95023e229c9c5", "exact_match": 0.0}
{"doc_id": 3400, "doc": {"question": "how did long john silver lose his leg in treasure island", "answer": ["in the Royal Navy"]}, "target": "['in the Royal Navy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how did long john silver lose his leg in treasure island?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" shot"]], "filtered_resps": ["shot"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9099793aeb51d9727f4618afb3e85407db7dab47df1ceb152123aef086311f62", "prompt_hash": "2db09c59c6d61f7520a03db20ef9a6051d08c02cf75ce7b80e9a17bcc9d40e06", "target_hash": "7d4a4cd70786e568b77994ebbbdc0badec1ad9374284adde428a9ff30d48864b", "exact_match": 0.0}
{"doc_id": 3401, "doc": {"question": "what is the spelling of lakh in english", "answer": ["lakh"]}, "target": "['lakh']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the spelling of lakh in english?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lac"]], "filtered_resps": ["lac"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "574a6e03d5b2dc24dcb880155b2c358fa7ff30834ed6f0530e072073a13275ae", "prompt_hash": "ab8c77b2ba1aa626bf00c2ee73985f483893a978796dd5eb0d1708cfaeadfef2", "target_hash": "4057549b6ac085dc6336ada2a619f8faf533b6bb6e9fa5a438ac930fc7ea01be", "exact_match": 0.0}
{"doc_id": 3402, "doc": {"question": "what level is a city and guilds qualification", "answer": ["entry level to level 7"]}, "target": "['entry level to level 7']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what level is a city and guilds qualification?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" level 2"]], "filtered_resps": ["level 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3fee9e7ce8c74ef77de167d70253d23c190c0235af56594d6cc902baa17f13d3", "prompt_hash": "2b3f2414e1d08a10b5e75a644ca1f77f5917bf519b642bcb86c72b0c7dffe6b3", "target_hash": "9598839bfd1aa2930c1c5261723d6c89fb94cb094984f67a466eb8510ef614f6", "exact_match": 0.0}
{"doc_id": 3403, "doc": {"question": "when is the last time the dolphins went to the superbowl", "answer": ["Super Bowl XIX", "1984"]}, "target": "['Super Bowl XIX', '1984']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the last time the dolphins went to the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2008"]], "filtered_resps": ["2008"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3374f7c9380a33dbd7c1f45082e01015cd54b2ccf2bea65d987395d45f68d7a0", "prompt_hash": "6b7e50f416129b823030e46e0fbc296194c69d5b049c615cc98cf3cb77f03d4e", "target_hash": "13f3d0a81abd7bc854194bd3c47663d38c521e7ffc3a2f34f9fc3dd2ae1270fa", "exact_match": 0.0}
{"doc_id": 3404, "doc": {"question": "how many seasons has greys anatomy been on tv", "answer": ["14"]}, "target": "['14']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many seasons has greys anatomy been on tv?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 15"]], "filtered_resps": ["15"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1ff718bd1c9ee5152e3d307610e5d2e36a1736ecb315f5daa686b54a59cb4721", "prompt_hash": "3bcebd3dda0fda0ae0d0ab6a1cc6e2670df7558cc7e67c17343eeff3bd34c38a", "target_hash": "1c4adeef9d9168604b52ef50b17fab3bebe77f9e001e8dcaa2e38dfcfc074f31", "exact_match": 0.0}
{"doc_id": 3405, "doc": {"question": "who is the ceo of crate and barrel", "answer": ["Neela Montgomery"]}, "target": "['Neela Montgomery']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the ceo of crate and barrel?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Michael A"]], "filtered_resps": ["Michael A"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d38e664d86bab9b864ad55cf4e4b4d6f3c3de0f78b99b6ddb0dd5494c28b7915", "prompt_hash": "0b69caacc7a81fd94de05126e1713b879abc7c53842f3c7b6ca102d15a5b3cd6", "target_hash": "6135949cad08a16d182e25732dfa7d43bf9cc8c5dbb91dd71a4d607d1b5da0ec", "exact_match": 0.0}
{"doc_id": 3406, "doc": {"question": "where did the tradition of the pinata come from", "answer": ["treats"]}, "target": "['treats']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did the tradition of the pinata come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mexico"]], "filtered_resps": ["Mexico"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1928404b2aa66757208cbdc195080241cc0b092ceb1173c1ba444e7678df55ff", "prompt_hash": "1506ae5abc325d0aa30b18fb503a5050210b4f6509d014328d8ed7d2a1dc6e69", "target_hash": "1110f5e349a9392c5eae5796261096b0dafcd8ddd03eaabc0bef8eb771f6dc5d", "exact_match": 0.0}
{"doc_id": 3407, "doc": {"question": "who is the head a in pretty little liars", "answer": ["CeCe Drake"]}, "target": "['CeCe Drake']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the head a in pretty little liars?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mona Vanderworth"]], "filtered_resps": ["Mona Vanderworth"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "322def9717ef2c70a2465d23daa50939fa38b343d1cc84a3d0c173733331801b", "prompt_hash": "2a492d610ebbc84d59ea56aca444802c77e7a67d34dff06f23cf1780a757d544", "target_hash": "79ed6b22568a6ba53a5f9a5f5e9fc2d423696b59721827d01650521c9ba9669d", "exact_match": 0.0}
{"doc_id": 3408, "doc": {"question": "how many students does monarch high school have", "answer": ["2,406"]}, "target": "['2,406']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many students does monarch high school have?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1"]], "filtered_resps": ["1"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03e61e8a2487fed91f00051361f782e7850365da719000567c3f376f0011a0ba", "prompt_hash": "afa07a8583d8db5263c97477eb528e2c6d11e8b97cfd190b088982a70d861e49", "target_hash": "b1c786d4afa3d23addbfcb53ed2bc0862450749b12ac27372dcc711791886b1a", "exact_match": 0.0}
{"doc_id": 3409, "doc": {"question": "who starred in the movie romancing the stone", "answer": ["Manuel Ojeda", "Alfonso Arau", "Kathleen Turner", "Michael Douglas", "Danny DeVito"]}, "target": "['Manuel Ojeda', 'Alfonso Arau', 'Kathleen Turner', 'Michael Douglas', 'Danny DeVito']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who starred in the movie romancing the stone?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Joan Collins"]], "filtered_resps": ["Joan Collins"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d8173b66629a449a14a414d4ba1fe38860d4744e023d1df1e6a6a5ff74df587", "prompt_hash": "5e834a232736ff5c091ccc1fb2b02003ec24dd0755a22a183228594ba4eb64d9", "target_hash": "bba801ec77a8e022d49ff1aba7f2e0316f40bb498be2f13cf859c4bd9688933b", "exact_match": 0.0}
{"doc_id": 3410, "doc": {"question": "what is the name of the dragon in eragon", "answer": ["Saphira"]}, "target": "['Saphira']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the name of the dragon in eragon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Saphira"]], "filtered_resps": ["Saphira"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "90f7e6c25947555363239f03d0a1b96dd3d169d111ccfb9a245ec51cf5d6529c", "prompt_hash": "269a76482068da315508d137505b2e2533c98e3ec553b71b8036e4a188ecebfd", "target_hash": "4d02a4103a3f2160a8da460402e812f2c54034c3e5fdd0339c58fd7e55fb0717", "exact_match": 1.0}
{"doc_id": 3411, "doc": {"question": "who has the most trophies in la liga", "answer": ["Real Madrid"]}, "target": "['Real Madrid']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most trophies in la liga?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Real Madrid"]], "filtered_resps": ["Real Madrid"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e31a908f6333720e600b738e196f49d46a34c8df0e56f3a013d47e31ff7673d1", "prompt_hash": "fc5e5b2f9c83ccd70894d84ff12f0fb580f84870fe2adb7ed42f0455853ac58b", "target_hash": "a1f9675d65afe7eef9e2d095ee7541ff6c66883ef6d8f4aedc1bd3a0943b4418", "exact_match": 1.0}
{"doc_id": 3412, "doc": {"question": "when did we decide to leave the eu", "answer": ["23 June 2016"]}, "target": "['23 June 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did we decide to leave the eu?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2016"]], "filtered_resps": ["2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e88429639e1f73b1eb05bf36709fb59a9184d808a7594ff5e1a0d8d396264d0e", "prompt_hash": "1311c12708267cc4de28d2a416ec2f4d9b83f5f1c27b913049253be94ab756c4", "target_hash": "19635683639a33ca3f187dd7d78260baf1574c3c6d3b600bf434983b286f0a0a", "exact_match": 0.0}
{"doc_id": 3413, "doc": {"question": "who issued ashwamedha coins after performing ashvamedha sacrifice", "answer": ["Samudragupta"]}, "target": "['Samudragupta']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who issued ashwamedha coins after performing ashvamedha sacrifice?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ashoka"]], "filtered_resps": ["Ashoka"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4c7c4c4747b9a36ce4707557f34baf55d8060f1db21bcb73eb139abc27a23513", "prompt_hash": "fb337fc6f60c77e72cb8e52bfc13729397126c1bdb2edb194157e6044bab7d3c", "target_hash": "4c0e3a5866687f0e8bdf349a9993d77b8bf6ea40627adc5919a7371036d49719", "exact_match": 0.0}
{"doc_id": 3414, "doc": {"question": "where do characters live in this is us", "answer": ["New Jersey", "Pittsburgh", "New York City", "Los Angeles"]}, "target": "['New Jersey', 'Pittsburgh', 'New York City', 'Los Angeles']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do characters live in this is us?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Boston"]], "filtered_resps": ["Boston"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b17722ea51630f4f7e7eb566b8a46a3db43981d9a8f755340ba9daaefc49642a", "prompt_hash": "b86c15b0993fd9ef1557cbe8e6122a37d452d42c19c59d05d115caa29fed711b", "target_hash": "a1be7a480825ea821bbe075a8d28fb78d402cfcc2eda9aec4b75b2adb7973804", "exact_match": 0.0}
{"doc_id": 3415, "doc": {"question": "in which regions are most of africa petroleum and natural gas found", "answer": ["Nigeria"]}, "target": "['Nigeria']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: in which regions are most of africa petroleum and natural gas found?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" North Africa"]], "filtered_resps": ["North Africa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "faa621d087be74f0faa17fec6fc4eafef38d8095c5b377d3b6eb0b1836bb9f00", "prompt_hash": "04aa23727e88514718f439c74f4dd4f72f85fd5219a95fd6224acd45922535d7", "target_hash": "c4603a446023182f3edb88bbc77373756e928da3545d71b7e2ae3ca09606280c", "exact_match": 0.0}
{"doc_id": 3416, "doc": {"question": "when will miraculous ladybug season 2 episode 12 come out", "answer": ["30 March 2018"]}, "target": "['30\\xa0March\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when will miraculous ladybug season 2 episode 12 come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 11"]], "filtered_resps": ["January 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "eaa601a45807bc2a7df2194c5259abf0797919e3b018380b60a1f695d3cf32bc", "prompt_hash": "35296fb95d4e934493d84ffbafbe8ccce5979dd0d05cdee2fe7fe575b67b3be9", "target_hash": "fff28d9a70393efafbf224eacac262fa6a94b8576e13fb7f3851ae41598ab1cb", "exact_match": 0.0}
{"doc_id": 3417, "doc": {"question": "who does betty end up with on riverdale", "answer": ["Archie Andrews"]}, "target": "['Archie Andrews']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does betty end up with on riverdale?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jughead"]], "filtered_resps": ["Jughead"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ecb23f00c184689c78f98e4bb471a9810e0327653700e7c908c3986bee8a6dd6", "prompt_hash": "ff17bc0ec8975dc55dc1cd7741481056712e0e1b7f1b4c95009faf4da9f281b2", "target_hash": "935d9b008110f1ca20358b2265d3330caf6ff825b1dbbd881fb6a1147b851d39", "exact_match": 0.0}
{"doc_id": 3418, "doc": {"question": "what is the most common cause of right ventricular heart failure", "answer": ["left-sided heart failure", "pulmonary heart disease"]}, "target": "['left-sided heart failure', 'pulmonary heart disease']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the most common cause of right ventricular heart failure?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Pulmonary hypertension"]], "filtered_resps": ["Pulmonary hypertension"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "42f9013fc427714701db524353c9bc2038aa976dc6a181d6aa8b72b7d6e49222", "prompt_hash": "f1c70d508d184cc2262ebee145ed6e8a18a1722eb2869ba17cb2c8978c030d90", "target_hash": "2447cad9793832d4a04617286860804d6cea6e96ea42a1bcf1ce5fd94147d6f7", "exact_match": 0.0}
{"doc_id": 3419, "doc": {"question": "what time do tam tams start in montreal", "answer": ["around 10:30am", "10:30am"]}, "target": "['around 10:30am', '10:30am']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what time do tam tams start in montreal?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 11:00"]], "filtered_resps": ["11:00"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d964addc6cf141319690c046a1aec34a8adb3656299e04cf4765c397cc8e9040", "prompt_hash": "747c5f3c1385d99b4bf8328e3a042377b2f7ade842cde14692fce0eeef6e4ec3", "target_hash": "339d916827d316c617e8ec61d3fb91ef474a468fae316959cddc6688333efdd0", "exact_match": 0.0}
{"doc_id": 3420, "doc": {"question": "what is money as a medium of exchange", "answer": ["fiat money"]}, "target": "['fiat money']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is money as a medium of exchange?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" commodity money"]], "filtered_resps": ["commodity money"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5ebdee122e01ef1203b9b6e482879c13fe45104bd13e2f90864f1c0cd4e09ff7", "prompt_hash": "420ec44cbd993152f3625c0ce3b1b37ee8c411f52966eb604e1f82266c71df0c", "target_hash": "f1f32f80cde87a73f690a374dbd649d5582da2e302673aeeb670756ce794c777", "exact_match": 0.0}
{"doc_id": 3421, "doc": {"question": "what is the main character's name in layer cake", "answer": ["XXXX", "XXXX,", "Daniel Craig"]}, "target": "['XXXX', 'XXXX,', 'Daniel Craig']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the main character's name in layer cake?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Frank Martin"]], "filtered_resps": ["Frank Martin"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7ae8209e2dac128bc784355ae8ded0444ca97a404ec7e208d839a16ba2ff292e", "prompt_hash": "200838da1f821f2a56c67177c5dbc132598979960de7ba9487eb6e24140e757f", "target_hash": "fb14e3c1f696fd3349b38c4dbfc9deca0a03f571dbb69dbb692ace3ba173ae90", "exact_match": 0.0}
{"doc_id": 3422, "doc": {"question": "who wrote the song if i were a boy", "answer": ["BC Jean and Toby Gad", "Toby Gad", "BC Jean"]}, "target": "['BC Jean and Toby Gad', 'Toby Gad', 'BC Jean']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the song if i were a boy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Beyoncé"]], "filtered_resps": ["Beyoncé"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a6ee360ac23e9b62412a26518809eb372266f901879d1da2805b743505dba28a", "prompt_hash": "fe215539b97fb42205e7388dd459c1b86d13386a42b1620d0e50f5b36340de2b", "target_hash": "b23147395aedf0b21ead5fde2389ff87bf000a1d8e0e985c579e899375cd8d41", "exact_match": 0.0}
{"doc_id": 3423, "doc": {"question": "when did cristiano ronaldo go to manchester united", "answer": ["2003", "at age 18 in 2003"]}, "target": "['2003', 'at age 18 in 2003']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did cristiano ronaldo go to manchester united?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2003"]], "filtered_resps": ["2003"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cb7a4d47785ee047ce7d7c60322634c266f7b04ea12e2f1e42ef71994e7a7b26", "prompt_hash": "df7dfd590146aa9dd60e0ab2333267bf7f9ffebbadc01be3ee254648c980cc93", "target_hash": "25aac91f14bcb9de32c598d9238af38bf510b6edaf0d3be960e556afc2d63740", "exact_match": 1.0}
{"doc_id": 3424, "doc": {"question": "who sang you'll be a woman soon in pulp fiction", "answer": ["rock band Urge Overkill", "Urge Overkill"]}, "target": "['rock band Urge Overkill', 'Urge Overkill']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang you'll be a woman soon in pulp fiction?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sheryl Crow"]], "filtered_resps": ["Sheryl Crow"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1fff7c924ae688f6b211c38bef18cd8e9273cc671a982ef37a86cdf21f38967a", "prompt_hash": "801c7e8fcf6b258283c9acfaeb7f2f4337547a1e038abf3bcc78aeba59a6cad6", "target_hash": "bf9749d4601d1cfae9c08ae297966db7b25c07ddc16e7fa2d33f5821ffb924c7", "exact_match": 0.0}
{"doc_id": 3425, "doc": {"question": "when was the cleveland browns last winning game", "answer": ["2016"]}, "target": "['2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the cleveland browns last winning game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" November 24"]], "filtered_resps": ["November 24"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c4522d4e79f4a0a9dd983d861ed420311f0a5bd136bbe35e1a8ff16e4ec085a4", "prompt_hash": "91df45d659a26a54d1ed659737d1fb1aa7eacbd4feecc8a3f9365c6d8c169d70", "target_hash": "a31e19f46d2b0d01eab5c71855279b219d9c5201e41a02b41a054a14541956ee", "exact_match": 0.0}
{"doc_id": 3426, "doc": {"question": "factories that assemble parts made in other countries", "answer": ["special economic zones"]}, "target": "['special economic zones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: factories that assemble parts made in other countries?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" multinational"]], "filtered_resps": ["multinational"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "05878391d06120d5b43e135574a6776e7930917cc56639e2f0107ca6c2630581", "prompt_hash": "a75623c53ebb11a07fc02125549bd8c99ebf5d6ec5f3235a5c28f1058e1fe672", "target_hash": "853e5a535559214f6df5d2af15b61c4b93f81f04381c273617c7e95c08d2bdbe", "exact_match": 0.0}
{"doc_id": 3427, "doc": {"question": "when did the not in this lifetime tour start", "answer": ["April 1, 2016", "April 8, 2016", "April 1, 2016"]}, "target": "['April 1, 2016', 'April 8, 2016', 'April\\xa01,\\xa02016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the not in this lifetime tour start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2016"]], "filtered_resps": ["2016"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cff7ba1c750cb3e997109041e0c01b20e59fd13dfb6bed74cd6195155929994b", "prompt_hash": "844df6928cd38d16aa5fbb9c888eb3f124ee48519a018aeaafda7db2bf5da747", "target_hash": "90a2214658d6e9ce2469b676d5210f1899a80213b8c73ff857dd2403c7e82dc0", "exact_match": 0.0}
{"doc_id": 3428, "doc": {"question": "the gulf stream the world's fastest ocean current flows along the western side of this water body", "answer": ["Atlantic ocean"]}, "target": "['Atlantic ocean']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the gulf stream the world's fastest ocean current flows along the western side of this water body?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" North Atlantic Ocean"]], "filtered_resps": ["North Atlantic Ocean"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "271d6a913c6b10d11586ff983daa061ec9fea939aa2af70c040eb1a17df0eee1", "prompt_hash": "716561d74543dcebc89bf4151b8ab310df2292d952ebd8cdff0acd27b2f55b97", "target_hash": "b01e16dde1a742a482fd782ab5f7298f9267ccc62e5814d501be58950a46692c", "exact_match": 0.0}
{"doc_id": 3429, "doc": {"question": "when is the last time the jaguars won a playoff game", "answer": ["2017"]}, "target": "['2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the last time the jaguars won a playoff game?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2006"]], "filtered_resps": ["2006"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bb8ce07b32574c4d74cdf4813fae02689b342714752ccf258e5d87801e39610c", "prompt_hash": "fd5d1b6ecd382d7c71152348440e220b7516e7b736164a929acb668b5dede8e7", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "exact_match": 0.0}
{"doc_id": 3430, "doc": {"question": "what is the minimum number of parties required to enter into a deed", "answer": ["two parties"]}, "target": "['two parties']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the minimum number of parties required to enter into a deed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" two"]], "filtered_resps": ["two"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "845db88145b79089921f013ff408441ebb2fa01549af9e564d0fecf6e42da213", "prompt_hash": "119ab4f3e559b5bfee8c084e02ed78d1b714ba73b0101009b2eb183bdc44a4fe", "target_hash": "34c88e215d2c40bd6461b13cabfd7ba4df2b8af2f696ac1989a27a2951e77651", "exact_match": 0.0}
{"doc_id": 3431, "doc": {"question": "who played cruella de vil in 101 dalmatians", "answer": ["Glenn Close"]}, "target": "['Glenn Close']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played cruella de vil in 101 dalmatians?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Glenn Close"]], "filtered_resps": ["Glenn Close"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f6fa29e0eb90dbbebf8a0c161e62b5564e64ba55018cf5095e687b644c0f54e8", "prompt_hash": "2895d02a393469e137d2ee09b69f309c58c18ca95aced96111707d5542090879", "target_hash": "e2c41abd4ffd576c016943b4d3cbc9eb71ef92b7088adeb6cb8a9d8775ae83c7", "exact_match": 1.0}
{"doc_id": 3432, "doc": {"question": "what genre is the magic tree house books", "answer": ["Children's historical fantasy"]}, "target": "[\"Children's historical fantasy\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what genre is the magic tree house books?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" adventure"]], "filtered_resps": ["adventure"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4cbd54b64d533ea10784cf7c30e4e0c12bb9293a37217377c665410ec3659a24", "prompt_hash": "fb6ba9d2c0ab2e4d59267c38e35d4432f6d2500c1b15a5843d3f0a69bb3f5d4a", "target_hash": "d5da69beb106eeaf85e3af2707eaaade58c86b4dfa275716a048e085b300117a", "exact_match": 0.0}
{"doc_id": 3433, "doc": {"question": "where did students for a democratic society start", "answer": ["Ann Arbor, Michigan"]}, "target": "['Ann Arbor, Michigan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did students for a democratic society start?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Chicago"]], "filtered_resps": ["Chicago"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "600dacf5ede768b9e9730ae3bb389e4e1938dfdc4f25baba6fc24fa8439a26bd", "prompt_hash": "e223422f10ea565208f7705054346914461e311ccb57fd5230ce57a0816902f4", "target_hash": "81387869acd09ab568ff9a868f820c27471597cf898b6fbd5c2917ac16154687", "exact_match": 0.0}
{"doc_id": 3434, "doc": {"question": "who was the first black person to register to vote", "answer": ["Thomas Mundy Peterson"]}, "target": "['Thomas Mundy Peterson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first black person to register to vote?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Henry Jackson"]], "filtered_resps": ["John Henry Jackson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "497ad97f8ce00bd17e77771e01801aa822cdf44448ac9928cdb7e32626e0c530", "prompt_hash": "bd065f962a85805b31e3663230824ad30ca7d566bda5b70a6c3052e1fa86dc47", "target_hash": "35e0a17d37b0e79f025c1a94bba0ece9e80d087c706af6fcd5c344aadf7fb294", "exact_match": 0.0}
{"doc_id": 3435, "doc": {"question": "who plays sheila carter on the bold and the beautiful", "answer": ["Kimberlin Brown", "Michelle Stafford"]}, "target": "['Kimberlin Brown', 'Michelle Stafford']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays sheila carter on the bold and the beautiful?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Taylor"]], "filtered_resps": ["Jennifer Taylor"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7e7ffcdc8faca9b1297ea5ed2df3164058d32ef94cb17f2cc6c64823ffbca539", "prompt_hash": "c77e8e417b6020d051881ddd7970c0fe7b841ed8d3c2a74b6879351ee27e21ca", "target_hash": "a10d529288c45c6febda35ac930cf1851014e196f50c63786a0de31469e06e47", "exact_match": 0.0}
{"doc_id": 3436, "doc": {"question": "where does sex and the city take place", "answer": ["New York City"]}, "target": "['New York City']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does sex and the city take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York"]], "filtered_resps": ["New York"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "04e1610e260106173ec429a1c744c7b27636eb73d08e2b6d1f11af50a62b6db4", "prompt_hash": "19c984c6fe468bd9086edd91fc50195f5eea95ea7f417f894e54d36ca3684e08", "target_hash": "34b5642e7ac3c37be810afca322277ff3e939f50d7aa576ecb54dd043fa4bbbe", "exact_match": 0.0}
{"doc_id": 3437, "doc": {"question": "what are the band members names of the rolling stones", "answer": ["Ronnie Wood", "Charlie Watts", "Keith Richards", "Mick Jagger"]}, "target": "['Ronnie Wood', 'Charlie\\xa0Watts', 'Keith Richards', 'Mick Jagger']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the band members names of the rolling stones?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mick Jagger"]], "filtered_resps": ["Mick Jagger"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f26480fa32a3ffd4cdd5c2547f04966bab4f4f8ed437449d9d34024d663368dc", "prompt_hash": "aecef61de7654c5c6897ea273fe6e321cfba15dfa66141d4c0fe2ff6b738c05d", "target_hash": "46b3666aeef896a34a447eb8209fbba4e522d2bc9a50e2519a4438619f7323df", "exact_match": 1.0}
{"doc_id": 3438, "doc": {"question": "who explored waters of cuba the bahamas and hispaniola", "answer": ["Christopher Columbus"]}, "target": "['Christopher Columbus']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who explored waters of cuba the bahamas and hispaniola?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Christopher Columbus"]], "filtered_resps": ["Christopher Columbus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "062c12e73d2485d6b34c33216a9ca3a1f310982dfda0e6f6202e6bf9f4988d04", "prompt_hash": "191de1b7a6efee6fe59c41320fc3b872f6925ae66776731dae694aef6dc214d2", "target_hash": "d9cbafdfba9432e8da683a3c26fef3c3e05b81dd01f3ad6281c8752befd6b2ce", "exact_match": 1.0}
{"doc_id": 3439, "doc": {"question": "who sang it my party and i'll cry if i want to in the 80", "answer": ["Barbara Gaskin", "Dave Stewart", "The Chipettes", "Dave Stewart and Barbara Gaskin"]}, "target": "['Barbara Gaskin', 'Dave Stewart', 'The Chipettes', 'Dave Stewart and Barbara Gaskin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang it my party and i'll cry if i want to in the 80?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Debbie Harry"]], "filtered_resps": ["Debbie Harry"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "168b2a8955f0f6e5fe6165d288763aff81644b1c0a960ba693e243bcd4d689c0", "prompt_hash": "b765550c411d2e66a77eb02ef80be8aa8d2f6b38aa9bcda8eff0127c0e3c58e8", "target_hash": "2d5351d6572dc5e7ef6282d63ec52badd76abf1ebcd083f75aa9a728b49e7296", "exact_match": 0.0}
{"doc_id": 3440, "doc": {"question": "who plays lefou in beauty and the beast 1991", "answer": ["Jesse Corti", "Venezuelan voice actor Jesse Corti"]}, "target": "['Jesse Corti', 'Venezuelan voice actor Jesse Corti']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays lefou in beauty and the beast 1991?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jerry Orbach"]], "filtered_resps": ["Jerry Orbach"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b3383a719aee1b35c32d9c6f2672f2d81a55e6c438ce9d629dc32fb939feb6ae", "prompt_hash": "7a6ac0700ecdf5ea16ffac26cc65f7d41c174471ac42a78bfe4e90c1ca4bb7f4", "target_hash": "ff3a8e55e0fb2a9e1fb7fa4dc462e10bc582348c606739f418df57d06bd8ced2", "exact_match": 0.0}
{"doc_id": 3441, "doc": {"question": "los angeles stadium at hollywood park opening date", "answer": ["2020"]}, "target": "['2020']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: los angeles stadium at hollywood park opening date?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1992"]], "filtered_resps": ["1992"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1edcb2e5cc311819dada0db1e7f10e393abc85f1d27077c55cac1621a3c8edd7", "prompt_hash": "efab48a5971f94c07ebdd92428164818535f14f2edc6117fe010f676040378f5", "target_hash": "e1c889ef9bce53ec45ad030136f2024254bcb0ad3d4ff2bf83f59b9b881297d6", "exact_match": 0.0}
{"doc_id": 3442, "doc": {"question": "who played raquel in only fools and horses", "answer": ["Tessa Peake-Jones"]}, "target": "['Tessa Peake-Jones']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played raquel in only fools and horses?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Liza Hanley"]], "filtered_resps": ["Liza Hanley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d89f3682689f25a2770c80b2beec2c8d679c96f87088b402cffcb80dea6986be", "prompt_hash": "ec80107539128ef25e0b5ce7e828eb1470304bcae325b58f1c34ab04754efe7b", "target_hash": "4d9426b91285b4bd89c1a5e757ac8918c90aa60f17d091d0804eec4f4ecef6aa", "exact_match": 0.0}
{"doc_id": 3443, "doc": {"question": "who does the voice of the beast in the new movie", "answer": ["Dan Stevens"]}, "target": "['Dan Stevens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the voice of the beast in the new movie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dan Stevens"]], "filtered_resps": ["Dan Stevens"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3085e4806662c20ec8e17eb0f57b9f47ce395bc98bac3b9528232fe33314e08f", "prompt_hash": "9b327e91a626fd5cbde6a0b241e4dce0a9e625c73e23a6faf505ddd726b76eb3", "target_hash": "6a04d8fe5396ef286d29fe0326bf1b89ad9f548594e9c7200756d5277e71c444", "exact_match": 1.0}
{"doc_id": 3444, "doc": {"question": "when was the first horrid henry book written", "answer": ["published in 1994", "1994", "in 1994"]}, "target": "['published in 1994', '1994', 'in 1994']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the first horrid henry book written?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1994"]], "filtered_resps": ["1994"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "43dd00f2c41f833df08abebf727db3ed33ae06db64d135237b9262f835f3040c", "prompt_hash": "37c231fdc5f67bfda484c0867307a2bf9b034b514ffd9be173f65e8910d0bc96", "target_hash": "c47fc85c9fb4aa4083bc3c9a3b6c3103ab293ca35f81baf15a11f9a7805156db", "exact_match": 1.0}
{"doc_id": 3445, "doc": {"question": "what year did return of the mack come out", "answer": ["18 March 1996", "March 1996", "1996"]}, "target": "['18\\xa0March\\xa01996', 'March 1996', '1996']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what year did return of the mack come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1995"]], "filtered_resps": ["1995"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "560c657c226cbd01cd94f9d37428f46c6c88bcfca590e12808db33ebe100b464", "prompt_hash": "2b025b523aee9d9a9ba323a7f49f413971dbf26fe6d8c06d2bc03d0eca6ee71d", "target_hash": "b2bdcf5d0f69d5dbe38f28cfd50f3ffe995f33f913817514cc8733c0e00fd962", "exact_match": 0.0}
{"doc_id": 3446, "doc": {"question": "who says that which we call a rose", "answer": ["Juliet", "William Shakespeare"]}, "target": "['Juliet', 'William Shakespeare']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who says that which we call a rose?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Juliet"]], "filtered_resps": ["Juliet"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5273a3fc6dfc4ac684e82443a532bcc43be2e8c6ed936c18ca3f150b885a734b", "prompt_hash": "7a00b2f84fd69e9b78d59efa56e1e85d647356a4dc69ad9ddee3c43af6f73f9e", "target_hash": "5123b54714bfedee5663d09b5f86c9e05810ee2f88098323c5f6ca1d1d98562e", "exact_match": 1.0}
{"doc_id": 3447, "doc": {"question": "what type of plate boundary is associated with iceland and its volcanic eruptions", "answer": ["divergent tectonic plate boundary", "a divergent tectonic plate boundary", "the mid-Atlantic Ridge"]}, "target": "['divergent tectonic plate boundary', 'a divergent tectonic plate boundary', 'the mid-Atlantic Ridge']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of plate boundary is associated with iceland and its volcanic eruptions?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" transform"]], "filtered_resps": ["transform"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "31c9d25164296b1279a63ad216ef9fd60b6c686cc3a57c3eae9942635b5ecd6b", "prompt_hash": "6c1e69e8ac0386e493241312692e0a03541f2636eccc5c97312f932430d9279d", "target_hash": "4bc6544556b3ba58d1b8850687cb8ccca0750f76a99563a9e220b57008f26b11", "exact_match": 0.0}
{"doc_id": 3448, "doc": {"question": "days of our lives cast doug and julie", "answer": ["Susan Seaforth Hayes", "Susan Seaforth", "Bill Hayes"]}, "target": "['Susan Seaforth Hayes', 'Susan Seaforth', 'Bill Hayes']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: days of our lives cast doug and julie?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Doug Williams"]], "filtered_resps": ["Doug Williams"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2cc013c3d9de669af1f1a38f8b38a1049d231ceb1a7ff1a89035ce3ae9bac44c", "prompt_hash": "851a7003030e2126cc536624f866aaab0bea50499f34d3845d3d5ce1ac1d380e", "target_hash": "5a9dcac60d6710cae7c318d34c2b4bbb56c6647627a52c8c2b447b4807b2c60e", "exact_match": 0.0}
{"doc_id": 3449, "doc": {"question": "who is known as the super fast boy in the series the icredible", "answer": ["Dashiell Robert Parr / Dash"]}, "target": "['Dashiell Robert Parr / Dash']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is known as the super fast boy in the series the icredible?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dash"]], "filtered_resps": ["Dash"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0137d2ee3ca420b1a0b5d42d408c8c2604f9609ffa3deed66a91361a3561cee2", "prompt_hash": "4e2aacb7f9a95e741131dd6846639ffe6f6f0572a67a96c3faf5cca6f8d3032d", "target_hash": "3ffb92e228e126c256ec1a21a289ce0b93a104b17c199a5157e1848bd1e48c00", "exact_match": 0.0}
{"doc_id": 3450, "doc": {"question": "who is stephanie's mom on the bold and the beautiful", "answer": ["Ridge Forrester", "Taylor Hayes", "Hunter Tylo"]}, "target": "['Ridge Forrester', 'Taylor Hayes', 'Hunter Tylo']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is stephanie's mom on the bold and the beautiful?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Stephanie's mom on The Bold and the Beautiful is Erica Kane"]], "filtered_resps": ["Stephanie's mom on The Bold and the Beautiful is Erica Kane"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c2d0baf27a04ee82b27196a210261af92b001637655465032fa96b4b931ba9f3", "prompt_hash": "ec6d959133935cf8fe8b173fe3955a0a4c00ab9edae77985bbc3c08ff12585b2", "target_hash": "c0a351772d0e9f5f18c3d298a82e104dfb9ef609e52a14dc4517cb24b9e3ed87", "exact_match": 0.0}
{"doc_id": 3451, "doc": {"question": "who was the first signatory of the israeli declaration of independence", "answer": ["David Ben-Gurion"]}, "target": "['David Ben-Gurion']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first signatory of the israeli declaration of independence?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Ben-Gurion"]], "filtered_resps": ["David Ben-Gurion"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a4b6c1a83215007723715ef0816e1ce4c4bd6f735fae20f159ab2a4639fd4e1", "prompt_hash": "45721f8a408c0601f104c314b08c7777007f98e872ce26f7f316569152bf2fd0", "target_hash": "2ad9caf3c29cd0ce7f8472ba0eeee19fe11411be997a33826dd61fd5a65e77e1", "exact_match": 1.0}
{"doc_id": 3452, "doc": {"question": "where does arsenic and old lace take place", "answer": ["Brooklyn, New York"]}, "target": "['Brooklyn, New York']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does arsenic and old lace take place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Brooklyn"]], "filtered_resps": ["Brooklyn"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "17d13d4c9833d8271c3e5de3afc7325ee386fac95742efaffde9c8ea8298fb16", "prompt_hash": "233eadeb89410d43ae9142747d34b6cd29b5101fa4b5de2a55a1a2f0c86fb5c6", "target_hash": "cedd369987cc20eb1bfec54067202fe99c6b4aa9571a06752006a7f26dcd5b0d", "exact_match": 0.0}
{"doc_id": 3453, "doc": {"question": "where can you find convergent boundaries on earth", "answer": ["Mariana Trench"]}, "target": "['Mariana Trench']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where can you find convergent boundaries on earth?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" ocean trenches"]], "filtered_resps": ["ocean trenches"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "61f6e9eb7d36ffadaac678c9bb74d84d70fbf58d8ec0b130b887498dd531c096", "prompt_hash": "1ee3c154911729958e66a92e00f370025317116dc37113dcd71c7d2f4d677d21", "target_hash": "e3f7fd5a31a4598d875fd11e3d796da24b3ba78ac88aa4ef22e41b412a931684", "exact_match": 0.0}
{"doc_id": 3454, "doc": {"question": "when did fortnite save the world first come out", "answer": ["July 25, 2017"]}, "target": "['July 25, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did fortnite save the world first come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" September 26"]], "filtered_resps": ["September 26"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3e86665f3fce81de2be1e4a8571658c22dd73664f7f0d79314d1f73aae155ab9", "prompt_hash": "473db1292919a421eebbddec51854b33f5ff5ebd7e4be5a2ae2b6c2e7aca333b", "target_hash": "d712a1516e2f92cef09469967119108c8d069f0c012cb762a2e67c4139ec111a", "exact_match": 0.0}
{"doc_id": 3455, "doc": {"question": "what kind of beast is the beast from beauty and the beast", "answer": ["a chimera"]}, "target": "['a chimera']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of beast is the beast from beauty and the beast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" dragon"]], "filtered_resps": ["dragon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5b947bf28a4871b992ddc1bd41dc6879da031a83bda23c6d3cbcf4d45249d8b4", "prompt_hash": "2a99e71c2e897a70b6e89d2bf868bb7fd10771c69eb9781dab9bc081b4a4862b", "target_hash": "4508a81198078b9c939f839fbb8b59a987a649cc99bba31a0d4b33da28bdfb16", "exact_match": 0.0}
{"doc_id": 3456, "doc": {"question": "when did somewhere over the rainbow come out", "answer": ["1939"]}, "target": "['1939']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did somewhere over the rainbow come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1939"]], "filtered_resps": ["1939"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9eab6e48a562eefc07a7003f5b591121727339a1571fdd143c9f7e40be15a7e9", "prompt_hash": "c9825b0ec6cb0308c53196eb89c56dae306c008197b3dcf6111780a3377c37c3", "target_hash": "38e0bf9ab20f85bb53523831e786bb1307fac89ce51efdab993a89423bae7a09", "exact_match": 1.0}
{"doc_id": 3457, "doc": {"question": "when was the chain first used for f1", "answer": ["1978"]}, "target": "['1978']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the chain first used for f1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1994"]], "filtered_resps": ["1994"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9e072d0181d76e0b433c6374cf69a45fb2c9f355712f2f26af118d2b7b1b9b58", "prompt_hash": "4abcf9f528d5f131aa92eb762b46f9cfd683f9442d511cc008020581c2b36d73", "target_hash": "effdfb2f73816e7029ea7c0fa637e38504ef44a4a5bac4f2d285b5aa95b6fce8", "exact_match": 0.0}
{"doc_id": 3458, "doc": {"question": "when was the death penalty reinstated in oregon", "answer": ["1984"]}, "target": "['1984']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the death penalty reinstated in oregon?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1976"]], "filtered_resps": ["1976"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f348c2eeefa646b0eb55e8ef397bd32ad75b8c4b8342399240c66ee6d0d023a", "prompt_hash": "7eede8d7d0b8baba0e65b5ccc4d7f9e5d58ea7d473ba3fca487a4e42027adeea", "target_hash": "8c1b2c2e658d9fadeeb5174a5acb23cdc07e29baf6e0886beeecb0ceac802b13", "exact_match": 0.0}
{"doc_id": 3459, "doc": {"question": "who told the story of the prodigal son", "answer": ["Jesus Christ"]}, "target": "['Jesus Christ']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who told the story of the prodigal son?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jesus"]], "filtered_resps": ["Jesus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ba2919235093b39707ba55f9db8e5093e84bb94e34894f02578b40d97c6d40dd", "prompt_hash": "3e1aa562e362937219265c023f62436000c46dc15af43688dd251c9fb5c55a76", "target_hash": "744a7cda540e4af78b7938d743cb5d3c903f9bdbc8a4e886d1635fbef0604aea", "exact_match": 0.0}
{"doc_id": 3460, "doc": {"question": "one piece english dubbed episode 564 release date", "answer": ["September 16, 2012"]}, "target": "['September 16, 2012']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: one piece english dubbed episode 564 release date?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2015-01-11"]], "filtered_resps": ["2015-01-11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "16dba894388f422ecf401bae0f20245dde25f18bdf8407d534151e6a56cc1a58", "prompt_hash": "f3885a81c2bab32486f5ead9db461199b19fc126553ff03e73b37ffab3122cb9", "target_hash": "a14fa2d58348bdcd9284ce40090d46c35630aeb67a086e78d452b83d09c26cd0", "exact_match": 0.0}
{"doc_id": 3461, "doc": {"question": "who stepped out of the dithyrambic chorus to become the first actor", "answer": ["Thespis"]}, "target": "['Thespis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who stepped out of the dithyrambic chorus to become the first actor?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Thespis"]], "filtered_resps": ["Thespis"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9709b2bbffe7491f3c16a95b4ca887b7a19039b49b6807a60fbe212413f53012", "prompt_hash": "d7ec229a78d7b4ec91a0f68baa7ac58cf7b142f5ad813bf91ea0e80a64c301af", "target_hash": "4b551ad81011667a9945a5c7d849ebc066d62ac3dd33ca29996dec6f18b91eb6", "exact_match": 1.0}
{"doc_id": 3462, "doc": {"question": "when does elijah first appear in vampire diaries", "answer": ["Rose", "\"Rose\"", "Season two"]}, "target": "['Rose', '\"Rose\"', 'Season two']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does elijah first appear in vampire diaries?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 4"]], "filtered_resps": ["season 4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d8a20544302c98a300c2f210c7058ae12eddb2f41991d33dcacbebeb13e2400e", "prompt_hash": "13ab756509b67bd7096afb2a35e34d11a16dcc4370b92963fda6f6b8422bccd4", "target_hash": "56019e346800066b56aaaac275e6f60f4e40a6679468a7b96d2376c43db1d5a9", "exact_match": 0.0}
{"doc_id": 3463, "doc": {"question": "what is the longest pier in the uk", "answer": ["Southend Pier"]}, "target": "['Southend Pier']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the longest pier in the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Blackpool Tower Pier"]], "filtered_resps": ["Blackpool Tower Pier"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "79bed8c4625688d75be6449274e3943e69ac3ee5742ce56073448d00bba3294e", "prompt_hash": "a9fe0fd8e9c5a9e849b8a96ca47c84a7d4b3e24a5118fcb0e91f5e50657becd0", "target_hash": "166802d5ff4f82f4c4c05551a681e3f8b50b0125150114f8a91fbc66ed9c152e", "exact_match": 0.0}
{"doc_id": 3464, "doc": {"question": "where is the tablet of the ten commandments", "answer": ["Ark of the Covenant"]}, "target": "['Ark of the Covenant']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is the tablet of the ten commandments?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Mount Sinai"]], "filtered_resps": ["Mount Sinai"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "82d35f004bcb5408fb2bce4612723005b1224032a5939b392df115332bfd6dc9", "prompt_hash": "4d9a230497fe3dd598f12b3aa05645695f64abe48e79fd5a9ef8d7edd390a090", "target_hash": "77c61afe13294325cf342830651053ed415d3d49897c8a4f432be37a472db1a7", "exact_match": 0.0}
{"doc_id": 3465, "doc": {"question": "do you need a permit or license to own a gun in florida", "answer": ["No"]}, "target": "['No']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: do you need a permit or license to own a gun in florida?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" no"]], "filtered_resps": ["no"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "04b150fb60ee307d337607eadf26ce74ab820c6898d360a603e5c6c937975e6a", "prompt_hash": "950ba9ae6ee9b21247e6fe8045c2b16026437e5ac6485d5e09f6a762a6877196", "target_hash": "624e19d6d4a6ca65cc7d7c2f1a2dfab282625eef3b071f7a31a0ca195ec62ba6", "exact_match": 1.0}
{"doc_id": 3466, "doc": {"question": "where does cleavage of the peptide bond by chymotrypsin occur", "answer": ["digestive systems"]}, "target": "['digestive systems']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does cleavage of the peptide bond by chymotrypsin occur?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" aromatic amino acids"]], "filtered_resps": ["aromatic amino acids"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "03eab79a8ca3f617d160b1cbf9fc7a933577ebe0aa8c45271a1120cfdd09ea95", "prompt_hash": "633bcc429f2c5120d8e1dd30a591fa0c804181f5a287ae335ab505e719174874", "target_hash": "5a3d18d50b9479bf642f0e0c079bbc33a568e6ccf33b2ce320a0311b368b5092", "exact_match": 0.0}
{"doc_id": 3467, "doc": {"question": "when does the nba season end this year", "answer": ["April 11, 2018"]}, "target": "['April 11, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the nba season end this year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" April 11"]], "filtered_resps": ["April 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e96a86404bb3de642a5e93d80e80ad52fe89ca486e7bb951cf21aa22947b291", "prompt_hash": "b003f02b93f3970cdcb1a4794a17937c7678fcb90df482ef8bf0a73bdb590c4a", "target_hash": "64fe0dea2e8712becdeb841316f689b983f0d30ac6c16bd3db81d0295b615da3", "exact_match": 0.0}
{"doc_id": 3468, "doc": {"question": "malcom in the middle season 1 episode 3", "answer": ["Home Alone 4"]}, "target": "['Home Alone 4']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: malcom in the middle season 1 episode 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Boy Who Cried Daddy"]], "filtered_resps": ["The Boy Who Cried Daddy"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f9c226092cb8b02959f993c6b96d432e9560396b80a642c51b4333312b2a999c", "prompt_hash": "9178376b51ea55ecc3d59c1f00e029e08176d2d880db3e8991ab6dd52497842c", "target_hash": "c71e33ad728b1ab84ed8d27026774c6e4f1f5765ddf601a1f8f318ebae053ba7", "exact_match": 0.0}
{"doc_id": 3469, "doc": {"question": "where are most nutrients absorbed in the human digestive tract", "answer": ["The small intestine", "small intestine"]}, "target": "['The small intestine', 'small intestine']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are most nutrients absorbed in the human digestive tract?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" small intestine"]], "filtered_resps": ["small intestine"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bae858fff2c3f7bf89efe7ff7303136156a2161a29a1fb7a6fe32539c49f8b79", "prompt_hash": "086e0f826fe62e3e833fe18ac02fd50c633bf00a5be8b39912c30383c9d2a05d", "target_hash": "30d42da74bcaf22bce25e5443f7a78b7ed813c059e3d32aff5a7bdaedd94f483", "exact_match": 1.0}
{"doc_id": 3470, "doc": {"question": "who was the leader of the soviet union when the berlin wall was built", "answer": ["Khrushchev", "Nikita Khrushchev", "Joseph Stalin"]}, "target": "['Khrushchev', 'Nikita Khrushchev', 'Joseph Stalin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the leader of the soviet union when the berlin wall was built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Nikita Khrushchev"]], "filtered_resps": ["Nikita Khrushchev"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b01f7fbf19c9a293d262a10a30ee6872368588188c0647a54aa80d2d2b224170", "prompt_hash": "a0440df2e1631a74725b99fadb22ef1a63d2df9466c01e6fbad6ac5bfa7d4655", "target_hash": "8f345a04690c485531cddb82ced9eb27f1da6faadb838bef345ea3d866fdb10c", "exact_match": 1.0}
{"doc_id": 3471, "doc": {"question": "when was the term prime minister first used", "answer": ["1624", "18th century"]}, "target": "['1624', '18th century']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the term prime minister first used?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1782"]], "filtered_resps": ["1782"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "767df4b9555fce0fc1c2053e0548c3bfdd800bb7ddd787953566f6a8908026a7", "prompt_hash": "c283d9acf590b5248428d1f1b80b2ee5f6f1adfca977bb9a01200651a067a81e", "target_hash": "4cfaf2f226553d537643d2ab7998d8c0ac304a4ffdb3f6224872a39fa3256618", "exact_match": 0.0}
{"doc_id": 3472, "doc": {"question": "when was the time the eagles won the superbowl", "answer": ["2017"]}, "target": "['2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the time the eagles won the superbowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1980"]], "filtered_resps": ["1980"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "99c05033919bd94c23aafeb3565875ae14628cb8fecccd185bf2620a5c8e8fae", "prompt_hash": "ad067d5db125ac378d9d06050e59edb3ae4036c55d074f550665d2319e5b3fd4", "target_hash": "2c05de14862a14e7ac9d20f9ce28474a8d24185341314bb815469592e3da38d5", "exact_match": 0.0}
{"doc_id": 3473, "doc": {"question": "who named all the animals in the world", "answer": ["Adam"]}, "target": "['Adam']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who named all the animals in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Noah"]], "filtered_resps": ["Noah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "27f0866f1e27fe8d7af164d174b6974730e68b47b890b2801aa47a281d46248d", "prompt_hash": "0cf0545a26a3930bc54157a9f1c9841498257ab4338f1309b0d3ff6a4b450c99", "target_hash": "0effd891bfe1eef87d19737b6587d06beb4767770060fd5653b4fce3b4f4b6e8", "exact_match": 0.0}
{"doc_id": 3474, "doc": {"question": "the most stable mineral at the earth's surface", "answer": ["quartz", "feldspar"]}, "target": "['quartz', 'feldspar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the most stable mineral at the earth's surface?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Quartz"]], "filtered_resps": ["Quartz"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "01be93ec306d2d699e3950e4a94dfb246c2c4c1b3e73a0ff1c9379c3ccdd1f6f", "prompt_hash": "9af0c8f8d3ec19409ebbb4d72befabb067072b8a88ca6b211b7dcb7175c40b54", "target_hash": "9122e77dc61fce0394ec9b7ac3768225d88a8e464d4e6a2b77534d41b820767b", "exact_match": 1.0}
{"doc_id": 3475, "doc": {"question": "who played caesar in planet of the apes war", "answer": ["Andy Serkis"]}, "target": "['Andy Serkis']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played caesar in planet of the apes war?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Roddy McDowall"]], "filtered_resps": ["Roddy McDowall"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cc8ef52877eaf9a8a605d2412c7de5fd1e8742440c990cce656e5eda79224298", "prompt_hash": "9d06d0df0954e4246515744777422e9bbb13571875014a14c209ed47008732d8", "target_hash": "68a452147a514270edb8f4be4410f8bc9fc99d58d1fb52d680dd7201717ca54d", "exact_match": 0.0}
{"doc_id": 3476, "doc": {"question": "who abandoned traditional harmony and created the twelve-tone system for composition", "answer": ["Arnold Schoenberg"]}, "target": "['Arnold Schoenberg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who abandoned traditional harmony and created the twelve-tone system for composition?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Arnold Schoenberg"]], "filtered_resps": ["Arnold Schoenberg"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9d7c4b715b8fee15751f89a5efee3ba3a3c9e59201508868c9a7f945bac76e87", "prompt_hash": "6f9abb0dd221dfa5f575ec75e91a3ce7dae39daa6661d458108944d02e7e7a7a", "target_hash": "a77a275a58182d17840eaf3479575f63d14b497c877c23d3402c6a6eebc41e44", "exact_match": 1.0}
{"doc_id": 3477, "doc": {"question": "who are the nbc olympic ice skating commentators", "answer": ["Jonny Moseley", "Tanith White", "Chris Vosters", "Luke Van Valin", "Charlie White", "Johnny Weir", "Tara Lipinski", "Andrea Joyce", "Terry Gannon"]}, "target": "['Jonny Moseley', 'Tanith White', 'Chris Vosters', 'Luke Van Valin', 'Charlie White', 'Johnny Weir', 'Tara Lipinski', 'Andrea Joyce', 'Terry Gannon']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who are the nbc olympic ice skating commentators?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Johnny Weir"]], "filtered_resps": ["Johnny Weir"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aaacfc97f9533e2bc9e8cd19693c78b4f7ce47a47b39d584ad2d8f2c66bd28ca", "prompt_hash": "9afa5f3e8709dc947e0de7e5a8a7f5bcfc6585586d23e7cdef265a1c386b4639", "target_hash": "b4bf17b3956b6429af88f992f6a600d468d55ca140603f3f59f293045da3fd65", "exact_match": 1.0}
{"doc_id": 3478, "doc": {"question": "where does the band imagine dragons come from", "answer": ["Las Vegas, Nevada"]}, "target": "['Las Vegas, Nevada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the band imagine dragons come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Las Vegas"]], "filtered_resps": ["Las Vegas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "06a1fa87dbaae0420685ee86b71a18d74714975f9d01890801df7c09f5abe0b7", "prompt_hash": "8603d4a82c0db89968adc8768940a93176827096ebdbb4aa76c839b886038676", "target_hash": "f13f4d36c363bbdf60f0d5bfa983e4b4be066dc38aa98f2cb3dc6d77c6a3d2bc", "exact_match": 0.0}
{"doc_id": 3479, "doc": {"question": "when do new episodes of ncis los angeles return", "answer": ["May 13, 2018", "October 1, 2017"]}, "target": "['May\\xa013,\\xa02018', 'October 1, 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do new episodes of ncis los angeles return?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" January 10"]], "filtered_resps": ["January 10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "89accbd504833f29d56511f0f0698495f2683a5210a91e32bffd2581de26d369", "prompt_hash": "272957f8b9bfb00b0a9de2a631a620ddfa2f21334f244424b35b7161f098fae0", "target_hash": "3f29c28e73735e6fe6902964c6e2e8242755811ac35b84040ab33bb6d12a8007", "exact_match": 0.0}
{"doc_id": 3480, "doc": {"question": "when does elena turn into a vampire in the tv series", "answer": ["season four", "fourth season"]}, "target": "['season four', 'fourth season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does elena turn into a vampire in the tv series?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" season 3"]], "filtered_resps": ["season 3"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a077c684bc3a4604a67aa9c5f97a25c14dae51d845b00fb58caa6663e7dc4a52", "prompt_hash": "d08e980c98160b504913af5306d1b7850a9ff14e20344e6378afe0ce68682657", "target_hash": "37bebf182c2f2652e7fb80915ebf4274fd01026518d0cc660afbe630752a7ef1", "exact_match": 0.0}
{"doc_id": 3481, "doc": {"question": "who wrote the country song i can only imagine", "answer": ["Bart Millard"]}, "target": "['Bart Millard']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who wrote the country song i can only imagine?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bart Millard"]], "filtered_resps": ["Bart Millard"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "369199f751d7dc6ca760010a46268c05c75ae09970be2ac49feb4892510ca99a", "prompt_hash": "8c3ddce859fdc7491e2b226dce31ce44431f8dc03f7504bcf3e5fd9e816a1a8a", "target_hash": "c721f0d30d6406be0d67857ed6d4d1237bdf20aa40e5b9d6b66ca424722eec8a", "exact_match": 1.0}
{"doc_id": 3482, "doc": {"question": "where does the water come from to fill lake eyre", "answer": ["Warburton River"]}, "target": "['Warburton River']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the water come from to fill lake eyre?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" rainfall"]], "filtered_resps": ["rainfall"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c34f9571a3cc303d55f2d5536b06b14a6017485a663a65c5bb34ee0659953635", "prompt_hash": "0d65807cadd9e573534f88dd8934fe38fad76ca68ac2090b94c50150978488fd", "target_hash": "df35b3622694454100f8bdd0b693e7b3356f96df0f0d6f15a9fd83f9af4147e7", "exact_match": 0.0}
{"doc_id": 3483, "doc": {"question": "when does season 2 of just add magic come out", "answer": ["October 14, 2016", "January 19, 2018"]}, "target": "['October\\xa014,\\xa02016', 'January\\xa019,\\xa02018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does season 2 of just add magic come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" October 11"]], "filtered_resps": ["October 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a725cc22de1077ceb0dcb9de13ae50c2b54d26f49e2bf412a50345f9d6778d9d", "prompt_hash": "980e539260e501492a42911d650ebf517e60119c3143253bb42605962f45bb67", "target_hash": "a70d3d73edffb9e549fe275360827e143c4b5d7eab72672689261b8a0f7d9d9e", "exact_match": 0.0}
{"doc_id": 3484, "doc": {"question": "latest season on keeping up with the kardashians", "answer": ["fourteen", "14"]}, "target": "['fourteen', '14']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: latest season on keeping up with the kardashians?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 20"]], "filtered_resps": ["20"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ef3958ef50ee55a8f96d6d4a40c00e6f0c5c728870717657c730324bd70e3452", "prompt_hash": "086d780c1e7340b8f2b10b620daca2d51f22c79190fcfb8195a85a3c48bad533", "target_hash": "e1469fcc52cad9d886bc38a026c458fd99fd4397e787189e193da99266e457a2", "exact_match": 0.0}
{"doc_id": 3485, "doc": {"question": "who won nfl football coach of the year", "answer": ["Sean McVay"]}, "target": "['Sean McVay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won nfl football coach of the year?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Sean McVay"]], "filtered_resps": ["Sean McVay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4beb6baf2102be397eef6067f8a0fc3e1af3520ad2f15f533014ffd75ab758bc", "prompt_hash": "032a13c95459da54e31d29bdfb0e5e8710a439cf272df7375add3c9067ba232d", "target_hash": "deba3d89f1f3be9f5176e538b29cc73efbc7038fa8dcbbf39d4c3c05e11cfbcd", "exact_match": 1.0}
{"doc_id": 3486, "doc": {"question": "who played the virgin in conan the destroyer", "answer": ["Olivia d'Abo"]}, "target": "[\"Olivia d'Abo\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played the virgin in conan the destroyer?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Linda Hamilton"]], "filtered_resps": ["Linda Hamilton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e3906fbdd4976890c4f0c16313b0b0bf7423b85eb6abf0f517d7c62dea3f32ee", "prompt_hash": "e295fb1fc2c57fd47132512305f716cf001f4db4d703701c3350dbc8acdce9b5", "target_hash": "3bb910bcef08d003dd2a7be51f87d65f447464d5b2748f648efbcdac684b2151", "exact_match": 0.0}
{"doc_id": 3487, "doc": {"question": "when did the black death end in england", "answer": ["December 1349"]}, "target": "['December 1349']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the black death end in england?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1477"]], "filtered_resps": ["1477"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ae9b8d0a240fd273b3f4505a4022ccafc0ca85a0a138938b5b1e72cdf78b716b", "prompt_hash": "55bd01a7fedb4461e9761296518da6c296fc73c7ffa44ed45aaba0b078ed1118", "target_hash": "ff6a10c4b2b79ffb890ef1607fb0e2b07a44ab1da861a7a14e70d38b7c86ab3e", "exact_match": 0.0}
{"doc_id": 3488, "doc": {"question": "paul walkers cars in fast and furious list", "answer": ["1995 Toyota Supra", "1995 Dodge Stealth", "1995 Mitsubishi Eclipse", "2002 Mitsubishi Lancer OZ Rally"]}, "target": "['1995 Toyota Supra', '1995 Dodge Stealth', '1995 Mitsubishi Eclipse', '2002 Mitsubishi Lancer OZ Rally']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: paul walkers cars in fast and furious list?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dodge Charger AWD AWD"]], "filtered_resps": ["Dodge Charger AWD AWD"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7490ec7f7fc813e0ce34f6c88f76a8b6e302139e0226b5ff6fee221bfbee3af4", "prompt_hash": "1d1c45b6ce2c022ed60c0c67c39a512579d6bbe118e40df2664959d1ab38b793", "target_hash": "0313a064b875932f34b6a128aff7d068f23dec80be6a72ceb4ac1355a93c4029", "exact_match": 0.0}
{"doc_id": 3489, "doc": {"question": "when boy meets girl waiting for a star to fall", "answer": ["1988"]}, "target": "['1988']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when boy meets girl waiting for a star to fall?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2010"]], "filtered_resps": ["2010"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2a58c40d05ca6547d8bcc515f16c396512a0accc5fa1cf25e1ee23e1e06168d2", "prompt_hash": "94828d33eee092d637c7bf8ea8440f4adb13234d55cf2ee7c9037a6202acfde6", "target_hash": "e071edb0a03ed21345f1ef6f9c0c506a8efd6860713d87f2bf676c9c0677b897", "exact_match": 0.0}
{"doc_id": 3490, "doc": {"question": "christmas though a christian holiday began as a celebration of", "answer": ["the winter solstice"]}, "target": "['the winter solstice']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: christmas though a christian holiday began as a celebration of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the birth of Jesus"]], "filtered_resps": ["the birth of Jesus"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f4f71a5205f492179c21a921cee5aa0e9d12ad2c99a581deea2e8dc205c8dd78", "prompt_hash": "7927327f552775528dbb1a7d6fd5e1ac3dbe4171bec5a56c04c40d3f86f30cb4", "target_hash": "f1d94570c74744bf67d2ce219490f9fb8d39219e366828e6e1860da477716837", "exact_match": 0.0}
{"doc_id": 3491, "doc": {"question": "who caused to build qutub minar in delhi", "answer": ["Iltutmish", "Firoz Shah Tughlaq"]}, "target": "['Iltutmish', 'Firoz Shah Tughlaq']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who caused to build qutub minar in delhi?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Qutb-ud-din Aibak"]], "filtered_resps": ["Qutb-ud-din Aibak"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "91518bf2606638d3397611d01a723915b86e81ce5717151ee62d53db03a3ebfe", "prompt_hash": "ee040ddc635b4dbc8095385a061edb868e235d2a202a480e4a0d9d344fb320ba", "target_hash": "893b78508aeaf6860755258bfb9bc35af7319aeda5dcba8f7a2402b062be9963", "exact_match": 0.0}
{"doc_id": 3492, "doc": {"question": "who changed the role of the first lady", "answer": ["Dolley Madison"]}, "target": "['Dolley Madison']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who changed the role of the first lady?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Eleanor Roosevelt"]], "filtered_resps": ["Eleanor Roosevelt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "95c9ec1d647981180736fdc503b6225d7a87f155f5adfd6529764b55c45e8689", "prompt_hash": "88ac1728a9562e50d995e40b4f35b62c27752436e831d4a308c1ec898d85603e", "target_hash": "fd016333b6f1f6ad03990a88df6fe2bb78b241de443d62f00a8d7e6a2fd801c6", "exact_match": 0.0}
{"doc_id": 3493, "doc": {"question": "who sings two pints of lager and a packet of crisps", "answer": ["Christian Henson", "Splodgenessabounds"]}, "target": "['Christian Henson', 'Splodgenessabounds']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings two pints of lager and a packet of crisps?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The La's"]], "filtered_resps": ["The La's"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "797831999da138ce9201dac5ae53f8b90a775af8e2f4aa5cd9de8944b0bcc2bf", "prompt_hash": "5f2d2de4101e88361bbf5cca123e865609a2c6f6c0e972740655bc2bdbca8690", "target_hash": "33b124a0a24b79cabc8dba5cd8929acd35749e8215c9681debab4ad2a4b46d13", "exact_match": 0.0}
{"doc_id": 3494, "doc": {"question": "when was the last time a hurricane hit the uk", "answer": ["the autumn of 2017", "2017", "autumn of 2017"]}, "target": "['the autumn of 2017', '2017', 'autumn of 2017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time a hurricane hit the uk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1987"]], "filtered_resps": ["1987"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9e5f9aea3bfca8c77dc3b388a15e913df89699a8e4dbb844facd52fde5d072be", "prompt_hash": "9d07f5307f7906e3fae030b45f7143f6154f732eb71f6d3a1fc89b9d721e0027", "target_hash": "c2c59f747f1fc5f499210c26d7c8b9f32eb5b3bfdf75d3bb11e69d5af8e09277", "exact_match": 0.0}
{"doc_id": 3495, "doc": {"question": "who was the declaration of independence written for", "answer": ["the Second Continental Congress"]}, "target": "['the Second Continental Congress']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the declaration of independence written for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" the people of the 13 American colonies"]], "filtered_resps": ["the people of the 13 American colonies"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "476f7f575ff5a92275f6df1afeaca595327bf2a95b7685e1b917df11af9ef5f7", "prompt_hash": "9957cafe77a529959c2cfcf19fc1d5a8ac0a17b47d2332e281df3d018c02f1e9", "target_hash": "0159b9a6660532cb1f560d38c29f057971004d020d3efd79d59ac5919bfad668", "exact_match": 0.0}
{"doc_id": 3496, "doc": {"question": "who played susanna in legends of the fall", "answer": ["Julia Ormond"]}, "target": "['Julia Ormond']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played susanna in legends of the fall?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cynthia Nixon"]], "filtered_resps": ["Cynthia Nixon"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "613683f6c46eb8c1dd05ff83a6f4d2a7d0c8ad25252c6e53285dde98a338adc2", "prompt_hash": "5486713e31b66ad82d6430af6985e0d9bf544251d4a2d71791ba4f94fe66688e", "target_hash": "1524571b1ed8ca98391fbb30914d1362634dc6da0424c1f4e89a561f3d3b3814", "exact_match": 0.0}
{"doc_id": 3497, "doc": {"question": "where are they building the new raiders stadium", "answer": ["Paradise, Nevada", "in Paradise, Nevada"]}, "target": "['Paradise, Nevada', 'in Paradise, Nevada']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are they building the new raiders stadium?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Las Vegas"]], "filtered_resps": ["Las Vegas"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ebfc56ddc668777461f09d8647dc2296b904c0514232f42e69bebde56bf9ca02", "prompt_hash": "05c9a9e1bef855165e54cd018c562b0c5fa0385009651b4a37377dcaf5edfc27", "target_hash": "6d98a9cbb526e8877ad0b122033cc2916e6fc1ba5055976f610d4b8847c120d6", "exact_match": 0.0}
{"doc_id": 3498, "doc": {"question": "who was the voice of templeton in charlotte's web", "answer": ["Paul Lynde"]}, "target": "['Paul Lynde']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the voice of templeton in charlotte's web?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Don Keyes"]], "filtered_resps": ["Don Keyes"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "acfe7814621d46f83a370450b8f87336a610d35040e9d94f84a32dfd16e26493", "prompt_hash": "5e4a86bf7600d596d2a7ca5300e3d56bd98eb12a8b7a64ca8d07607fb00f762b", "target_hash": "c8af525ab9594c38a2e297c0f6f9ffc633f6cde0e36517c84b54016df10d72f3", "exact_match": 0.0}
{"doc_id": 3499, "doc": {"question": "who has the most rings in the nba right now 2017", "answer": ["Boston Celtics center Bill Russell", "Bill Russell"]}, "target": "['Boston Celtics center Bill Russell', 'Bill Russell']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who has the most rings in the nba right now 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kobe Bryant"]], "filtered_resps": ["Kobe Bryant"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9eabbea63ac4bbbbdcc464ff7c2f4b9ca1ba51cbb2fcbbeb4cc0f64de07839ae", "prompt_hash": "9aca68433fe19ce3fd93790728d2722c8fd5178ec7a4d7e17c98cba9108c477f", "target_hash": "9d023bbfd0a343e9665ac2f8f476c30ea03c7e3c92f7f9f586c34e9ea630384a", "exact_match": 0.0}
{"doc_id": 3500, "doc": {"question": "the actual name of the confederate force at gettysburg was", "answer": ["Confederate States Army"]}, "target": "['Confederate States Army']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the actual name of the confederate force at gettysburg was?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Army of Northern Virginia"]], "filtered_resps": ["Army of Northern Virginia"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a61c191398c50603d4128c7f7773ef5eba361ccfb9fdb2bee68a25e27b844ba1", "prompt_hash": "68af3b5c329eb20854418ece0bd6a7a7dd77c539b8b8267b36cd37b9e138b00b", "target_hash": "9e6c3c5f72a9ca20262242a4a85a76b15a08512a9d174a5245a4494a81299e32", "exact_match": 0.0}
{"doc_id": 3501, "doc": {"question": "when was the last wolf pack killed in yellowstone", "answer": ["1926", "by 1926"]}, "target": "['1926', 'by 1926']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last wolf pack killed in yellowstone?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1926"]], "filtered_resps": ["1926"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "74510e55fe2bc272f4f0fea3ad32524c85deed119a80c2df98755b20ea9d52b1", "prompt_hash": "374c35f0f5d0e5390e571ee81eb974f655407e74e8e109594da616a736ce7aec", "target_hash": "dc77c13f774edeea7787b78d1ddb73638263c932512f898975372a7eac6376b9", "exact_match": 1.0}
{"doc_id": 3502, "doc": {"question": "new anime from the creators of attack on titan", "answer": ["Vinland Saga"]}, "target": "['Vinland Saga']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: new anime from the creators of attack on titan?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Shingeki no Kyojin: The Final Season"]], "filtered_resps": ["Shingeki no Kyojin: The Final Season"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "99b45dc5b6e6595274d7e035ea442083899cad70d43f8f0cf899d51425a8eeaf", "prompt_hash": "349ba8598069a7752ee0acabf0fbf1dc69c2ffc5717fb179c426485053a1339f", "target_hash": "86b8bf69d41614dbbe13bed75599838d0a96b853ef54516c491f94b122f57dbd", "exact_match": 0.0}
{"doc_id": 3503, "doc": {"question": "who lives at the end of king lear", "answer": ["Kent", "Albany", "Edgar"]}, "target": "['Kent', 'Albany', 'Edgar']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who lives at the end of king lear?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Edgar"]], "filtered_resps": ["Edgar"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d212c3ba75642bf222e1b63da4104101a2a423bd6f92d7a4ef4691d64a819b1d", "prompt_hash": "b576bfcdbfca25d3b832b27f6d4061b2ca7a6833b5d027d3a64ab352c96f307c", "target_hash": "51858a768c0c4340ec8cc42e1f206dab6544799a4c203e2bed9ff6cd21acedb0", "exact_match": 1.0}
{"doc_id": 3504, "doc": {"question": "deep water fishing boat with many baited hooks", "answer": ["Longline fishing", "Longliners", "Longline"]}, "target": "['Longline fishing', 'Longliners', 'Longline']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: deep water fishing boat with many baited hooks?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" longline"]], "filtered_resps": ["longline"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ce8b8d90fe40f971cac2b11ab31f0d3bbeb687f166bf6d35b2c076e540f0e1ca", "prompt_hash": "aea2383f81c198fd87d37a84c09431190c6532d66666ad2ec0a41cb5df92eeee", "target_hash": "e67354b052a87e1b5d4c3a2ffe64bab7b7d405e2fc8594e2af9aca0e70d26510", "exact_match": 1.0}
{"doc_id": 3505, "doc": {"question": "where was gunsmoke to the last man filmed", "answer": ["Tucson, Arizona"]}, "target": "['Tucson, Arizona']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where was gunsmoke to the last man filmed?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New Mexico"]], "filtered_resps": ["New Mexico"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "74a58ca56a01fce35cdc4468035923e6b0975929a30a86f0c31b4ace101884b6", "prompt_hash": "1c8c14f8a2a99ab01cca879fe04aa3ebe52a3258c5df1b1f669b21a9687dc778", "target_hash": "648037473b9b3a640374d2dc46971ce4eef37abf7d79677c1955a75439395dde", "exact_match": 0.0}
{"doc_id": 3506, "doc": {"question": "when was the defensive 3 second rule implemented", "answer": ["the 2001–2002 season", "2001–2002 season"]}, "target": "['the 2001–2002 season', '2001–2002 season']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the defensive 3 second rule implemented?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1931"]], "filtered_resps": ["1931"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "02a9b0c1b33501e9e2573408597cc5b12e70e0e97ed2cdf4fbcdabab526f0be0", "prompt_hash": "0ffb935bfb57e207a2ad5b6f96a401e4564a517465dd40d149b9ee09cdb576e6", "target_hash": "e7aae07ff8a4e3c28ed9c2a255e3e56be285b5ec32ac581456f83895553fd87e", "exact_match": 0.0}
{"doc_id": 3507, "doc": {"question": "who represents the arizona 9th district in the house", "answer": ["Kyrsten Sinema"]}, "target": "['Kyrsten Sinema']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who represents the arizona 9th district in the house?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ann Kirkpatrick"]], "filtered_resps": ["Ann Kirkpatrick"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bbaad7faf7c0de2179aba73813e16de4bc143dc7d090f188f861a6c678c594e3", "prompt_hash": "90fb80df098f1b1bf23fc062b09d65830d8b9793b6be8bb151857b6aa201a4b3", "target_hash": "5b67005cdc00d6e011aab268e88f2b2af6c60831752caeceb5e146330e2e9882", "exact_match": 0.0}
{"doc_id": 3508, "doc": {"question": "this type of nuclear decay effectively turns a proton into a neutron", "answer": ["beta decay"]}, "target": "['beta decay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: this type of nuclear decay effectively turns a proton into a neutron?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" beta decay"]], "filtered_resps": ["beta decay"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "21c46c6cb025d8c750c5eb557fddbd83e6fd15e083250080ed35000bb881f548", "prompt_hash": "2dbe557cb7bdbe4d95a138929fff5d14f88fb7ccff9d2de9d5adf88ff99faf0c", "target_hash": "b2848797967a527ac97af1ac74ed5adf7f4248089fd15708838ff940a26f0be5", "exact_match": 1.0}
{"doc_id": 3509, "doc": {"question": "who drove the number 18 car in nascar", "answer": ["Kyle Busch"]}, "target": "['Kyle Busch']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who drove the number 18 car in nascar?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dale Jarrett"]], "filtered_resps": ["Dale Jarrett"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bac7cdfd7589878b2fe0d7466ef6d09cc46e0467743e84e8c8d8a24dd9f91184", "prompt_hash": "e01ade717cd788c077e772d5765bdf48b3f09068c3d5c580911f841bcab934d4", "target_hash": "c986c9287f293cc1208907bdc1261bd563276a7fbd823ff93650e52c026067f9", "exact_match": 0.0}
{"doc_id": 3510, "doc": {"question": "who sang buddy can you spare a dime", "answer": ["Rudy Vallee", "Bing Crosby"]}, "target": "['Rudy Vallee', 'Bing Crosby']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang buddy can you spare a dime?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Frank Sinatra"]], "filtered_resps": ["Frank Sinatra"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "029bad02abb8225cc07457327962d1cf14a57fcbd53537ff278cb3dcd2fcc103", "prompt_hash": "c10b934d0563354fd3ae9a647bb521430369fdbc15b32a92c14baa6cc97887c5", "target_hash": "77cd2da89291453ae98316604b42ff10e09d1da1a4889130240c6895eabe9982", "exact_match": 0.0}
{"doc_id": 3511, "doc": {"question": "what is the first covenant in the old testament", "answer": ["the Noahic Covenant"]}, "target": "['the Noahic Covenant']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the first covenant in the old testament?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Noahic"]], "filtered_resps": ["Noahic"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c5a281dfb2a1ff62b328da4a1f81af3803a55d4014d6680868f7d497ea8f5f00", "prompt_hash": "ac10245f05e5eab4d1cd4afc2f132156f912ee4309248a311098c8b49423cee8", "target_hash": "a7f0c3b745165a4f4035eda0cec430d6ba7dc138d8fe068ec70ab08965d12660", "exact_match": 0.0}
{"doc_id": 3512, "doc": {"question": "who validated the civil rights movement by proclaiming we shall overcome", "answer": ["President Lyndon Johnson", "Guy Carawan", "Lucille Simmons"]}, "target": "['President Lyndon Johnson', 'Guy Carawan', 'Lucille Simmons']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who validated the civil rights movement by proclaiming we shall overcome?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Martin Luther King"]], "filtered_resps": ["Martin Luther King"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "65d64cf85f5df11ad547b797fa8475ea1b9adedefcc76714edc7a9d6c8020287", "prompt_hash": "3661a4f58e99df1d6b72bf9ad15877be1eb79ea45c1946b702e425c5cf7ad328", "target_hash": "5555f0a264bb6c1acac4137e91d2b15f874e39025b559eb78a90ad4688328361", "exact_match": 0.0}
{"doc_id": 3513, "doc": {"question": "when was fingerprinting first used by the police", "answer": ["the early 20th century", "In 1840"]}, "target": "['the early 20th century', 'In 1840']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was fingerprinting first used by the police?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1905"]], "filtered_resps": ["1905"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "082c260f1d8822819f2e890e5c44ce1774c70df7d3712a32ccaa7b40092a2dba", "prompt_hash": "9352e98cf5c24c967295c2bf6d6512ed667bcf626e736d58b2790dd5110add9b", "target_hash": "f0caa1818eede43ab7adc63a4d430b8b0f6c68aa2465b68df97895cf5ddcd1f1", "exact_match": 0.0}
{"doc_id": 3514, "doc": {"question": "where does aarp fall on the political spectrum", "answer": ["non-partisan"]}, "target": "['non-partisan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does aarp fall on the political spectrum?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" center-left"]], "filtered_resps": ["center-left"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4ce0d568a4254d79e4639b331ffff9d98e592535e6112210519818707fc22397", "prompt_hash": "fcd91326c59091ebb9c0e47c53c3e59090adedc5f99c86a0bfd4dc0d3cea4118", "target_hash": "775268427e567131978b2ac9e31dc41f4ad0f4585b9951da1e77ac21a47d176a", "exact_match": 0.0}
{"doc_id": 3515, "doc": {"question": "when did the united states acquired puerto rico", "answer": ["1898"]}, "target": "['1898']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the united states acquired puerto rico?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1898"]], "filtered_resps": ["1898"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ab27d9d4d9f5ac41b90b71d2fdb7e6b8d62557e44d236a33354d01f5c662532d", "prompt_hash": "51e5e704c2224115b83d74748c43c8871d4acc97c60056720b531bf68b14dcd8", "target_hash": "d24377e00d603d57f6b06cf8fe63e117c28df36077e7854da03067d8611ebbfd", "exact_match": 1.0}
{"doc_id": 3516, "doc": {"question": "who's the original singer of help me make it through the night", "answer": ["Kris Kristofferson"]}, "target": "['Kris Kristofferson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who's the original singer of help me make it through the night?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Bowie"]], "filtered_resps": ["David Bowie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a555f80baa1877e1d03e640b3b2b2b7f09158f992258a88607b82ad7aa4fbe7e", "prompt_hash": "c3f7fd7ce780dccd2561c75f829ab66fdb3386a0f12708e5062f60882369b8c4", "target_hash": "6f4505ea478ab91bf374a66a2f6d25fe9ceb65a5008f6c8f13341f12d1a0c0af", "exact_match": 0.0}
{"doc_id": 3517, "doc": {"question": "nodding your head no is an example of", "answer": ["a gesture"]}, "target": "['a gesture']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: nodding your head no is an example of?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" non-verbal communication"]], "filtered_resps": ["non-verbal communication"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1b3f7f5927d3563bf10952c3df4fdba1946fb4e0bbaaec97429283d5515f6951", "prompt_hash": "671a4864ab06d4897dcc57e89b4d50c78fb027584d162d1551a43c0a086efe28", "target_hash": "445f886d960b9ccad95240ecf5b639105b3fca52e056a754bf732d060c55ec16", "exact_match": 0.0}
{"doc_id": 3518, "doc": {"question": "who was running as vice president in 1984", "answer": ["George H. W. Bush", "Ferraro", "Vice President George H. W. Bush", "Congresswoman Ferraro"]}, "target": "['George H. W. Bush', 'Ferraro', 'Vice President George H. W. Bush', 'Congresswoman Ferraro']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was running as vice president in 1984?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Walter Mondale"]], "filtered_resps": ["Walter Mondale"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a89e708612fd1cb89ceef6a00f814651dd80e533218c1bb139db6df58d87bd43", "prompt_hash": "5865d8b347c110d888668c48257e6f38a47fae00290cc449089e4f50e361c622", "target_hash": "62ce8f62bd201c14750202a355812da934f44bd23f8ba270987d77d702650ccf", "exact_match": 0.0}
{"doc_id": 3519, "doc": {"question": "how long did the democrats control the house and senate", "answer": ["57 yrs", "35 yrs"]}, "target": "['57 yrs', '35 yrs']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how long did the democrats control the house and senate?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1931"]], "filtered_resps": ["1931"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b51da6c6e50b937190943ea283ce37fdaa9c9ba01d0a97907d0408b2711e1202", "prompt_hash": "9592935e15c19bc75ff081f887f26a612a2132fce8a2594bee1c64e78e3d970f", "target_hash": "e60eb128a82b5ed3dd0394b8b34194e0edef1a88d2383d3f9e5bfed733e4cf43", "exact_match": 0.0}
{"doc_id": 3520, "doc": {"question": "what kind of book is where's waldo", "answer": ["puzzle"]}, "target": "['puzzle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what kind of book is where's waldo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" puzzle"]], "filtered_resps": ["puzzle"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "330c1d6e2a9aaca0d271c3089c2b763d778c69fdc8d317eec35e9a297096b38d", "prompt_hash": "eac05f9a66a2d347216e832a86249b45ba5b6281623a8fd26dff4e999e577e94", "target_hash": "252f9428a1ef1197e5c3846c43376ab2ecfba8c33531f0b2fff36ce978d19a30", "exact_match": 1.0}
{"doc_id": 3521, "doc": {"question": "when is beyblade burst super z coming out", "answer": ["April 2, 2018"]}, "target": "['April 2, 2018']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is beyblade burst super z coming out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ebf47028362eeff21deb18eddcfde92d816a375db1fe2e68861f90239d079cd7", "prompt_hash": "8a2aae130e06f99bfe1572c294950758a1431d04216f06d3b9bc7390af4e9dd9", "target_hash": "2397ab23719c90e15a6a2e5ab8ca2d3d5c0e7150517d4736552e4a3cea42fc38", "exact_match": 0.0}
{"doc_id": 3522, "doc": {"question": "who is the leader of the ontario pc party", "answer": ["Patrick Brown", "Patrick Walter Brown MPP", "Patrick Walter Brown"]}, "target": "['Patrick Brown', 'Patrick Walter Brown MPP', 'Patrick Walter Brown']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the leader of the ontario pc party?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dalton McGuinty"]], "filtered_resps": ["Dalton McGuinty"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6be2cd54d2a014d590e25d745fdaa67d28992f8029c163f5d2d838cae1d34056", "prompt_hash": "93130c9d6bf0faba2f0aaf08f59bf0207483c2c837d6e714b5973882c1e45852", "target_hash": "d0fae969e2850ff27dbaca5ff95a20cc9c5c9dd3db4b0d78909b4b1679d7834f", "exact_match": 0.0}
{"doc_id": 3523, "doc": {"question": "who plays the bad guy in the good place", "answer": ["Ted Danson"]}, "target": "['Ted Danson']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the bad guy in the good place?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Schwimmer"]], "filtered_resps": ["David Schwimmer"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d604c362cda8634fd0c1b304daa31b19495d8ec804441951476b377f9dfc6dc8", "prompt_hash": "9b26bf2c8cd007da243727b86243b5c75cccfa0b867680614714c86daaeccdb8", "target_hash": "ab62e2384f8d39744b837e455fc225c386c3f94f3e356629cc1a3ebfffdeb2e6", "exact_match": 0.0}
{"doc_id": 3524, "doc": {"question": "who is covering the winter olympics in canada", "answer": ["TLN", "Rogers Media", "CBC/Radio-Canada", "Bell Media"]}, "target": "['TLN', 'Rogers Media', 'CBC/Radio-Canada', 'Bell Media']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is covering the winter olympics in canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" NBC"]], "filtered_resps": ["NBC"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "39761c624f3f597d1289d3763494e9e6c8c9b3b979118b04ff86ae7faf956674", "prompt_hash": "df5367c556a994d4d056284ffdb190a33edb600e21076b454350ec708489a66c", "target_hash": "92391b72d5d7a4175a890e4b96e0774cc47e56b36ffae34adddd8d414fc26553", "exact_match": 0.0}
{"doc_id": 3525, "doc": {"question": "most polluted city in the world according to world health organization", "answer": ["Zabol"]}, "target": "['Zabol']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: most polluted city in the world according to world health organization?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Delhi"]], "filtered_resps": ["Delhi"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3015606e9ed1fff8d53b86b18014cf901652aff9c19f150112755ec49ca0b1fa", "prompt_hash": "45f733295409c6c5e2275c5c16e2ad1fbbd5c015c4fb3de5e76f341eb9d11a23", "target_hash": "050c257e33bcef97fcf151ba0209cbfaeabb91749d61919e89e7fd6bb5e2618d", "exact_match": 0.0}
{"doc_id": 3526, "doc": {"question": "public companies are part of what economic sector", "answer": ["public sector", "The public sector"]}, "target": "['public sector', 'The public sector']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: public companies are part of what economic sector?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" private sector"]], "filtered_resps": ["private sector"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e9d0116e8cd94b0e6dbb3c71d1a5c6baddce1f73f36fd52dc98ab006bb79d697", "prompt_hash": "94b5a608d951405954e4a4f53e18df6a84eb6c2047814d242c2124ff96c98959", "target_hash": "a7240ba8487c09d4d136281aab2d9f0491df99ac9e753f7f44a9748489852571", "exact_match": 0.0}
{"doc_id": 3527, "doc": {"question": "how many episodes in season 2 of fargo", "answer": ["10", "ten"]}, "target": "['10', 'ten']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how many episodes in season 2 of fargo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 10"]], "filtered_resps": ["10"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4d83bc785724fc3338e3f34fffdab7f383ad1230b59fb1173bfc8637736beaa5", "prompt_hash": "e859fc1cf1f5d932367bda5e0eda45e07f515e61f2f0c0b3182a19ff9e73eedf", "target_hash": "a5556ec28cc852b3a5bed65ed5fa2a162430f96c9825252cbb55f6aa0c956137", "exact_match": 1.0}
{"doc_id": 3528, "doc": {"question": "south african tree with thorns on the trunk", "answer": ["silk floss tree"]}, "target": "['silk floss tree']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: south african tree with thorns on the trunk?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" protea"]], "filtered_resps": ["protea"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9b6c070dc443f6685d21179fb57cbe39c377e1ce6ead6fddbc21fb5a9d351fad", "prompt_hash": "f6c439e8ca47b3447b32e979873d0a9c320313bef65b3079d385112c7e36a1ec", "target_hash": "fdc73153e4529aab8e100a500cf5b1fbf80a6da7a0ef56ddfe73363083c97588", "exact_match": 0.0}
{"doc_id": 3529, "doc": {"question": "when was the jury system abolished in india", "answer": ["1959"]}, "target": "['1959']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the jury system abolished in india?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1962"]], "filtered_resps": ["1962"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f281d62beeefb17c1f3d420546b5a5a68c43b9d080f2117548cdb82326726b5f", "prompt_hash": "5fee78dbe04133da29924f1bd8f0f2b81a421a44d181f49766cfe67d89f35f39", "target_hash": "f0d444a417e14e93a838f61282eafa9683308e2f4ca4a967d5c1c1281dfe3a0d", "exact_match": 0.0}
{"doc_id": 3530, "doc": {"question": "when was the us department of homeland security created", "answer": ["November 25, 2002", "November 25, 2002"]}, "target": "['November 25, 2002', 'November\\xa025, 2002']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the us department of homeland security created?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" November 25"]], "filtered_resps": ["November 25"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77fab99cb20033b78c9728168c4f3ce6613c1c5bb0b9d3a3c07728e3680354d2", "prompt_hash": "483c9a2d10929d0d46541500f8be68f4fd45616784d140a4f0ca6ed9552006a0", "target_hash": "bef57d4865384cd5a5e7be7e2c5568732c1aa19cb5bfe3a73ea2906423038a93", "exact_match": 0.0}
{"doc_id": 3531, "doc": {"question": "when did god save the queen became the national anthem", "answer": ["in the 1780s and 1790s", "After the Battle of Culloden"]}, "target": "['in the 1780s and 1790s', 'After the Battle of Culloden']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did god save the queen became the national anthem?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1952"]], "filtered_resps": ["1952"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "154581c50e08052d39c9f72005ee909b6e0ea570ce54671281922f9e3bc691ee", "prompt_hash": "3c421d3e733de57ccbb595733d8256a2501ada63efbc6ed2d80740578cc77ed8", "target_hash": "0672bc535f6dfee9c8e1af45f088dcae732adde000d3755a3c9e7c0134e0954b", "exact_match": 0.0}
{"doc_id": 3532, "doc": {"question": "who plays the coach in old spice commercial", "answer": ["Isaiah Amir Mustafa"]}, "target": "['Isaiah Amir Mustafa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the coach in old spice commercial?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Isaiah Mustafa"]], "filtered_resps": ["Isaiah Mustafa"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b1f63c77a90285f612354dc3eb438efe071a40c954682d77ccba9a604e810258", "prompt_hash": "6a039f9390d314d365115fbfccbefb0b8418faef5f9744e55d840cd295460c04", "target_hash": "d18275bcc06d37b5772dbc741ea15c1be146e23eeeea59b3d7d179b6667632fe", "exact_match": 0.0}
{"doc_id": 3533, "doc": {"question": "where can i get a state issued id", "answer": ["each state's DMV", "DMV"]}, "target": "[\"each state's DMV\", 'DMV']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where can i get a state issued id?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Department of Motor Vehicles"]], "filtered_resps": ["Department of Motor Vehicles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c4663143393ccf4caca7087a5bae1d131704adb0fcfc9d337758b09206a00bf", "prompt_hash": "78fdc89e9c0692fb0f517ddcbed3234c779e8545a4c1795561c097f4cb8ebda6", "target_hash": "7e13cea19cb2ea3bd536218f62c3c3bc7f426f7ba835ddd6be09da6797617493", "exact_match": 0.0}
{"doc_id": 3534, "doc": {"question": "where did hope on bold and beautiful go", "answer": ["Milan, Italy"]}, "target": "['Milan, Italy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did hope on bold and beautiful go?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" deceased"]], "filtered_resps": ["deceased"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e652b35210d2acfda9bd7d42cef86ff92c4a50504c7765f2d105d97551b0fa5b", "prompt_hash": "f84896dc639c914cc061dc214c3f38dc48e2cf6c67c7e633723e9b66df1cf752", "target_hash": "70a59e4e396310a0da3bcbd9ebef040d430efb9310d33c10822b1ef494c8042f", "exact_match": 0.0}
{"doc_id": 3535, "doc": {"question": "who sang on great gig in the sky", "answer": ["Clare Torry", "Pink Floyd"]}, "target": "['Clare Torry', 'Pink Floyd']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang on great gig in the sky?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Clare Torry"]], "filtered_resps": ["Clare Torry"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9cbe89aab1bbdde17df01420e4f1362fe0fe89c70e39db573729c00b26a7cf23", "prompt_hash": "b7e9a466ef8edab933f5aeb163a428a19d5b3422f8e6283b4fe9fe41ba81e227", "target_hash": "3061bf726c7483632170b9ff24e27e07a2932abc5bddb76966ef3d1f1a292ef2", "exact_match": 1.0}
{"doc_id": 3536, "doc": {"question": "who said some are born great in twelfth night", "answer": ["Malvolio", "letter that Maria wrote"]}, "target": "['Malvolio', 'letter that Maria wrote']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who said some are born great in twelfth night?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Malvolio"]], "filtered_resps": ["Malvolio"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "81cde27c650c7ba8660b9f0f5f423d460a4fde3f485000f273d8c66d445d1b86", "prompt_hash": "a63c178e7cf5ed81258011321a1dbe605badc8a385b5c56c6d669a4f3bc8b1be", "target_hash": "0cbeeb4b4f4099b79cd7a41c55314a000f9382ba4aecde05a151320feb0c2163", "exact_match": 1.0}
{"doc_id": 3537, "doc": {"question": "where is thank you for your service based", "answer": ["Kansas", "Fort Riley, Kansas"]}, "target": "['Kansas', 'Fort Riley, Kansas']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is thank you for your service based?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Afghanistan"]], "filtered_resps": ["Afghanistan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c3fbb59fc4045397da115a308ec0e0230e8af9366271e0d79dc6351325a2e0e3", "prompt_hash": "7fe74c94bd119e30d6b8ac02cd5d4cf9982e8726c5fd61fcd9043a7bba9cf47d", "target_hash": "e76d199d0d32eafdf6cbb446fa862c9986892b5f99c2faedf7c4a0714312829a", "exact_match": 0.0}
{"doc_id": 3538, "doc": {"question": "who sang the national anthem at the 2002 super bowl", "answer": ["Mariah Carey"]}, "target": "['Mariah Carey']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang the national anthem at the 2002 super bowl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Whitney Houston"]], "filtered_resps": ["Whitney Houston"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea071275d945a1bbb1f770a87006644edf0286b58dbffa6a3832659857fb215b", "prompt_hash": "47aeb928a8f70d116980094331158b2128b4d3bd222c9cbbff80970155bec6de", "target_hash": "82b007b2c1b87c06d7f4314a3387cf3ec9b1b1ed6e309af36a8940be142f0e43", "exact_match": 0.0}
{"doc_id": 3539, "doc": {"question": "where does the last name waters come from", "answer": ["Wales and Yorkshire", "Yorkshire", "Wales"]}, "target": "['Wales and Yorkshire', 'Yorkshire', 'Wales']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does the last name waters come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" English"]], "filtered_resps": ["English"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ee1f118aa6c76d3590ec854b8688f8302ea6fb427ff02464e03ed92c5d69b83e", "prompt_hash": "ef538562547a0f6eb6b6712f1ffc2011c2cd9d0e26985300ac999bc6afa89ca0", "target_hash": "ac2cc61339a9ec7ab4d138eab776d24aa25ce581a82ed23496c9da3ad97516e3", "exact_match": 0.0}
{"doc_id": 3540, "doc": {"question": "who played sonny in lemony snicket's a series of unfortunate events", "answer": ["Presley Smith"]}, "target": "['Presley Smith']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played sonny in lemony snicket's a series of unfortunate events?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Troughton"]], "filtered_resps": ["David Troughton"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8e0829794e3674c561ca209877a7dc028da12481b29e1ff92b3bfc415f895795", "prompt_hash": "0efc9b801ddcaf295c4ce05509777f2b70d013e0349726deed2f44a3db202a24", "target_hash": "755d0da0a00ecc79a0931fdab2306c3591d11c2edb6f56a05e04c8f6d1fb6ad4", "exact_match": 0.0}
{"doc_id": 3541, "doc": {"question": "when did the dust bowl end in oklahoma", "answer": ["1940"]}, "target": "['1940']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the dust bowl end in oklahoma?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1939"]], "filtered_resps": ["1939"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c037582f5a26f7c64ab24ec47f4ed788a02e2a5b99cfbf6446bd2cd51f197a09", "prompt_hash": "ca3003823bf3dd21cbfa9c86dcc43a50df9946826bd23df6f9181b521f113f99", "target_hash": "c0a04f24805cbc8a624fa2b86f8e91fbc157178d899ea8625545874db22764a9", "exact_match": 0.0}
{"doc_id": 3542, "doc": {"question": "how does the cash cab guy read the questions", "answer": ["walkie-talkie and earpiece"]}, "target": "['walkie-talkie and earpiece']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: how does the cash cab guy read the questions?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" by reading the questions"]], "filtered_resps": ["by reading the questions"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4171f82b66fca88a2e479291451d2e46037d841160bd85ecab86784f0773d356", "prompt_hash": "a74aa50048f45e95ba729c59e17d2a777105e861d413d74c0bef19bb07bf43cb", "target_hash": "6078dbd82e89c153da608f0dea0e6d19e0dba14434f4f5f14608347989d47ca5", "exact_match": 0.0}
{"doc_id": 3543, "doc": {"question": "who did the music for your lie in april", "answer": ["Masaru Yokoyama"]}, "target": "['Masaru Yokoyama']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the music for your lie in april?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Yuki Kajiura"]], "filtered_resps": ["Yuki Kajiura"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "dade8b1ffa3313d09ac607773c8265a8610ec566434bbcd86f850458923a32dd", "prompt_hash": "6ed4ece993c88a01b7d16fc52a0184a68f946caacd068468ff06b0e2ba01b7cc", "target_hash": "c1674a1df96ca49c049aa62470a134c1cbb593ebdea38a684755079dbc8c176f", "exact_match": 0.0}
{"doc_id": 3544, "doc": {"question": "what are the value of style guides and style sheet", "answer": ["improve communication"]}, "target": "['improve communication']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what are the value of style guides and style sheet?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" HTML"]], "filtered_resps": ["HTML"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "311061b416b3fde3e06fc05cfb35d06175ee409fcca41fa213fc4d6f8abec62b", "prompt_hash": "679f4cb6a66f79718279a7d41761c4604561bd694e2e93e0d9b11d14694ab45d", "target_hash": "6f7d7827a8419d7cb7e1e7c85620f84936e46948cc7b503c2d6bbd73a25fcd4a", "exact_match": 0.0}
{"doc_id": 3545, "doc": {"question": "where are unipolar neurons found in spinal cord", "answer": ["the distal dorsal root"]}, "target": "['the distal dorsal root']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where are unipolar neurons found in spinal cord?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sensory neurons"]], "filtered_resps": ["sensory neurons"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e0c1d79410da46864ada61d4aa4d9f43c8ab5eb84f7102290921240ca316a9a5", "prompt_hash": "d8b01cb0eea759b556ecd8d0f9377c8503530077925680e9fc71dfe16d2dab23", "target_hash": "b27ac415834f7b3a1c926edd2874bb983c672d2cbd1c93458f7e703e98590b81", "exact_match": 0.0}
{"doc_id": 3546, "doc": {"question": "what was the date of the signing of the declaration of independence", "answer": ["August 2, 1776"]}, "target": "['August\\xa02,\\xa01776']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what was the date of the signing of the declaration of independence?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" July 4"]], "filtered_resps": ["July 4"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9fbeafc8e8a74520f2f8e486771f2e523e2c05b2d31aa408fdf325c4775ce731", "prompt_hash": "0d40914b55b479156879a5189ce8a0a3f16efa27405c3564445b95e0363352c1", "target_hash": "313999103aa68a29e6005023cdd11778526cdfdc3c18170f6f958ecada7d1c67", "exact_match": 0.0}
{"doc_id": 3547, "doc": {"question": "who signed the largest on the declaration of independence", "answer": ["John Hancock"]}, "target": "['John Hancock']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who signed the largest on the declaration of independence?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Hancock"]], "filtered_resps": ["John Hancock"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "84610d809f3db0b7f57a26fcee614328727aa274f83680009212cb45b25506e9", "prompt_hash": "2a47aeb02ac377d531308a2f04d458b03a29bd6638eceb0e65a420d022815c13", "target_hash": "345b6b9f8d04eb9a645c4fdb528cbcaaae174a6a90c6520d242965a7412d4f7b", "exact_match": 1.0}
{"doc_id": 3548, "doc": {"question": "when do liam and annie get together season 3", "answer": ["Holiday Madness"]}, "target": "['Holiday Madness']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when do liam and annie get together season 3?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" episode 11"]], "filtered_resps": ["episode 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "feafb238d5c5afbaf79898bbbee9b50c9435106cc0c5f0efd25808fc9c53dbe0", "prompt_hash": "04c727e34c598e7b8cecc25d5747c9201f9965a26ef7251f2d59409650269656", "target_hash": "ff78735e06a4112ff959b10927a587e17dc4aa8ea976ea813b6c8510fe506079", "exact_match": 0.0}
{"doc_id": 3549, "doc": {"question": "tujhe dekha toh yeh jana sanam movie name", "answer": ["Dilwale Dulhania Le Jayenge"]}, "target": "['Dilwale Dulhania Le Jayenge']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: tujhe dekha toh yeh jana sanam movie name?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Dilwale Dulhania Le Jayenge"]], "filtered_resps": ["Dilwale Dulhania Le Jayenge"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "20ad1fc129ce30e21fcccad6cf597c1cc41ffa12cecbbd0eca8286b354c4d849", "prompt_hash": "3b9982c1df1fddb943a61c49aad3d99019e754c5ee4cd7056a379938eba4b431", "target_hash": "be6f06abbce0510ad53de86a1ecdc0e53739d14f1b43364e48f9e56e3b986782", "exact_match": 1.0}
{"doc_id": 3550, "doc": {"question": "where do they film young and the restless", "answer": ["CBS Television City"]}, "target": "['CBS Television City']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where do they film young and the restless?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Los Angeles"]], "filtered_resps": ["Los Angeles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1fb143e80cb5b98908e6ef51ff5f99206d468c3f72c5a12e0bf3811d01cd1c44", "prompt_hash": "d309b04cba825f01e51657dfb5547834175babcaddfbbbb4268d38e2b8e5d02d", "target_hash": "5134139960e0f1c515db800624c149ea9b8c86d463274cdee172f5a7bb1f4d8a", "exact_match": 0.0}
{"doc_id": 3551, "doc": {"question": "new mexico was admitted to the union as the . . . state", "answer": ["47th"]}, "target": "['47th']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: new mexico was admitted to the union as the . . . state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 47"]], "filtered_resps": ["47"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "bcb38ce901c2166b0d8911b6ff35cc0eb3b769eb6f4b7ef22661082eb4740707", "prompt_hash": "bf34f4979e79621de4b336e32e24f25e23feef3f90e880af544b63bac6f1e5ff", "target_hash": "611579cccdb4eb1ba770c4a1a8aa53fccaf20be18580fd8e905b6b1464512383", "exact_match": 0.0}
{"doc_id": 3552, "doc": {"question": "what episode does marnie come back in geordie shore", "answer": ["5"]}, "target": "['5']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what episode does marnie come back in geordie shore?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Episode 11"]], "filtered_resps": ["Episode 11"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "c3b2524f90b0775d0968b9e911ba86ff2f59dcc48c9878de91ec044f3021928e", "prompt_hash": "17a538c205c979748227f79d6c68f07aa897889b81c084c1d0af2fc13c72c756", "target_hash": "494a843c2b5d43b794ff50c14c5647bf4ff556a365c58f564af6c235a53602e3", "exact_match": 0.0}
{"doc_id": 3553, "doc": {"question": "the vascular layer of the eye is the", "answer": ["middle layer", "The uvea", "uvea"]}, "target": "['middle layer', 'The uvea', 'uvea']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the vascular layer of the eye is the?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" retina"]], "filtered_resps": ["retina"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1fe2a6aa9023170dd1167faa0ad82df9e97f23fcca8d53d74c35e73ec24d797d", "prompt_hash": "d4ef0ebd4206f9d7359d58023c71e2627fba68106da20e31884cb98fbd13b1cf", "target_hash": "0f1dfcd41ed14a5ca6b7e939ef4630330c5d9ecb7018ab5e94ff967a37e5f3dc", "exact_match": 0.0}
{"doc_id": 3554, "doc": {"question": "who was the actor who played crocodile dundee", "answer": ["Paul Hogan", "Paul Hogan, AM"]}, "target": "['Paul Hogan', 'Paul Hogan, AM']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the actor who played crocodile dundee?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Paul Hogan"]], "filtered_resps": ["Paul Hogan"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "5d0ead8816c59c3a663cdbd77b8ef588c4dd5884ca21d37738e279416a7e1566", "prompt_hash": "f8306002a46e65e6b00f86b1392c181153e8421f99892087beaeeec44a8110e7", "target_hash": "7c86b68b802eb7be67b2eba63d3e73d0b6f926c100a876846e42ad22738cc43f", "exact_match": 1.0}
{"doc_id": 3555, "doc": {"question": "a bond that the issuer has the right to pay off before its maturity date", "answer": ["callable", "callable bonds"]}, "target": "['callable', 'callable bonds']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: a bond that the issuer has the right to pay off before its maturity date?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" callable"]], "filtered_resps": ["callable"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "31f514e137d350eb0d245f6d9540a2b7d65c7db2e20cca820e5c5721eed43fa9", "prompt_hash": "b6397c97c1373993ca492912b09ad6d47ef733b5a30463a3232e4ee07ff02219", "target_hash": "6272f5df287956116c366cb1cc414a82306906f76f7d231a866d8f8ba9e2cac2", "exact_match": 1.0}
{"doc_id": 3556, "doc": {"question": "who makes the decisions about what to produce in a market economy", "answer": ["capital and financial markets"]}, "target": "['capital and financial markets']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who makes the decisions about what to produce in a market economy?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" consumers"]], "filtered_resps": ["consumers"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "879f94d9f8cccf45553e7067a12e490fdb0a2758c48e374d32ceb51f50707996", "prompt_hash": "ca79a4da3588cdbc1aa404215f4a74b204bea514bf18fb7d892f844e38f89694", "target_hash": "c0514ebe5494c0c516c6278d5c79775456d957677cd1b5441e6cd77b16ad0f29", "exact_match": 0.0}
{"doc_id": 3557, "doc": {"question": "who did the dominican republic gain its independence from", "answer": ["Haiti"]}, "target": "['Haiti']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who did the dominican republic gain its independence from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Spain"]], "filtered_resps": ["Spain"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f12fcd2a70056734a12b71296a1cb9d11b06184e55672461606e0cd2db3ed6ab", "prompt_hash": "257f80d8502b834e9dd67f10646adb5a7cc5d212a76e6cff54a298273029d7c3", "target_hash": "eff30373159bd84f1ca69995afef59a0319e6ea0acf8c65e03c831925ebfab39", "exact_match": 0.0}
{"doc_id": 3558, "doc": {"question": "who played little ricky on i love lucy show", "answer": ["Keith Thibodeaux"]}, "target": "['Keith Thibodeaux']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who played little ricky on i love lucy show?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" William Frawley"]], "filtered_resps": ["William Frawley"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2177e7f7cf5e5b690d00d007f7ab70509f43200217f43314f549a7bd965e4eba", "prompt_hash": "74efd4e3b8c4f8cd68c835e39e793f5f2e062ab3eeaee87d5c9da233fbfd15f4", "target_hash": "f014c021709fcf11a762460c155e810f09055deed8a860541c83d2f65a192f04", "exact_match": 0.0}
{"doc_id": 3559, "doc": {"question": "who sang with eternal on i wanna be the only one", "answer": ["BeBe Winans"]}, "target": "['BeBe Winans']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sang with eternal on i wanna be the only one?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" David Bowie"]], "filtered_resps": ["David Bowie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "d688b0004d350252baec97684de9c374c1348694c5b4eacdcf5218b3f6172cf3", "prompt_hash": "afee7d9ac6eafbd07d6cbe17ac237d1ab76c616056ba3fe402f131efbec3fbe1", "target_hash": "c18f3b463dff5f84532efd75790ed48158c324e884bbddf13a2925de6d714f1b", "exact_match": 0.0}
{"doc_id": 3560, "doc": {"question": "where is most of the worlds gold held", "answer": ["United States"]}, "target": "['United States']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where is most of the worlds gold held?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Fort Knox"]], "filtered_resps": ["Fort Knox"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "90964dd95be7f24e81e54447810dbccf4621597fdb1239d0086b7afe10deeea6", "prompt_hash": "3eeb0289fae660bbe1c00bfc2cf0cd730e133947d1f81bdd04659dc9d4957d02", "target_hash": "248d02cf1d61215522b5495553dd8d7a3b62df655aa003cdc541d7fb5faee586", "exact_match": 0.0}
{"doc_id": 3561, "doc": {"question": "what is a home run with a girl", "answer": ["\"full\" sexual intercourse"]}, "target": "['\"full\" sexual intercourse']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is a home run with a girl?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" a strike"]], "filtered_resps": ["a strike"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7835b4f0d10e281c632626da3a9b5665b7f873a331cae5ebdec62c4d990a5af2", "prompt_hash": "9e3f5190cce5f9f0119f7918d3ad029ecee2a0b85a2f662afb8591814fcc0f4b", "target_hash": "cc8c9a879217642eea183b23f54d599ef5786a84fc20880b3c8a88fb0c4aa1ea", "exact_match": 0.0}
{"doc_id": 3562, "doc": {"question": "who was elected governor of tennessee in 1865", "answer": ["Edward H. East", "William G. Brownlow"]}, "target": "['Edward H. East', 'William G. Brownlow']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was elected governor of tennessee in 1865?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Andrew Johnson"]], "filtered_resps": ["Andrew Johnson"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a0534775307986566bf2ec4cb169d19abbc2916eee91a78c8b17c59fe37e41ad", "prompt_hash": "4b2c8be67b1479c22932780b79e0ffee7fc453b57a9974ffd996dc607c9f503d", "target_hash": "355c5f5bb0c165f37045811fcfe4fe1ccf1f0303ac35c12f74c5693e1173b33a", "exact_match": 0.0}
{"doc_id": 3563, "doc": {"question": "band who had a hit with heart and soul crossword", "answer": ["British pop band T'Pau", "T'Pau"]}, "target": "[\"British pop band T'Pau\", \"T'Pau\"]", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: band who had a hit with heart and soul crossword?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" The Beatles"]], "filtered_resps": ["The Beatles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1696b7b20004505768149889558c9f4f71e15d1cfb61f433e84bc014eb78ed40", "prompt_hash": "3f3515d95ba76abe6aa6a97dfd7774fec00b7a98b4557336b584e931eb215172", "target_hash": "1a4934a83a6e8be77cfc615601967ba44fbb452c64a4634afbdd4084c4f147ee", "exact_match": 0.0}
{"doc_id": 3564, "doc": {"question": "when was the last time tug-of-war was an official olympic sport", "answer": ["1920"]}, "target": "['1920']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time tug-of-war was an official olympic sport?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1920"]], "filtered_resps": ["1920"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4172f892f17a2706802a2dc9ad4951cf16347ed2f3cbb008a2fafa0c677c48ae", "prompt_hash": "c996c2a3ec2718bdbf2d4c8b243e4ed29e7f6f646e9758d4e5f7b1fec033936f", "target_hash": "32f4dcd676bddeffdeed489f8a5f470d99668be4c1d92f476fbf5af19c3f403e", "exact_match": 1.0}
{"doc_id": 3565, "doc": {"question": "when does the sa node begin electrical signaling", "answer": ["spontaneously generated", "spontaneously"]}, "target": "['spontaneously generated', 'spontaneously']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the sa node begin electrical signaling?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" during diastole"]], "filtered_resps": ["during diastole"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b7ce9802d1e0d35eb9c08766ad4faff228836a58c800347f3e23ffe115f81619", "prompt_hash": "9f998b44a6c660ab8552c134e0ad06d538cb17a32412d8977e0b9cc0918b4e0c", "target_hash": "022b165e90ff506b1f7336aa46572bd16473981ea1b4d0f05e0e56eef79d3283", "exact_match": 0.0}
{"doc_id": 3566, "doc": {"question": "who was the first elected president of pakistan", "answer": ["Iskander Mirza", "JB Talha Khan"]}, "target": "['Iskander Mirza', 'JB Talha Khan']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was the first elected president of pakistan?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Muhammad Ali Jinnah"]], "filtered_resps": ["Muhammad Ali Jinnah"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "74460fa05f337b757930c66d6d536262db5087ce947ae9d66bdcecb45085c619", "prompt_hash": "8a086e94260e4117a3d4e903edd5ca61e4ac7c4f8cbbc6fb4ff3d01b86d1c2de", "target_hash": "56cb8c1b74af132b630a94bb4d5602b50837478cc919da9d2452b9efe3beb3e8", "exact_match": 0.0}
{"doc_id": 3567, "doc": {"question": "who is the founder of el pollo loco", "answer": ["Juan Francisco Ochoa"]}, "target": "['Juan Francisco Ochoa']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the founder of el pollo loco?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Francisco \"Bobby\" Ghilardotti"]], "filtered_resps": ["Francisco \"Bobby\" Ghilardotti"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "b9cd64c5ec9a221d8d6a2ace52be840abd48f4524513f2f56b61a2c64e6cceb0", "prompt_hash": "4748b89a67853a0238b995c427e0529db31a4eba35eedab59b420abb3042ca31", "target_hash": "d10cdc8290ebf948f09528fcf7de0ffd08441aa9a566432a0c45373e3ab5508d", "exact_match": 0.0}
{"doc_id": 3568, "doc": {"question": "who plays dawn on nicky ricky dicky and dawn", "answer": ["Lizzy Greene"]}, "target": "['Lizzy Greene']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays dawn on nicky ricky dicky and dawn?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Love Hewitt"]], "filtered_resps": ["Jennifer Love Hewitt"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9db9d71716c54ab70ce984a2f492f7aa03aef58d07be145e9dc3966d46da2623", "prompt_hash": "f077b06c58752942b6ad458ceb8112a4698251113516dcfc36d636c5d1af7879", "target_hash": "06e1f031e8301dda2827797e5d9e8723e4008a251002322042c6cf06d5effa08", "exact_match": 0.0}
{"doc_id": 3569, "doc": {"question": "what is the central idea of the poem the rime of ancient mariner", "answer": ["a violation of nature"]}, "target": "['a violation of nature']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the central idea of the poem the rime of ancient mariner?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" guilt and punishment"]], "filtered_resps": ["guilt and punishment"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "64035f056573418f6a1f9f97e28bee48156a54cb43ee978ec3134577c1ce8468", "prompt_hash": "aa68f6b74647c9b96ed34f285690db0664fcf443f312897f7207d88fca67ff6d", "target_hash": "d331039ccb4c1bce55706a95bd81c517d02cfe1a3141406031a58a0b5db71860", "exact_match": 0.0}
{"doc_id": 3570, "doc": {"question": "who is rose in the fall season 2", "answer": ["Valene Kane"]}, "target": "['Valene Kane']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is rose in the fall season 2?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rose is a recurring character in the fall season 2"]], "filtered_resps": ["Rose is a recurring character in the fall season 2"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "3723cd44bf2bf6af3c2190d29e05224800c161f47356735494db67b139f31c54", "prompt_hash": "e68938da8e06c47efcc75731f7b6c96790aeccf1aae6d557f3afe34c1953f5e9", "target_hash": "d5aef1a7403eed7b6b38129405e3f7a408eb63b47fcc55d9e8ff857bdd3cf32c", "exact_match": 0.0}
{"doc_id": 3571, "doc": {"question": "who plays the judge in drop dead diva", "answer": ["Lex Medlin"]}, "target": "['Lex Medlin']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the judge in drop dead diva?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Esposito"]], "filtered_resps": ["Jennifer Esposito"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a20d39bc1467085027d0f1cb92694410edc001ceab99e864990a5a7d88fc1231", "prompt_hash": "a702e76d63a2e460c02ed47337d0c13543e22248890d712e9d3d2535a16ca1f5", "target_hash": "80677dfdd883133868a396e3b3e7a15467e4fa62cfaf967e54416331dda15dd2", "exact_match": 0.0}
{"doc_id": 3572, "doc": {"question": "why is the indian ocean the warmest in the world", "answer": ["human induced greenhouse warming"]}, "target": "['human induced greenhouse warming']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: why is the indian ocean the warmest in the world?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Indian Ocean Dipole"]], "filtered_resps": ["Indian Ocean Dipole"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6346103673bd39bd7e31e220b039b7dd1b7a0cc4ab20a0b7b09c0bcf1525a0a6", "prompt_hash": "69088fafc3a14558a0242ca92e3030fe335acf766a5d7754d8591d82ceade0cf", "target_hash": "2826d65f5a23cd1146cbe7c28a2096a07e6ccc36972a7566aab84af37526dde2", "exact_match": 0.0}
{"doc_id": 3573, "doc": {"question": "when did michael jordan get his last ring", "answer": ["1998"]}, "target": "['1998']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did michael jordan get his last ring?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1998"]], "filtered_resps": ["1998"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8dba9c4a58183c38897cdac73560c8ee80d728c51b027b296e8fa0ff45d44680", "prompt_hash": "af1b24e6af410b27e3b21f005ffecade6de26d09473a4859cec70b653b9f62d3", "target_hash": "af1dd11d685d07f55fbfc760446204424f6189f35a956c9f3b09a3b25cbebbea", "exact_match": 1.0}
{"doc_id": 3574, "doc": {"question": "who sings blame it on the bossa nova", "answer": ["Eydie Gormé"]}, "target": "['Eydie Gormé']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who sings blame it on the bossa nova?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Jennifer Lopez"]], "filtered_resps": ["Jennifer Lopez"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2fe1e3cee62608f91d69b163b6b0b6881da608e33126fdd0cdd707e57174f8f1", "prompt_hash": "9dac812c5afd32642a25d9e6dfadb252317ed3b541fdfe1ae666b9295aa0eb93", "target_hash": "b81ec54c8b68e14819d1dd7f0b49adc5e5af674ba22bd3fb280b2b333ce08770", "exact_match": 0.0}
{"doc_id": 3575, "doc": {"question": "who do the characters represent in 8 mile", "answer": ["loosely on Eminem"]}, "target": "['loosely on Eminem']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who do the characters represent in 8 mile?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Eminem"]], "filtered_resps": ["Eminem"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cb48d76e3d59c02fb7cfdfd8717c19de59e1bb59bdb32e221afc5b3ab407afa0", "prompt_hash": "6c5f052cb8f08820eec50a415074a9e933db813b72e90c46ea3295362198b173", "target_hash": "52bff0ebf85c3f20d3da7ef7c00763c08d9d6dd1697524661c3ec5356a76bea6", "exact_match": 0.0}
{"doc_id": 3576, "doc": {"question": "when was the last time there was a school shooting in canada", "answer": ["January 22, 2016"]}, "target": "['January 22, 2016']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time there was a school shooting in canada?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2014"]], "filtered_resps": ["2014"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a350ff410908407735d6961cf8830c7d24a3351d9bd6ea8df5d10b10c9c2a974", "prompt_hash": "35fb7c31131ea6a5c9abffdb1cfcf92df877239ec0a149361e8ee4a6eec0f3a2", "target_hash": "8bab3ccebe22ceba4f6c98a7b80f7c67c56623bcb70cc9611b3a491407bf0a48", "exact_match": 0.0}
{"doc_id": 3577, "doc": {"question": "when was the last time.michigan beat ohio state", "answer": ["2011"]}, "target": "['2011']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last time.michigan beat ohio state?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2013"]], "filtered_resps": ["2013"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6f6bcc2330d8917d9c5c33e5b79831fa95800e9909039fea331c02bf0e8971fe", "prompt_hash": "3a036c32401f0cd4f3cebd24b16368c87af7ab94514005653bb96921d710271e", "target_hash": "3fd77e6a3c8aa1507fbee4fc02c1a1a3ef1013d169b015b0c71135a2990b6664", "exact_match": 0.0}
{"doc_id": 3578, "doc": {"question": "when does the stock market open in hong kong", "answer": ["09:15"]}, "target": "['09:15']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when does the stock market open in hong kong?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 9:30 am"]], "filtered_resps": ["9:30 am"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f468889e38d08ca0b4704d9275730a924752325d5aa67a30ff1aef1924fa55d", "prompt_hash": "1f0ed94beca796d4f9f461a2f3f1fcbd8f477cb2c88ae0ff8cb0514a7d2b3a0a", "target_hash": "c9738de50bb8d54b377d91f7f45b89f03a40f083fef2f47798e03b847207ff47", "exact_match": 0.0}
{"doc_id": 3579, "doc": {"question": "who does the vocals for feel it still", "answer": ["Portugal. The Man"]}, "target": "['Portugal. The Man']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who does the vocals for feel it still?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Kanye West"]], "filtered_resps": ["Kanye West"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f7cff4cff06748f79fe729f6e1a501a49f278aa69175e4628379b6a32a74cdfa", "prompt_hash": "50b5ff59c4f9ed171b584b492b0f5e079973b9bf474575f14d8a81957c390691", "target_hash": "ffe0b4a05d15ace15cdf9121a1f6798dc6e35d33fb427bfecf5c12dd5e262300", "exact_match": 0.0}
{"doc_id": 3580, "doc": {"question": "what type of fuel goes in a zippo", "answer": ["lighter fluid", "butane"]}, "target": "['lighter fluid', 'butane']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of fuel goes in a zippo?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" lighter fluid"]], "filtered_resps": ["lighter fluid"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "96b501c468a7b4b9dd7209e40fdad20f21cae8fe606d11f7a96435e20eb2afce", "prompt_hash": "100705ff2b0339875a9f70a7c04ed00eebd5967ba8123797c16922c0dc59a6e5", "target_hash": "1a851e03ea1d8d3cbeb752bae38b3434fa782aec0e1a4275ae2a971553b139d7", "exact_match": 1.0}
{"doc_id": 3581, "doc": {"question": "ben 10 ultimate alien episode 2 season 1", "answer": ["Duped"]}, "target": "['Duped']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: ben 10 ultimate alien episode 2 season 1?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ben 10: Alien Force"]], "filtered_resps": ["Ben 10: Alien Force"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ca60a7a46ea92e006d5565e660b2e54a2056823f8aba2bb294315cb8dd7da7a1", "prompt_hash": "2ac0d04985247802db65adeee394f3be7fbf8de7f766466aca9f5ddbf1073a4a", "target_hash": "537274b0716d17d03d824700d5110ee3c58a2a0b06764b7ffdccc6878caab6b7", "exact_match": 0.0}
{"doc_id": 3582, "doc": {"question": "where does florida natural orange juice come from", "answer": ["Lake Wales, Florida"]}, "target": "['Lake Wales, Florida']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where does florida natural orange juice come from?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Florida"]], "filtered_resps": ["Florida"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "7a3a2bcf57b05c36ec653a06797b1524d050b9ea59897908bd1f845a2a1bd476", "prompt_hash": "c8f719022dbad5e379ca5e0090ad06b97523a094b872d6ec27a02f7432aff918", "target_hash": "c27d2bcd12bdd2b67ee7b1e739169376b6b7b732597d84a9e22fb171d2af8637", "exact_match": 0.0}
{"doc_id": 3583, "doc": {"question": "when is the new dumping ground coming out 2017", "answer": ["15 December 2017"]}, "target": "['15\\xa0December\\xa02017']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when is the new dumping ground coming out 2017?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2017"]], "filtered_resps": ["2017"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6ada75d418b87acd320b931c5a8c1a2899996161e59ce3bd17ad3ce700eb2e04", "prompt_hash": "779b606861fd49a5447e933b61bf8104c3d5fa079df99d4816419c281e8a4ae7", "target_hash": "3f46a8ed1066b02d45551b5e01cb669dffa51991d2dbf3cecedc229032100462", "exact_match": 0.0}
{"doc_id": 3584, "doc": {"question": "what type of rock dominates the earth's surface", "answer": ["Sedimentary", "sedimentary rock", "sedimentary"]}, "target": "['Sedimentary', 'sedimentary rock', 'sedimentary']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what type of rock dominates the earth's surface?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" sedimentary"]], "filtered_resps": ["sedimentary"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e62b71ee4a7c0daaee688d1fe563fdb91cea160b7eb3f326b369c583e3468396", "prompt_hash": "8d04b3c1912596bd1e23d1ee3fd62b743f6fd1a95b3a3ca6d774d4b353aef2a9", "target_hash": "1cda361b775d4a0d5f93bcfec19f82b05c0c69f14fbe0d8988e9b76e0cec1402", "exact_match": 1.0}
{"doc_id": 3585, "doc": {"question": "forney's sister in where the heart is", "answer": ["Margaret Hoard", "Mary Elizabeth"]}, "target": "['Margaret Hoard', 'Mary Elizabeth']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: forney's sister in where the heart is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Rosie"]], "filtered_resps": ["Rosie"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "51a14a236d7ed85b28001d96f4c8b9f10646d1a9c121061cd67463138c2d72a8", "prompt_hash": "20f0f6246ad5277c59250d35edbf4609373e65671e2788c26102ec1ae48ee943", "target_hash": "feab3549bd075b48a27492e4182860bd7cb9ab2b47c90078788c4b004e5e76ef", "exact_match": 0.0}
{"doc_id": 3586, "doc": {"question": "when did the first pair of yeezys come out", "answer": ["February 14, 2015"]}, "target": "['February 14, 2015']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when did the first pair of yeezys come out?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 2005"]], "filtered_resps": ["2005"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "ea3349efe4d71bd49649591db8f1eab05041bfcd6d24350045dc8ec7044d6c55", "prompt_hash": "45282bda849926612f200adf791f2e411bef5543260e2e3133727d3ecb80e526", "target_hash": "12bfdcfed962690d994ffef66b23c3b0f3266b7b4b8a9b0ee191a61d44eaf844", "exact_match": 0.0}
{"doc_id": 3587, "doc": {"question": "when was the tower of london finished being built", "answer": ["1078"]}, "target": "['1078']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the tower of london finished being built?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1277"]], "filtered_resps": ["1277"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "e972a459835315e0012e0f541b07084ed32a84004c03dcebc0f2cf2aa07d09a3", "prompt_hash": "e119ddcbae5c2c0f0f7f82e06784b3c7d2cfcd06aaf4042b77546ac56bb434ae", "target_hash": "9314595ab3faf3a2f6f7eace216a55f8cba18a81ad96d027fd58b3b182d3be5c", "exact_match": 0.0}
{"doc_id": 3588, "doc": {"question": "who decides the number of judges in the high court", "answer": ["the President of India"]}, "target": "['the President of India']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who decides the number of judges in the high court?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Parliament"]], "filtered_resps": ["Parliament"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "44f09a747409a8f893156649e068dbe5f6958a82d2c6bba4434cc6c9de8123ce", "prompt_hash": "fe1ab6a02067d9fa88fcd7794f7279074b3df2037bd5bb80015d9bfbf4cef28e", "target_hash": "16c0c8baae112afc2c066503fb7c4264a27406d28f0a67296f724416c8777dbe", "exact_match": 0.0}
{"doc_id": 3589, "doc": {"question": "the radiographic term used to describe the dense bone of the socket and septal crest is", "answer": ["lamina dura", "alveolar process", "the lamina dura"]}, "target": "['lamina dura', 'alveolar process', 'the lamina dura']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: the radiographic term used to describe the dense bone of the socket and septal crest is?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" alveolar crest"]], "filtered_resps": ["alveolar crest"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "afb62684dcdda67de3f520e114004eec90104f2f58bc4a09f83aec9aa6f7bc89", "prompt_hash": "94a237d4af4a2e245e31195696977ce639b059af66a17f6fa91c31c2e08f5e69", "target_hash": "1d92d91cf528fdeb5ebc0247c89d521dfaa6a6fdd6c43623e5758f4682bf1b90", "exact_match": 0.0}
{"doc_id": 3590, "doc": {"question": "who is the secretary of state for northern ireland", "answer": ["Karen Bradley"]}, "target": "['Karen Bradley']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who is the secretary of state for northern ireland?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" James Brokenshire"]], "filtered_resps": ["James Brokenshire"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "a83b07971fe38ddbb7cade6f723a1165d3f18560b31152988445378c633b39ec", "prompt_hash": "b35b7ab2c7c4435a94e3d7945c7507e8ee13a8c891338bb2e2c7b6b8cb14ce05", "target_hash": "206e0429ae905e7ec9b32983bdf2e1de4609bc9e6f19d9a45a9a3df1b1d3b21f", "exact_match": 0.0}
{"doc_id": 3591, "doc": {"question": "who was selected for the 2018 football hall of fame", "answer": ["Ray Lewis", "Brian Urlacher", "Jerry Kramer", "Robert Brazile", "Bobby Beathard", "Brian Dawkins", "Randy Moss", "Terrell Owens"]}, "target": "['Ray Lewis', 'Brian Urlacher', 'Jerry Kramer', 'Robert Brazile', 'Bobby Beathard', 'Brian Dawkins', 'Randy Moss', 'Terrell Owens']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who was selected for the 2018 football hall of fame?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" John Madden"]], "filtered_resps": ["John Madden"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "aace3707a605952652ee2f7f9ae967ff941512ae7ab6187b5728c5bb99b407e3", "prompt_hash": "b7c2ceecc38fa8561ecb757b64bc06cdbdf69d7e004e767762e8a2e6774c67c5", "target_hash": "8b7401f15c2ab48047083d3dfec3e0bf45acdce2093103c4c0e1efc45feebf06", "exact_match": 0.0}
{"doc_id": 3592, "doc": {"question": "what is the third book in the lunar chronicles", "answer": ["Cress"]}, "target": "['Cress']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the third book in the lunar chronicles?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Cinder"]], "filtered_resps": ["Cinder"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "9f6eec200490a73c388bd213dbf96db73affd65a35e2336121713fc6d65c9029", "prompt_hash": "6c8fb067a53343fac2bbf611b4a9cf5ab80c8bc7e3238638feea4e99b1a6b49a", "target_hash": "e9b40ece43389339358ecb63d0fa5d3c51bd8a017e07b8a8d488803360ad363b", "exact_match": 0.0}
{"doc_id": 3593, "doc": {"question": "who plays the mother of howard on big bang theory", "answer": ["Carol Ann Susi"]}, "target": "['Carol Ann Susi']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who plays the mother of howard on big bang theory?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Melissa Rauch"]], "filtered_resps": ["Melissa Rauch"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "8f9a77b5bb49cbbf856b3a7824d6e2941c2bfeac2d9a87df5254a09e046f594f", "prompt_hash": "ef8a0e04ac4a78e11d4566b33cee8b427c5f6bb26534af10768f770a0134ebfb", "target_hash": "d9b981fa44b817cb8638978da6a2fba73f823cf496985443da0195ba5be11e04", "exact_match": 0.0}
{"doc_id": 3594, "doc": {"question": "what makes a mini bike a mini bike", "answer": ["a miniature motorcycle"]}, "target": "['a miniature motorcycle']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what makes a mini bike a mini bike?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" engine size"]], "filtered_resps": ["engine size"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77e8733011bbd61f0f64bd00a2bfe849d90bad6a5527d0fee27af31d5cf31467", "prompt_hash": "e6fec336f07a926363c47167c68cfdcdc5fc4f351f1226c6c5240eccb39d1de3", "target_hash": "1be384c32d4dc4170fc3c2bac0a232223d9fde38c64c423923dad159cd46b520", "exact_match": 0.0}
{"doc_id": 3595, "doc": {"question": "which british city is farthest from the sea", "answer": ["Coton in the Elms"]}, "target": "['Coton in the Elms']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: which british city is farthest from the sea?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" inland city of Manchester"]], "filtered_resps": ["inland city of Manchester"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "6ed49dbdf32ac056458c79b3cafa52b5164b94f68183c29ad4a1ff2e0dfc8e97", "prompt_hash": "933ce1d87cfcdab08558c819b90468486887137c3e781af98fc2ca6e32366e81", "target_hash": "acd53163046639e7a8fceb15b01eed9eeca519a4aae15d0ecad155c1cbdde2e5", "exact_match": 0.0}
{"doc_id": 3596, "doc": {"question": "what mall did they use in back to the future", "answer": ["Puente Hills Mall"]}, "target": "['Puente Hills Mall']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what mall did they use in back to the future?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Hill Valley Mall"]], "filtered_resps": ["Hill Valley Mall"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "2f3908c836ee5141ebc4de2aeb3c19b206f5c3694434f9854bd7c8da53717256", "prompt_hash": "51385505ef565d1d8232aa6e1706b0c48f285f35f941e5c56ce4b6c83b05450c", "target_hash": "603372194e9f1817397f694a9b092ce999d537a1159d06c6af5ae2f839bd9866", "exact_match": 0.0}
{"doc_id": 3597, "doc": {"question": "turkey stuffed with a duck stuffed with a chicken", "answer": ["Turducken"]}, "target": "['Turducken']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: turkey stuffed with a duck stuffed with a chicken?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Turkey stuffed with a duck stuffed with a chicken"]], "filtered_resps": ["Turkey stuffed with a duck stuffed with a chicken"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "179a36a0a2bd3113d38880b810700bf1537d20470804d5c3445d3c2c9be2b60e", "prompt_hash": "b383b6da4073e73f8c3cd261acbf608ef60679804650f5a026abc62aa9ea6f73", "target_hash": "673697211d90bd105455a40da32f1b4e7534e0fe7f3db25ec0f45d3e281a07c0", "exact_match": 0.0}
{"doc_id": 3598, "doc": {"question": "who came up with the initial concept of protons and neutrons", "answer": ["Dmitri Ivanenko", "Werner Heisenberg"]}, "target": "['Dmitri Ivanenko', 'Werner Heisenberg']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who came up with the initial concept of protons and neutrons?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ernest Rutherford"]], "filtered_resps": ["Ernest Rutherford"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "f59225026256365495d996a0c6ac6b78c94bb20ace8f319daa41c6e87fdd891a", "prompt_hash": "dd535dac71f6c04effb006349989950ee4822fcc5cc4df5a7ddd651918496a71", "target_hash": "10e8dba7b55a5ed78020b7d63aadb6eb5d91b163c2ac39c07899c86fbd037478", "exact_match": 0.0}
{"doc_id": 3599, "doc": {"question": "when was the last summer olympics in the usa", "answer": ["1996"]}, "target": "['1996']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: when was the last summer olympics in the usa?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" 1984"]], "filtered_resps": ["1984"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "671801d2d052c383bb2b0a70047f4af4532f186c00d5439d94376c78309b8961", "prompt_hash": "2a07c6124cc40870021fdd10e73651df54bb8248929550c5b8c01d6f370c0254", "target_hash": "4884bad91c23e17f70a396eecdc381a658567383863ece0c7474966e045c229b", "exact_match": 0.0}
{"doc_id": 3600, "doc": {"question": "oklahoma's 10 geographic regions are defined by surface features called", "answer": ["ecological regions"]}, "target": "['ecological regions']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: oklahoma's 10 geographic regions are defined by surface features called?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" physiographic regions"]], "filtered_resps": ["physiographic regions"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1c85482264fb2454e8821e705e7d16b815b504e1f826eefbc8b5df59f42a3c58", "prompt_hash": "7c88bff044ab05b236d7f2940a30761ae8e643012844e7c02419852e937a7ab8", "target_hash": "5ac8b961c6eb2237fa48133126b9820fa57069dc4de73ea1bd280e2ce4fd312b", "exact_match": 0.0}
{"doc_id": 3601, "doc": {"question": "who won the 2017 ncaa mens basketball tournament", "answer": ["North Carolina"]}, "target": "['North Carolina']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who won the 2017 ncaa mens basketball tournament?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" North Carolina"]], "filtered_resps": ["North Carolina"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "08759de3b3848ae4fa0aff61a7a1e67a3017114310e0c7b21c7d109048e103f2", "prompt_hash": "9c04a18f2c3cff0ef3aba977982019b3c58549215ce393c894dbe48d12491351", "target_hash": "c1ebfefe2790aaf6b990bce421fee6ef6ccbbd0ec352b67d48457fcf28ec0b23", "exact_match": 1.0}
{"doc_id": 3602, "doc": {"question": "where did immigrants enter the us on the west coast", "answer": ["Angel Island Immigration Station", "San Francisco Bay"]}, "target": "['Angel Island Immigration Station', 'San Francisco Bay']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: where did immigrants enter the us on the west coast?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Ellis Island"]], "filtered_resps": ["Ellis Island"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "46d3817583345c9e8b3732911d358134239912d5742affa88c564f84ec107ead", "prompt_hash": "df324c6e60d9a3d38a6e91134826bfcd0f8b74525efc767626ac80868873ea88", "target_hash": "f9e28ded74b23590e4547bfa2a27814f5bf9c75d2e20d5102d8877edea9d5513", "exact_match": 0.0}
{"doc_id": 3603, "doc": {"question": "isdn uses b & d channels. what is d channel use for", "answer": ["control and signaling information", "data", "carries control and signaling information"]}, "target": "['control and signaling information', 'data', 'carries control and signaling information']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: isdn uses b & d channels. what is d channel use for?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" signaling"]], "filtered_resps": ["signaling"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "71adbe5e39fef1b84ba8b354bea5df959f5ef33ca07f87d408359653238ae084", "prompt_hash": "a97c0da124d47c4f60863f40fd86b9404a08eadab493190564fee5aebc5b8f6e", "target_hash": "634294fb6054a3d857e8cf891047f2b6e6e6be0f215c0ec7df20c73763736165", "exact_match": 0.0}
{"doc_id": 3604, "doc": {"question": "natural resources can be divided into what two categories", "answer": ["Biotic", "Abiotic"]}, "target": "['Biotic', 'Abiotic']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: natural resources can be divided into what two categories?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" renewable"]], "filtered_resps": ["renewable"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "cab3c868bc20bbffe695851e563ec895cc5b3a197a86c1d6326c415056592f99", "prompt_hash": "b050ae4863124fa0967a6499f899bbaa7cc51a9417b6ea66ee376bf91dc93f27", "target_hash": "75cd747240e58708f2f75be588b7b044f3db5e3b512731af11158fe980790311", "exact_match": 0.0}
{"doc_id": 3605, "doc": {"question": "what is the value of the currency in paraguay", "answer": ["126 PYG to 1 USD"]}, "target": "['126 PYG to 1 USD']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the value of the currency in paraguay?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" guaraní"]], "filtered_resps": ["guaraní"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "0e327447fc3b93377c31a5781d9b5c4f5f12e03c5b68b0a7a76ee9dbe31640ff", "prompt_hash": "b862fe6e50bfca37abea60d778aa97daedb76da58f88efb8aedc492f9cad5f77", "target_hash": "8c8c0228902f28ac2ccc751a4505db49d0181c504f0ee0ac210a36b5a40c2b40", "exact_match": 0.0}
{"doc_id": 3606, "doc": {"question": "what college does everyone in gossip girl go to", "answer": ["New York University", "Columbia University"]}, "target": "['New York University', 'Columbia University']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what college does everyone in gossip girl go to?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" New York University"]], "filtered_resps": ["New York University"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "4e4c24521418262aa5de97361fb6ef02e4dc337e839dc462971311886354107d", "prompt_hash": "4bd05f29b915d7f2fa30c24544c0785df99522d1bfb6467fb9b936e81c61134b", "target_hash": "224ca6620a3f772644ff673c80ab13cac9b2383a2676771c822384b99859909c", "exact_match": 1.0}
{"doc_id": 3607, "doc": {"question": "who will take the throne after the queen dies", "answer": ["Charles, Prince of Wales"]}, "target": "['Charles, Prince of Wales']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: who will take the throne after the queen dies?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Charles"]], "filtered_resps": ["Charles"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "35836b2a8716773ecb6bdf98a1d81a5fee5df05f77c9ee88b0233f19ef7e0133", "prompt_hash": "9e928bec49f5518b75666f4816870c77184951dd051654208417a219834ecb58", "target_hash": "0c8a6e555eef107c4940c8f23cc8341d0e6a50425add1ba0d8ef4a81f66360df", "exact_match": 0.0}
{"doc_id": 3608, "doc": {"question": "what domain has more individuals than all other domains combined do", "answer": ["the com TLD"]}, "target": "['the com TLD']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what domain has more individuals than all other domains combined do?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" Bacteria"]], "filtered_resps": ["Bacteria"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "1efe4cd510170ba005773d844515b42f6300afccbd0c40565566760182b67546", "prompt_hash": "f7825ed22c51e5eb408bb288c3bc03461a62b5344a7f68e928cc0427f91b0efb", "target_hash": "33e1b59538352663c48343ec38c09816f693e8963ef5e344c2f55602698ffcf3", "exact_match": 0.0}
{"doc_id": 3609, "doc": {"question": "what is the meaning of the name comanche", "answer": ["enemy"]}, "target": "['enemy']", "arguments": {"gen_args_0": {"arg_0": "Answer these questions:\n\nQ: what is the meaning of the name comanche?\nA:", "arg_1": {"until": ["\n", ".", ","], "do_sample": false, "temperature": 0.0}}}, "resps": [[" fierce"]], "filtered_resps": ["fierce"], "filter": "remove_whitespace", "metrics": ["exact_match"], "doc_hash": "77c33919af05de3d03fdab341d2656a8c64b95c870ffaeb5d34c8badca173780", "prompt_hash": "7b99dd8d0c227b1c522cd5537708f416ce4aa30aa5c2bb62dec50edbd0ae6771", "target_hash": "76a630f596294efb8735bc7c1922e0af600452d63f407fe74985aedea5a0bd98", "exact_match": 0.0}
